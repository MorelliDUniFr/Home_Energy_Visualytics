{
 "cells": [
  {
   "cell_type": "code",
   "id": "10b447d45ff6bd21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:56:45.093635Z",
     "start_time": "2025-07-08T13:56:43.154231Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from config_loader import load_config\n",
    "from joblib import load\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:56:46.190654Z",
     "start_time": "2025-07-08T13:56:45.219972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "household_id = '05'\n",
    "\n",
    "config, config_dir = load_config()\n",
    "env = config['Settings']['environment']\n",
    "models_dir = config['Data']['models_dir']\n",
    "scalers_dir = config['Data']['scalers_dir']\n",
    "target_scalers_dir = config['Data']['target_scalers_dir']\n",
    "data_path = config[env]['data_path']\n",
    "training_dataset_file = config['Data']['training_dataset_file']\n",
    "infer_data_file = config['Data']['infer_data_file']\n",
    "demo_dataset_ground_truth_file = config['Data']['demo_dataset_ground_truth_file']\n",
    "inference_timestamp = config['Inference']['inference_timestamp']\n",
    "model_file = config['Data']['model_file']\n",
    "\n",
    "inferred_data_file = config['Data']['inferred_data_file']\n",
    "column_names_file = config['Data']['training_dataset_columns_file']\n",
    "input_scaler_file = config['Data']['input_scaler_file']\n",
    "target_scalers_file = config['Data']['target_scalers_file']\n",
    "batch_size = int(config['Inference']['batch_size'])\n",
    "\n",
    "model_path = os.path.join(data_path, models_dir)\n",
    "inferred_data_path = os.path.join(data_path, inferred_data_file)\n",
    "infer_data_path = os.path.join(data_path, infer_data_file)\n",
    "input_scaler_path = os.path.join(data_path, scalers_dir, input_scaler_file)\n",
    "target_scalers_path = os.path.join(data_path, scalers_dir, target_scalers_dir)\n",
    "\n",
    "input_scaler = load(input_scaler_path)\n",
    "\n",
    "device = torch.device('cpu')"
   ],
   "id": "bdd7bdde823b7d2f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:56:46.506937Z",
     "start_time": "2025-07-08T13:56:46.196229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the file\n",
    "df = pd.read_parquet(os.path.join(data_path, training_dataset_file))\n",
    "df_gt = df.copy()\n",
    "\n",
    "# Read appliance names from the text file\n",
    "with open(os.path.join(data_path, column_names_file), 'r') as file:\n",
    "    column_names_json = json.load(file)\n",
    "\n",
    "appliances_list = column_names_json['appliances']"
   ],
   "id": "cfe287b2134c0ddc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:56:49.870234Z",
     "start_time": "2025-07-08T13:56:46.560935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Keep only certain columns\n",
    "df = df[df.columns[:11]]\n",
    "df_gt = df_gt.iloc[:, [0, 1] + list(range(11, df_gt.shape[1]))]\n",
    "df = df[df['household_id'] == household_id]\n",
    "df_gt = df_gt[df_gt['household_id'] == household_id]\n",
    "df.drop(columns=['household_id'], inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Convert 'timestamp' to datetime if it's not already\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df_gt['timestamp'] = pd.to_datetime(df_gt['timestamp'])\n",
    "\n",
    "# Get the 71 unique dates (just the date part)\n",
    "unique_dates = sorted(df['timestamp'].dt.date.unique())\n",
    "\n",
    "# Create the new continuous date range: yesterday to 71 days before\n",
    "new_dates = [datetime.now().date() - timedelta(days=i+1) for i in range(len(unique_dates))]\n",
    "new_dates = sorted(new_dates)  # Sort to maintain original order if needed\n",
    "\n",
    "# Map old dates to new dates\n",
    "date_mapping = dict(zip(unique_dates, new_dates))\n",
    "\n",
    "# Replace the 'timestamp' with the mapped date (keeping the time component)\n",
    "df['timestamp'] = df['timestamp'].apply(\n",
    "    lambda x: datetime.combine(date_mapping[x.date()], x.time())\n",
    ")\n",
    "df_gt['timestamp'] = df_gt['timestamp'].apply(\n",
    "    lambda x: datetime.combine(date_mapping[x.date()], x.time())\n",
    ")"
   ],
   "id": "10500b1035427eae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(613440, 10)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:56:50.445736Z",
     "start_time": "2025-07-08T13:56:49.887899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save df to parquet file\n",
    "df.to_parquet(os.path.join(data_path, infer_data_file))\n",
    "df_gt.to_parquet(os.path.join(data_path, demo_dataset_ground_truth_file))"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T14:19:08.355051Z",
     "start_time": "2025-07-08T13:56:50.462391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read appliance names from the text file\n",
    "with open(os.path.join(data_path, column_names_file), 'r') as file:\n",
    "    column_names_json = json.load(file)\n",
    "\n",
    "appliances_list = column_names_json['appliances']\n",
    "\n",
    "def create_day_dataset_from_file():\n",
    "    df = pd.read_parquet(infer_data_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    timestamps = df['timestamp'].reset_index(drop=True)  # Keep timestamps for later\n",
    "    df = df.drop(columns=['timestamp'])\n",
    "\n",
    "    # Apply normalization\n",
    "    X_day = input_scaler.transform(df)\n",
    "    return X_day, timestamps\n",
    "\n",
    "\n",
    "def run_inference(X_day, appliance):\n",
    "    appliance_name = appliance.lower().replace(' ', '_')\n",
    "    model = torch.jit.load(os.path.join(model_path, appliance_name + model_file))\n",
    "    model.eval()\n",
    "\n",
    "    if len(X_day.shape) == 2:\n",
    "        X_day = np.expand_dims(X_day, axis=0)  # Add batch dimension at axis=0\n",
    "\n",
    "    X_day_tensor = torch.tensor(X_day, dtype=torch.float32).to(device)\n",
    "    day_dataset = TensorDataset(X_day_tensor)\n",
    "    day_loader = DataLoader(day_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions_all = []\n",
    "    with torch.no_grad():\n",
    "        for (batch_X,) in day_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_predictions = model(batch_X)\n",
    "\n",
    "            # Clamp predictions to be non-negative (power >= 0)\n",
    "            batch_predictions = torch.clamp(batch_predictions, min=0.0)\n",
    "\n",
    "            # Convert to NumPy after clamping\n",
    "            predictions_all.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "    predictions_np = np.concatenate(predictions_all, axis=0)\n",
    "    return predictions_np\n",
    "\n",
    "def melt_dataframe(df):\n",
    "    # Melt the DataFrame to long format\n",
    "    df_long = pd.melt(df,\n",
    "                      id_vars=['timestamp'],   # Columns to keep\n",
    "                      var_name='appliance',    # New column for appliance names\n",
    "                      value_name='value')      # New column for values\n",
    "\n",
    "    # Convert timestamp and extract date, hour, month\n",
    "    df_long['timestamp'] = pd.to_datetime(df_long['timestamp'])\n",
    "    df_long['date'] = df_long['timestamp'].dt.date\n",
    "    df_long['minute'] = df_long['timestamp'].dt.minute\n",
    "    df_long['hour'] = df_long['timestamp'].dt.hour\n",
    "    df_long['month'] = df_long['timestamp'].dt.to_period('M')\n",
    "\n",
    "    # Sort by timestamp (and optionally by appliance if you want consistent order)\n",
    "    df_long = df_long.sort_values(by=['timestamp'])\n",
    "\n",
    "    return df_long\n",
    "\n",
    "\n",
    "def compute_other_column(p_df, sm_df):\n",
    "    # Ensure timestamps are datetime and sorted\n",
    "    p_df['timestamp'] = pd.to_datetime(p_df['timestamp'])\n",
    "    sm_df['timestamp'] = pd.to_datetime(sm_df['timestamp'])\n",
    "\n",
    "    p_df = p_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    sm_df = sm_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Sum predicted appliance power per timestamp (exclude 'timestamp' column)\n",
    "    appliance_cols = p_df.columns.difference(['timestamp'])\n",
    "    p_df['total_pred_power'] = p_df[appliance_cols].sum(axis=1)\n",
    "\n",
    "    # Sum smart meter phases to get total power per timestamp\n",
    "    phase_cols = [col for col in sm_df.columns if col.lower() in ['powerl1', 'powerl2', 'powerl3']]\n",
    "    sm_df['total_sm_power'] = sm_df[phase_cols].sum(axis=1)\n",
    "\n",
    "    # Merge on timestamp to align rows\n",
    "    merged = pd.merge(p_df, sm_df[['timestamp', 'total_sm_power']], on='timestamp', how='inner')\n",
    "\n",
    "    # Compute 'Other' = smart meter total - sum predicted appliances\n",
    "    merged['Other'] = merged['total_sm_power'] - merged['total_pred_power']\n",
    "\n",
    "    # Clip negative values to zero\n",
    "    merged['Other'] = merged['Other'].clip(lower=0)\n",
    "\n",
    "    # Optional: keep original columns + Other column, drop helper cols\n",
    "    result = merged.drop(columns=['total_pred_power', 'total_sm_power'])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def append_predictions(timestamps, predictions_dict):\n",
    "    \"\"\"\n",
    "    timestamps: list of timestamps (len = total timesteps)\n",
    "    predictions_dict: dict of {appliance_name: np.ndarray of shape (total_timesteps,)}\n",
    "                      or (1, seq_len, 1) / (batch, seq_len, 1)\n",
    "    \"\"\"\n",
    "    pred_df = pd.DataFrame({'timestamp': timestamps})\n",
    "\n",
    "    for appliance, pred in predictions_dict.items():\n",
    "        print(f\"Processing {appliance} - shape before reshape:\", pred.shape)\n",
    "        appliance_name = appliance.lower().replace(' ', '_')\n",
    "\n",
    "        # Remove batch dimension if necessary\n",
    "        if pred.ndim == 3 and pred.shape[0] == 1:\n",
    "            pred = pred[0]  # shape: (seq_len, 1)\n",
    "        if pred.ndim == 2 and pred.shape[1] == 1:\n",
    "            pred = pred[:, 0]  # shape: (seq_len,)\n",
    "        elif pred.ndim == 3:\n",
    "            pred = pred.reshape(-1, pred.shape[2])[:, 0]  # flatten and squeeze\n",
    "\n",
    "        print(f\"Shape after reshape for {appliance}:\", pred.shape)\n",
    "\n",
    "        # Inverse scale\n",
    "        target_scaler = load(os.path.join(target_scalers_path, appliance_name + target_scalers_file))\n",
    "        pred_reshaped = pred.reshape(-1, 1)\n",
    "        pred_inverse = target_scaler.inverse_transform(pred_reshaped).flatten()\n",
    "\n",
    "        pred_df[appliance] = pred_inverse\n",
    "\n",
    "    pred_df = compute_other_column(pred_df, df)\n",
    "\n",
    "    # Melt and save\n",
    "    pred_df = melt_dataframe(pred_df)\n",
    "    pred_df.to_parquet(inferred_data_path, index=False)\n",
    "\n",
    "\n",
    "def split_parquet_by_date(input_parquet_path, output_root):\n",
    "    df = pd.read_parquet(input_parquet_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    grouped = df.groupby('date')\n",
    "\n",
    "    for date, group in grouped:\n",
    "        folder_name = f'date={date}'\n",
    "        folder_path = os.path.join(output_root, folder_name)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        group = group.drop(columns=['date'])\n",
    "        output_path = os.path.join(folder_path, 'predictions.parquet')\n",
    "        group.to_parquet(output_path, index=False)\n",
    "\n",
    "    print(\"Done splitting into daily folders.\")\n",
    "\n",
    "\n",
    "X_day, timestamps = create_day_dataset_from_file()\n",
    "predictions = {}\n",
    "for appliance in appliances_list:\n",
    "    predictions_np = run_inference(X_day, appliance=appliance)\n",
    "    predictions[appliance] = predictions_np\n",
    "\n",
    "append_predictions(timestamps, predictions)\n",
    "daily_partitions_root = os.path.join(data_path, 'demo_dataset')  # same as your original output_root\n",
    "split_parquet_by_date(inferred_data_path, daily_partitions_root)"
   ],
   "id": "955a8dbed4061cba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Coffee Machine - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Coffee Machine: (613440,)\n",
      "Processing Dryer - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Dryer: (613440,)\n",
      "Processing Freezer - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Freezer: (613440,)\n",
      "Processing Fridge - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Fridge: (613440,)\n",
      "Processing Lamp - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Lamp: (613440,)\n",
      "Processing Laptop - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Laptop: (613440,)\n",
      "Processing Microwave - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Microwave: (613440,)\n",
      "Processing PC - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for PC: (613440,)\n",
      "Processing Router - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Router: (613440,)\n",
      "Processing Tablet - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Tablet: (613440,)\n",
      "Processing Washing Machine - shape before reshape: (1, 613440, 1)\n",
      "Shape after reshape for Washing Machine: (613440,)\n",
      "Done splitting into daily folders.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T14:19:08.394863Z",
     "start_time": "2025-07-08T14:19:08.392928Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "67b9a68b6f1a7e7d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
