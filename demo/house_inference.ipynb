{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:30:38.027506Z",
     "start_time": "2025-07-03T08:30:38.021373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "# Load the file\n",
    "df = pd.read_parquet(os.path.join(data_path, infer_data_file))\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "print(df.tail(10))\n",
    "\n",
    "columns_to_drop = ['SMid', 'Po', 'P1o', 'P2o', 'P3o', 'Ei', 'Ei1', 'Ei2', 'Eo', 'Eo1', 'Eo2']\n",
    "for col in columns_to_drop:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'P1i': 'powerl1',\n",
    "    'P2i': 'powerl2',\n",
    "    'P3i': 'powerl3',\n",
    "    'I1': 'currentl1',\n",
    "    'I2': 'currentl2',\n",
    "    'I3': 'currentl3',\n",
    "    'V1': 'voltagel1',\n",
    "    'V2': 'voltagel2',\n",
    "    'V3': 'voltagel3',\n",
    "})\n",
    "\"\"\""
   ],
   "id": "cfe287b2134c0ddc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Load the file\\ndf = pd.read_parquet(os.path.join(data_path, infer_data_file))\\n\\nprint(df.shape)\\nprint(df.head(10))\\nprint(df.tail(10))\\n\\ncolumns_to_drop = ['SMid', 'Po', 'P1o', 'P2o', 'P3o', 'Ei', 'Ei1', 'Ei2', 'Eo', 'Eo1', 'Eo2']\\nfor col in columns_to_drop:\\n    if col in df.columns:\\n        df = df.drop(columns=[col])\\n\\ndf = df.rename(columns={\\n    'P1i': 'powerl1',\\n    'P2i': 'powerl2',\\n    'P3i': 'powerl3',\\n    'I1': 'currentl1',\\n    'I2': 'currentl2',\\n    'I3': 'currentl3',\\n    'V1': 'voltagel1',\\n    'V2': 'voltagel2',\\n    'V3': 'voltagel3',\\n})\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:30:39.900200Z",
     "start_time": "2025-07-03T08:30:38.032589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from config_loader import load_config, logger\n",
    "from joblib import load\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json"
   ],
   "id": "10b447d45ff6bd21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:30:40.494740Z",
     "start_time": "2025-07-03T08:30:40.027775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config, config_dir = load_config()\n",
    "env = config['Settings']['environment']\n",
    "models_dir = config['Data']['models_dir']\n",
    "scalers_dir = config['Data']['scalers_dir']\n",
    "data_path = config[env]['data_path']\n",
    "training_dataset_file = config['Data']['training_dataset_file']\n",
    "# infer_data_file = config['Data']['infer_data_file']\n",
    "infer_data_file = 'data/mqtt_data_whole.parquet'\n",
    "# infer_data_file = 'mqtt_data_daily.parquet'\n",
    "demo_dataset_ground_truth_file = config['Data']['demo_dataset_ground_truth_file']\n",
    "inference_timestamp = config['Inference']['inference_timestamp']\n",
    "model_file = config['Data']['model_file']\n",
    "# inferred_data_file = config['Data']['inferred_data_file']\n",
    "inferred_data_file = 'real_inferred_whole.parquet'\n",
    "column_names_file = config['Data']['training_dataset_columns_file']\n",
    "input_scaler_file = config['Data']['input_scaler_file']\n",
    "target_scalers_file = config['Data']['target_scalers_file']\n",
    "batch_size = int(config['Inference']['batch_size'])\n",
    "\n",
    "model_path = os.path.join(data_path, models_dir)\n",
    "inferred_data_path = os.path.join(data_path, inferred_data_file)\n",
    "infer_data_path = os.path.join(data_path, infer_data_file)\n",
    "input_scaler_path = os.path.join(data_path, input_scaler_file)\n",
    "target_scalers_path = os.path.join(data_path, scalers_dir)\n",
    "\n",
    "input_scaler = load(input_scaler_path)\n",
    "\n",
    "device = torch.device('cpu')"
   ],
   "id": "bdd7bdde823b7d2f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-03 10:30:40] INFO: Transfer time: 1900-01-01 11:50:00, Inference time: 1900-01-01 11:53:00\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:30:40.509167Z",
     "start_time": "2025-07-03T08:30:40.505741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read appliance names\n",
    "with open(os.path.join(data_path, column_names_file), 'r') as file:\n",
    "    column_names_json = json.load(file)\n",
    "\n",
    "# Create the list of appliances dynamically from the model files\n",
    "appliances_list = [f.replace('.pt', '').rsplit('_', 1)[0].replace('_', ' ').title() for f in os.listdir(model_path)]\n"
   ],
   "id": "829329e1fd654b71",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:32:36.517122Z",
     "start_time": "2025-07-03T08:30:40.521217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_inference_input(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = ['SMid', 'Pi', 'Po', 'P1o', 'P2o', 'P3o', 'Ei', 'Ei1', 'Ei2', 'Eo', 'Eo1', 'Eo2']\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Rename to match training schema\n",
    "    rename_map = {\n",
    "        'P1i': 'powerl1', 'P2i': 'powerl2', 'P3i': 'powerl3',\n",
    "        'I1': 'currentl1', 'I2': 'currentl2', 'I3': 'currentl3',\n",
    "        'V1': 'voltagel1', 'V2': 'voltagel2', 'V3': 'voltagel3',\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Extract and drop timestamp\n",
    "    ts = df.pop('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Ensure column order\n",
    "    expected_cols = list(rename_map.values())\n",
    "    # if missing := set(expected_cols) - df.columns.to_set():\n",
    "      #   raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    df = df[expected_cols]\n",
    "\n",
    "    # Normalize input\n",
    "    X = input_scaler.transform(df)\n",
    "    return X, ts\n",
    "\n",
    "\n",
    "def run_inference(day_input, app):\n",
    "    appliance_name = app.lower().replace(' ', '_')\n",
    "    model = torch.jit.load(os.path.join(model_path, appliance_name + model_file))\n",
    "    model.eval()\n",
    "\n",
    "    day_input = np.expand_dims(day_input, axis=0)\n",
    "\n",
    "    day_input_tensor = torch.tensor(day_input, dtype=torch.float32).to(device)\n",
    "    day_dataset = TensorDataset(day_input_tensor)\n",
    "    day_loader = DataLoader(day_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions_all = []\n",
    "    with torch.no_grad():\n",
    "        for (batch_X,) in day_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_predictions = model(batch_X)\n",
    "\n",
    "            # Clamp predictions to be non-negative (power >= 0)\n",
    "            batch_predictions = torch.clamp(batch_predictions, min=0.0)\n",
    "\n",
    "            # Convert to NumPy after clamping\n",
    "            predictions_all.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(predictions_all, axis=0)\n",
    "\n",
    "def melt_dataframe(df):\n",
    "    # Melt the DataFrame to long format\n",
    "    df_long = pd.melt(df,\n",
    "                      id_vars=['timestamp'],   # Columns to keep\n",
    "                      var_name='appliance',    # New column for appliance names\n",
    "                      value_name='value')      # New column for values\n",
    "\n",
    "    # Convert timestamp and extract date, hour, month\n",
    "    df_long['timestamp'] = pd.to_datetime(df_long['timestamp'])\n",
    "    df_long['date'] = df_long['timestamp'].dt.date\n",
    "    df_long['minute'] = df_long['timestamp'].dt.minute\n",
    "    df_long['hour'] = df_long['timestamp'].dt.hour\n",
    "    df_long['month'] = df_long['timestamp'].dt.to_period('M')\n",
    "\n",
    "    # Sort by timestamp\n",
    "    return df_long.sort_values(by=['timestamp'])\n",
    "\n",
    "\n",
    "def compute_other_column(p_df, sm_df):\n",
    "    # Ensure timestamps are datetime and sorted\n",
    "    p_df['timestamp'] = pd.to_datetime(p_df['timestamp'])\n",
    "    sm_df['timestamp'] = pd.to_datetime(sm_df['timestamp'])\n",
    "\n",
    "    p_df = p_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    sm_df = sm_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Sum predicted appliance power per timestamp (exclude 'timestamp' column)\n",
    "    appliance_cols = p_df.columns.difference(['timestamp'])\n",
    "    p_df['total_pred_power'] = p_df[appliance_cols].sum(axis=1)\n",
    "\n",
    "    # Sum smart meter phases to get total power per timestamp\n",
    "    phase_cols = [col for col in sm_df.columns if col.lower() in ['powerl1', 'powerl2', 'powerl3']]\n",
    "    sm_df['total_sm_power'] = sm_df[phase_cols].sum(axis=1)\n",
    "\n",
    "    # Merge on timestamp to align rows\n",
    "    merged = pd.merge(p_df, sm_df[['timestamp', 'total_sm_power']], on='timestamp', how='inner')\n",
    "\n",
    "    # Compute 'Other' = smart meter total - sum predicted appliances\n",
    "    merged['Other'] = merged['total_sm_power'] - merged['total_pred_power']\n",
    "\n",
    "    # Clip negative values to zero\n",
    "    merged['Other'] = merged['Other'].clip(lower=0)\n",
    "\n",
    "    result = merged.drop(columns=['total_pred_power', 'total_sm_power'])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def append_predictions(ts, predictions_dict):\n",
    "    \"\"\"\n",
    "    timestamps: list of timestamps (len = total timesteps)\n",
    "    predictions_dict: dict of {appliance_name: np.ndarray of shape (total_timesteps,)}\n",
    "                      or (1, seq_len, 1) / (batch, seq_len, 1)\n",
    "    \"\"\"\n",
    "    pred_df = pd.DataFrame({'timestamp': ts})\n",
    "\n",
    "    for app, pred in predictions_dict.items():\n",
    "        appliance_name = app.lower().replace(' ', '_')\n",
    "\n",
    "        pred = np.squeeze(pred)\n",
    "\n",
    "        # Inverse scale\n",
    "        target_scaler = load(os.path.join(target_scalers_path, appliance_name + target_scalers_file))\n",
    "        pred_reshaped = pred.reshape(-1, 1)\n",
    "        pred_inverse = target_scaler.inverse_transform(pred_reshaped).flatten()\n",
    "\n",
    "        pred_df[appliance] = pred_inverse\n",
    "\n",
    "    # Compute 'Other' column\n",
    "    sm_df = pd.read_parquet(infer_data_path)\n",
    "    pred_df = compute_other_column(pred_df, sm_df)\n",
    "\n",
    "    # Melt and save\n",
    "    pred_df = melt_dataframe(pred_df)\n",
    "\n",
    "    # Load previous data if exists\n",
    "    if os.path.exists(inferred_data_path):\n",
    "        try:\n",
    "            existing_df = pd.read_parquet(inferred_data_path)\n",
    "            pred_df = pd.concat([existing_df, pred_df], ignore_index=True)\n",
    "        except Exception as ex:\n",
    "            logger.error(f\"Error loading existing file: {ex}\")\n",
    "\n",
    "    pred_df.to_parquet(inferred_data_path, index=False)\n",
    "    logger.info(f\"[{datetime.now()}] Predictions for appliances {list(predictions_dict.keys())} appended to {inferred_data_path}\")\n",
    "\n",
    "\n",
    "\n",
    "input_data, timestamps = prepare_inference_input(infer_data_path)\n",
    "predictions = {}\n",
    "for appliance in appliances_list:\n",
    "    predictions_np = run_inference(input_data, app=appliance)\n",
    "    predictions[appliance] = predictions_np\n",
    "\n",
    "append_predictions(timestamps, predictions)"
   ],
   "id": "955a8dbed4061cba",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 146\u001B[39m\n\u001B[32m    144\u001B[39m predictions = {}\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m appliance \u001B[38;5;129;01min\u001B[39;00m appliances_list:\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m     predictions_np = \u001B[43mrun_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mappliance\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    147\u001B[39m     predictions[appliance] = predictions_np\n\u001B[32m    149\u001B[39m append_predictions(timestamps, predictions)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 45\u001B[39m, in \u001B[36mrun_inference\u001B[39m\u001B[34m(day_input, app)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m (batch_X,) \u001B[38;5;129;01min\u001B[39;00m day_loader:\n\u001B[32m     44\u001B[39m     batch_X = batch_X.to(device)\n\u001B[32m---> \u001B[39m\u001B[32m45\u001B[39m     batch_predictions = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_X\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     47\u001B[39m     \u001B[38;5;66;03m# Clamp predictions to be non-negative (power >= 0)\u001B[39;00m\n\u001B[32m     48\u001B[39m     batch_predictions = torch.clamp(batch_predictions, \u001B[38;5;28mmin\u001B[39m=\u001B[32m0.0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "67b9a68b6f1a7e7d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
