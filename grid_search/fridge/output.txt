Using device: cuda

Run 1/72: hidden=128, seq_len=120, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000732
Validation Loss: 0.00064460
Epoch [2/300], Train Loss: 0.000656
Validation Loss: 0.00062209
Epoch [3/300], Train Loss: 0.000629
Validation Loss: 0.00060413
Epoch [4/300], Train Loss: 0.000615
Validation Loss: 0.00059585
Epoch [5/300], Train Loss: 0.000609
Validation Loss: 0.00059141
Epoch [6/300], Train Loss: 0.000604
Validation Loss: 0.00058872
Epoch [7/300], Train Loss: 0.000602
Validation Loss: 0.00058511
Epoch [8/300], Train Loss: 0.000600
Validation Loss: 0.00058417
Epoch [9/300], Train Loss: 0.000597
Validation Loss: 0.00057958
Epoch [10/300], Train Loss: 0.000594
Validation Loss: 0.00057573
Epoch [11/300], Train Loss: 0.000591
Validation Loss: 0.00057627
Epoch [12/300], Train Loss: 0.000589
Validation Loss: 0.00057254
Epoch [13/300], Train Loss: 0.000587
Validation Loss: 0.00056857
Epoch [14/300], Train Loss: 0.000583
Validation Loss: 0.00056636
Epoch [15/300], Train Loss: 0.000582
Validation Loss: 0.00056390
Epoch [16/300], Train Loss: 0.000579
Validation Loss: 0.00056196
Epoch [17/300], Train Loss: 0.000575
Validation Loss: 0.00055829
Epoch [18/300], Train Loss: 0.000572
Validation Loss: 0.00055519
Epoch [19/300], Train Loss: 0.000569
Validation Loss: 0.00055352
Epoch [20/300], Train Loss: 0.000566
Validation Loss: 0.00055020
Epoch [21/300], Train Loss: 0.000563
Validation Loss: 0.00054764
Epoch [22/300], Train Loss: 0.000559
Validation Loss: 0.00054424
Epoch [23/300], Train Loss: 0.000556
Validation Loss: 0.00054149
Epoch [24/300], Train Loss: 0.000553
Validation Loss: 0.00054074
Epoch [25/300], Train Loss: 0.000550
Validation Loss: 0.00053872
Epoch [26/300], Train Loss: 0.000547
Validation Loss: 0.00053604
Epoch [27/300], Train Loss: 0.000541
Validation Loss: 0.00052550
Epoch [28/300], Train Loss: 0.000536
Validation Loss: 0.00051747
Epoch [29/300], Train Loss: 0.000528
Validation Loss: 0.00051665
Epoch [30/300], Train Loss: 0.000519
Validation Loss: 0.00049854
Epoch [31/300], Train Loss: 0.000507
Validation Loss: 0.00048457
Epoch [32/300], Train Loss: 0.000498
Validation Loss: 0.00047352
Epoch [33/300], Train Loss: 0.000489
Validation Loss: 0.00046634
Epoch [34/300], Train Loss: 0.000482
Validation Loss: 0.00046139
Epoch [35/300], Train Loss: 0.000477
Validation Loss: 0.00045965
Epoch [36/300], Train Loss: 0.000468
Validation Loss: 0.00045727
Epoch [37/300], Train Loss: 0.000459
Validation Loss: 0.00044793
Epoch [38/300], Train Loss: 0.000454
Validation Loss: 0.00043123
Epoch [39/300], Train Loss: 0.000442
Validation Loss: 0.00042690
Epoch [40/300], Train Loss: 0.000439
Validation Loss: 0.00042240
Epoch [41/300], Train Loss: 0.000432
Validation Loss: 0.00041627
Epoch [42/300], Train Loss: 0.000427
Validation Loss: 0.00041604
Epoch [43/300], Train Loss: 0.000420
Validation Loss: 0.00040601
Epoch [44/300], Train Loss: 0.000416
Validation Loss: 0.00039921
Epoch [45/300], Train Loss: 0.000410
Validation Loss: 0.00040682
Epoch [46/300], Train Loss: 0.000407
Validation Loss: 0.00039455
Epoch [47/300], Train Loss: 0.000405
Validation Loss: 0.00039337
Epoch [48/300], Train Loss: 0.000401
Validation Loss: 0.00039491
Epoch [49/300], Train Loss: 0.000401
Validation Loss: 0.00039878
Epoch [50/300], Train Loss: 0.000397
Validation Loss: 0.00038718
Epoch [51/300], Train Loss: 0.000395
Validation Loss: 0.00039157
Epoch [52/300], Train Loss: 0.000392
Validation Loss: 0.00037834
Epoch [53/300], Train Loss: 0.000389
Validation Loss: 0.00038077
Epoch [54/300], Train Loss: 0.000387
Validation Loss: 0.00038272
Epoch [55/300], Train Loss: 0.000387
Validation Loss: 0.00037598
Epoch [56/300], Train Loss: 0.000383
Validation Loss: 0.00037791
Epoch [57/300], Train Loss: 0.000381
Validation Loss: 0.00037845
Epoch [58/300], Train Loss: 0.000379
Validation Loss: 0.00037179
Epoch [59/300], Train Loss: 0.000379
Validation Loss: 0.00036701
Epoch [60/300], Train Loss: 0.000376
Validation Loss: 0.00036450
Epoch [61/300], Train Loss: 0.000375
Validation Loss: 0.00036157
Epoch [62/300], Train Loss: 0.000371
Validation Loss: 0.00036230
Epoch [63/300], Train Loss: 0.000369
Validation Loss: 0.00035913
Epoch [64/300], Train Loss: 0.000369
Validation Loss: 0.00036264
Epoch [65/300], Train Loss: 0.000368
Validation Loss: 0.00035508
Epoch [66/300], Train Loss: 0.000366
Validation Loss: 0.00035456
Epoch [67/300], Train Loss: 0.000364
Validation Loss: 0.00035097
Epoch [68/300], Train Loss: 0.000363
Validation Loss: 0.00035393
Epoch [69/300], Train Loss: 0.000360
Validation Loss: 0.00035101
Epoch [70/300], Train Loss: 0.000360
Validation Loss: 0.00034871
Epoch [71/300], Train Loss: 0.000356
Validation Loss: 0.00034768
Epoch [72/300], Train Loss: 0.000356
Validation Loss: 0.00034403
Epoch [73/300], Train Loss: 0.000353
Validation Loss: 0.00034587
Epoch [74/300], Train Loss: 0.000355
Validation Loss: 0.00034045
Epoch [75/300], Train Loss: 0.000351
Validation Loss: 0.00033892
Epoch [76/300], Train Loss: 0.000349
Validation Loss: 0.00033728
Epoch [77/300], Train Loss: 0.000347
Validation Loss: 0.00033816
Epoch [78/300], Train Loss: 0.000346
Validation Loss: 0.00033693
Epoch [79/300], Train Loss: 0.000345
Validation Loss: 0.00034055
Epoch [80/300], Train Loss: 0.000344
Validation Loss: 0.00033204
Epoch [81/300], Train Loss: 0.000342
Validation Loss: 0.00033159
Epoch [82/300], Train Loss: 0.000341
Validation Loss: 0.00032944
Epoch [83/300], Train Loss: 0.000339
Validation Loss: 0.00032917
Epoch [84/300], Train Loss: 0.000338
Validation Loss: 0.00032861
Epoch [85/300], Train Loss: 0.000336
Validation Loss: 0.00033014
Epoch [86/300], Train Loss: 0.000336
Validation Loss: 0.00032598
Epoch [87/300], Train Loss: 0.000334
Validation Loss: 0.00032368
Epoch [88/300], Train Loss: 0.000333
Validation Loss: 0.00032066
Epoch [89/300], Train Loss: 0.000330
Validation Loss: 0.00032222
Epoch [90/300], Train Loss: 0.000329
Validation Loss: 0.00031656
Epoch [91/300], Train Loss: 0.000327
Validation Loss: 0.00031731
Epoch [92/300], Train Loss: 0.000326
Validation Loss: 0.00031417
Epoch [93/300], Train Loss: 0.000324
Validation Loss: 0.00031466
Epoch [94/300], Train Loss: 0.000323
Validation Loss: 0.00031138
Epoch [95/300], Train Loss: 0.000322
Validation Loss: 0.00031264
Epoch [96/300], Train Loss: 0.000321
Validation Loss: 0.00031121
Epoch [97/300], Train Loss: 0.000319
Validation Loss: 0.00031110
Epoch [98/300], Train Loss: 0.000319
Validation Loss: 0.00030695
Epoch [99/300], Train Loss: 0.000317
Validation Loss: 0.00030802
Epoch [100/300], Train Loss: 0.000316
Validation Loss: 0.00030371
Epoch [101/300], Train Loss: 0.000315
Validation Loss: 0.00030924
Epoch [102/300], Train Loss: 0.000312
Validation Loss: 0.00030074
Epoch [103/300], Train Loss: 0.000311
Validation Loss: 0.00030177
Epoch [104/300], Train Loss: 0.000309
Validation Loss: 0.00030253
Epoch [105/300], Train Loss: 0.000311
Validation Loss: 0.00030867
Epoch [106/300], Train Loss: 0.000308
Validation Loss: 0.00029540
Epoch [107/300], Train Loss: 0.000305
Validation Loss: 0.00029533
Epoch [108/300], Train Loss: 0.000306
Validation Loss: 0.00029879
Epoch [109/300], Train Loss: 0.000303
Validation Loss: 0.00029102
Epoch [110/300], Train Loss: 0.000302
Validation Loss: 0.00029495
Epoch [111/300], Train Loss: 0.000303
Validation Loss: 0.00029116
Epoch [112/300], Train Loss: 0.000300
Validation Loss: 0.00029082
Epoch [113/300], Train Loss: 0.000299
Validation Loss: 0.00028569
Epoch [114/300], Train Loss: 0.000299
Validation Loss: 0.00028736
Epoch [115/300], Train Loss: 0.000298
Validation Loss: 0.00028704
Epoch [116/300], Train Loss: 0.000298
Validation Loss: 0.00028385
Epoch [117/300], Train Loss: 0.000295
Validation Loss: 0.00028446
Epoch [118/300], Train Loss: 0.000294
Validation Loss: 0.00028488
Epoch [119/300], Train Loss: 0.000294
Validation Loss: 0.00027833
Epoch [120/300], Train Loss: 0.000292
Validation Loss: 0.00028926
Epoch [121/300], Train Loss: 0.000292
Validation Loss: 0.00027862
Epoch [122/300], Train Loss: 0.000291
Validation Loss: 0.00027861
Epoch [123/300], Train Loss: 0.000289
Validation Loss: 0.00028355
Epoch [124/300], Train Loss: 0.000288
Validation Loss: 0.00027744
Epoch [125/300], Train Loss: 0.000288
Validation Loss: 0.00027350
Epoch [126/300], Train Loss: 0.000287
Validation Loss: 0.00028072
Epoch [127/300], Train Loss: 0.000286
Validation Loss: 0.00027813
Epoch [128/300], Train Loss: 0.000285
Validation Loss: 0.00027194
Epoch [129/300], Train Loss: 0.000285
Validation Loss: 0.00027214
Epoch [130/300], Train Loss: 0.000283
Validation Loss: 0.00027038
Epoch [131/300], Train Loss: 0.000283
Validation Loss: 0.00027338
Epoch [132/300], Train Loss: 0.000282
Validation Loss: 0.00027684
Epoch [133/300], Train Loss: 0.000282
Validation Loss: 0.00026704
Epoch [134/300], Train Loss: 0.000280
Validation Loss: 0.00026739
Epoch [135/300], Train Loss: 0.000279
Validation Loss: 0.00026829
Epoch [136/300], Train Loss: 0.000279
Validation Loss: 0.00026504
Epoch [137/300], Train Loss: 0.000278
Validation Loss: 0.00026837
Epoch [138/300], Train Loss: 0.000278
Validation Loss: 0.00026465
Epoch [139/300], Train Loss: 0.000277
Validation Loss: 0.00026408
Epoch [140/300], Train Loss: 0.000278
Validation Loss: 0.00026260
Epoch [141/300], Train Loss: 0.000275
Validation Loss: 0.00026102
Epoch [142/300], Train Loss: 0.000274
Validation Loss: 0.00026344
Epoch [143/300], Train Loss: 0.000274
Validation Loss: 0.00025968
Epoch [144/300], Train Loss: 0.000275
Validation Loss: 0.00025877
Epoch [145/300], Train Loss: 0.000272
Validation Loss: 0.00026242
Epoch [146/300], Train Loss: 0.000272
Validation Loss: 0.00025950
Epoch [147/300], Train Loss: 0.000271
Validation Loss: 0.00025791
Epoch [148/300], Train Loss: 0.000270
Validation Loss: 0.00026810
Epoch [149/300], Train Loss: 0.000271
Validation Loss: 0.00025768
Epoch [150/300], Train Loss: 0.000271
Validation Loss: 0.00025691
Epoch [151/300], Train Loss: 0.000269
Validation Loss: 0.00025539
Epoch [152/300], Train Loss: 0.000267
Validation Loss: 0.00025486
Epoch [153/300], Train Loss: 0.000268
Validation Loss: 0.00025610
Epoch [154/300], Train Loss: 0.000267
Validation Loss: 0.00025403
Epoch [155/300], Train Loss: 0.000267
Validation Loss: 0.00025233
Epoch [156/300], Train Loss: 0.000266
Validation Loss: 0.00025082
Epoch [157/300], Train Loss: 0.000265
Validation Loss: 0.00025298
Epoch [158/300], Train Loss: 0.000265
Validation Loss: 0.00025041
Epoch [159/300], Train Loss: 0.000265
Validation Loss: 0.00026564
Epoch [160/300], Train Loss: 0.000263
Validation Loss: 0.00025060
Epoch [161/300], Train Loss: 0.000262
Validation Loss: 0.00024892
Epoch [162/300], Train Loss: 0.000262
Validation Loss: 0.00025225
Epoch [163/300], Train Loss: 0.000262
Validation Loss: 0.00024796
Epoch [164/300], Train Loss: 0.000262
Validation Loss: 0.00024736
Epoch [165/300], Train Loss: 0.000261
Validation Loss: 0.00025096
Epoch [166/300], Train Loss: 0.000261
Validation Loss: 0.00025163
Epoch [167/300], Train Loss: 0.000259
Validation Loss: 0.00024697
Epoch [168/300], Train Loss: 0.000260
Validation Loss: 0.00024582
Epoch [169/300], Train Loss: 0.000258
Validation Loss: 0.00024536
Epoch [170/300], Train Loss: 0.000258
Validation Loss: 0.00024547
Epoch [171/300], Train Loss: 0.000257
Validation Loss: 0.00024926
Epoch [172/300], Train Loss: 0.000258
Validation Loss: 0.00024450
Epoch [173/300], Train Loss: 0.000257
Validation Loss: 0.00024407
Epoch [174/300], Train Loss: 0.000257
Validation Loss: 0.00024340
Epoch [175/300], Train Loss: 0.000256
Validation Loss: 0.00024277
Epoch [176/300], Train Loss: 0.000256
Validation Loss: 0.00024143
Epoch [177/300], Train Loss: 0.000254
Validation Loss: 0.00024244
Epoch [178/300], Train Loss: 0.000255
Validation Loss: 0.00024040
Epoch [179/300], Train Loss: 0.000255
Validation Loss: 0.00024189
Epoch [180/300], Train Loss: 0.000254
Validation Loss: 0.00023880
Epoch [181/300], Train Loss: 0.000252
Validation Loss: 0.00024005
Epoch [182/300], Train Loss: 0.000253
Validation Loss: 0.00024013
Epoch [183/300], Train Loss: 0.000253
Validation Loss: 0.00023911
Epoch [184/300], Train Loss: 0.000252
Validation Loss: 0.00023798
Epoch [185/300], Train Loss: 0.000252
Validation Loss: 0.00023986
Epoch [186/300], Train Loss: 0.000251
Validation Loss: 0.00023853
Epoch [187/300], Train Loss: 0.000250
Validation Loss: 0.00023858
Epoch [188/300], Train Loss: 0.000251
Validation Loss: 0.00023875
Epoch [189/300], Train Loss: 0.000249
Validation Loss: 0.00024068
Epoch [190/300], Train Loss: 0.000250
Validation Loss: 0.00023803
Epoch [191/300], Train Loss: 0.000249
Validation Loss: 0.00023538
Epoch [192/300], Train Loss: 0.000249
Validation Loss: 0.00023500
Epoch [193/300], Train Loss: 0.000248
Validation Loss: 0.00023815
Epoch [194/300], Train Loss: 0.000248
Validation Loss: 0.00023403
Epoch [195/300], Train Loss: 0.000248
Validation Loss: 0.00023334
Epoch [196/300], Train Loss: 0.000247
Validation Loss: 0.00023509
Epoch [197/300], Train Loss: 0.000246
Validation Loss: 0.00023538
Epoch [198/300], Train Loss: 0.000246
Validation Loss: 0.00023359
Epoch [199/300], Train Loss: 0.000245
Validation Loss: 0.00023345
Epoch [200/300], Train Loss: 0.000245
Validation Loss: 0.00023389
Epoch [201/300], Train Loss: 0.000245
Validation Loss: 0.00023249
Epoch [202/300], Train Loss: 0.000245
Validation Loss: 0.00023224
Epoch [203/300], Train Loss: 0.000245
Validation Loss: 0.00023520
Epoch [204/300], Train Loss: 0.000244
Validation Loss: 0.00023231
Epoch [205/300], Train Loss: 0.000244
Validation Loss: 0.00023131
Epoch [206/300], Train Loss: 0.000244
Validation Loss: 0.00023161
Epoch [207/300], Train Loss: 0.000243
Validation Loss: 0.00022969
Epoch [208/300], Train Loss: 0.000243
Validation Loss: 0.00022941
Epoch [209/300], Train Loss: 0.000243
Validation Loss: 0.00022901
Epoch [210/300], Train Loss: 0.000242
Validation Loss: 0.00022995
Epoch [211/300], Train Loss: 0.000242
Validation Loss: 0.00022878
Epoch [212/300], Train Loss: 0.000241
Validation Loss: 0.00022880
Epoch [213/300], Train Loss: 0.000241
Validation Loss: 0.00022769
Epoch [214/300], Train Loss: 0.000241
Validation Loss: 0.00022698
Epoch [215/300], Train Loss: 0.000241
Validation Loss: 0.00023059
Epoch [216/300], Train Loss: 0.000241
Validation Loss: 0.00023499
Epoch [217/300], Train Loss: 0.000240
Validation Loss: 0.00023086
Epoch [218/300], Train Loss: 0.000240
Validation Loss: 0.00022567
Epoch [219/300], Train Loss: 0.000240
Validation Loss: 0.00022540
Epoch [220/300], Train Loss: 0.000239
Validation Loss: 0.00022720
Epoch [221/300], Train Loss: 0.000240
Validation Loss: 0.00023251
Epoch [222/300], Train Loss: 0.000240
Validation Loss: 0.00022552
Epoch [223/300], Train Loss: 0.000239
Validation Loss: 0.00022790
Epoch [224/300], Train Loss: 0.000238
Validation Loss: 0.00022374
Epoch [225/300], Train Loss: 0.000238
Validation Loss: 0.00022863
Epoch [226/300], Train Loss: 0.000238
Validation Loss: 0.00022602
Epoch [227/300], Train Loss: 0.000238
Validation Loss: 0.00022322
Epoch [228/300], Train Loss: 0.000237
Validation Loss: 0.00023155
Epoch [229/300], Train Loss: 0.000238
Validation Loss: 0.00022638
Epoch [230/300], Train Loss: 0.000237
Validation Loss: 0.00022754
Epoch [231/300], Train Loss: 0.000237
Validation Loss: 0.00022554
Epoch [232/300], Train Loss: 0.000236
Validation Loss: 0.00022841
Epoch [233/300], Train Loss: 0.000237
Validation Loss: 0.00022216
Epoch [234/300], Train Loss: 0.000236
Validation Loss: 0.00022217
Epoch [235/300], Train Loss: 0.000236
Validation Loss: 0.00022303
Epoch [236/300], Train Loss: 0.000236
Validation Loss: 0.00022472
Epoch [237/300], Train Loss: 0.000236
Validation Loss: 0.00022228
Epoch [238/300], Train Loss: 0.000235
Validation Loss: 0.00022355
Epoch [239/300], Train Loss: 0.000236
Validation Loss: 0.00022141
Epoch [240/300], Train Loss: 0.000235
Validation Loss: 0.00022485
Epoch [241/300], Train Loss: 0.000234
Validation Loss: 0.00022158
Epoch [242/300], Train Loss: 0.000234
Validation Loss: 0.00022443
Epoch [243/300], Train Loss: 0.000235
Validation Loss: 0.00022040
Epoch [244/300], Train Loss: 0.000233
Validation Loss: 0.00022217
Epoch [245/300], Train Loss: 0.000234
Validation Loss: 0.00022401
Epoch [246/300], Train Loss: 0.000233
Validation Loss: 0.00021946
Epoch [247/300], Train Loss: 0.000233
Validation Loss: 0.00022007
Epoch [248/300], Train Loss: 0.000233
Validation Loss: 0.00022250
Epoch [249/300], Train Loss: 0.000233
Validation Loss: 0.00022067
Epoch [250/300], Train Loss: 0.000233
Validation Loss: 0.00021966
Epoch [251/300], Train Loss: 0.000233
Validation Loss: 0.00022135
Epoch [252/300], Train Loss: 0.000233
Validation Loss: 0.00021889
Epoch [253/300], Train Loss: 0.000232
Validation Loss: 0.00021911
Epoch [254/300], Train Loss: 0.000232
Validation Loss: 0.00022118
Epoch [255/300], Train Loss: 0.000232
Validation Loss: 0.00022010
Epoch [256/300], Train Loss: 0.000232
Validation Loss: 0.00022332
Epoch [257/300], Train Loss: 0.000232
Validation Loss: 0.00021821
Epoch [258/300], Train Loss: 0.000232
Validation Loss: 0.00021825
Epoch [259/300], Train Loss: 0.000232
Validation Loss: 0.00021866
Epoch [260/300], Train Loss: 0.000231
Validation Loss: 0.00022049
Epoch [261/300], Train Loss: 0.000232
Validation Loss: 0.00021815
Epoch [262/300], Train Loss: 0.000231
Validation Loss: 0.00021945
Epoch [263/300], Train Loss: 0.000231
Validation Loss: 0.00021987
Epoch [264/300], Train Loss: 0.000232
Validation Loss: 0.00021725
Epoch [265/300], Train Loss: 0.000231
Validation Loss: 0.00021849
Epoch [266/300], Train Loss: 0.000230
Validation Loss: 0.00021731
Epoch [267/300], Train Loss: 0.000230
Validation Loss: 0.00021968
Epoch [268/300], Train Loss: 0.000231
Validation Loss: 0.00021731
Epoch [269/300], Train Loss: 0.000231
Validation Loss: 0.00021768
Epoch [270/300], Train Loss: 0.000230
Validation Loss: 0.00021877
Epoch [271/300], Train Loss: 0.000230
Validation Loss: 0.00021813
Epoch [272/300], Train Loss: 0.000229
Validation Loss: 0.00021594
Epoch [273/300], Train Loss: 0.000229
Validation Loss: 0.00021635
Epoch [274/300], Train Loss: 0.000229
Validation Loss: 0.00021768
Epoch [275/300], Train Loss: 0.000229
Validation Loss: 0.00021616
Epoch [276/300], Train Loss: 0.000229
Validation Loss: 0.00021886
Epoch [277/300], Train Loss: 0.000229
Validation Loss: 0.00021990
Epoch [278/300], Train Loss: 0.000229
Validation Loss: 0.00021660
Epoch [279/300], Train Loss: 0.000229
Validation Loss: 0.00021703
Epoch [280/300], Train Loss: 0.000228
Validation Loss: 0.00021641
Epoch [281/300], Train Loss: 0.000228
Validation Loss: 0.00021650
Epoch [282/300], Train Loss: 0.000228
Validation Loss: 0.00021535
Epoch [283/300], Train Loss: 0.000228
Validation Loss: 0.00021559
Epoch [284/300], Train Loss: 0.000228
Validation Loss: 0.00021488
Epoch [285/300], Train Loss: 0.000228
Validation Loss: 0.00021497
Epoch [286/300], Train Loss: 0.000228
Validation Loss: 0.00021565
Epoch [287/300], Train Loss: 0.000227
Validation Loss: 0.00021978
Epoch [288/300], Train Loss: 0.000228
Validation Loss: 0.00021502
Epoch [289/300], Train Loss: 0.000227
Validation Loss: 0.00021510
Epoch [290/300], Train Loss: 0.000228
Validation Loss: 0.00021481
Epoch [291/300], Train Loss: 0.000227
Validation Loss: 0.00021528
Epoch [292/300], Train Loss: 0.000227
Validation Loss: 0.00021394
Epoch [293/300], Train Loss: 0.000227
Validation Loss: 0.00021490
Epoch [294/300], Train Loss: 0.000226
Validation Loss: 0.00021434
Epoch [295/300], Train Loss: 0.000227
Validation Loss: 0.00021427
Epoch [296/300], Train Loss: 0.000226
Validation Loss: 0.00021440
Epoch [297/300], Train Loss: 0.000226
Validation Loss: 0.00021379
Epoch [298/300], Train Loss: 0.000226
Validation Loss: 0.00021399
Epoch [299/300], Train Loss: 0.000226
Validation Loss: 0.00021298
Epoch [300/300], Train Loss: 0.000226
Validation Loss: 0.00021373

Evaluating model for: Fridge
Run 1/72 completed in 17225.12 seconds with: {'MAE': np.float32(17.549725), 'MSE': np.float32(695.0687), 'RMSE': np.float32(26.364157), 'SAE': np.float32(0.0055028982), 'NDE': np.float32(0.4567449)}

Run 2/72: hidden=128, seq_len=120, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000822
Validation Loss: 0.00067001
Epoch [2/300], Train Loss: 0.000686
Validation Loss: 0.00064717
Epoch [3/300], Train Loss: 0.000660
Validation Loss: 0.00062384
Epoch [4/300], Train Loss: 0.000637
Validation Loss: 0.00060778
Epoch [5/300], Train Loss: 0.000624
Validation Loss: 0.00060188
Epoch [6/300], Train Loss: 0.000618
Validation Loss: 0.00059900
Epoch [7/300], Train Loss: 0.000614
Validation Loss: 0.00059449
Epoch [8/300], Train Loss: 0.000611
Validation Loss: 0.00059177
Epoch [9/300], Train Loss: 0.000608
Validation Loss: 0.00058837
Epoch [10/300], Train Loss: 0.000605
Validation Loss: 0.00058635
Epoch [11/300], Train Loss: 0.000603
Validation Loss: 0.00058865
Epoch [12/300], Train Loss: 0.000601
Validation Loss: 0.00058442
Epoch [13/300], Train Loss: 0.000599
Validation Loss: 0.00058012
Epoch [14/300], Train Loss: 0.000596
Validation Loss: 0.00057950
Epoch [15/300], Train Loss: 0.000596
Validation Loss: 0.00057678
Epoch [16/300], Train Loss: 0.000593
Validation Loss: 0.00057579
Epoch [17/300], Train Loss: 0.000591
Validation Loss: 0.00057401
Epoch [18/300], Train Loss: 0.000590
Validation Loss: 0.00057235
Epoch [19/300], Train Loss: 0.000587
Validation Loss: 0.00057094
Epoch [20/300], Train Loss: 0.000585
Validation Loss: 0.00056794
Epoch [21/300], Train Loss: 0.000582
Validation Loss: 0.00056603
Epoch [22/300], Train Loss: 0.000578
Validation Loss: 0.00055950
Epoch [23/300], Train Loss: 0.000572
Validation Loss: 0.00055206
Epoch [24/300], Train Loss: 0.000566
Validation Loss: 0.00054868
Epoch [25/300], Train Loss: 0.000561
Validation Loss: 0.00054600
Epoch [26/300], Train Loss: 0.000557
Validation Loss: 0.00054310
Epoch [27/300], Train Loss: 0.000550
Validation Loss: 0.00053251
Epoch [28/300], Train Loss: 0.000541
Validation Loss: 0.00051990
Epoch [29/300], Train Loss: 0.000530
Validation Loss: 0.00052642
Epoch [30/300], Train Loss: 0.000518
Validation Loss: 0.00049508
Epoch [31/300], Train Loss: 0.000509
Validation Loss: 0.00048775
Epoch [32/300], Train Loss: 0.000503
Validation Loss: 0.00048859
Epoch [33/300], Train Loss: 0.000496
Validation Loss: 0.00047862
Epoch [34/300], Train Loss: 0.000488
Validation Loss: 0.00047276
Epoch [35/300], Train Loss: 0.000484
Validation Loss: 0.00046918
Epoch [36/300], Train Loss: 0.000478
Validation Loss: 0.00047201
Epoch [37/300], Train Loss: 0.000472
Validation Loss: 0.00046085
Epoch [38/300], Train Loss: 0.000469
Validation Loss: 0.00045496
Epoch [39/300], Train Loss: 0.000463
Validation Loss: 0.00045113
Epoch [40/300], Train Loss: 0.000459
Validation Loss: 0.00044541
Epoch [41/300], Train Loss: 0.000455
Validation Loss: 0.00044053
Epoch [42/300], Train Loss: 0.000453
Validation Loss: 0.00043687
Epoch [43/300], Train Loss: 0.000446
Validation Loss: 0.00043481
Epoch [44/300], Train Loss: 0.000442
Validation Loss: 0.00043212
Epoch [45/300], Train Loss: 0.000439
Validation Loss: 0.00042911
Epoch [46/300], Train Loss: 0.000435
Validation Loss: 0.00042604
Epoch [47/300], Train Loss: 0.000435
Validation Loss: 0.00042247
Epoch [48/300], Train Loss: 0.000432
Validation Loss: 0.00042891
Epoch [49/300], Train Loss: 0.000429
Validation Loss: 0.00042343
Epoch [50/300], Train Loss: 0.000430
Validation Loss: 0.00041995
Epoch [51/300], Train Loss: 0.000428
Validation Loss: 0.00043312
Epoch [52/300], Train Loss: 0.000423
Validation Loss: 0.00041416
Epoch [53/300], Train Loss: 0.000420
Validation Loss: 0.00041707
Epoch [54/300], Train Loss: 0.000420
Validation Loss: 0.00042469
Epoch [55/300], Train Loss: 0.000419
Validation Loss: 0.00041210
Epoch [56/300], Train Loss: 0.000417
Validation Loss: 0.00041050
Epoch [57/300], Train Loss: 0.000415
Validation Loss: 0.00041826
Epoch [58/300], Train Loss: 0.000414
Validation Loss: 0.00040683
Epoch [59/300], Train Loss: 0.000413
Validation Loss: 0.00040471
Epoch [60/300], Train Loss: 0.000410
Validation Loss: 0.00040038
Epoch [61/300], Train Loss: 0.000409
Validation Loss: 0.00040479
Epoch [62/300], Train Loss: 0.000406
Validation Loss: 0.00040561
Epoch [63/300], Train Loss: 0.000406
Validation Loss: 0.00040024
Epoch [64/300], Train Loss: 0.000405
Validation Loss: 0.00040452
Epoch [65/300], Train Loss: 0.000405
Validation Loss: 0.00039860
Epoch [66/300], Train Loss: 0.000402
Validation Loss: 0.00040232
Epoch [67/300], Train Loss: 0.000401
Validation Loss: 0.00039342
Epoch [68/300], Train Loss: 0.000399
Validation Loss: 0.00039373
Epoch [69/300], Train Loss: 0.000398
Validation Loss: 0.00039319
Epoch [70/300], Train Loss: 0.000395
Validation Loss: 0.00038888
Epoch [71/300], Train Loss: 0.000392
Validation Loss: 0.00038863
Epoch [72/300], Train Loss: 0.000391
Validation Loss: 0.00039080
Epoch [73/300], Train Loss: 0.000389
Validation Loss: 0.00038855
Epoch [74/300], Train Loss: 0.000391
Validation Loss: 0.00038844
Epoch [75/300], Train Loss: 0.000388
Validation Loss: 0.00038550
Epoch [76/300], Train Loss: 0.000385
Validation Loss: 0.00038408
Epoch [77/300], Train Loss: 0.000383
Validation Loss: 0.00037621
Epoch [78/300], Train Loss: 0.000381
Validation Loss: 0.00038034
Epoch [79/300], Train Loss: 0.000380
Validation Loss: 0.00037155
Epoch [80/300], Train Loss: 0.000379
Validation Loss: 0.00037146
Epoch [81/300], Train Loss: 0.000376
Validation Loss: 0.00037155
Epoch [82/300], Train Loss: 0.000374
Validation Loss: 0.00036548
Epoch [83/300], Train Loss: 0.000373
Validation Loss: 0.00036554
Epoch [84/300], Train Loss: 0.000370
Validation Loss: 0.00036105
Epoch [85/300], Train Loss: 0.000366
Validation Loss: 0.00036221
Epoch [86/300], Train Loss: 0.000365
Validation Loss: 0.00035758
Epoch [87/300], Train Loss: 0.000362
Validation Loss: 0.00035543
Epoch [88/300], Train Loss: 0.000359
Validation Loss: 0.00035036
Epoch [89/300], Train Loss: 0.000356
Validation Loss: 0.00034887
Epoch [90/300], Train Loss: 0.000354
Validation Loss: 0.00034277
Epoch [91/300], Train Loss: 0.000350
Validation Loss: 0.00033973
Epoch [92/300], Train Loss: 0.000347
Validation Loss: 0.00033707
Epoch [93/300], Train Loss: 0.000344
Validation Loss: 0.00034157
Epoch [94/300], Train Loss: 0.000342
Validation Loss: 0.00033140
Epoch [95/300], Train Loss: 0.000339
Validation Loss: 0.00032965
Epoch [96/300], Train Loss: 0.000337
Validation Loss: 0.00032928
Epoch [97/300], Train Loss: 0.000337
Validation Loss: 0.00032773
Epoch [98/300], Train Loss: 0.000334
Validation Loss: 0.00032513
Epoch [99/300], Train Loss: 0.000333
Validation Loss: 0.00032865
Epoch [100/300], Train Loss: 0.000330
Validation Loss: 0.00032249
Epoch [101/300], Train Loss: 0.000329
Validation Loss: 0.00032168
Epoch [102/300], Train Loss: 0.000325
Validation Loss: 0.00032045
Epoch [103/300], Train Loss: 0.000325
Validation Loss: 0.00031680
Epoch [104/300], Train Loss: 0.000322
Validation Loss: 0.00031249
Epoch [105/300], Train Loss: 0.000324
Validation Loss: 0.00031605
Epoch [106/300], Train Loss: 0.000320
Validation Loss: 0.00030916
Epoch [107/300], Train Loss: 0.000317
Validation Loss: 0.00030726
Epoch [108/300], Train Loss: 0.000316
Validation Loss: 0.00030971
Epoch [109/300], Train Loss: 0.000315
Validation Loss: 0.00030614
Epoch [110/300], Train Loss: 0.000313
Validation Loss: 0.00030678
Epoch [111/300], Train Loss: 0.000313
Validation Loss: 0.00030591
Epoch [112/300], Train Loss: 0.000310
Validation Loss: 0.00030244
Epoch [113/300], Train Loss: 0.000310
Validation Loss: 0.00029926
Epoch [114/300], Train Loss: 0.000308
Validation Loss: 0.00029964
Epoch [115/300], Train Loss: 0.000307
Validation Loss: 0.00030220
Epoch [116/300], Train Loss: 0.000307
Validation Loss: 0.00029745
Epoch [117/300], Train Loss: 0.000306
Validation Loss: 0.00029494
Epoch [118/300], Train Loss: 0.000304
Validation Loss: 0.00029405
Epoch [119/300], Train Loss: 0.000302
Validation Loss: 0.00029089
Epoch [120/300], Train Loss: 0.000300
Validation Loss: 0.00029556
Epoch [121/300], Train Loss: 0.000300
Validation Loss: 0.00028568
Epoch [122/300], Train Loss: 0.000299
Validation Loss: 0.00028848
Epoch [123/300], Train Loss: 0.000297
Validation Loss: 0.00029087
Epoch [124/300], Train Loss: 0.000294
Validation Loss: 0.00028582
Epoch [125/300], Train Loss: 0.000293
Validation Loss: 0.00028054
Epoch [126/300], Train Loss: 0.000291
Validation Loss: 0.00028351
Epoch [127/300], Train Loss: 0.000291
Validation Loss: 0.00027821
Epoch [128/300], Train Loss: 0.000289
Validation Loss: 0.00027534
Epoch [129/300], Train Loss: 0.000289
Validation Loss: 0.00027664
Epoch [130/300], Train Loss: 0.000286
Validation Loss: 0.00027334
Epoch [131/300], Train Loss: 0.000286
Validation Loss: 0.00027610
Epoch [132/300], Train Loss: 0.000285
Validation Loss: 0.00027507
Epoch [133/300], Train Loss: 0.000283
Validation Loss: 0.00027400
Epoch [134/300], Train Loss: 0.000281
Validation Loss: 0.00026660
Epoch [135/300], Train Loss: 0.000281
Validation Loss: 0.00027604
Epoch [136/300], Train Loss: 0.000281
Validation Loss: 0.00026635
Epoch [137/300], Train Loss: 0.000279
Validation Loss: 0.00026426
Epoch [138/300], Train Loss: 0.000278
Validation Loss: 0.00026529
Epoch [139/300], Train Loss: 0.000276
Validation Loss: 0.00026494
Epoch [140/300], Train Loss: 0.000277
Validation Loss: 0.00026532
Epoch [141/300], Train Loss: 0.000274
Validation Loss: 0.00026121
Epoch [142/300], Train Loss: 0.000272
Validation Loss: 0.00026903
Epoch [143/300], Train Loss: 0.000273
Validation Loss: 0.00026150
Epoch [144/300], Train Loss: 0.000274
Validation Loss: 0.00025460
Epoch [145/300], Train Loss: 0.000269
Validation Loss: 0.00025771
Epoch [146/300], Train Loss: 0.000269
Validation Loss: 0.00025384
Epoch [147/300], Train Loss: 0.000268
Validation Loss: 0.00025957
Epoch [148/300], Train Loss: 0.000267
Validation Loss: 0.00026249
Epoch [149/300], Train Loss: 0.000267
Validation Loss: 0.00025905
Epoch [150/300], Train Loss: 0.000268
Validation Loss: 0.00025582
Epoch [151/300], Train Loss: 0.000264
Validation Loss: 0.00025239
Epoch [152/300], Train Loss: 0.000263
Validation Loss: 0.00024913
Epoch [153/300], Train Loss: 0.000262
Validation Loss: 0.00025062
Epoch [154/300], Train Loss: 0.000261
Validation Loss: 0.00024919
Epoch [155/300], Train Loss: 0.000261
Validation Loss: 0.00024806
Epoch [156/300], Train Loss: 0.000258
Validation Loss: 0.00024774
Epoch [157/300], Train Loss: 0.000258
Validation Loss: 0.00024377
Epoch [158/300], Train Loss: 0.000259
Validation Loss: 0.00024747
Epoch [159/300], Train Loss: 0.000260
Validation Loss: 0.00025443
Epoch [160/300], Train Loss: 0.000255
Validation Loss: 0.00024302
Epoch [161/300], Train Loss: 0.000255
Validation Loss: 0.00023982
Epoch [162/300], Train Loss: 0.000255
Validation Loss: 0.00024034
Epoch [163/300], Train Loss: 0.000252
Validation Loss: 0.00023992
Epoch [164/300], Train Loss: 0.000252
Validation Loss: 0.00023964
Epoch [165/300], Train Loss: 0.000253
Validation Loss: 0.00024007
Epoch [166/300], Train Loss: 0.000251
Validation Loss: 0.00024032
Epoch [167/300], Train Loss: 0.000249
Validation Loss: 0.00023810
Epoch [168/300], Train Loss: 0.000249
Validation Loss: 0.00023632
Epoch [169/300], Train Loss: 0.000248
Validation Loss: 0.00023441
Epoch [170/300], Train Loss: 0.000249
Validation Loss: 0.00023277
Epoch [171/300], Train Loss: 0.000247
Validation Loss: 0.00023330
Epoch [172/300], Train Loss: 0.000246
Validation Loss: 0.00023893
Epoch [173/300], Train Loss: 0.000247
Validation Loss: 0.00023113
Epoch [174/300], Train Loss: 0.000245
Validation Loss: 0.00023104
Epoch [175/300], Train Loss: 0.000245
Validation Loss: 0.00023545
Epoch [176/300], Train Loss: 0.000245
Validation Loss: 0.00023069
Epoch [177/300], Train Loss: 0.000243
Validation Loss: 0.00023329
Epoch [178/300], Train Loss: 0.000243
Validation Loss: 0.00023288
Epoch [179/300], Train Loss: 0.000243
Validation Loss: 0.00023697
Epoch [180/300], Train Loss: 0.000243
Validation Loss: 0.00022815
Epoch [181/300], Train Loss: 0.000241
Validation Loss: 0.00022875
Epoch [182/300], Train Loss: 0.000241
Validation Loss: 0.00022953
Epoch [183/300], Train Loss: 0.000241
Validation Loss: 0.00022749
Epoch [184/300], Train Loss: 0.000240
Validation Loss: 0.00022542
Epoch [185/300], Train Loss: 0.000239
Validation Loss: 0.00022901
Epoch [186/300], Train Loss: 0.000239
Validation Loss: 0.00022451
Epoch [187/300], Train Loss: 0.000238
Validation Loss: 0.00023537
Epoch [188/300], Train Loss: 0.000238
Validation Loss: 0.00022687
Epoch [189/300], Train Loss: 0.000237
Validation Loss: 0.00022727
Epoch [190/300], Train Loss: 0.000238
Validation Loss: 0.00022880
Epoch [191/300], Train Loss: 0.000238
Validation Loss: 0.00023053
Epoch [192/300], Train Loss: 0.000236
Validation Loss: 0.00022586
Epoch [193/300], Train Loss: 0.000236
Validation Loss: 0.00022283
Epoch [194/300], Train Loss: 0.000236
Validation Loss: 0.00022375
Epoch [195/300], Train Loss: 0.000234
Validation Loss: 0.00022880
Epoch [196/300], Train Loss: 0.000235
Validation Loss: 0.00022309
Epoch [197/300], Train Loss: 0.000234
Validation Loss: 0.00022099
Epoch [198/300], Train Loss: 0.000233
Validation Loss: 0.00022588
Epoch [199/300], Train Loss: 0.000235
Validation Loss: 0.00022155
Epoch [200/300], Train Loss: 0.000233
Validation Loss: 0.00022104
Epoch [201/300], Train Loss: 0.000232
Validation Loss: 0.00022104
Epoch [202/300], Train Loss: 0.000232
Validation Loss: 0.00022151
Epoch [203/300], Train Loss: 0.000234
Validation Loss: 0.00022281
Epoch [204/300], Train Loss: 0.000231
Validation Loss: 0.00022033
Epoch [205/300], Train Loss: 0.000231
Validation Loss: 0.00022060
Epoch [206/300], Train Loss: 0.000231
Validation Loss: 0.00022185
Epoch [207/300], Train Loss: 0.000232
Validation Loss: 0.00022022
Epoch [208/300], Train Loss: 0.000232
Validation Loss: 0.00021902
Epoch [209/300], Train Loss: 0.000231
Validation Loss: 0.00021901
Epoch [210/300], Train Loss: 0.000229
Validation Loss: 0.00021962
Epoch [211/300], Train Loss: 0.000231
Validation Loss: 0.00021818
Epoch [212/300], Train Loss: 0.000230
Validation Loss: 0.00021863
Epoch [213/300], Train Loss: 0.000228
Validation Loss: 0.00021832
Epoch [214/300], Train Loss: 0.000228
Validation Loss: 0.00021662
Epoch [215/300], Train Loss: 0.000228
Validation Loss: 0.00021739
Epoch [216/300], Train Loss: 0.000229
Validation Loss: 0.00025574
Epoch [217/300], Train Loss: 0.000229
Validation Loss: 0.00022022
Epoch [218/300], Train Loss: 0.000227
Validation Loss: 0.00021957
Epoch [219/300], Train Loss: 0.000229
Validation Loss: 0.00021568
Epoch [220/300], Train Loss: 0.000227
Validation Loss: 0.00021925
Epoch [221/300], Train Loss: 0.000226
Validation Loss: 0.00021606
Epoch [222/300], Train Loss: 0.000227
Validation Loss: 0.00021640
Epoch [223/300], Train Loss: 0.000226
Validation Loss: 0.00021625
Epoch [224/300], Train Loss: 0.000228
Validation Loss: 0.00021709
Epoch [225/300], Train Loss: 0.000226
Validation Loss: 0.00021568
Epoch [226/300], Train Loss: 0.000224
Validation Loss: 0.00024273
Epoch [227/300], Train Loss: 0.000226
Validation Loss: 0.00021474
Epoch [228/300], Train Loss: 0.000225
Validation Loss: 0.00021508
Epoch [229/300], Train Loss: 0.000225
Validation Loss: 0.00022332
Epoch [230/300], Train Loss: 0.000227
Validation Loss: 0.00021790
Epoch [231/300], Train Loss: 0.000225
Validation Loss: 0.00021603
Epoch [232/300], Train Loss: 0.000223
Validation Loss: 0.00021388
Epoch [233/300], Train Loss: 0.000224
Validation Loss: 0.00021340
Epoch [234/300], Train Loss: 0.000223
Validation Loss: 0.00021276
Epoch [235/300], Train Loss: 0.000224
Validation Loss: 0.00022221
Epoch [236/300], Train Loss: 0.000229
Validation Loss: 0.00022018
Epoch [237/300], Train Loss: 0.000222
Validation Loss: 0.00021472
Epoch [238/300], Train Loss: 0.000223
Validation Loss: 0.00021212
Epoch [239/300], Train Loss: 0.000222
Validation Loss: 0.00021214
Epoch [240/300], Train Loss: 0.000222
Validation Loss: 0.00021233
Epoch [241/300], Train Loss: 0.000222
Validation Loss: 0.00021118
Epoch [242/300], Train Loss: 0.000221
Validation Loss: 0.00022072
Epoch [243/300], Train Loss: 0.000223
Validation Loss: 0.00021119
Epoch [244/300], Train Loss: 0.000221
Validation Loss: 0.00021074
Epoch [245/300], Train Loss: 0.000223
Validation Loss: 0.00021249
Epoch [246/300], Train Loss: 0.000220
Validation Loss: 0.00021032
Epoch [247/300], Train Loss: 0.000221
Validation Loss: 0.00021156
Epoch [248/300], Train Loss: 0.000221
Validation Loss: 0.00021110
Epoch [249/300], Train Loss: 0.000222
Validation Loss: 0.00021117
Epoch [250/300], Train Loss: 0.000220
Validation Loss: 0.00021045
Epoch [251/300], Train Loss: 0.000220
Validation Loss: 0.00021133
Epoch [252/300], Train Loss: 0.000220
Validation Loss: 0.00021156
Epoch [253/300], Train Loss: 0.000219
Validation Loss: 0.00021060
Epoch [254/300], Train Loss: 0.000221
Validation Loss: 0.00021042
Epoch [255/300], Train Loss: 0.000219
Validation Loss: 0.00021238
Epoch [256/300], Train Loss: 0.000219
Validation Loss: 0.00020958
Epoch [257/300], Train Loss: 0.000219
Validation Loss: 0.00021062
Epoch [258/300], Train Loss: 0.000219
Validation Loss: 0.00021021
Epoch [259/300], Train Loss: 0.000218
Validation Loss: 0.00021029
Epoch [260/300], Train Loss: 0.000219
Validation Loss: 0.00021081
Epoch [261/300], Train Loss: 0.000218
Validation Loss: 0.00020880
Epoch [262/300], Train Loss: 0.000219
Validation Loss: 0.00024398
Epoch [263/300], Train Loss: 0.000219
Validation Loss: 0.00020979
Epoch [264/300], Train Loss: 0.000218
Validation Loss: 0.00020925
Epoch [265/300], Train Loss: 0.000218
Validation Loss: 0.00020787
Epoch [266/300], Train Loss: 0.000217
Validation Loss: 0.00020761
Epoch [267/300], Train Loss: 0.000220
Validation Loss: 0.00021163
Epoch [268/300], Train Loss: 0.000217
Validation Loss: 0.00020785
Epoch [269/300], Train Loss: 0.000217
Validation Loss: 0.00020875
Epoch [270/300], Train Loss: 0.000218
Validation Loss: 0.00020711
Epoch [271/300], Train Loss: 0.000219
Validation Loss: 0.00020791
Epoch [272/300], Train Loss: 0.000216
Validation Loss: 0.00020754
Epoch [273/300], Train Loss: 0.000216
Validation Loss: 0.00020806
Epoch [274/300], Train Loss: 0.000217
Validation Loss: 0.00020808
Epoch [275/300], Train Loss: 0.000216
Validation Loss: 0.00020756
Epoch [276/300], Train Loss: 0.000217
Validation Loss: 0.00020713
Epoch [277/300], Train Loss: 0.000215
Validation Loss: 0.00021792
Epoch [278/300], Train Loss: 0.000217
Validation Loss: 0.00020801
Epoch [279/300], Train Loss: 0.000216
Validation Loss: 0.00020839
Epoch [280/300], Train Loss: 0.000216
Validation Loss: 0.00020551
Epoch [281/300], Train Loss: 0.000215
Validation Loss: 0.00020832
Epoch [282/300], Train Loss: 0.000216
Validation Loss: 0.00020671
Epoch [283/300], Train Loss: 0.000215
Validation Loss: 0.00020629
Epoch [284/300], Train Loss: 0.000215
Validation Loss: 0.00020531
Epoch [285/300], Train Loss: 0.000218
Validation Loss: 0.00020639
Epoch [286/300], Train Loss: 0.000214
Validation Loss: 0.00020964
Epoch [287/300], Train Loss: 0.000214
Validation Loss: 0.00020864
Epoch [288/300], Train Loss: 0.000214
Validation Loss: 0.00021072
Epoch [289/300], Train Loss: 0.000217
Validation Loss: 0.00020617
Epoch [290/300], Train Loss: 0.000215
Validation Loss: 0.00020526
Epoch [291/300], Train Loss: 0.000214
Validation Loss: 0.00020483
Epoch [292/300], Train Loss: 0.000215
Validation Loss: 0.00020765
Epoch [293/300], Train Loss: 0.000213
Validation Loss: 0.00020879
Epoch [294/300], Train Loss: 0.000214
Validation Loss: 0.00020770
Epoch [295/300], Train Loss: 0.000214
Validation Loss: 0.00020685
Epoch [296/300], Train Loss: 0.000213
Validation Loss: 0.00020577
Epoch [297/300], Train Loss: 0.000214
Validation Loss: 0.00020358
Epoch [298/300], Train Loss: 0.000213
Validation Loss: 0.00020551
Epoch [299/300], Train Loss: 0.000213
Validation Loss: 0.00020514
Epoch [300/300], Train Loss: 0.000213
Validation Loss: 0.00020465

Evaluating model for: Fridge
Run 2/72 completed in 17777.57 seconds with: {'MAE': np.float32(16.858461), 'MSE': np.float32(660.0195), 'RMSE': np.float32(25.690844), 'SAE': np.float32(0.0071315286), 'NDE': np.float32(0.4450801)}

Run 3/72: hidden=128, seq_len=120, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000662
Validation Loss: 0.00063654
Epoch [2/300], Train Loss: 0.000633
Validation Loss: 0.00060503
Epoch [3/300], Train Loss: 0.000615
Validation Loss: 0.00059877
Epoch [4/300], Train Loss: 0.000609
Validation Loss: 0.00059612
Epoch [5/300], Train Loss: 0.000606
Validation Loss: 0.00059177
Epoch [6/300], Train Loss: 0.000603
Validation Loss: 0.00059042
Epoch [7/300], Train Loss: 0.000600
Validation Loss: 0.00058713
Epoch [8/300], Train Loss: 0.000599
Validation Loss: 0.00059511
Epoch [9/300], Train Loss: 0.000597
Validation Loss: 0.00058459
Epoch [10/300], Train Loss: 0.000594
Validation Loss: 0.00058162
Epoch [11/300], Train Loss: 0.000592
Validation Loss: 0.00057986
Epoch [12/300], Train Loss: 0.000589
Validation Loss: 0.00057659
Epoch [13/300], Train Loss: 0.000587
Validation Loss: 0.00057100
Epoch [14/300], Train Loss: 0.000581
Validation Loss: 0.00056367
Epoch [15/300], Train Loss: 0.000575
Validation Loss: 0.00056029
Epoch [16/300], Train Loss: 0.000569
Validation Loss: 0.00055165
Epoch [17/300], Train Loss: 0.000563
Validation Loss: 0.00054900
Epoch [18/300], Train Loss: 0.000562
Validation Loss: 0.00054576
Epoch [19/300], Train Loss: 0.000558
Validation Loss: 0.00054724
Epoch [20/300], Train Loss: 0.000554
Validation Loss: 0.00054689
Epoch [21/300], Train Loss: 0.000550
Validation Loss: 0.00053528
Epoch [22/300], Train Loss: 0.000545
Validation Loss: 0.00053320
Epoch [23/300], Train Loss: 0.000539
Validation Loss: 0.00052479
Epoch [24/300], Train Loss: 0.000529
Validation Loss: 0.00050660
Epoch [25/300], Train Loss: 0.000518
Validation Loss: 0.00051015
Epoch [26/300], Train Loss: 0.000505
Validation Loss: 0.00049526
Epoch [27/300], Train Loss: 0.000498
Validation Loss: 0.00047410
Epoch [28/300], Train Loss: 0.000486
Validation Loss: 0.00047051
Epoch [29/300], Train Loss: 0.000482
Validation Loss: 0.00051494
Epoch [30/300], Train Loss: 0.000477
Validation Loss: 0.00045997
Epoch [31/300], Train Loss: 0.000468
Validation Loss: 0.00045466
Epoch [32/300], Train Loss: 0.000466
Validation Loss: 0.00044866
Epoch [33/300], Train Loss: 0.000460
Validation Loss: 0.00045057
Epoch [34/300], Train Loss: 0.000460
Validation Loss: 0.00044600
Epoch [35/300], Train Loss: 0.000452
Validation Loss: 0.00043691
Epoch [36/300], Train Loss: 0.000456
Validation Loss: 0.00044723
Epoch [37/300], Train Loss: 0.000447
Validation Loss: 0.00043406
Epoch [38/300], Train Loss: 0.000443
Validation Loss: 0.00043076
Epoch [39/300], Train Loss: 0.000440
Validation Loss: 0.00047178
Epoch [40/300], Train Loss: 0.000438
Validation Loss: 0.00042679
Epoch [41/300], Train Loss: 0.000434
Validation Loss: 0.00042463
Epoch [42/300], Train Loss: 0.000431
Validation Loss: 0.00041541
Epoch [43/300], Train Loss: 0.000427
Validation Loss: 0.00042379
Epoch [44/300], Train Loss: 0.000422
Validation Loss: 0.00041149
Epoch [45/300], Train Loss: 0.000421
Validation Loss: 0.00042940
Epoch [46/300], Train Loss: 0.000417
Validation Loss: 0.00042142
Epoch [47/300], Train Loss: 0.000415
Validation Loss: 0.00040307
Epoch [48/300], Train Loss: 0.000413
Validation Loss: 0.00040802
Epoch [49/300], Train Loss: 0.000410
Validation Loss: 0.00040145
Epoch [50/300], Train Loss: 0.000411
Validation Loss: 0.00039840
Epoch [51/300], Train Loss: 0.000408
Validation Loss: 0.00039198
Epoch [52/300], Train Loss: 0.000403
Validation Loss: 0.00039548
Epoch [53/300], Train Loss: 0.000403
Validation Loss: 0.00039622
Epoch [54/300], Train Loss: 0.000403
Validation Loss: 0.00039870
Epoch [55/300], Train Loss: 0.000399
Validation Loss: 0.00039123
Epoch [56/300], Train Loss: 0.000396
Validation Loss: 0.00038453
Epoch [57/300], Train Loss: 0.000393
Validation Loss: 0.00039233
Epoch [58/300], Train Loss: 0.000390
Validation Loss: 0.00042195
Epoch [59/300], Train Loss: 0.000392
Validation Loss: 0.00037639
Epoch [60/300], Train Loss: 0.000387
Validation Loss: 0.00038432
Epoch [61/300], Train Loss: 0.000386
Validation Loss: 0.00037136
Epoch [62/300], Train Loss: 0.000385
Validation Loss: 0.00037299
Epoch [63/300], Train Loss: 0.000381
Validation Loss: 0.00037223
Epoch [64/300], Train Loss: 0.000382
Validation Loss: 0.00036828
Epoch [65/300], Train Loss: 0.000377
Validation Loss: 0.00036568
Epoch [66/300], Train Loss: 0.000377
Validation Loss: 0.00039741
Epoch [67/300], Train Loss: 0.000373
Validation Loss: 0.00035856
Epoch [68/300], Train Loss: 0.000370
Validation Loss: 0.00035511
Epoch [69/300], Train Loss: 0.000368
Validation Loss: 0.00037292
Epoch [70/300], Train Loss: 0.000368
Validation Loss: 0.00036577
Epoch [71/300], Train Loss: 0.000367
Validation Loss: 0.00035368
Epoch [72/300], Train Loss: 0.000366
Validation Loss: 0.00034745
Epoch [73/300], Train Loss: 0.000361
Validation Loss: 0.00035880
Epoch [74/300], Train Loss: 0.000359
Validation Loss: 0.00034457
Epoch [75/300], Train Loss: 0.000358
Validation Loss: 0.00034953
Epoch [76/300], Train Loss: 0.000357
Validation Loss: 0.00034578
Epoch [77/300], Train Loss: 0.000355
Validation Loss: 0.00034023
Epoch [78/300], Train Loss: 0.000350
Validation Loss: 0.00034071
Epoch [79/300], Train Loss: 0.000349
Validation Loss: 0.00034151
Epoch [80/300], Train Loss: 0.000349
Validation Loss: 0.00033180
Epoch [81/300], Train Loss: 0.000346
Validation Loss: 0.00033320
Epoch [82/300], Train Loss: 0.000342
Validation Loss: 0.00032478
Epoch [83/300], Train Loss: 0.000340
Validation Loss: 0.00032597
Epoch [84/300], Train Loss: 0.000337
Validation Loss: 0.00033160
Epoch [85/300], Train Loss: 0.000339
Validation Loss: 0.00034134
Epoch [86/300], Train Loss: 0.000337
Validation Loss: 0.00032490
Epoch [87/300], Train Loss: 0.000330
Validation Loss: 0.00031734
Epoch [88/300], Train Loss: 0.000328
Validation Loss: 0.00031273
Epoch [89/300], Train Loss: 0.000329
Validation Loss: 0.00030989
Epoch [90/300], Train Loss: 0.000324
Validation Loss: 0.00031750
Epoch [91/300], Train Loss: 0.000322
Validation Loss: 0.00030903
Epoch [92/300], Train Loss: 0.000317
Validation Loss: 0.00030688
Epoch [93/300], Train Loss: 0.000317
Validation Loss: 0.00030460
Epoch [94/300], Train Loss: 0.000314
Validation Loss: 0.00031089
Epoch [95/300], Train Loss: 0.000314
Validation Loss: 0.00030385
Epoch [96/300], Train Loss: 0.000310
Validation Loss: 0.00029606
Epoch [97/300], Train Loss: 0.000308
Validation Loss: 0.00029492
Epoch [98/300], Train Loss: 0.000307
Validation Loss: 0.00031453
Epoch [99/300], Train Loss: 0.000303
Validation Loss: 0.00029159
Epoch [100/300], Train Loss: 0.000301
Validation Loss: 0.00029513
Epoch [101/300], Train Loss: 0.000302
Validation Loss: 0.00030788
Epoch [102/300], Train Loss: 0.000299
Validation Loss: 0.00028381
Epoch [103/300], Train Loss: 0.000294
Validation Loss: 0.00028178
Epoch [104/300], Train Loss: 0.000294
Validation Loss: 0.00028281
Epoch [105/300], Train Loss: 0.000294
Validation Loss: 0.00027783
Epoch [106/300], Train Loss: 0.000290
Validation Loss: 0.00027331
Epoch [107/300], Train Loss: 0.000287
Validation Loss: 0.00027122
Epoch [108/300], Train Loss: 0.000287
Validation Loss: 0.00027380
Epoch [109/300], Train Loss: 0.000286
Validation Loss: 0.00026963
Epoch [110/300], Train Loss: 0.000283
Validation Loss: 0.00027456
Epoch [111/300], Train Loss: 0.000282
Validation Loss: 0.00026844
Epoch [112/300], Train Loss: 0.000282
Validation Loss: 0.00026780
Epoch [113/300], Train Loss: 0.000279
Validation Loss: 0.00026593
Epoch [114/300], Train Loss: 0.000280
Validation Loss: 0.00026583
Epoch [115/300], Train Loss: 0.000277
Validation Loss: 0.00026810
Epoch [116/300], Train Loss: 0.000277
Validation Loss: 0.00025992
Epoch [117/300], Train Loss: 0.000275
Validation Loss: 0.00026302
Epoch [118/300], Train Loss: 0.000275
Validation Loss: 0.00026034
Epoch [119/300], Train Loss: 0.000274
Validation Loss: 0.00026483
Epoch [120/300], Train Loss: 0.000275
Validation Loss: 0.00026232
Epoch [121/300], Train Loss: 0.000271
Validation Loss: 0.00025858
Epoch [122/300], Train Loss: 0.000270
Validation Loss: 0.00026951
Epoch [123/300], Train Loss: 0.000269
Validation Loss: 0.00025702
Epoch [124/300], Train Loss: 0.000270
Validation Loss: 0.00025695
Epoch [125/300], Train Loss: 0.000269
Validation Loss: 0.00025973
Epoch [126/300], Train Loss: 0.000267
Validation Loss: 0.00026233
Epoch [127/300], Train Loss: 0.000266
Validation Loss: 0.00025521
Epoch [128/300], Train Loss: 0.000266
Validation Loss: 0.00027699
Epoch [129/300], Train Loss: 0.000264
Validation Loss: 0.00025325
Epoch [130/300], Train Loss: 0.000264
Validation Loss: 0.00024987
Epoch [131/300], Train Loss: 0.000266
Validation Loss: 0.00025092
Epoch [132/300], Train Loss: 0.000264
Validation Loss: 0.00024977
Epoch [133/300], Train Loss: 0.000264
Validation Loss: 0.00025239
Epoch [134/300], Train Loss: 0.000266
Validation Loss: 0.00025062
Epoch [135/300], Train Loss: 0.000262
Validation Loss: 0.00024716
Epoch [136/300], Train Loss: 0.000264
Validation Loss: 0.00024685
Epoch [137/300], Train Loss: 0.000260
Validation Loss: 0.00024942
Epoch [138/300], Train Loss: 0.000261
Validation Loss: 0.00024823
Epoch [139/300], Train Loss: 0.000257
Validation Loss: 0.00024739
Epoch [140/300], Train Loss: 0.000259
Validation Loss: 0.00025290
Epoch [141/300], Train Loss: 0.000258
Validation Loss: 0.00024562
Epoch [142/300], Train Loss: 0.000259
Validation Loss: 0.00025133
Epoch [143/300], Train Loss: 0.000257
Validation Loss: 0.00024451
Epoch [144/300], Train Loss: 0.000264
Validation Loss: 0.00024600
Epoch [145/300], Train Loss: 0.000266
Validation Loss: 0.00024508
Epoch [146/300], Train Loss: 0.000257
Validation Loss: 0.00025352
Epoch [147/300], Train Loss: 0.000255
Validation Loss: 0.00024248
Epoch [148/300], Train Loss: 0.000254
Validation Loss: 0.00024863
Epoch [149/300], Train Loss: 0.000255
Validation Loss: 0.00026988
Epoch [150/300], Train Loss: 0.000257
Validation Loss: 0.00025791
Epoch [151/300], Train Loss: 0.000253
Validation Loss: 0.00024217
Epoch [152/300], Train Loss: 0.000252
Validation Loss: 0.00024189
Epoch [153/300], Train Loss: 0.000254
Validation Loss: 0.00025098
Epoch [154/300], Train Loss: 0.000253
Validation Loss: 0.00024681
Epoch [155/300], Train Loss: 0.000254
Validation Loss: 0.00024603
Epoch [156/300], Train Loss: 0.000251
Validation Loss: 0.00024195
Epoch [157/300], Train Loss: 0.000251
Validation Loss: 0.00024122
Epoch [158/300], Train Loss: 0.000252
Validation Loss: 0.00024370
Epoch [159/300], Train Loss: 0.000253
Validation Loss: 0.00025210
Epoch [160/300], Train Loss: 0.000251
Validation Loss: 0.00024804
Epoch [161/300], Train Loss: 0.000249
Validation Loss: 0.00025045
Epoch [162/300], Train Loss: 0.000255
Validation Loss: 0.00025310
Epoch [163/300], Train Loss: 0.000251
Validation Loss: 0.00024421
Epoch [164/300], Train Loss: 0.000250
Validation Loss: 0.00025592
Epoch [165/300], Train Loss: 0.000249
Validation Loss: 0.00023826
Epoch [166/300], Train Loss: 0.000248
Validation Loss: 0.00024308
Epoch [167/300], Train Loss: 0.000249
Validation Loss: 0.00024096
Epoch [168/300], Train Loss: 0.000251
Validation Loss: 0.00025768
Epoch [169/300], Train Loss: 0.000253
Validation Loss: 0.00023583
Epoch [170/300], Train Loss: 0.000248
Validation Loss: 0.00023785
Epoch [171/300], Train Loss: 0.000246
Validation Loss: 0.00023541
Epoch [172/300], Train Loss: 0.000246
Validation Loss: 0.00023687
Epoch [173/300], Train Loss: 0.000247
Validation Loss: 0.00023712
Epoch [174/300], Train Loss: 0.000316
Validation Loss: 0.00028008
Epoch [175/300], Train Loss: 0.000254
Validation Loss: 0.00024272
Epoch [176/300], Train Loss: 0.000246
Validation Loss: 0.00023584
Epoch [177/300], Train Loss: 0.000244
Validation Loss: 0.00023447
Epoch [178/300], Train Loss: 0.000247
Validation Loss: 0.00023406
Epoch [179/300], Train Loss: 0.000245
Validation Loss: 0.00024350
Epoch [180/300], Train Loss: 0.000244
Validation Loss: 0.00023461
Epoch [181/300], Train Loss: 0.000244
Validation Loss: 0.00023413
Epoch [182/300], Train Loss: 0.000244
Validation Loss: 0.00023177
Epoch [183/300], Train Loss: 0.000243
Validation Loss: 0.00023424
Epoch [184/300], Train Loss: 0.000245
Validation Loss: 0.00023153
Epoch [185/300], Train Loss: 0.000243
Validation Loss: 0.00024082
Epoch [186/300], Train Loss: 0.000242
Validation Loss: 0.00024073
Epoch [187/300], Train Loss: 0.000245
Validation Loss: 0.00023212
Epoch [188/300], Train Loss: 0.000242
Validation Loss: 0.00023202
Epoch [189/300], Train Loss: 0.000241
Validation Loss: 0.00023242
Epoch [190/300], Train Loss: 0.000244
Validation Loss: 0.00023272
Epoch [191/300], Train Loss: 0.000244
Validation Loss: 0.00023400
Epoch [192/300], Train Loss: 0.000241
Validation Loss: 0.00023042
Epoch [193/300], Train Loss: 0.000241
Validation Loss: 0.00023242
Epoch [194/300], Train Loss: 0.000240
Validation Loss: 0.00023019
Epoch [195/300], Train Loss: 0.000240
Validation Loss: 0.00023026
Epoch [196/300], Train Loss: 0.000240
Validation Loss: 0.00023148
Epoch [197/300], Train Loss: 0.000240
Validation Loss: 0.00023351
Epoch [198/300], Train Loss: 0.000243
Validation Loss: 0.00022970
Epoch [199/300], Train Loss: 0.000241
Validation Loss: 0.00025106
Epoch [200/300], Train Loss: 0.000242
Validation Loss: 0.00023261
Epoch [201/300], Train Loss: 0.000238
Validation Loss: 0.00022888
Epoch [202/300], Train Loss: 0.000239
Validation Loss: 0.00022811
Epoch [203/300], Train Loss: 0.000236
Validation Loss: 0.00026705
Epoch [204/300], Train Loss: 0.000239
Validation Loss: 0.00023059
Epoch [205/300], Train Loss: 0.000238
Validation Loss: 0.00022716
Epoch [206/300], Train Loss: 0.000239
Validation Loss: 0.00022789
Epoch [207/300], Train Loss: 0.000235
Validation Loss: 0.00022721
Epoch [208/300], Train Loss: 0.000236
Validation Loss: 0.00023783
Epoch [209/300], Train Loss: 0.000236
Validation Loss: 0.00022649
Epoch [210/300], Train Loss: 0.000235
Validation Loss: 0.00022691
Epoch [211/300], Train Loss: 0.000239
Validation Loss: 0.00022624
Epoch [212/300], Train Loss: 0.000237
Validation Loss: 0.00022836
Epoch [213/300], Train Loss: 0.000234
Validation Loss: 0.00022526
Epoch [214/300], Train Loss: 0.000235
Validation Loss: 0.00022467
Epoch [215/300], Train Loss: 0.000235
Validation Loss: 0.00022667
Epoch [216/300], Train Loss: 0.000234
Validation Loss: 0.00023381
Epoch [217/300], Train Loss: 0.000235
Validation Loss: 0.00022987
Epoch [218/300], Train Loss: 0.000234
Validation Loss: 0.00022677
Epoch [219/300], Train Loss: 0.000234
Validation Loss: 0.00022513
Epoch [220/300], Train Loss: 0.000235
Validation Loss: 0.00022590
Epoch [221/300], Train Loss: 0.000245
Validation Loss: 0.00022548
Epoch [222/300], Train Loss: 0.000236
Validation Loss: 0.00022530
Epoch [223/300], Train Loss: 0.000233
Validation Loss: 0.00022384
Epoch [224/300], Train Loss: 0.000241
Validation Loss: 0.00022846
Epoch [225/300], Train Loss: 0.000238
Validation Loss: 0.00028072
Epoch [226/300], Train Loss: 0.000232
Validation Loss: 0.00022354
Epoch [227/300], Train Loss: 0.000231
Validation Loss: 0.00022305
Epoch [228/300], Train Loss: 0.000232
Validation Loss: 0.00026322
Epoch [229/300], Train Loss: 0.000234
Validation Loss: 0.00022354
Epoch [230/300], Train Loss: 0.000232
Validation Loss: 0.00022320
Epoch [231/300], Train Loss: 0.000231
Validation Loss: 0.00022166
Epoch [232/300], Train Loss: 0.000231
Validation Loss: 0.00022359
Epoch [233/300], Train Loss: 0.000232
Validation Loss: 0.00022366
Epoch [234/300], Train Loss: 0.000231
Validation Loss: 0.00022153
Epoch [235/300], Train Loss: 0.000232
Validation Loss: 0.00022193
Epoch [236/300], Train Loss: 0.000230
Validation Loss: 0.00023903
Epoch [237/300], Train Loss: 0.000230
Validation Loss: 0.00022097
Epoch [238/300], Train Loss: 0.000230
Validation Loss: 0.00022028
Epoch [239/300], Train Loss: 0.000234
Validation Loss: 0.00022346
Epoch [240/300], Train Loss: 0.000229
Validation Loss: 0.00022196
Epoch [241/300], Train Loss: 0.000230
Validation Loss: 0.00021993
Epoch [242/300], Train Loss: 0.000229
Validation Loss: 0.00022099
Epoch [243/300], Train Loss: 0.000229
Validation Loss: 0.00022248
Epoch [244/300], Train Loss: 0.000229
Validation Loss: 0.00021971
Epoch [245/300], Train Loss: 0.000229
Validation Loss: 0.00022066
Epoch [246/300], Train Loss: 0.000229
Validation Loss: 0.00022602
Epoch [247/300], Train Loss: 0.000229
Validation Loss: 0.00021918
Epoch [248/300], Train Loss: 0.000228
Validation Loss: 0.00022074
Epoch [249/300], Train Loss: 0.000228
Validation Loss: 0.00022508
Epoch [250/300], Train Loss: 0.000227
Validation Loss: 0.00021901
Epoch [251/300], Train Loss: 0.000229
Validation Loss: 0.00022023
Epoch [252/300], Train Loss: 0.000227
Validation Loss: 0.00021951
Epoch [253/300], Train Loss: 0.000228
Validation Loss: 0.00022231
Epoch [254/300], Train Loss: 0.000227
Validation Loss: 0.00022038
Epoch [255/300], Train Loss: 0.000232
Validation Loss: 0.00021930
Epoch [256/300], Train Loss: 0.000226
Validation Loss: 0.00021858
Epoch [257/300], Train Loss: 0.000230
Validation Loss: 0.00021961
Epoch [258/300], Train Loss: 0.000226
Validation Loss: 0.00021788
Epoch [259/300], Train Loss: 0.000227
Validation Loss: 0.00021918
Epoch [260/300], Train Loss: 0.000226
Validation Loss: 0.00021831
Epoch [261/300], Train Loss: 0.000226
Validation Loss: 0.00021779
Epoch [262/300], Train Loss: 0.000226
Validation Loss: 0.00022053
Epoch [263/300], Train Loss: 0.000225
Validation Loss: 0.00021827
Epoch [264/300], Train Loss: 0.000225
Validation Loss: 0.00021728
Epoch [265/300], Train Loss: 0.000226
Validation Loss: 0.00021705
Epoch [266/300], Train Loss: 0.000228
Validation Loss: 0.00021759
Epoch [267/300], Train Loss: 0.000226
Validation Loss: 0.00021926
Epoch [268/300], Train Loss: 0.000226
Validation Loss: 0.00021700
Epoch [269/300], Train Loss: 0.000231
Validation Loss: 0.00021959
Epoch [270/300], Train Loss: 0.000225
Validation Loss: 0.00021664
Epoch [271/300], Train Loss: 0.000225
Validation Loss: 0.00021758
Epoch [272/300], Train Loss: 0.000225
Validation Loss: 0.00021613
Epoch [273/300], Train Loss: 0.000223
Validation Loss: 0.00021578
Epoch [274/300], Train Loss: 0.000223
Validation Loss: 0.00021584
Epoch [275/300], Train Loss: 0.000223
Validation Loss: 0.00021725
Epoch [276/300], Train Loss: 0.000223
Validation Loss: 0.00021716
Epoch [277/300], Train Loss: 0.000223
Validation Loss: 0.00021685
Epoch [278/300], Train Loss: 0.000225
Validation Loss: 0.00021678
Epoch [279/300], Train Loss: 0.000223
Validation Loss: 0.00021639
Epoch [280/300], Train Loss: 0.000225
Validation Loss: 0.00022013
Epoch [281/300], Train Loss: 0.000223
Validation Loss: 0.00021825
Epoch [282/300], Train Loss: 0.000223
Validation Loss: 0.00021474
Epoch [283/300], Train Loss: 0.000222
Validation Loss: 0.00021421
Epoch [284/300], Train Loss: 0.000223
Validation Loss: 0.00021570
Epoch [285/300], Train Loss: 0.000223
Validation Loss: 0.00021490
Epoch [286/300], Train Loss: 0.000223
Validation Loss: 0.00021453
Epoch [287/300], Train Loss: 0.000221
Validation Loss: 0.00021467
Epoch [288/300], Train Loss: 0.000222
Validation Loss: 0.00021704
Epoch [289/300], Train Loss: 0.000223
Validation Loss: 0.00021502
Epoch [290/300], Train Loss: 0.000222
Validation Loss: 0.00021454
Epoch [291/300], Train Loss: 0.000221
Validation Loss: 0.00021428
Epoch [292/300], Train Loss: 0.000222
Validation Loss: 0.00021374
Epoch [293/300], Train Loss: 0.000241
Validation Loss: 0.00021324
Epoch [294/300], Train Loss: 0.000220
Validation Loss: 0.00021698
Epoch [295/300], Train Loss: 0.000224
Validation Loss: 0.00021570
Epoch [296/300], Train Loss: 0.000221
Validation Loss: 0.00021366
Epoch [297/300], Train Loss: 0.000220
Validation Loss: 0.00021508
Epoch [298/300], Train Loss: 0.000221
Validation Loss: 0.00021517
Epoch [299/300], Train Loss: 0.000221
Validation Loss: 0.00021579
Epoch [300/300], Train Loss: 0.000219
Validation Loss: 0.00021407

Evaluating model for: Fridge
Run 3/72 completed in 17945.20 seconds with: {'MAE': np.float32(17.334805), 'MSE': np.float32(702.38043), 'RMSE': np.float32(26.50246), 'SAE': np.float32(0.017795617), 'NDE': np.float32(0.45914093)}

Run 4/72: hidden=128, seq_len=120, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000795
Validation Loss: 0.00066885
Epoch [2/300], Train Loss: 0.000685
Validation Loss: 0.00063016
Epoch [3/300], Train Loss: 0.000638
Validation Loss: 0.00061101
Epoch [4/300], Train Loss: 0.000627
Validation Loss: 0.00060243
Epoch [5/300], Train Loss: 0.000620
Validation Loss: 0.00059880
Epoch [6/300], Train Loss: 0.000616
Validation Loss: 0.00060110
Epoch [7/300], Train Loss: 0.000612
Validation Loss: 0.00059341
Epoch [8/300], Train Loss: 0.000610
Validation Loss: 0.00059355
Epoch [9/300], Train Loss: 0.000607
Validation Loss: 0.00058899
Epoch [10/300], Train Loss: 0.000605
Validation Loss: 0.00058800
Epoch [11/300], Train Loss: 0.000603
Validation Loss: 0.00058916
Epoch [12/300], Train Loss: 0.000601
Validation Loss: 0.00058665
Epoch [13/300], Train Loss: 0.000600
Validation Loss: 0.00058469
Epoch [14/300], Train Loss: 0.000596
Validation Loss: 0.00058080
Epoch [15/300], Train Loss: 0.000595
Validation Loss: 0.00057710
Epoch [16/300], Train Loss: 0.000591
Validation Loss: 0.00057444
Epoch [17/300], Train Loss: 0.000586
Validation Loss: 0.00056688
Epoch [18/300], Train Loss: 0.000575
Validation Loss: 0.00056195
Epoch [19/300], Train Loss: 0.000567
Validation Loss: 0.00056173
Epoch [20/300], Train Loss: 0.000564
Validation Loss: 0.00055269
Epoch [21/300], Train Loss: 0.000559
Validation Loss: 0.00055270
Epoch [22/300], Train Loss: 0.000556
Validation Loss: 0.00054969
Epoch [23/300], Train Loss: 0.000558
Validation Loss: 0.00054930
Epoch [24/300], Train Loss: 0.000561
Validation Loss: 0.00054491
Epoch [25/300], Train Loss: 0.000593
Validation Loss: 0.00056987
Epoch [26/300], Train Loss: 0.000579
Validation Loss: 0.00056285
Epoch [27/300], Train Loss: 0.000572
Validation Loss: 0.00055709
Epoch [28/300], Train Loss: 0.000566
Validation Loss: 0.00055076
Epoch [29/300], Train Loss: 0.000558
Validation Loss: 0.00054135
Epoch [30/300], Train Loss: 0.000542
Validation Loss: 0.00052647
Epoch [31/300], Train Loss: 0.000532
Validation Loss: 0.00051711
Epoch [32/300], Train Loss: 0.000520
Validation Loss: 0.00051087
Epoch [33/300], Train Loss: 0.000517
Validation Loss: 0.00050771
Epoch [34/300], Train Loss: 0.000521
Validation Loss: 0.00053943
Epoch [35/300], Train Loss: 0.000516
Validation Loss: 0.00050694
Epoch [36/300], Train Loss: 0.000506
Validation Loss: 0.00050009
Epoch [37/300], Train Loss: 0.000515
Validation Loss: 0.00056641
Epoch [38/300], Train Loss: 0.000501
Validation Loss: 0.00049047
Epoch [39/300], Train Loss: 0.000493
Validation Loss: 0.00051358
Epoch [40/300], Train Loss: 0.000493
Validation Loss: 0.00047904
Epoch [41/300], Train Loss: 0.000484
Validation Loss: 0.00047243
Epoch [42/300], Train Loss: 0.000481
Validation Loss: 0.00046740
Epoch [43/300], Train Loss: 0.000476
Validation Loss: 0.00046742
Epoch [44/300], Train Loss: 0.000528
Validation Loss: 0.00053273
Epoch [45/300], Train Loss: 0.000524
Validation Loss: 0.00050159
Epoch [46/300], Train Loss: 0.000490
Validation Loss: 0.00047653
Epoch [47/300], Train Loss: 0.000477
Validation Loss: 0.00047203
Epoch [48/300], Train Loss: 0.000478
Validation Loss: 0.00046347
Epoch [49/300], Train Loss: 0.000468
Validation Loss: 0.00046260
Epoch [50/300], Train Loss: 0.000470
Validation Loss: 0.00045496
Epoch [51/300], Train Loss: 0.000459
Validation Loss: 0.00045597
Epoch [52/300], Train Loss: 0.000457
Validation Loss: 0.00044748
Epoch [53/300], Train Loss: 0.000453
Validation Loss: 0.00044442
Epoch [54/300], Train Loss: 0.000452
Validation Loss: 0.00044122
Epoch [55/300], Train Loss: 0.000449
Validation Loss: 0.00043947
Epoch [56/300], Train Loss: 0.000483
Validation Loss: 0.00047058
Epoch [57/300], Train Loss: 0.000470
Validation Loss: 0.00045993
Epoch [58/300], Train Loss: 0.000464
Validation Loss: 0.00045642
Epoch [59/300], Train Loss: 0.000459
Validation Loss: 0.00044711
Epoch [60/300], Train Loss: 0.000452
Validation Loss: 0.00045002
Epoch [61/300], Train Loss: 0.000445
Validation Loss: 0.00043700
Epoch [62/300], Train Loss: 0.000451
Validation Loss: 0.00043653
Epoch [63/300], Train Loss: 0.000438
Validation Loss: 0.00042952
Epoch [64/300], Train Loss: 0.000436
Validation Loss: 0.00043016
Epoch [65/300], Train Loss: 0.000434
Validation Loss: 0.00042396
Epoch [66/300], Train Loss: 0.000434
Validation Loss: 0.00042403
Epoch [67/300], Train Loss: 0.000428
Validation Loss: 0.00041886
Epoch [68/300], Train Loss: 0.000426
Validation Loss: 0.00041626
Epoch [69/300], Train Loss: 0.000425
Validation Loss: 0.00042401
Epoch [70/300], Train Loss: 0.000422
Validation Loss: 0.00041702
Epoch [71/300], Train Loss: 0.000418
Validation Loss: 0.00041704
Epoch [72/300], Train Loss: 0.000417
Validation Loss: 0.00041286
Epoch [73/300], Train Loss: 0.000418
Validation Loss: 0.00040818
Epoch [74/300], Train Loss: 0.000413
Validation Loss: 0.00040414
Epoch [75/300], Train Loss: 0.000410
Validation Loss: 0.00039701
Epoch [76/300], Train Loss: 0.000406
Validation Loss: 0.00039758
Epoch [77/300], Train Loss: 0.000399
Validation Loss: 0.00038824
Epoch [78/300], Train Loss: 0.000401
Validation Loss: 0.00039239
Epoch [79/300], Train Loss: 0.000396
Validation Loss: 0.00038038
Epoch [80/300], Train Loss: 0.000389
Validation Loss: 0.00037408
Epoch [81/300], Train Loss: 0.000389
Validation Loss: 0.00037078
Epoch [82/300], Train Loss: 0.000381
Validation Loss: 0.00036618
Epoch [83/300], Train Loss: 0.000375
Validation Loss: 0.00035900
Epoch [84/300], Train Loss: 0.000372
Validation Loss: 0.00035595
Epoch [85/300], Train Loss: 0.000367
Validation Loss: 0.00036560
Epoch [86/300], Train Loss: 0.000363
Validation Loss: 0.00035359
Epoch [87/300], Train Loss: 0.000359
Validation Loss: 0.00034667
Epoch [88/300], Train Loss: 0.000357
Validation Loss: 0.00033999
Epoch [89/300], Train Loss: 0.000355
Validation Loss: 0.00034242
Epoch [90/300], Train Loss: 0.000349
Validation Loss: 0.00034136
Epoch [91/300], Train Loss: 0.000346
Validation Loss: 0.00033799
Epoch [92/300], Train Loss: 0.000360
Validation Loss: 0.00034202
Epoch [93/300], Train Loss: 0.000344
Validation Loss: 0.00033166
Epoch [94/300], Train Loss: 0.000341
Validation Loss: 0.00032820
Epoch [95/300], Train Loss: 0.000338
Validation Loss: 0.00033050
Epoch [96/300], Train Loss: 0.000339
Validation Loss: 0.00032524
Epoch [97/300], Train Loss: 0.000334
Validation Loss: 0.00032775
Epoch [98/300], Train Loss: 0.000334
Validation Loss: 0.00032251
Epoch [99/300], Train Loss: 0.000331
Validation Loss: 0.00032161
Epoch [100/300], Train Loss: 0.000331
Validation Loss: 0.00031923
Epoch [101/300], Train Loss: 0.000327
Validation Loss: 0.00031528
Epoch [102/300], Train Loss: 0.000324
Validation Loss: 0.00031333
Epoch [103/300], Train Loss: 0.000324
Validation Loss: 0.00031748
Epoch [104/300], Train Loss: 0.000322
Validation Loss: 0.00031045
Epoch [105/300], Train Loss: 0.000321
Validation Loss: 0.00032565
Epoch [106/300], Train Loss: 0.000317
Validation Loss: 0.00030872
Epoch [107/300], Train Loss: 0.000331
Validation Loss: 0.00030710
Epoch [108/300], Train Loss: 0.000330
Validation Loss: 0.00032272
Epoch [109/300], Train Loss: 0.000315
Validation Loss: 0.00030662
Epoch [110/300], Train Loss: 0.000313
Validation Loss: 0.00030601
Epoch [111/300], Train Loss: 0.000313
Validation Loss: 0.00030607
Epoch [112/300], Train Loss: 0.000309
Validation Loss: 0.00030248
Epoch [113/300], Train Loss: 0.000318
Validation Loss: 0.00030980
Epoch [114/300], Train Loss: 0.000309
Validation Loss: 0.00029891
Epoch [115/300], Train Loss: 0.000307
Validation Loss: 0.00029789
Epoch [116/300], Train Loss: 0.000304
Validation Loss: 0.00029685
Epoch [117/300], Train Loss: 0.000303
Validation Loss: 0.00029926
Epoch [118/300], Train Loss: 0.000304
Validation Loss: 0.00029608
Epoch [119/300], Train Loss: 0.000305
Validation Loss: 0.00029806
Epoch [120/300], Train Loss: 0.000312
Validation Loss: 0.00029636
Epoch [121/300], Train Loss: 0.000300
Validation Loss: 0.00029295
Epoch [122/300], Train Loss: 0.000298
Validation Loss: 0.00029551
Epoch [123/300], Train Loss: 0.000296
Validation Loss: 0.00029108
Epoch [124/300], Train Loss: 0.000296
Validation Loss: 0.00028924
Epoch [125/300], Train Loss: 0.000295
Validation Loss: 0.00031572
Epoch [126/300], Train Loss: 0.000293
Validation Loss: 0.00028865
Epoch [127/300], Train Loss: 0.000293
Validation Loss: 0.00028598
Epoch [128/300], Train Loss: 0.000292
Validation Loss: 0.00028435
Epoch [129/300], Train Loss: 0.000298
Validation Loss: 0.00029956
Epoch [130/300], Train Loss: 0.000290
Validation Loss: 0.00028334
Epoch [131/300], Train Loss: 0.000288
Validation Loss: 0.00028358
Epoch [132/300], Train Loss: 0.000286
Validation Loss: 0.00028441
Epoch [133/300], Train Loss: 0.000288
Validation Loss: 0.00029367
Epoch [134/300], Train Loss: 0.000286
Validation Loss: 0.00028151
Epoch [135/300], Train Loss: 0.000286
Validation Loss: 0.00027818
Epoch [136/300], Train Loss: 0.000282
Validation Loss: 0.00028174
Epoch [137/300], Train Loss: 0.000284
Validation Loss: 0.00027875
Epoch [138/300], Train Loss: 0.000283
Validation Loss: 0.00028111
Epoch [139/300], Train Loss: 0.000282
Validation Loss: 0.00027822
Epoch [140/300], Train Loss: 0.000280
Validation Loss: 0.00028334
Epoch [141/300], Train Loss: 0.000281
Validation Loss: 0.00028308
Epoch [142/300], Train Loss: 0.000278
Validation Loss: 0.00027655
Epoch [143/300], Train Loss: 0.000279
Validation Loss: 0.00027781
Epoch [144/300], Train Loss: 0.000278
Validation Loss: 0.00027707
Epoch [145/300], Train Loss: 0.000287
Validation Loss: 0.00027259
Epoch [146/300], Train Loss: 0.000280
Validation Loss: 0.00027549
Epoch [147/300], Train Loss: 0.000274
Validation Loss: 0.00027101
Epoch [148/300], Train Loss: 0.000273
Validation Loss: 0.00027094
Epoch [149/300], Train Loss: 0.000274
Validation Loss: 0.00027294
Epoch [150/300], Train Loss: 0.000274
Validation Loss: 0.00027229
Epoch [151/300], Train Loss: 0.000274
Validation Loss: 0.00027086
Epoch [152/300], Train Loss: 0.000271
Validation Loss: 0.00026903
Epoch [153/300], Train Loss: 0.000380
Validation Loss: 0.00037846
Epoch [154/300], Train Loss: 0.000368
Validation Loss: 0.00034204
Epoch [155/300], Train Loss: 0.000339
Validation Loss: 0.00031667
Epoch [156/300], Train Loss: 0.000320
Validation Loss: 0.00030373
Epoch [157/300], Train Loss: 0.000308
Validation Loss: 0.00029652
Epoch [158/300], Train Loss: 0.000299
Validation Loss: 0.00028503
Epoch [159/300], Train Loss: 0.000290
Validation Loss: 0.00028587
Epoch [160/300], Train Loss: 0.000285
Validation Loss: 0.00028022
Epoch [161/300], Train Loss: 0.000281
Validation Loss: 0.00027480
Epoch [162/300], Train Loss: 0.000277
Validation Loss: 0.00027470
Early stopping triggered

Evaluating model for: Fridge
Run 4/72 completed in 9499.54 seconds with: {'MAE': np.float32(20.14642), 'MSE': np.float32(886.8061), 'RMSE': np.float32(29.77929), 'SAE': np.float32(0.011106849), 'NDE': np.float32(0.5159104)}

Run 5/72: hidden=128, seq_len=120, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000670
Validation Loss: 0.00067425
Epoch [2/300], Train Loss: 0.000652
Validation Loss: 0.00065775
Epoch [3/300], Train Loss: 0.000634
Validation Loss: 0.00063253
Epoch [4/300], Train Loss: 0.000616
Validation Loss: 0.00061991
Epoch [5/300], Train Loss: 0.000607
Validation Loss: 0.00061436
Epoch [6/300], Train Loss: 0.000603
Validation Loss: 0.00061073
Epoch [7/300], Train Loss: 0.000602
Validation Loss: 0.00061035
Epoch [8/300], Train Loss: 0.000602
Validation Loss: 0.00060550
Epoch [9/300], Train Loss: 0.000600
Validation Loss: 0.00060819
Epoch [10/300], Train Loss: 0.000599
Validation Loss: 0.00060474
Epoch [11/300], Train Loss: 0.000598
Validation Loss: 0.00060353
Epoch [12/300], Train Loss: 0.000596
Validation Loss: 0.00060030
Epoch [13/300], Train Loss: 0.000595
Validation Loss: 0.00060024
Epoch [14/300], Train Loss: 0.000594
Validation Loss: 0.00060220
Epoch [15/300], Train Loss: 0.000592
Validation Loss: 0.00060002
Epoch [16/300], Train Loss: 0.000590
Validation Loss: 0.00059753
Epoch [17/300], Train Loss: 0.000590
Validation Loss: 0.00059335
Epoch [18/300], Train Loss: 0.000588
Validation Loss: 0.00059462
Epoch [19/300], Train Loss: 0.000587
Validation Loss: 0.00058984
Epoch [20/300], Train Loss: 0.000587
Validation Loss: 0.00058876
Epoch [21/300], Train Loss: 0.000585
Validation Loss: 0.00059074
Epoch [22/300], Train Loss: 0.000584
Validation Loss: 0.00058443
Epoch [23/300], Train Loss: 0.000585
Validation Loss: 0.00058994
Epoch [24/300], Train Loss: 0.000582
Validation Loss: 0.00058593
Epoch [25/300], Train Loss: 0.000584
Validation Loss: 0.00058264
Epoch [26/300], Train Loss: 0.000581
Validation Loss: 0.00058211
Epoch [27/300], Train Loss: 0.000580
Validation Loss: 0.00058295
Epoch [28/300], Train Loss: 0.000580
Validation Loss: 0.00058837
Epoch [29/300], Train Loss: 0.000578
Validation Loss: 0.00058057
Epoch [30/300], Train Loss: 0.000578
Validation Loss: 0.00057831
Epoch [31/300], Train Loss: 0.000576
Validation Loss: 0.00057493
Epoch [32/300], Train Loss: 0.000575
Validation Loss: 0.00057559
Epoch [33/300], Train Loss: 0.000574
Validation Loss: 0.00057615
Epoch [34/300], Train Loss: 0.000573
Validation Loss: 0.00057162
Epoch [35/300], Train Loss: 0.000572
Validation Loss: 0.00057022
Epoch [36/300], Train Loss: 0.000572
Validation Loss: 0.00057061
Epoch [37/300], Train Loss: 0.000570
Validation Loss: 0.00056950
Epoch [38/300], Train Loss: 0.000569
Validation Loss: 0.00056623
Epoch [39/300], Train Loss: 0.000568
Validation Loss: 0.00056879
Epoch [40/300], Train Loss: 0.000567
Validation Loss: 0.00056488
Epoch [41/300], Train Loss: 0.000566
Validation Loss: 0.00056401
Epoch [42/300], Train Loss: 0.000565
Validation Loss: 0.00056216
Epoch [43/300], Train Loss: 0.000562
Validation Loss: 0.00055811
Epoch [44/300], Train Loss: 0.000562
Validation Loss: 0.00055760
Epoch [45/300], Train Loss: 0.000561
Validation Loss: 0.00055543
Epoch [46/300], Train Loss: 0.000559
Validation Loss: 0.00055340
Epoch [47/300], Train Loss: 0.000558
Validation Loss: 0.00055363
Epoch [48/300], Train Loss: 0.000556
Validation Loss: 0.00055164
Epoch [49/300], Train Loss: 0.000554
Validation Loss: 0.00054852
Epoch [50/300], Train Loss: 0.000553
Validation Loss: 0.00054875
Epoch [51/300], Train Loss: 0.000551
Validation Loss: 0.00054759
Epoch [52/300], Train Loss: 0.000549
Validation Loss: 0.00054606
Epoch [53/300], Train Loss: 0.000548
Validation Loss: 0.00054132
Epoch [54/300], Train Loss: 0.000547
Validation Loss: 0.00053962
Epoch [55/300], Train Loss: 0.000544
Validation Loss: 0.00053881
Epoch [56/300], Train Loss: 0.000543
Validation Loss: 0.00054033
Epoch [57/300], Train Loss: 0.000541
Validation Loss: 0.00053675
Epoch [58/300], Train Loss: 0.000539
Validation Loss: 0.00053259
Epoch [59/300], Train Loss: 0.000536
Validation Loss: 0.00053055
Epoch [60/300], Train Loss: 0.000533
Validation Loss: 0.00052732
Epoch [61/300], Train Loss: 0.000533
Validation Loss: 0.00052487
Epoch [62/300], Train Loss: 0.000529
Validation Loss: 0.00053756
Epoch [63/300], Train Loss: 0.000527
Validation Loss: 0.00051892
Epoch [64/300], Train Loss: 0.000522
Validation Loss: 0.00053289
Epoch [65/300], Train Loss: 0.000523
Validation Loss: 0.00051780
Epoch [66/300], Train Loss: 0.000518
Validation Loss: 0.00052126
Epoch [67/300], Train Loss: 0.000517
Validation Loss: 0.00050642
Epoch [68/300], Train Loss: 0.000513
Validation Loss: 0.00050439
Epoch [69/300], Train Loss: 0.000509
Validation Loss: 0.00050128
Epoch [70/300], Train Loss: 0.000508
Validation Loss: 0.00050120
Epoch [71/300], Train Loss: 0.000505
Validation Loss: 0.00050415
Epoch [72/300], Train Loss: 0.000503
Validation Loss: 0.00049289
Epoch [73/300], Train Loss: 0.000500
Validation Loss: 0.00048949
Epoch [74/300], Train Loss: 0.000497
Validation Loss: 0.00049453
Epoch [75/300], Train Loss: 0.000496
Validation Loss: 0.00048693
Epoch [76/300], Train Loss: 0.000492
Validation Loss: 0.00048483
Epoch [77/300], Train Loss: 0.000491
Validation Loss: 0.00047911
Epoch [78/300], Train Loss: 0.000488
Validation Loss: 0.00048066
Epoch [79/300], Train Loss: 0.000487
Validation Loss: 0.00049229
Epoch [80/300], Train Loss: 0.000485
Validation Loss: 0.00047274
Epoch [81/300], Train Loss: 0.000483
Validation Loss: 0.00047066
Epoch [82/300], Train Loss: 0.000483
Validation Loss: 0.00047690
Epoch [83/300], Train Loss: 0.000482
Validation Loss: 0.00046555
Epoch [84/300], Train Loss: 0.000476
Validation Loss: 0.00046607
Epoch [85/300], Train Loss: 0.000473
Validation Loss: 0.00047705
Epoch [86/300], Train Loss: 0.000473
Validation Loss: 0.00046590
Epoch [87/300], Train Loss: 0.000469
Validation Loss: 0.00046113
Epoch [88/300], Train Loss: 0.000467
Validation Loss: 0.00045995
Epoch [89/300], Train Loss: 0.000467
Validation Loss: 0.00045360
Epoch [90/300], Train Loss: 0.000462
Validation Loss: 0.00044886
Epoch [91/300], Train Loss: 0.000458
Validation Loss: 0.00045036
Epoch [92/300], Train Loss: 0.000457
Validation Loss: 0.00044949
Epoch [93/300], Train Loss: 0.000454
Validation Loss: 0.00044403
Epoch [94/300], Train Loss: 0.000451
Validation Loss: 0.00043604
Epoch [95/300], Train Loss: 0.000448
Validation Loss: 0.00043364
Epoch [96/300], Train Loss: 0.000447
Validation Loss: 0.00043243
Epoch [97/300], Train Loss: 0.000443
Validation Loss: 0.00042942
Epoch [98/300], Train Loss: 0.000442
Validation Loss: 0.00042732
Epoch [99/300], Train Loss: 0.000438
Validation Loss: 0.00042327
Epoch [100/300], Train Loss: 0.000438
Validation Loss: 0.00042050
Epoch [101/300], Train Loss: 0.000435
Validation Loss: 0.00044066
Epoch [102/300], Train Loss: 0.000434
Validation Loss: 0.00042341
Epoch [103/300], Train Loss: 0.000431
Validation Loss: 0.00041267
Epoch [104/300], Train Loss: 0.000430
Validation Loss: 0.00042039
Epoch [105/300], Train Loss: 0.000427
Validation Loss: 0.00042296
Epoch [106/300], Train Loss: 0.000425
Validation Loss: 0.00041852
Epoch [107/300], Train Loss: 0.000422
Validation Loss: 0.00040928
Epoch [108/300], Train Loss: 0.000421
Validation Loss: 0.00040518
Epoch [109/300], Train Loss: 0.000422
Validation Loss: 0.00040840
Epoch [110/300], Train Loss: 0.000418
Validation Loss: 0.00040329
Epoch [111/300], Train Loss: 0.000416
Validation Loss: 0.00043805
Epoch [112/300], Train Loss: 0.000415
Validation Loss: 0.00040111
Epoch [113/300], Train Loss: 0.000413
Validation Loss: 0.00040665
Epoch [114/300], Train Loss: 0.000412
Validation Loss: 0.00039839
Epoch [115/300], Train Loss: 0.000410
Validation Loss: 0.00040141
Epoch [116/300], Train Loss: 0.000411
Validation Loss: 0.00039619
Epoch [117/300], Train Loss: 0.000408
Validation Loss: 0.00039588
Epoch [118/300], Train Loss: 0.000406
Validation Loss: 0.00039421
Epoch [119/300], Train Loss: 0.000404
Validation Loss: 0.00038912
Epoch [120/300], Train Loss: 0.000404
Validation Loss: 0.00039041
Epoch [121/300], Train Loss: 0.000403
Validation Loss: 0.00038937
Epoch [122/300], Train Loss: 0.000400
Validation Loss: 0.00038657
Epoch [123/300], Train Loss: 0.000402
Validation Loss: 0.00038990
Epoch [124/300], Train Loss: 0.000400
Validation Loss: 0.00039728
Epoch [125/300], Train Loss: 0.000404
Validation Loss: 0.00039636
Epoch [126/300], Train Loss: 0.000399
Validation Loss: 0.00038266
Epoch [127/300], Train Loss: 0.000398
Validation Loss: 0.00039218
Epoch [128/300], Train Loss: 0.000398
Validation Loss: 0.00038257
Epoch [129/300], Train Loss: 0.000395
Validation Loss: 0.00037931
Epoch [130/300], Train Loss: 0.000394
Validation Loss: 0.00038147
Epoch [131/300], Train Loss: 0.000392
Validation Loss: 0.00038398
Epoch [132/300], Train Loss: 0.000392
Validation Loss: 0.00038482
Epoch [133/300], Train Loss: 0.000393
Validation Loss: 0.00038307
Epoch [134/300], Train Loss: 0.000392
Validation Loss: 0.00039272
Epoch [135/300], Train Loss: 0.000389
Validation Loss: 0.00037570
Epoch [136/300], Train Loss: 0.000389
Validation Loss: 0.00037531
Epoch [137/300], Train Loss: 0.000388
Validation Loss: 0.00037535
Epoch [138/300], Train Loss: 0.000389
Validation Loss: 0.00038002
Epoch [139/300], Train Loss: 0.000387
Validation Loss: 0.00037195
Epoch [140/300], Train Loss: 0.000385
Validation Loss: 0.00038319
Epoch [141/300], Train Loss: 0.000388
Validation Loss: 0.00037644
Epoch [142/300], Train Loss: 0.000386
Validation Loss: 0.00038158
Epoch [143/300], Train Loss: 0.000386
Validation Loss: 0.00037416
Epoch [144/300], Train Loss: 0.000386
Validation Loss: 0.00037158
Epoch [145/300], Train Loss: 0.000383
Validation Loss: 0.00036942
Epoch [146/300], Train Loss: 0.000381
Validation Loss: 0.00036898
Epoch [147/300], Train Loss: 0.000381
Validation Loss: 0.00036723
Epoch [148/300], Train Loss: 0.000381
Validation Loss: 0.00037211
Epoch [149/300], Train Loss: 0.000379
Validation Loss: 0.00036651
Epoch [150/300], Train Loss: 0.000380
Validation Loss: 0.00036659
Epoch [151/300], Train Loss: 0.000379
Validation Loss: 0.00037313
Epoch [152/300], Train Loss: 0.000378
Validation Loss: 0.00036430
Epoch [153/300], Train Loss: 0.000377
Validation Loss: 0.00036381
Epoch [154/300], Train Loss: 0.000376
Validation Loss: 0.00036846
Epoch [155/300], Train Loss: 0.000377
Validation Loss: 0.00037073
Epoch [156/300], Train Loss: 0.000375
Validation Loss: 0.00036274
Epoch [157/300], Train Loss: 0.000375
Validation Loss: 0.00037341
Epoch [158/300], Train Loss: 0.000374
Validation Loss: 0.00036126
Epoch [159/300], Train Loss: 0.000374
Validation Loss: 0.00037124
Epoch [160/300], Train Loss: 0.000374
Validation Loss: 0.00036193
Epoch [161/300], Train Loss: 0.000373
Validation Loss: 0.00036306
Epoch [162/300], Train Loss: 0.000373
Validation Loss: 0.00036079
Epoch [163/300], Train Loss: 0.000371
Validation Loss: 0.00036182
Epoch [164/300], Train Loss: 0.000372
Validation Loss: 0.00035918
Epoch [165/300], Train Loss: 0.000370
Validation Loss: 0.00036503
Epoch [166/300], Train Loss: 0.000369
Validation Loss: 0.00036437
Epoch [167/300], Train Loss: 0.000369
Validation Loss: 0.00035842
Epoch [168/300], Train Loss: 0.000370
Validation Loss: 0.00035562
Epoch [169/300], Train Loss: 0.000368
Validation Loss: 0.00036753
Epoch [170/300], Train Loss: 0.000366
Validation Loss: 0.00035513
Epoch [171/300], Train Loss: 0.000366
Validation Loss: 0.00035808
Epoch [172/300], Train Loss: 0.000365
Validation Loss: 0.00035333
Epoch [173/300], Train Loss: 0.000364
Validation Loss: 0.00035531
Epoch [174/300], Train Loss: 0.000364
Validation Loss: 0.00035554
Epoch [175/300], Train Loss: 0.000365
Validation Loss: 0.00035249
Epoch [176/300], Train Loss: 0.000363
Validation Loss: 0.00035352
Epoch [177/300], Train Loss: 0.000361
Validation Loss: 0.00036107
Epoch [178/300], Train Loss: 0.000363
Validation Loss: 0.00035131
Epoch [179/300], Train Loss: 0.000362
Validation Loss: 0.00035023
Epoch [180/300], Train Loss: 0.000361
Validation Loss: 0.00035000
Epoch [181/300], Train Loss: 0.000359
Validation Loss: 0.00034758
Epoch [182/300], Train Loss: 0.000361
Validation Loss: 0.00034990
Epoch [183/300], Train Loss: 0.000361
Validation Loss: 0.00034791
Epoch [184/300], Train Loss: 0.000360
Validation Loss: 0.00035137
Epoch [185/300], Train Loss: 0.000357
Validation Loss: 0.00034785
Epoch [186/300], Train Loss: 0.000358
Validation Loss: 0.00035049
Epoch [187/300], Train Loss: 0.000358
Validation Loss: 0.00034493
Epoch [188/300], Train Loss: 0.000356
Validation Loss: 0.00034472
Epoch [189/300], Train Loss: 0.000358
Validation Loss: 0.00034434
Epoch [190/300], Train Loss: 0.000355
Validation Loss: 0.00034812
Epoch [191/300], Train Loss: 0.000356
Validation Loss: 0.00034497
Epoch [192/300], Train Loss: 0.000354
Validation Loss: 0.00034275
Epoch [193/300], Train Loss: 0.000353
Validation Loss: 0.00034864
Epoch [194/300], Train Loss: 0.000355
Validation Loss: 0.00034517
Epoch [195/300], Train Loss: 0.000353
Validation Loss: 0.00034224
Epoch [196/300], Train Loss: 0.000352
Validation Loss: 0.00034120
Epoch [197/300], Train Loss: 0.000352
Validation Loss: 0.00034258
Epoch [198/300], Train Loss: 0.000352
Validation Loss: 0.00034168
Epoch [199/300], Train Loss: 0.000350
Validation Loss: 0.00033987
Epoch [200/300], Train Loss: 0.000352
Validation Loss: 0.00033967
Epoch [201/300], Train Loss: 0.000349
Validation Loss: 0.00034344
Epoch [202/300], Train Loss: 0.000351
Validation Loss: 0.00033820
Epoch [203/300], Train Loss: 0.000349
Validation Loss: 0.00034109
Epoch [204/300], Train Loss: 0.000348
Validation Loss: 0.00034559
Epoch [205/300], Train Loss: 0.000348
Validation Loss: 0.00033702
Epoch [206/300], Train Loss: 0.000348
Validation Loss: 0.00033672
Epoch [207/300], Train Loss: 0.000347
Validation Loss: 0.00034253
Epoch [208/300], Train Loss: 0.000346
Validation Loss: 0.00033729
Epoch [209/300], Train Loss: 0.000347
Validation Loss: 0.00033873
Epoch [210/300], Train Loss: 0.000345
Validation Loss: 0.00033388
Epoch [211/300], Train Loss: 0.000346
Validation Loss: 0.00033685
Epoch [212/300], Train Loss: 0.000345
Validation Loss: 0.00033588
Epoch [213/300], Train Loss: 0.000344
Validation Loss: 0.00033395
Epoch [214/300], Train Loss: 0.000343
Validation Loss: 0.00033372
Epoch [215/300], Train Loss: 0.000343
Validation Loss: 0.00033446
Epoch [216/300], Train Loss: 0.000342
Validation Loss: 0.00033733
Epoch [217/300], Train Loss: 0.000343
Validation Loss: 0.00033426
Epoch [218/300], Train Loss: 0.000341
Validation Loss: 0.00033118
Epoch [219/300], Train Loss: 0.000342
Validation Loss: 0.00033206
Epoch [220/300], Train Loss: 0.000341
Validation Loss: 0.00034106
Epoch [221/300], Train Loss: 0.000341
Validation Loss: 0.00033034
Epoch [222/300], Train Loss: 0.000340
Validation Loss: 0.00033282
Epoch [223/300], Train Loss: 0.000341
Validation Loss: 0.00033009
Epoch [224/300], Train Loss: 0.000339
Validation Loss: 0.00032873
Epoch [225/300], Train Loss: 0.000341
Validation Loss: 0.00033020
Epoch [226/300], Train Loss: 0.000339
Validation Loss: 0.00033020
Epoch [227/300], Train Loss: 0.000338
Validation Loss: 0.00033538
Epoch [228/300], Train Loss: 0.000337
Validation Loss: 0.00032829
Epoch [229/300], Train Loss: 0.000337
Validation Loss: 0.00033183
Epoch [230/300], Train Loss: 0.000338
Validation Loss: 0.00032720
Epoch [231/300], Train Loss: 0.000336
Validation Loss: 0.00032726
Epoch [232/300], Train Loss: 0.000336
Validation Loss: 0.00033088
Epoch [233/300], Train Loss: 0.000336
Validation Loss: 0.00033000
Epoch [234/300], Train Loss: 0.000337
Validation Loss: 0.00032729
Epoch [235/300], Train Loss: 0.000335
Validation Loss: 0.00032694
Epoch [236/300], Train Loss: 0.000335
Validation Loss: 0.00034302
Epoch [237/300], Train Loss: 0.000335
Validation Loss: 0.00032359
Epoch [238/300], Train Loss: 0.000335
Validation Loss: 0.00032400
Epoch [239/300], Train Loss: 0.000334
Validation Loss: 0.00033299
Epoch [240/300], Train Loss: 0.000333
Validation Loss: 0.00032349
Epoch [241/300], Train Loss: 0.000333
Validation Loss: 0.00032221
Epoch [242/300], Train Loss: 0.000332
Validation Loss: 0.00032275
Epoch [243/300], Train Loss: 0.000332
Validation Loss: 0.00032572
Epoch [244/300], Train Loss: 0.000331
Validation Loss: 0.00032219
Epoch [245/300], Train Loss: 0.000332
Validation Loss: 0.00032599
Epoch [246/300], Train Loss: 0.000332
Validation Loss: 0.00032182
Epoch [247/300], Train Loss: 0.000330
Validation Loss: 0.00032035
Epoch [248/300], Train Loss: 0.000330
Validation Loss: 0.00032001
Epoch [249/300], Train Loss: 0.000331
Validation Loss: 0.00032857
Epoch [250/300], Train Loss: 0.000329
Validation Loss: 0.00032049
Epoch [251/300], Train Loss: 0.000330
Validation Loss: 0.00031920
Epoch [252/300], Train Loss: 0.000328
Validation Loss: 0.00031848
Epoch [253/300], Train Loss: 0.000328
Validation Loss: 0.00031967
Epoch [254/300], Train Loss: 0.000328
Validation Loss: 0.00031895
Epoch [255/300], Train Loss: 0.000328
Validation Loss: 0.00032020
Epoch [256/300], Train Loss: 0.000326
Validation Loss: 0.00031763
Epoch [257/300], Train Loss: 0.000327
Validation Loss: 0.00031678
Epoch [258/300], Train Loss: 0.000327
Validation Loss: 0.00032402
Epoch [259/300], Train Loss: 0.000326
Validation Loss: 0.00031674
Epoch [260/300], Train Loss: 0.000326
Validation Loss: 0.00031831
Epoch [261/300], Train Loss: 0.000326
Validation Loss: 0.00031700
Epoch [262/300], Train Loss: 0.000324
Validation Loss: 0.00031531
Epoch [263/300], Train Loss: 0.000325
Validation Loss: 0.00031490
Epoch [264/300], Train Loss: 0.000326
Validation Loss: 0.00032037
Epoch [265/300], Train Loss: 0.000324
Validation Loss: 0.00031604
Epoch [266/300], Train Loss: 0.000324
Validation Loss: 0.00031544
Epoch [267/300], Train Loss: 0.000324
Validation Loss: 0.00031782
Epoch [268/300], Train Loss: 0.000323
Validation Loss: 0.00031338
Epoch [269/300], Train Loss: 0.000322
Validation Loss: 0.00031692
Epoch [270/300], Train Loss: 0.000322
Validation Loss: 0.00031516
Epoch [271/300], Train Loss: 0.000323
Validation Loss: 0.00031291
Epoch [272/300], Train Loss: 0.000323
Validation Loss: 0.00031357
Epoch [273/300], Train Loss: 0.000322
Validation Loss: 0.00031225
Epoch [274/300], Train Loss: 0.000322
Validation Loss: 0.00031534
Epoch [275/300], Train Loss: 0.000321
Validation Loss: 0.00031348
Epoch [276/300], Train Loss: 0.000320
Validation Loss: 0.00031422
Epoch [277/300], Train Loss: 0.000321
Validation Loss: 0.00031315
Epoch [278/300], Train Loss: 0.000320
Validation Loss: 0.00031167
Epoch [279/300], Train Loss: 0.000319
Validation Loss: 0.00031186
Epoch [280/300], Train Loss: 0.000320
Validation Loss: 0.00030933
Epoch [281/300], Train Loss: 0.000320
Validation Loss: 0.00030968
Epoch [282/300], Train Loss: 0.000319
Validation Loss: 0.00031038
Epoch [283/300], Train Loss: 0.000318
Validation Loss: 0.00031102
Epoch [284/300], Train Loss: 0.000319
Validation Loss: 0.00030887
Epoch [285/300], Train Loss: 0.000318
Validation Loss: 0.00031047
Epoch [286/300], Train Loss: 0.000317
Validation Loss: 0.00030862
Epoch [287/300], Train Loss: 0.000318
Validation Loss: 0.00031235
Epoch [288/300], Train Loss: 0.000318
Validation Loss: 0.00030871
Epoch [289/300], Train Loss: 0.000317
Validation Loss: 0.00030746
Epoch [290/300], Train Loss: 0.000318
Validation Loss: 0.00030736
Epoch [291/300], Train Loss: 0.000317
Validation Loss: 0.00030846
Epoch [292/300], Train Loss: 0.000317
Validation Loss: 0.00031137
Epoch [293/300], Train Loss: 0.000318
Validation Loss: 0.00030752
Epoch [294/300], Train Loss: 0.000317
Validation Loss: 0.00030718
Epoch [295/300], Train Loss: 0.000316
Validation Loss: 0.00030854
Epoch [296/300], Train Loss: 0.000316
Validation Loss: 0.00030780
Epoch [297/300], Train Loss: 0.000316
Validation Loss: 0.00030948
Epoch [298/300], Train Loss: 0.000316
Validation Loss: 0.00030993
Epoch [299/300], Train Loss: 0.000315
Validation Loss: 0.00030713
Epoch [300/300], Train Loss: 0.000316
Validation Loss: 0.00030637

Evaluating model for: Fridge
Run 5/72 completed in 8498.19 seconds with: {'MAE': np.float32(22.431704), 'MSE': np.float32(989.8543), 'RMSE': np.float32(31.46195), 'SAE': np.float32(0.018502418), 'NDE': np.float32(0.54123497)}

Run 6/72: hidden=128, seq_len=120, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.001197
Validation Loss: 0.00074392
Epoch [2/300], Train Loss: 0.000736
Validation Loss: 0.00073134
Epoch [3/300], Train Loss: 0.000720
Validation Loss: 0.00071525
Epoch [4/300], Train Loss: 0.000702
Validation Loss: 0.00069700
Epoch [5/300], Train Loss: 0.000682
Validation Loss: 0.00067459
Epoch [6/300], Train Loss: 0.000667
Validation Loss: 0.00066565
Epoch [7/300], Train Loss: 0.000654
Validation Loss: 0.00065356
Epoch [8/300], Train Loss: 0.000643
Validation Loss: 0.00064018
Epoch [9/300], Train Loss: 0.000637
Validation Loss: 0.00063426
Epoch [10/300], Train Loss: 0.000633
Validation Loss: 0.00063071
Epoch [11/300], Train Loss: 0.000629
Validation Loss: 0.00062541
Epoch [12/300], Train Loss: 0.000625
Validation Loss: 0.00062220
Epoch [13/300], Train Loss: 0.000622
Validation Loss: 0.00062041
Epoch [14/300], Train Loss: 0.000620
Validation Loss: 0.00062297
Epoch [15/300], Train Loss: 0.000618
Validation Loss: 0.00062005
Epoch [16/300], Train Loss: 0.000616
Validation Loss: 0.00061667
Epoch [17/300], Train Loss: 0.000614
Validation Loss: 0.00061295
Epoch [18/300], Train Loss: 0.000613
Validation Loss: 0.00061449
Epoch [19/300], Train Loss: 0.000612
Validation Loss: 0.00061368
Epoch [20/300], Train Loss: 0.000612
Validation Loss: 0.00061152
Epoch [21/300], Train Loss: 0.000610
Validation Loss: 0.00061156
Epoch [22/300], Train Loss: 0.000609
Validation Loss: 0.00060655
Epoch [23/300], Train Loss: 0.000608
Validation Loss: 0.00060778
Epoch [24/300], Train Loss: 0.000606
Validation Loss: 0.00060638
Epoch [25/300], Train Loss: 0.000607
Validation Loss: 0.00060658
Epoch [26/300], Train Loss: 0.000605
Validation Loss: 0.00060421
Epoch [27/300], Train Loss: 0.000603
Validation Loss: 0.00060231
Epoch [28/300], Train Loss: 0.000602
Validation Loss: 0.00061623
Epoch [29/300], Train Loss: 0.000600
Validation Loss: 0.00059999
Epoch [30/300], Train Loss: 0.000599
Validation Loss: 0.00059757
Epoch [31/300], Train Loss: 0.000597
Validation Loss: 0.00059434
Epoch [32/300], Train Loss: 0.000595
Validation Loss: 0.00059189
Epoch [33/300], Train Loss: 0.000592
Validation Loss: 0.00059243
Epoch [34/300], Train Loss: 0.000590
Validation Loss: 0.00058588
Epoch [35/300], Train Loss: 0.000588
Validation Loss: 0.00058367
Epoch [36/300], Train Loss: 0.000585
Validation Loss: 0.00057764
Epoch [37/300], Train Loss: 0.000581
Validation Loss: 0.00057557
Epoch [38/300], Train Loss: 0.000577
Validation Loss: 0.00057023
Epoch [39/300], Train Loss: 0.000574
Validation Loss: 0.00056225
Epoch [40/300], Train Loss: 0.000569
Validation Loss: 0.00056121
Epoch [41/300], Train Loss: 0.000566
Validation Loss: 0.00056016
Epoch [42/300], Train Loss: 0.000561
Validation Loss: 0.00054839
Epoch [43/300], Train Loss: 0.000555
Validation Loss: 0.00054559
Epoch [44/300], Train Loss: 0.000553
Validation Loss: 0.00055820
Epoch [45/300], Train Loss: 0.000547
Validation Loss: 0.00053404
Epoch [46/300], Train Loss: 0.000542
Validation Loss: 0.00053073
Epoch [47/300], Train Loss: 0.000539
Validation Loss: 0.00052615
Epoch [48/300], Train Loss: 0.000534
Validation Loss: 0.00052095
Epoch [49/300], Train Loss: 0.000530
Validation Loss: 0.00051801
Epoch [50/300], Train Loss: 0.000526
Validation Loss: 0.00051302
Epoch [51/300], Train Loss: 0.000521
Validation Loss: 0.00051947
Epoch [52/300], Train Loss: 0.000518
Validation Loss: 0.00050528
Epoch [53/300], Train Loss: 0.000517
Validation Loss: 0.00049974
Epoch [54/300], Train Loss: 0.000514
Validation Loss: 0.00050366
Epoch [55/300], Train Loss: 0.000513
Validation Loss: 0.00049547
Epoch [56/300], Train Loss: 0.000508
Validation Loss: 0.00049306
Epoch [57/300], Train Loss: 0.000506
Validation Loss: 0.00048982
Epoch [58/300], Train Loss: 0.000501
Validation Loss: 0.00049301
Epoch [59/300], Train Loss: 0.000498
Validation Loss: 0.00048789
Epoch [60/300], Train Loss: 0.000496
Validation Loss: 0.00047736
Epoch [61/300], Train Loss: 0.000494
Validation Loss: 0.00048038
Epoch [62/300], Train Loss: 0.000490
Validation Loss: 0.00047881
Epoch [63/300], Train Loss: 0.000488
Validation Loss: 0.00047037
Epoch [64/300], Train Loss: 0.000483
Validation Loss: 0.00047720
Epoch [65/300], Train Loss: 0.000482
Validation Loss: 0.00046583
Epoch [66/300], Train Loss: 0.000479
Validation Loss: 0.00047268
Epoch [67/300], Train Loss: 0.000480
Validation Loss: 0.00046062
Epoch [68/300], Train Loss: 0.000473
Validation Loss: 0.00045882
Epoch [69/300], Train Loss: 0.000471
Validation Loss: 0.00046169
Epoch [70/300], Train Loss: 0.000468
Validation Loss: 0.00045045
Epoch [71/300], Train Loss: 0.000465
Validation Loss: 0.00045987
Epoch [72/300], Train Loss: 0.000464
Validation Loss: 0.00044074
Epoch [73/300], Train Loss: 0.000461
Validation Loss: 0.00045321
Epoch [74/300], Train Loss: 0.000461
Validation Loss: 0.00044085
Epoch [75/300], Train Loss: 0.000455
Validation Loss: 0.00043487
Epoch [76/300], Train Loss: 0.000451
Validation Loss: 0.00043234
Epoch [77/300], Train Loss: 0.000448
Validation Loss: 0.00042934
Epoch [78/300], Train Loss: 0.000446
Validation Loss: 0.00042616
Epoch [79/300], Train Loss: 0.000445
Validation Loss: 0.00042540
Epoch [80/300], Train Loss: 0.000445
Validation Loss: 0.00042480
Epoch [81/300], Train Loss: 0.000442
Validation Loss: 0.00041914
Epoch [82/300], Train Loss: 0.000442
Validation Loss: 0.00041774
Epoch [83/300], Train Loss: 0.000439
Validation Loss: 0.00041940
Epoch [84/300], Train Loss: 0.000438
Validation Loss: 0.00042466
Epoch [85/300], Train Loss: 0.000440
Validation Loss: 0.00043210
Epoch [86/300], Train Loss: 0.000437
Validation Loss: 0.00042109
Epoch [87/300], Train Loss: 0.000434
Validation Loss: 0.00041726
Epoch [88/300], Train Loss: 0.000437
Validation Loss: 0.00041439
Epoch [89/300], Train Loss: 0.000433
Validation Loss: 0.00041174
Epoch [90/300], Train Loss: 0.000433
Validation Loss: 0.00044058
Epoch [91/300], Train Loss: 0.000432
Validation Loss: 0.00040874
Epoch [92/300], Train Loss: 0.000430
Validation Loss: 0.00041149
Epoch [93/300], Train Loss: 0.000428
Validation Loss: 0.00040698
Epoch [94/300], Train Loss: 0.000427
Validation Loss: 0.00040710
Epoch [95/300], Train Loss: 0.000428
Validation Loss: 0.00041003
Epoch [96/300], Train Loss: 0.000429
Validation Loss: 0.00041853
Epoch [97/300], Train Loss: 0.000426
Validation Loss: 0.00040499
Epoch [98/300], Train Loss: 0.000425
Validation Loss: 0.00040391
Epoch [99/300], Train Loss: 0.000423
Validation Loss: 0.00040130
Epoch [100/300], Train Loss: 0.000426
Validation Loss: 0.00040543
Epoch [101/300], Train Loss: 0.000425
Validation Loss: 0.00040540
Epoch [102/300], Train Loss: 0.000423
Validation Loss: 0.00040287
Epoch [103/300], Train Loss: 0.000420
Validation Loss: 0.00040510
Epoch [104/300], Train Loss: 0.000420
Validation Loss: 0.00039840
Epoch [105/300], Train Loss: 0.000418
Validation Loss: 0.00039712
Epoch [106/300], Train Loss: 0.000419
Validation Loss: 0.00041518
Epoch [107/300], Train Loss: 0.000418
Validation Loss: 0.00039867
Epoch [108/300], Train Loss: 0.000417
Validation Loss: 0.00039540
Epoch [109/300], Train Loss: 0.000417
Validation Loss: 0.00039550
Epoch [110/300], Train Loss: 0.000416
Validation Loss: 0.00039462
Epoch [111/300], Train Loss: 0.000415
Validation Loss: 0.00040948
Epoch [112/300], Train Loss: 0.000414
Validation Loss: 0.00039673
Epoch [113/300], Train Loss: 0.000413
Validation Loss: 0.00039859
Epoch [114/300], Train Loss: 0.000414
Validation Loss: 0.00039803
Epoch [115/300], Train Loss: 0.000414
Validation Loss: 0.00040700
Epoch [116/300], Train Loss: 0.000414
Validation Loss: 0.00039201
Epoch [117/300], Train Loss: 0.000412
Validation Loss: 0.00040308
Epoch [118/300], Train Loss: 0.000411
Validation Loss: 0.00039194
Epoch [119/300], Train Loss: 0.000410
Validation Loss: 0.00038893
Epoch [120/300], Train Loss: 0.000410
Validation Loss: 0.00039424
Epoch [121/300], Train Loss: 0.000409
Validation Loss: 0.00039043
Epoch [122/300], Train Loss: 0.000408
Validation Loss: 0.00038813
Epoch [123/300], Train Loss: 0.000408
Validation Loss: 0.00039411
Epoch [124/300], Train Loss: 0.000407
Validation Loss: 0.00039133
Epoch [125/300], Train Loss: 0.000407
Validation Loss: 0.00038842
Epoch [126/300], Train Loss: 0.000406
Validation Loss: 0.00039436
Epoch [127/300], Train Loss: 0.000405
Validation Loss: 0.00039825
Epoch [128/300], Train Loss: 0.000405
Validation Loss: 0.00038985
Epoch [129/300], Train Loss: 0.000405
Validation Loss: 0.00038717
Epoch [130/300], Train Loss: 0.000403
Validation Loss: 0.00038309
Epoch [131/300], Train Loss: 0.000402
Validation Loss: 0.00038819
Epoch [132/300], Train Loss: 0.000401
Validation Loss: 0.00038978
Epoch [133/300], Train Loss: 0.000401
Validation Loss: 0.00038520
Epoch [134/300], Train Loss: 0.000402
Validation Loss: 0.00038184
Epoch [135/300], Train Loss: 0.000400
Validation Loss: 0.00038503
Epoch [136/300], Train Loss: 0.000400
Validation Loss: 0.00038118
Epoch [137/300], Train Loss: 0.000400
Validation Loss: 0.00038487
Epoch [138/300], Train Loss: 0.000400
Validation Loss: 0.00038314
Epoch [139/300], Train Loss: 0.000397
Validation Loss: 0.00038393
Epoch [140/300], Train Loss: 0.000397
Validation Loss: 0.00037944
Epoch [141/300], Train Loss: 0.000399
Validation Loss: 0.00038375
Epoch [142/300], Train Loss: 0.000397
Validation Loss: 0.00038549
Epoch [143/300], Train Loss: 0.000397
Validation Loss: 0.00038031
Epoch [144/300], Train Loss: 0.000396
Validation Loss: 0.00038250
Epoch [145/300], Train Loss: 0.000395
Validation Loss: 0.00038418
Epoch [146/300], Train Loss: 0.000395
Validation Loss: 0.00037730
Epoch [147/300], Train Loss: 0.000395
Validation Loss: 0.00038421
Epoch [148/300], Train Loss: 0.000393
Validation Loss: 0.00037893
Epoch [149/300], Train Loss: 0.000392
Validation Loss: 0.00037547
Epoch [150/300], Train Loss: 0.000393
Validation Loss: 0.00037504
Epoch [151/300], Train Loss: 0.000392
Validation Loss: 0.00037435
Epoch [152/300], Train Loss: 0.000392
Validation Loss: 0.00037558
Epoch [153/300], Train Loss: 0.000391
Validation Loss: 0.00037283
Epoch [154/300], Train Loss: 0.000390
Validation Loss: 0.00037153
Epoch [155/300], Train Loss: 0.000390
Validation Loss: 0.00037746
Epoch [156/300], Train Loss: 0.000389
Validation Loss: 0.00037145
Epoch [157/300], Train Loss: 0.000388
Validation Loss: 0.00037115
Epoch [158/300], Train Loss: 0.000388
Validation Loss: 0.00037077
Epoch [159/300], Train Loss: 0.000387
Validation Loss: 0.00037479
Epoch [160/300], Train Loss: 0.000387
Validation Loss: 0.00036739
Epoch [161/300], Train Loss: 0.000385
Validation Loss: 0.00036965
Epoch [162/300], Train Loss: 0.000385
Validation Loss: 0.00037836
Epoch [163/300], Train Loss: 0.000385
Validation Loss: 0.00036913
Epoch [164/300], Train Loss: 0.000386
Validation Loss: 0.00037026
Epoch [165/300], Train Loss: 0.000385
Validation Loss: 0.00037355
Epoch [166/300], Train Loss: 0.000384
Validation Loss: 0.00036685
Epoch [167/300], Train Loss: 0.000384
Validation Loss: 0.00036841
Epoch [168/300], Train Loss: 0.000383
Validation Loss: 0.00036398
Epoch [169/300], Train Loss: 0.000382
Validation Loss: 0.00036970
Epoch [170/300], Train Loss: 0.000382
Validation Loss: 0.00036366
Epoch [171/300], Train Loss: 0.000381
Validation Loss: 0.00037008
Epoch [172/300], Train Loss: 0.000380
Validation Loss: 0.00036381
Epoch [173/300], Train Loss: 0.000381
Validation Loss: 0.00036465
Epoch [174/300], Train Loss: 0.000379
Validation Loss: 0.00036650
Epoch [175/300], Train Loss: 0.000380
Validation Loss: 0.00036258
Epoch [176/300], Train Loss: 0.000378
Validation Loss: 0.00036027
Epoch [177/300], Train Loss: 0.000377
Validation Loss: 0.00036419
Epoch [178/300], Train Loss: 0.000377
Validation Loss: 0.00036103
Epoch [179/300], Train Loss: 0.000377
Validation Loss: 0.00036157
Epoch [180/300], Train Loss: 0.000376
Validation Loss: 0.00036036
Epoch [181/300], Train Loss: 0.000375
Validation Loss: 0.00036023
Epoch [182/300], Train Loss: 0.000375
Validation Loss: 0.00035987
Epoch [183/300], Train Loss: 0.000375
Validation Loss: 0.00036733
Epoch [184/300], Train Loss: 0.000375
Validation Loss: 0.00035734
Epoch [185/300], Train Loss: 0.000373
Validation Loss: 0.00036084
Epoch [186/300], Train Loss: 0.000373
Validation Loss: 0.00036352
Epoch [187/300], Train Loss: 0.000372
Validation Loss: 0.00035441
Epoch [188/300], Train Loss: 0.000372
Validation Loss: 0.00035549
Epoch [189/300], Train Loss: 0.000372
Validation Loss: 0.00035810
Epoch [190/300], Train Loss: 0.000371
Validation Loss: 0.00035777
Epoch [191/300], Train Loss: 0.000370
Validation Loss: 0.00035475
Epoch [192/300], Train Loss: 0.000369
Validation Loss: 0.00035343
Epoch [193/300], Train Loss: 0.000369
Validation Loss: 0.00035265
Epoch [194/300], Train Loss: 0.000370
Validation Loss: 0.00035333
Epoch [195/300], Train Loss: 0.000368
Validation Loss: 0.00035206
Epoch [196/300], Train Loss: 0.000368
Validation Loss: 0.00035353
Epoch [197/300], Train Loss: 0.000367
Validation Loss: 0.00035886
Epoch [198/300], Train Loss: 0.000366
Validation Loss: 0.00035007
Epoch [199/300], Train Loss: 0.000367
Validation Loss: 0.00035291
Epoch [200/300], Train Loss: 0.000366
Validation Loss: 0.00034768
Epoch [201/300], Train Loss: 0.000365
Validation Loss: 0.00034769
Epoch [202/300], Train Loss: 0.000364
Validation Loss: 0.00034911
Epoch [203/300], Train Loss: 0.000363
Validation Loss: 0.00034962
Epoch [204/300], Train Loss: 0.000364
Validation Loss: 0.00034692
Epoch [205/300], Train Loss: 0.000363
Validation Loss: 0.00034569
Epoch [206/300], Train Loss: 0.000362
Validation Loss: 0.00034598
Epoch [207/300], Train Loss: 0.000362
Validation Loss: 0.00034745
Epoch [208/300], Train Loss: 0.000362
Validation Loss: 0.00034619
Epoch [209/300], Train Loss: 0.000360
Validation Loss: 0.00034477
Epoch [210/300], Train Loss: 0.000360
Validation Loss: 0.00034355
Epoch [211/300], Train Loss: 0.000360
Validation Loss: 0.00034376
Epoch [212/300], Train Loss: 0.000358
Validation Loss: 0.00034395
Epoch [213/300], Train Loss: 0.000358
Validation Loss: 0.00034232
Epoch [214/300], Train Loss: 0.000358
Validation Loss: 0.00034135
Epoch [215/300], Train Loss: 0.000357
Validation Loss: 0.00034079
Epoch [216/300], Train Loss: 0.000356
Validation Loss: 0.00033918
Epoch [217/300], Train Loss: 0.000356
Validation Loss: 0.00034032
Epoch [218/300], Train Loss: 0.000356
Validation Loss: 0.00033810
Epoch [219/300], Train Loss: 0.000354
Validation Loss: 0.00033877
Epoch [220/300], Train Loss: 0.000355
Validation Loss: 0.00033691
Epoch [221/300], Train Loss: 0.000354
Validation Loss: 0.00033933
Epoch [222/300], Train Loss: 0.000353
Validation Loss: 0.00033825
Epoch [223/300], Train Loss: 0.000352
Validation Loss: 0.00033457
Epoch [224/300], Train Loss: 0.000352
Validation Loss: 0.00033406
Epoch [225/300], Train Loss: 0.000353
Validation Loss: 0.00033945
Epoch [226/300], Train Loss: 0.000351
Validation Loss: 0.00033223
Epoch [227/300], Train Loss: 0.000350
Validation Loss: 0.00033974
Epoch [228/300], Train Loss: 0.000349
Validation Loss: 0.00033394
Epoch [229/300], Train Loss: 0.000349
Validation Loss: 0.00033026
Epoch [230/300], Train Loss: 0.000348
Validation Loss: 0.00033024
Epoch [231/300], Train Loss: 0.000347
Validation Loss: 0.00033020
Epoch [232/300], Train Loss: 0.000346
Validation Loss: 0.00033070
Epoch [233/300], Train Loss: 0.000347
Validation Loss: 0.00032853
Epoch [234/300], Train Loss: 0.000345
Validation Loss: 0.00033030
Epoch [235/300], Train Loss: 0.000344
Validation Loss: 0.00032903
Epoch [236/300], Train Loss: 0.000344
Validation Loss: 0.00032722
Epoch [237/300], Train Loss: 0.000344
Validation Loss: 0.00032894
Epoch [238/300], Train Loss: 0.000344
Validation Loss: 0.00033107
Epoch [239/300], Train Loss: 0.000343
Validation Loss: 0.00032353
Epoch [240/300], Train Loss: 0.000341
Validation Loss: 0.00032433
Epoch [241/300], Train Loss: 0.000341
Validation Loss: 0.00032312
Epoch [242/300], Train Loss: 0.000340
Validation Loss: 0.00032353
Epoch [243/300], Train Loss: 0.000339
Validation Loss: 0.00032256
Epoch [244/300], Train Loss: 0.000339
Validation Loss: 0.00032139
Epoch [245/300], Train Loss: 0.000338
Validation Loss: 0.00032089
Epoch [246/300], Train Loss: 0.000339
Validation Loss: 0.00032078
Epoch [247/300], Train Loss: 0.000338
Validation Loss: 0.00032030
Epoch [248/300], Train Loss: 0.000337
Validation Loss: 0.00032509
Epoch [249/300], Train Loss: 0.000338
Validation Loss: 0.00031967
Epoch [250/300], Train Loss: 0.000337
Validation Loss: 0.00032521
Epoch [251/300], Train Loss: 0.000335
Validation Loss: 0.00031728
Epoch [252/300], Train Loss: 0.000334
Validation Loss: 0.00031737
Epoch [253/300], Train Loss: 0.000334
Validation Loss: 0.00031682
Epoch [254/300], Train Loss: 0.000333
Validation Loss: 0.00031445
Epoch [255/300], Train Loss: 0.000332
Validation Loss: 0.00031379
Epoch [256/300], Train Loss: 0.000332
Validation Loss: 0.00031635
Epoch [257/300], Train Loss: 0.000333
Validation Loss: 0.00031361
Epoch [258/300], Train Loss: 0.000330
Validation Loss: 0.00031273
Epoch [259/300], Train Loss: 0.000330
Validation Loss: 0.00031381
Epoch [260/300], Train Loss: 0.000329
Validation Loss: 0.00031244
Epoch [261/300], Train Loss: 0.000330
Validation Loss: 0.00031384
Epoch [262/300], Train Loss: 0.000328
Validation Loss: 0.00031092
Epoch [263/300], Train Loss: 0.000328
Validation Loss: 0.00031157
Epoch [264/300], Train Loss: 0.000328
Validation Loss: 0.00030820
Epoch [265/300], Train Loss: 0.000327
Validation Loss: 0.00030874
Epoch [266/300], Train Loss: 0.000326
Validation Loss: 0.00030887
Epoch [267/300], Train Loss: 0.000325
Validation Loss: 0.00030946
Epoch [268/300], Train Loss: 0.000324
Validation Loss: 0.00030630
Epoch [269/300], Train Loss: 0.000324
Validation Loss: 0.00030527
Epoch [270/300], Train Loss: 0.000325
Validation Loss: 0.00030772
Epoch [271/300], Train Loss: 0.000323
Validation Loss: 0.00030556
Epoch [272/300], Train Loss: 0.000324
Validation Loss: 0.00030366
Epoch [273/300], Train Loss: 0.000323
Validation Loss: 0.00030328
Epoch [274/300], Train Loss: 0.000322
Validation Loss: 0.00030320
Epoch [275/300], Train Loss: 0.000321
Validation Loss: 0.00030469
Epoch [276/300], Train Loss: 0.000320
Validation Loss: 0.00030495
Epoch [277/300], Train Loss: 0.000321
Validation Loss: 0.00030255
Epoch [278/300], Train Loss: 0.000319
Validation Loss: 0.00030187
Epoch [279/300], Train Loss: 0.000321
Validation Loss: 0.00030063
Epoch [280/300], Train Loss: 0.000319
Validation Loss: 0.00030047
Epoch [281/300], Train Loss: 0.000318
Validation Loss: 0.00030015
Epoch [282/300], Train Loss: 0.000318
Validation Loss: 0.00030000
Epoch [283/300], Train Loss: 0.000317
Validation Loss: 0.00030214
Epoch [284/300], Train Loss: 0.000318
Validation Loss: 0.00030027
Epoch [285/300], Train Loss: 0.000316
Validation Loss: 0.00029778
Epoch [286/300], Train Loss: 0.000316
Validation Loss: 0.00029944
Epoch [287/300], Train Loss: 0.000316
Validation Loss: 0.00030008
Epoch [288/300], Train Loss: 0.000315
Validation Loss: 0.00029618
Epoch [289/300], Train Loss: 0.000314
Validation Loss: 0.00029640
Epoch [290/300], Train Loss: 0.000315
Validation Loss: 0.00029603
Epoch [291/300], Train Loss: 0.000314
Validation Loss: 0.00029611
Epoch [292/300], Train Loss: 0.000313
Validation Loss: 0.00029510
Epoch [293/300], Train Loss: 0.000314
Validation Loss: 0.00029489
Epoch [294/300], Train Loss: 0.000314
Validation Loss: 0.00029562
Epoch [295/300], Train Loss: 0.000312
Validation Loss: 0.00029435
Epoch [296/300], Train Loss: 0.000312
Validation Loss: 0.00029404
Epoch [297/300], Train Loss: 0.000311
Validation Loss: 0.00029520
Epoch [298/300], Train Loss: 0.000311
Validation Loss: 0.00029572
Epoch [299/300], Train Loss: 0.000310
Validation Loss: 0.00029211
Epoch [300/300], Train Loss: 0.000310
Validation Loss: 0.00029160

Evaluating model for: Fridge
Run 6/72 completed in 8595.07 seconds with: {'MAE': np.float32(21.526926), 'MSE': np.float32(971.11127), 'RMSE': np.float32(31.162659), 'SAE': np.float32(0.009544104), 'NDE': np.float32(0.53608626)}

Run 7/72: hidden=128, seq_len=120, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000675
Validation Loss: 0.00068180
Epoch [2/300], Train Loss: 0.000661
Validation Loss: 0.00067860
Epoch [3/300], Train Loss: 0.000649
Validation Loss: 0.00064438
Epoch [4/300], Train Loss: 0.000624
Validation Loss: 0.00062578
Epoch [5/300], Train Loss: 0.000611
Validation Loss: 0.00062579
Epoch [6/300], Train Loss: 0.000606
Validation Loss: 0.00061458
Epoch [7/300], Train Loss: 0.000605
Validation Loss: 0.00061572
Epoch [8/300], Train Loss: 0.000604
Validation Loss: 0.00060955
Epoch [9/300], Train Loss: 0.000602
Validation Loss: 0.00061369
Epoch [10/300], Train Loss: 0.000599
Validation Loss: 0.00060787
Epoch [11/300], Train Loss: 0.000598
Validation Loss: 0.00060646
Epoch [12/300], Train Loss: 0.000597
Validation Loss: 0.00060362
Epoch [13/300], Train Loss: 0.000596
Validation Loss: 0.00060837
Epoch [14/300], Train Loss: 0.000594
Validation Loss: 0.00060372
Epoch [15/300], Train Loss: 0.000594
Validation Loss: 0.00060299
Epoch [16/300], Train Loss: 0.000591
Validation Loss: 0.00059621
Epoch [17/300], Train Loss: 0.000590
Validation Loss: 0.00059298
Epoch [18/300], Train Loss: 0.000588
Validation Loss: 0.00059203
Epoch [19/300], Train Loss: 0.000585
Validation Loss: 0.00058435
Epoch [20/300], Train Loss: 0.000582
Validation Loss: 0.00058350
Epoch [21/300], Train Loss: 0.000579
Validation Loss: 0.00058680
Epoch [22/300], Train Loss: 0.000575
Validation Loss: 0.00057370
Epoch [23/300], Train Loss: 0.000574
Validation Loss: 0.00057518
Epoch [24/300], Train Loss: 0.000570
Validation Loss: 0.00057776
Epoch [25/300], Train Loss: 0.000569
Validation Loss: 0.00057402
Epoch [26/300], Train Loss: 0.000566
Validation Loss: 0.00056485
Epoch [27/300], Train Loss: 0.000561
Validation Loss: 0.00055824
Epoch [28/300], Train Loss: 0.000556
Validation Loss: 0.00054802
Epoch [29/300], Train Loss: 0.000548
Validation Loss: 0.00054274
Epoch [30/300], Train Loss: 0.000543
Validation Loss: 0.00052425
Epoch [31/300], Train Loss: 0.000535
Validation Loss: 0.00052288
Epoch [32/300], Train Loss: 0.000526
Validation Loss: 0.00050487
Epoch [33/300], Train Loss: 0.000516
Validation Loss: 0.00049767
Epoch [34/300], Train Loss: 0.000503
Validation Loss: 0.00049307
Epoch [35/300], Train Loss: 0.000498
Validation Loss: 0.00047400
Epoch [36/300], Train Loss: 0.000489
Validation Loss: 0.00047696
Epoch [37/300], Train Loss: 0.000488
Validation Loss: 0.00047288
Epoch [38/300], Train Loss: 0.000486
Validation Loss: 0.00047301
Epoch [39/300], Train Loss: 0.000474
Validation Loss: 0.00046370
Epoch [40/300], Train Loss: 0.000472
Validation Loss: 0.00048135
Epoch [41/300], Train Loss: 0.000469
Validation Loss: 0.00046442
Epoch [42/300], Train Loss: 0.000466
Validation Loss: 0.00048611
Epoch [43/300], Train Loss: 0.000470
Validation Loss: 0.00045401
Epoch [44/300], Train Loss: 0.000463
Validation Loss: 0.00047206
Epoch [45/300], Train Loss: 0.000463
Validation Loss: 0.00045545
Epoch [46/300], Train Loss: 0.000459
Validation Loss: 0.00045112
Epoch [47/300], Train Loss: 0.000457
Validation Loss: 0.00049589
Epoch [48/300], Train Loss: 0.000454
Validation Loss: 0.00044441
Epoch [49/300], Train Loss: 0.000450
Validation Loss: 0.00044114
Epoch [50/300], Train Loss: 0.000453
Validation Loss: 0.00044233
Epoch [51/300], Train Loss: 0.000449
Validation Loss: 0.00043859
Epoch [52/300], Train Loss: 0.000445
Validation Loss: 0.00043862
Epoch [53/300], Train Loss: 0.000447
Validation Loss: 0.00043654
Epoch [54/300], Train Loss: 0.000443
Validation Loss: 0.00043316
Epoch [55/300], Train Loss: 0.000442
Validation Loss: 0.00043242
Epoch [56/300], Train Loss: 0.000439
Validation Loss: 0.00043153
Epoch [57/300], Train Loss: 0.000442
Validation Loss: 0.00042801
Epoch [58/300], Train Loss: 0.000439
Validation Loss: 0.00046641
Epoch [59/300], Train Loss: 0.000438
Validation Loss: 0.00043913
Epoch [60/300], Train Loss: 0.000435
Validation Loss: 0.00043119
Epoch [61/300], Train Loss: 0.000432
Validation Loss: 0.00042658
Epoch [62/300], Train Loss: 0.000433
Validation Loss: 0.00041906
Epoch [63/300], Train Loss: 0.000429
Validation Loss: 0.00041840
Epoch [64/300], Train Loss: 0.000429
Validation Loss: 0.00041745
Epoch [65/300], Train Loss: 0.000428
Validation Loss: 0.00041869
Epoch [66/300], Train Loss: 0.000429
Validation Loss: 0.00041796
Epoch [67/300], Train Loss: 0.000425
Validation Loss: 0.00042139
Epoch [68/300], Train Loss: 0.000424
Validation Loss: 0.00041173
Epoch [69/300], Train Loss: 0.000424
Validation Loss: 0.00042597
Epoch [70/300], Train Loss: 0.000428
Validation Loss: 0.00041192
Epoch [71/300], Train Loss: 0.000422
Validation Loss: 0.00041246
Epoch [72/300], Train Loss: 0.000421
Validation Loss: 0.00041192
Epoch [73/300], Train Loss: 0.000418
Validation Loss: 0.00042628
Epoch [74/300], Train Loss: 0.000421
Validation Loss: 0.00040658
Epoch [75/300], Train Loss: 0.000418
Validation Loss: 0.00040503
Epoch [76/300], Train Loss: 0.000418
Validation Loss: 0.00040605
Epoch [77/300], Train Loss: 0.000415
Validation Loss: 0.00040309
Epoch [78/300], Train Loss: 0.000413
Validation Loss: 0.00040364
Epoch [79/300], Train Loss: 0.000414
Validation Loss: 0.00041218
Epoch [80/300], Train Loss: 0.000412
Validation Loss: 0.00039878
Epoch [81/300], Train Loss: 0.000412
Validation Loss: 0.00040240
Epoch [82/300], Train Loss: 0.000411
Validation Loss: 0.00040211
Epoch [83/300], Train Loss: 0.000410
Validation Loss: 0.00041762
Epoch [84/300], Train Loss: 0.000412
Validation Loss: 0.00039858
Epoch [85/300], Train Loss: 0.000415
Validation Loss: 0.00042215
Epoch [86/300], Train Loss: 0.000413
Validation Loss: 0.00040654
Epoch [87/300], Train Loss: 0.000409
Validation Loss: 0.00040234
Epoch [88/300], Train Loss: 0.000410
Validation Loss: 0.00041548
Epoch [89/300], Train Loss: 0.000408
Validation Loss: 0.00039624
Epoch [90/300], Train Loss: 0.000407
Validation Loss: 0.00039683
Epoch [91/300], Train Loss: 0.000406
Validation Loss: 0.00039296
Epoch [92/300], Train Loss: 0.000407
Validation Loss: 0.00039562
Epoch [93/300], Train Loss: 0.000406
Validation Loss: 0.00039582
Epoch [94/300], Train Loss: 0.000404
Validation Loss: 0.00039130
Epoch [95/300], Train Loss: 0.000404
Validation Loss: 0.00039759
Epoch [96/300], Train Loss: 0.000404
Validation Loss: 0.00039817
Epoch [97/300], Train Loss: 0.000403
Validation Loss: 0.00039464
Epoch [98/300], Train Loss: 0.000401
Validation Loss: 0.00038773
Epoch [99/300], Train Loss: 0.000401
Validation Loss: 0.00039673
Epoch [100/300], Train Loss: 0.000400
Validation Loss: 0.00038769
Epoch [101/300], Train Loss: 0.000401
Validation Loss: 0.00039695
Epoch [102/300], Train Loss: 0.000400
Validation Loss: 0.00040194
Epoch [103/300], Train Loss: 0.000399
Validation Loss: 0.00038434
Epoch [104/300], Train Loss: 0.000397
Validation Loss: 0.00039028
Epoch [105/300], Train Loss: 0.000397
Validation Loss: 0.00039570
Epoch [106/300], Train Loss: 0.000397
Validation Loss: 0.00042009
Epoch [107/300], Train Loss: 0.000397
Validation Loss: 0.00038422
Epoch [108/300], Train Loss: 0.000395
Validation Loss: 0.00038480
Epoch [109/300], Train Loss: 0.000396
Validation Loss: 0.00039692
Epoch [110/300], Train Loss: 0.000396
Validation Loss: 0.00038763
Epoch [111/300], Train Loss: 0.000393
Validation Loss: 0.00038159
Epoch [112/300], Train Loss: 0.000391
Validation Loss: 0.00037853
Epoch [113/300], Train Loss: 0.000393
Validation Loss: 0.00037966
Epoch [114/300], Train Loss: 0.000392
Validation Loss: 0.00038998
Epoch [115/300], Train Loss: 0.000392
Validation Loss: 0.00038778
Epoch [116/300], Train Loss: 0.000393
Validation Loss: 0.00037780
Epoch [117/300], Train Loss: 0.000390
Validation Loss: 0.00038080
Epoch [118/300], Train Loss: 0.000392
Validation Loss: 0.00037566
Epoch [119/300], Train Loss: 0.000392
Validation Loss: 0.00037777
Epoch [120/300], Train Loss: 0.000389
Validation Loss: 0.00037424
Epoch [121/300], Train Loss: 0.000389
Validation Loss: 0.00037555
Epoch [122/300], Train Loss: 0.000387
Validation Loss: 0.00037677
Epoch [123/300], Train Loss: 0.000388
Validation Loss: 0.00037247
Epoch [124/300], Train Loss: 0.000388
Validation Loss: 0.00037356
Epoch [125/300], Train Loss: 0.000387
Validation Loss: 0.00038733
Epoch [126/300], Train Loss: 0.000386
Validation Loss: 0.00037180
Epoch [127/300], Train Loss: 0.000385
Validation Loss: 0.00037268
Epoch [128/300], Train Loss: 0.000385
Validation Loss: 0.00036994
Epoch [129/300], Train Loss: 0.000384
Validation Loss: 0.00036993
Epoch [130/300], Train Loss: 0.000383
Validation Loss: 0.00037057
Epoch [131/300], Train Loss: 0.000382
Validation Loss: 0.00036898
Epoch [132/300], Train Loss: 0.000382
Validation Loss: 0.00036869
Epoch [133/300], Train Loss: 0.000383
Validation Loss: 0.00037086
Epoch [134/300], Train Loss: 0.000382
Validation Loss: 0.00036775
Epoch [135/300], Train Loss: 0.000384
Validation Loss: 0.00036839
Epoch [136/300], Train Loss: 0.000381
Validation Loss: 0.00036813
Epoch [137/300], Train Loss: 0.000381
Validation Loss: 0.00036778
Epoch [138/300], Train Loss: 0.000380
Validation Loss: 0.00036579
Epoch [139/300], Train Loss: 0.000378
Validation Loss: 0.00036847
Epoch [140/300], Train Loss: 0.000378
Validation Loss: 0.00036848
Epoch [141/300], Train Loss: 0.000379
Validation Loss: 0.00036781
Epoch [142/300], Train Loss: 0.000380
Validation Loss: 0.00038016
Epoch [143/300], Train Loss: 0.000380
Validation Loss: 0.00036862
Epoch [144/300], Train Loss: 0.000379
Validation Loss: 0.00036224
Epoch [145/300], Train Loss: 0.000377
Validation Loss: 0.00037288
Epoch [146/300], Train Loss: 0.000379
Validation Loss: 0.00036253
Epoch [147/300], Train Loss: 0.000376
Validation Loss: 0.00036152
Epoch [148/300], Train Loss: 0.000377
Validation Loss: 0.00036694
Epoch [149/300], Train Loss: 0.000374
Validation Loss: 0.00036052
Epoch [150/300], Train Loss: 0.000376
Validation Loss: 0.00035968
Epoch [151/300], Train Loss: 0.000374
Validation Loss: 0.00036062
Epoch [152/300], Train Loss: 0.000374
Validation Loss: 0.00035913
Epoch [153/300], Train Loss: 0.000374
Validation Loss: 0.00036436
Epoch [154/300], Train Loss: 0.000373
Validation Loss: 0.00035764
Epoch [155/300], Train Loss: 0.000372
Validation Loss: 0.00036131
Epoch [156/300], Train Loss: 0.000374
Validation Loss: 0.00036019
Epoch [157/300], Train Loss: 0.000373
Validation Loss: 0.00036108
Epoch [158/300], Train Loss: 0.000373
Validation Loss: 0.00037203
Epoch [159/300], Train Loss: 0.000373
Validation Loss: 0.00036516
Epoch [160/300], Train Loss: 0.000372
Validation Loss: 0.00035947
Epoch [161/300], Train Loss: 0.000373
Validation Loss: 0.00035702
Epoch [162/300], Train Loss: 0.000370
Validation Loss: 0.00035587
Epoch [163/300], Train Loss: 0.000370
Validation Loss: 0.00035456
Epoch [164/300], Train Loss: 0.000370
Validation Loss: 0.00035452
Epoch [165/300], Train Loss: 0.000369
Validation Loss: 0.00035433
Epoch [166/300], Train Loss: 0.000371
Validation Loss: 0.00035405
Epoch [167/300], Train Loss: 0.000370
Validation Loss: 0.00035454
Epoch [168/300], Train Loss: 0.000367
Validation Loss: 0.00035173
Epoch [169/300], Train Loss: 0.000368
Validation Loss: 0.00035739
Epoch [170/300], Train Loss: 0.000368
Validation Loss: 0.00035399
Epoch [171/300], Train Loss: 0.000365
Validation Loss: 0.00035033
Epoch [172/300], Train Loss: 0.000366
Validation Loss: 0.00035439
Epoch [173/300], Train Loss: 0.000365
Validation Loss: 0.00035150
Epoch [174/300], Train Loss: 0.000366
Validation Loss: 0.00036364
Epoch [175/300], Train Loss: 0.000369
Validation Loss: 0.00035003
Epoch [176/300], Train Loss: 0.000364
Validation Loss: 0.00034996
Epoch [177/300], Train Loss: 0.000363
Validation Loss: 0.00035747
Epoch [178/300], Train Loss: 0.000364
Validation Loss: 0.00035423
Epoch [179/300], Train Loss: 0.000363
Validation Loss: 0.00035656
Epoch [180/300], Train Loss: 0.000364
Validation Loss: 0.00035576
Epoch [181/300], Train Loss: 0.000363
Validation Loss: 0.00034845
Epoch [182/300], Train Loss: 0.000362
Validation Loss: 0.00034709
Epoch [183/300], Train Loss: 0.000362
Validation Loss: 0.00034961
Epoch [184/300], Train Loss: 0.000362
Validation Loss: 0.00034534
Epoch [185/300], Train Loss: 0.000360
Validation Loss: 0.00034760
Epoch [186/300], Train Loss: 0.000361
Validation Loss: 0.00034950
Epoch [187/300], Train Loss: 0.000361
Validation Loss: 0.00034613
Epoch [188/300], Train Loss: 0.000361
Validation Loss: 0.00034747
Epoch [189/300], Train Loss: 0.000359
Validation Loss: 0.00034726
Epoch [190/300], Train Loss: 0.000359
Validation Loss: 0.00034982
Epoch [191/300], Train Loss: 0.000359
Validation Loss: 0.00035033
Epoch [192/300], Train Loss: 0.000357
Validation Loss: 0.00034474
Epoch [193/300], Train Loss: 0.000358
Validation Loss: 0.00034219
Epoch [194/300], Train Loss: 0.000358
Validation Loss: 0.00034587
Epoch [195/300], Train Loss: 0.000356
Validation Loss: 0.00034337
Epoch [196/300], Train Loss: 0.000360
Validation Loss: 0.00034550
Epoch [197/300], Train Loss: 0.000355
Validation Loss: 0.00034406
Epoch [198/300], Train Loss: 0.000356
Validation Loss: 0.00034950
Epoch [199/300], Train Loss: 0.000356
Validation Loss: 0.00034128
Epoch [200/300], Train Loss: 0.000358
Validation Loss: 0.00034057
Epoch [201/300], Train Loss: 0.000355
Validation Loss: 0.00034461
Epoch [202/300], Train Loss: 0.000353
Validation Loss: 0.00033903
Epoch [203/300], Train Loss: 0.000352
Validation Loss: 0.00033877
Epoch [204/300], Train Loss: 0.000352
Validation Loss: 0.00034719
Epoch [205/300], Train Loss: 0.000353
Validation Loss: 0.00033820
Epoch [206/300], Train Loss: 0.000355
Validation Loss: 0.00034084
Epoch [207/300], Train Loss: 0.000354
Validation Loss: 0.00034565
Epoch [208/300], Train Loss: 0.000351
Validation Loss: 0.00033673
Epoch [209/300], Train Loss: 0.000351
Validation Loss: 0.00034244
Epoch [210/300], Train Loss: 0.000351
Validation Loss: 0.00033610
Epoch [211/300], Train Loss: 0.000349
Validation Loss: 0.00034063
Epoch [212/300], Train Loss: 0.000350
Validation Loss: 0.00033359
Epoch [213/300], Train Loss: 0.000349
Validation Loss: 0.00034526
Epoch [214/300], Train Loss: 0.000348
Validation Loss: 0.00033342
Epoch [215/300], Train Loss: 0.000348
Validation Loss: 0.00033298
Epoch [216/300], Train Loss: 0.000347
Validation Loss: 0.00033225
Epoch [217/300], Train Loss: 0.000348
Validation Loss: 0.00033220
Epoch [218/300], Train Loss: 0.000347
Validation Loss: 0.00033201
Epoch [219/300], Train Loss: 0.000348
Validation Loss: 0.00034328
Epoch [220/300], Train Loss: 0.000347
Validation Loss: 0.00033005
Epoch [221/300], Train Loss: 0.000345
Validation Loss: 0.00032979
Epoch [222/300], Train Loss: 0.000344
Validation Loss: 0.00032915
Epoch [223/300], Train Loss: 0.000344
Validation Loss: 0.00032778
Epoch [224/300], Train Loss: 0.000346
Validation Loss: 0.00034192
Epoch [225/300], Train Loss: 0.000345
Validation Loss: 0.00033415
Epoch [226/300], Train Loss: 0.000344
Validation Loss: 0.00032638
Epoch [227/300], Train Loss: 0.000341
Validation Loss: 0.00032614
Epoch [228/300], Train Loss: 0.000340
Validation Loss: 0.00032689
Epoch [229/300], Train Loss: 0.000340
Validation Loss: 0.00032461
Epoch [230/300], Train Loss: 0.000340
Validation Loss: 0.00032404
Epoch [231/300], Train Loss: 0.000340
Validation Loss: 0.00032706
Epoch [232/300], Train Loss: 0.000340
Validation Loss: 0.00032382
Epoch [233/300], Train Loss: 0.000339
Validation Loss: 0.00032233
Epoch [234/300], Train Loss: 0.000338
Validation Loss: 0.00032459
Epoch [235/300], Train Loss: 0.000337
Validation Loss: 0.00032850
Epoch [236/300], Train Loss: 0.000337
Validation Loss: 0.00032215
Epoch [237/300], Train Loss: 0.000336
Validation Loss: 0.00032188
Epoch [238/300], Train Loss: 0.000336
Validation Loss: 0.00031822
Epoch [239/300], Train Loss: 0.000336
Validation Loss: 0.00031887
Epoch [240/300], Train Loss: 0.000335
Validation Loss: 0.00031744
Epoch [241/300], Train Loss: 0.000333
Validation Loss: 0.00031819
Epoch [242/300], Train Loss: 0.000332
Validation Loss: 0.00031584
Epoch [243/300], Train Loss: 0.000332
Validation Loss: 0.00032026
Epoch [244/300], Train Loss: 0.000333
Validation Loss: 0.00031415
Epoch [245/300], Train Loss: 0.000332
Validation Loss: 0.00031455
Epoch [246/300], Train Loss: 0.000331
Validation Loss: 0.00031861
Epoch [247/300], Train Loss: 0.000330
Validation Loss: 0.00031361
Epoch [248/300], Train Loss: 0.000333
Validation Loss: 0.00032615
Epoch [249/300], Train Loss: 0.000331
Validation Loss: 0.00031665
Epoch [250/300], Train Loss: 0.000329
Validation Loss: 0.00031510
Epoch [251/300], Train Loss: 0.000328
Validation Loss: 0.00031116
Epoch [252/300], Train Loss: 0.000327
Validation Loss: 0.00031060
Epoch [253/300], Train Loss: 0.000327
Validation Loss: 0.00031404
Epoch [254/300], Train Loss: 0.000327
Validation Loss: 0.00031042
Epoch [255/300], Train Loss: 0.000337
Validation Loss: 0.00031239
Epoch [256/300], Train Loss: 0.000329
Validation Loss: 0.00031007
Epoch [257/300], Train Loss: 0.000326
Validation Loss: 0.00030775
Epoch [258/300], Train Loss: 0.000325
Validation Loss: 0.00031044
Epoch [259/300], Train Loss: 0.000324
Validation Loss: 0.00030731
Epoch [260/300], Train Loss: 0.000324
Validation Loss: 0.00030935
Epoch [261/300], Train Loss: 0.000323
Validation Loss: 0.00030573
Epoch [262/300], Train Loss: 0.000323
Validation Loss: 0.00030674
Epoch [263/300], Train Loss: 0.000322
Validation Loss: 0.00031009
Epoch [264/300], Train Loss: 0.000322
Validation Loss: 0.00030603
Epoch [265/300], Train Loss: 0.000322
Validation Loss: 0.00030699
Epoch [266/300], Train Loss: 0.000321
Validation Loss: 0.00030367
Epoch [267/300], Train Loss: 0.000321
Validation Loss: 0.00030686
Epoch [268/300], Train Loss: 0.000321
Validation Loss: 0.00030516
Epoch [269/300], Train Loss: 0.000319
Validation Loss: 0.00030240
Epoch [270/300], Train Loss: 0.000320
Validation Loss: 0.00030862
Epoch [271/300], Train Loss: 0.000320
Validation Loss: 0.00030259
Epoch [272/300], Train Loss: 0.000318
Validation Loss: 0.00030407
Epoch [273/300], Train Loss: 0.000319
Validation Loss: 0.00030191
Epoch [274/300], Train Loss: 0.000319
Validation Loss: 0.00030074
Epoch [275/300], Train Loss: 0.000317
Validation Loss: 0.00030155
Epoch [276/300], Train Loss: 0.000318
Validation Loss: 0.00029981
Epoch [277/300], Train Loss: 0.000316
Validation Loss: 0.00029939
Epoch [278/300], Train Loss: 0.000317
Validation Loss: 0.00030026
Epoch [279/300], Train Loss: 0.000315
Validation Loss: 0.00030014
Epoch [280/300], Train Loss: 0.000317
Validation Loss: 0.00029850
Epoch [281/300], Train Loss: 0.000315
Validation Loss: 0.00029919
Epoch [282/300], Train Loss: 0.000315
Validation Loss: 0.00030066
Epoch [283/300], Train Loss: 0.000315
Validation Loss: 0.00029914
Epoch [284/300], Train Loss: 0.000315
Validation Loss: 0.00029897
Epoch [285/300], Train Loss: 0.000313
Validation Loss: 0.00029754
Epoch [286/300], Train Loss: 0.000313
Validation Loss: 0.00029592
Epoch [287/300], Train Loss: 0.000314
Validation Loss: 0.00029829
Epoch [288/300], Train Loss: 0.000313
Validation Loss: 0.00029619
Epoch [289/300], Train Loss: 0.000312
Validation Loss: 0.00029924
Epoch [290/300], Train Loss: 0.000314
Validation Loss: 0.00029538
Epoch [291/300], Train Loss: 0.000312
Validation Loss: 0.00029545
Epoch [292/300], Train Loss: 0.000312
Validation Loss: 0.00029676
Epoch [293/300], Train Loss: 0.000313
Validation Loss: 0.00029594
Epoch [294/300], Train Loss: 0.000311
Validation Loss: 0.00029560
Epoch [295/300], Train Loss: 0.000311
Validation Loss: 0.00029543
Epoch [296/300], Train Loss: 0.000311
Validation Loss: 0.00029485
Epoch [297/300], Train Loss: 0.000310
Validation Loss: 0.00029501
Epoch [298/300], Train Loss: 0.000310
Validation Loss: 0.00029387
Epoch [299/300], Train Loss: 0.000311
Validation Loss: 0.00029787
Epoch [300/300], Train Loss: 0.000310
Validation Loss: 0.00029246

Evaluating model for: Fridge
Run 7/72 completed in 8542.38 seconds with: {'MAE': np.float32(21.379236), 'MSE': np.float32(962.6933), 'RMSE': np.float32(31.0273), 'SAE': np.float32(0.012364507), 'NDE': np.float32(0.5337577)}

Run 8/72: hidden=128, seq_len=120, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000725
Validation Loss: 0.00069172
Epoch [2/300], Train Loss: 0.000675
Validation Loss: 0.00068979
Epoch [3/300], Train Loss: 0.000669
Validation Loss: 0.00068644
Epoch [4/300], Train Loss: 0.000658
Validation Loss: 0.00064243
Epoch [5/300], Train Loss: 0.000626
Validation Loss: 0.00062890
Epoch [6/300], Train Loss: 0.000616
Validation Loss: 0.00062271
Epoch [7/300], Train Loss: 0.000612
Validation Loss: 0.00062420
Epoch [8/300], Train Loss: 0.000611
Validation Loss: 0.00061411
Epoch [9/300], Train Loss: 0.000608
Validation Loss: 0.00061229
Epoch [10/300], Train Loss: 0.000606
Validation Loss: 0.00061243
Epoch [11/300], Train Loss: 0.000605
Validation Loss: 0.00060963
Epoch [12/300], Train Loss: 0.000603
Validation Loss: 0.00060719
Epoch [13/300], Train Loss: 0.000603
Validation Loss: 0.00060604
Epoch [14/300], Train Loss: 0.000601
Validation Loss: 0.00061173
Epoch [15/300], Train Loss: 0.000600
Validation Loss: 0.00061242
Epoch [16/300], Train Loss: 0.000599
Validation Loss: 0.00060689
Epoch [17/300], Train Loss: 0.000598
Validation Loss: 0.00060184
Epoch [18/300], Train Loss: 0.000597
Validation Loss: 0.00060484
Epoch [19/300], Train Loss: 0.000595
Validation Loss: 0.00060058
Epoch [20/300], Train Loss: 0.000594
Validation Loss: 0.00060024
Epoch [21/300], Train Loss: 0.000593
Validation Loss: 0.00060087
Epoch [22/300], Train Loss: 0.000592
Validation Loss: 0.00059443
Epoch [23/300], Train Loss: 0.000592
Validation Loss: 0.00059728
Epoch [24/300], Train Loss: 0.000589
Validation Loss: 0.00059820
Epoch [25/300], Train Loss: 0.000590
Validation Loss: 0.00059462
Epoch [26/300], Train Loss: 0.000588
Validation Loss: 0.00059045
Epoch [27/300], Train Loss: 0.000585
Validation Loss: 0.00059217
Epoch [28/300], Train Loss: 0.000583
Validation Loss: 0.00059588
Epoch [29/300], Train Loss: 0.000581
Validation Loss: 0.00058666
Epoch [30/300], Train Loss: 0.000578
Validation Loss: 0.00057364
Epoch [31/300], Train Loss: 0.000569
Validation Loss: 0.00056463
Epoch [32/300], Train Loss: 0.000568
Validation Loss: 0.00055835
Epoch [33/300], Train Loss: 0.000562
Validation Loss: 0.00056785
Epoch [34/300], Train Loss: 0.000564
Validation Loss: 0.00055882
Epoch [35/300], Train Loss: 0.000558
Validation Loss: 0.00055196
Epoch [36/300], Train Loss: 0.000559
Validation Loss: 0.00054955
Epoch [37/300], Train Loss: 0.000549
Validation Loss: 0.00054304
Epoch [38/300], Train Loss: 0.000545
Validation Loss: 0.00053876
Epoch [39/300], Train Loss: 0.000541
Validation Loss: 0.00053696
Epoch [40/300], Train Loss: 0.000541
Validation Loss: 0.00054758
Epoch [41/300], Train Loss: 0.000539
Validation Loss: 0.00053090
Epoch [42/300], Train Loss: 0.000533
Validation Loss: 0.00052570
Epoch [43/300], Train Loss: 0.000559
Validation Loss: 0.00053395
Epoch [44/300], Train Loss: 0.000562
Validation Loss: 0.00060998
Epoch [45/300], Train Loss: 0.000591
Validation Loss: 0.00057384
Epoch [46/300], Train Loss: 0.000571
Validation Loss: 0.00055565
Epoch [47/300], Train Loss: 0.000558
Validation Loss: 0.00053820
Epoch [48/300], Train Loss: 0.000546
Validation Loss: 0.00053312
Epoch [49/300], Train Loss: 0.000540
Validation Loss: 0.00052633
Epoch [50/300], Train Loss: 0.000535
Validation Loss: 0.00052629
Epoch [51/300], Train Loss: 0.000538
Validation Loss: 0.00052436
Epoch [52/300], Train Loss: 0.000533
Validation Loss: 0.00052336
Epoch [53/300], Train Loss: 0.000528
Validation Loss: 0.00051657
Epoch [54/300], Train Loss: 0.000528
Validation Loss: 0.00051365
Epoch [55/300], Train Loss: 0.000537
Validation Loss: 0.00051272
Epoch [56/300], Train Loss: 0.000523
Validation Loss: 0.00051900
Epoch [57/300], Train Loss: 0.000524
Validation Loss: 0.00051108
Epoch [58/300], Train Loss: 0.000522
Validation Loss: 0.00050809
Epoch [59/300], Train Loss: 0.000521
Validation Loss: 0.00052668
Epoch [60/300], Train Loss: 0.000521
Validation Loss: 0.00050747
Epoch [61/300], Train Loss: 0.000521
Validation Loss: 0.00050491
Epoch [62/300], Train Loss: 0.000527
Validation Loss: 0.00059150
Epoch [63/300], Train Loss: 0.000536
Validation Loss: 0.00051271
Epoch [64/300], Train Loss: 0.000523
Validation Loss: 0.00050788
Epoch [65/300], Train Loss: 0.000521
Validation Loss: 0.00051884
Epoch [66/300], Train Loss: 0.000519
Validation Loss: 0.00050679
Epoch [67/300], Train Loss: 0.000517
Validation Loss: 0.00058638
Epoch [68/300], Train Loss: 0.000522
Validation Loss: 0.00049895
Epoch [69/300], Train Loss: 0.000512
Validation Loss: 0.00050151
Epoch [70/300], Train Loss: 0.000511
Validation Loss: 0.00049904
Epoch [71/300], Train Loss: 0.000510
Validation Loss: 0.00049727
Epoch [72/300], Train Loss: 0.000510
Validation Loss: 0.00049148
Epoch [73/300], Train Loss: 0.000506
Validation Loss: 0.00049398
Epoch [74/300], Train Loss: 0.000508
Validation Loss: 0.00049313
Epoch [75/300], Train Loss: 0.000507
Validation Loss: 0.00048584
Epoch [76/300], Train Loss: 0.000509
Validation Loss: 0.00049171
Epoch [77/300], Train Loss: 0.000538
Validation Loss: 0.00056784
Epoch [78/300], Train Loss: 0.000561
Validation Loss: 0.00055290
Epoch [79/300], Train Loss: 0.000549
Validation Loss: 0.00053735
Epoch [80/300], Train Loss: 0.000531
Validation Loss: 0.00051255
Epoch [81/300], Train Loss: 0.000518
Validation Loss: 0.00050477
Epoch [82/300], Train Loss: 0.000511
Validation Loss: 0.00049859
Epoch [83/300], Train Loss: 0.000527
Validation Loss: 0.00050976
Epoch [84/300], Train Loss: 0.000518
Validation Loss: 0.00049653
Epoch [85/300], Train Loss: 0.000510
Validation Loss: 0.00049197
Early stopping triggered

Evaluating model for: Fridge
Run 8/72 completed in 2395.97 seconds with: {'MAE': np.float32(32.465157), 'MSE': np.float32(1709.7557), 'RMSE': np.float32(41.349194), 'SAE': np.float32(0.006927398), 'NDE': np.float32(0.71132374)}

Run 9/72: hidden=128, seq_len=360, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000663
Validation Loss: 0.00065577
Epoch [2/300], Train Loss: 0.000650
Validation Loss: 0.00064437
Epoch [3/300], Train Loss: 0.000637
Validation Loss: 0.00062963
Epoch [4/300], Train Loss: 0.000619
Validation Loss: 0.00061022
Epoch [5/300], Train Loss: 0.000608
Validation Loss: 0.00060248
Epoch [6/300], Train Loss: 0.000603
Validation Loss: 0.00060232
Epoch [7/300], Train Loss: 0.000600
Validation Loss: 0.00059678
Epoch [8/300], Train Loss: 0.000597
Validation Loss: 0.00059799
Epoch [9/300], Train Loss: 0.000597
Validation Loss: 0.00059610
Epoch [10/300], Train Loss: 0.000595
Validation Loss: 0.00059261
Epoch [11/300], Train Loss: 0.000593
Validation Loss: 0.00059331
Epoch [12/300], Train Loss: 0.000593
Validation Loss: 0.00059171
Epoch [13/300], Train Loss: 0.000591
Validation Loss: 0.00058964
Epoch [14/300], Train Loss: 0.000590
Validation Loss: 0.00058870
Epoch [15/300], Train Loss: 0.000589
Validation Loss: 0.00058894
Epoch [16/300], Train Loss: 0.000587
Validation Loss: 0.00059043
Epoch [17/300], Train Loss: 0.000587
Validation Loss: 0.00058711
Epoch [18/300], Train Loss: 0.000585
Validation Loss: 0.00058482
Epoch [19/300], Train Loss: 0.000584
Validation Loss: 0.00058249
Epoch [20/300], Train Loss: 0.000582
Validation Loss: 0.00058096
Epoch [21/300], Train Loss: 0.000580
Validation Loss: 0.00058038
Epoch [22/300], Train Loss: 0.000579
Validation Loss: 0.00057806
Epoch [23/300], Train Loss: 0.000577
Validation Loss: 0.00058049
Epoch [24/300], Train Loss: 0.000577
Validation Loss: 0.00057365
Epoch [25/300], Train Loss: 0.000573
Validation Loss: 0.00057293
Epoch [26/300], Train Loss: 0.000571
Validation Loss: 0.00057003
Epoch [27/300], Train Loss: 0.000569
Validation Loss: 0.00056722
Epoch [28/300], Train Loss: 0.000567
Validation Loss: 0.00056843
Epoch [29/300], Train Loss: 0.000564
Validation Loss: 0.00056051
Epoch [30/300], Train Loss: 0.000561
Validation Loss: 0.00055557
Epoch [31/300], Train Loss: 0.000556
Validation Loss: 0.00055242
Epoch [32/300], Train Loss: 0.000553
Validation Loss: 0.00055298
Epoch [33/300], Train Loss: 0.000551
Validation Loss: 0.00054759
Epoch [34/300], Train Loss: 0.000547
Validation Loss: 0.00054716
Epoch [35/300], Train Loss: 0.000546
Validation Loss: 0.00054244
Epoch [36/300], Train Loss: 0.000543
Validation Loss: 0.00054233
Epoch [37/300], Train Loss: 0.000540
Validation Loss: 0.00054018
Epoch [38/300], Train Loss: 0.000537
Validation Loss: 0.00052949
Epoch [39/300], Train Loss: 0.000534
Validation Loss: 0.00052547
Epoch [40/300], Train Loss: 0.000529
Validation Loss: 0.00052241
Epoch [41/300], Train Loss: 0.000526
Validation Loss: 0.00051816
Epoch [42/300], Train Loss: 0.000524
Validation Loss: 0.00051877
Epoch [43/300], Train Loss: 0.000520
Validation Loss: 0.00051419
Epoch [44/300], Train Loss: 0.000516
Validation Loss: 0.00050973
Epoch [45/300], Train Loss: 0.000512
Validation Loss: 0.00050499
Epoch [46/300], Train Loss: 0.000509
Validation Loss: 0.00050755
Epoch [47/300], Train Loss: 0.000507
Validation Loss: 0.00049549
Epoch [48/300], Train Loss: 0.000504
Validation Loss: 0.00049331
Epoch [49/300], Train Loss: 0.000499
Validation Loss: 0.00049622
Epoch [50/300], Train Loss: 0.000497
Validation Loss: 0.00048618
Epoch [51/300], Train Loss: 0.000493
Validation Loss: 0.00048421
Epoch [52/300], Train Loss: 0.000492
Validation Loss: 0.00048168
Epoch [53/300], Train Loss: 0.000487
Validation Loss: 0.00047852
Epoch [54/300], Train Loss: 0.000484
Validation Loss: 0.00047407
Epoch [55/300], Train Loss: 0.000484
Validation Loss: 0.00048518
Epoch [56/300], Train Loss: 0.000480
Validation Loss: 0.00047490
Epoch [57/300], Train Loss: 0.000477
Validation Loss: 0.00046858
Epoch [58/300], Train Loss: 0.000476
Validation Loss: 0.00047083
Epoch [59/300], Train Loss: 0.000475
Validation Loss: 0.00046174
Epoch [60/300], Train Loss: 0.000473
Validation Loss: 0.00046479
Epoch [61/300], Train Loss: 0.000469
Validation Loss: 0.00045968
Epoch [62/300], Train Loss: 0.000467
Validation Loss: 0.00045776
Epoch [63/300], Train Loss: 0.000466
Validation Loss: 0.00046188
Epoch [64/300], Train Loss: 0.000464
Validation Loss: 0.00045794
Epoch [65/300], Train Loss: 0.000462
Validation Loss: 0.00045347
Epoch [66/300], Train Loss: 0.000460
Validation Loss: 0.00045784
Epoch [67/300], Train Loss: 0.000461
Validation Loss: 0.00045039
Epoch [68/300], Train Loss: 0.000457
Validation Loss: 0.00044645
Epoch [69/300], Train Loss: 0.000454
Validation Loss: 0.00044360
Epoch [70/300], Train Loss: 0.000454
Validation Loss: 0.00044368
Epoch [71/300], Train Loss: 0.000454
Validation Loss: 0.00044934
Epoch [72/300], Train Loss: 0.000453
Validation Loss: 0.00044096
Epoch [73/300], Train Loss: 0.000450
Validation Loss: 0.00044011
Epoch [74/300], Train Loss: 0.000449
Validation Loss: 0.00044280
Epoch [75/300], Train Loss: 0.000446
Validation Loss: 0.00043795
Epoch [76/300], Train Loss: 0.000444
Validation Loss: 0.00043731
Epoch [77/300], Train Loss: 0.000445
Validation Loss: 0.00043640
Epoch [78/300], Train Loss: 0.000443
Validation Loss: 0.00043269
Epoch [79/300], Train Loss: 0.000440
Validation Loss: 0.00043067
Epoch [80/300], Train Loss: 0.000439
Validation Loss: 0.00043461
Epoch [81/300], Train Loss: 0.000437
Validation Loss: 0.00042788
Epoch [82/300], Train Loss: 0.000438
Validation Loss: 0.00042850
Epoch [83/300], Train Loss: 0.000436
Validation Loss: 0.00043022
Epoch [84/300], Train Loss: 0.000434
Validation Loss: 0.00042819
Epoch [85/300], Train Loss: 0.000434
Validation Loss: 0.00042452
Epoch [86/300], Train Loss: 0.000503
Validation Loss: 0.00044267
Epoch [87/300], Train Loss: 0.000442
Validation Loss: 0.00042706
Epoch [88/300], Train Loss: 0.000432
Validation Loss: 0.00042239
Epoch [89/300], Train Loss: 0.000429
Validation Loss: 0.00041975
Epoch [90/300], Train Loss: 0.000428
Validation Loss: 0.00042300
Epoch [91/300], Train Loss: 0.000427
Validation Loss: 0.00041957
Epoch [92/300], Train Loss: 0.000426
Validation Loss: 0.00041870
Epoch [93/300], Train Loss: 0.000424
Validation Loss: 0.00041778
Epoch [94/300], Train Loss: 0.000423
Validation Loss: 0.00041560
Epoch [95/300], Train Loss: 0.000422
Validation Loss: 0.00041530
Epoch [96/300], Train Loss: 0.000423
Validation Loss: 0.00041695
Epoch [97/300], Train Loss: 0.000419
Validation Loss: 0.00041155
Epoch [98/300], Train Loss: 0.000419
Validation Loss: 0.00041472
Epoch [99/300], Train Loss: 0.000417
Validation Loss: 0.00041270
Epoch [100/300], Train Loss: 0.000417
Validation Loss: 0.00040862
Epoch [101/300], Train Loss: 0.000415
Validation Loss: 0.00040805
Epoch [102/300], Train Loss: 0.000415
Validation Loss: 0.00040600
Epoch [103/300], Train Loss: 0.000413
Validation Loss: 0.00040626
Epoch [104/300], Train Loss: 0.000411
Validation Loss: 0.00040564
Epoch [105/300], Train Loss: 0.000412
Validation Loss: 0.00040731
Epoch [106/300], Train Loss: 0.000411
Validation Loss: 0.00040198
Epoch [107/300], Train Loss: 0.000420
Validation Loss: 0.00040574
Epoch [108/300], Train Loss: 0.000409
Validation Loss: 0.00040344
Epoch [109/300], Train Loss: 0.000408
Validation Loss: 0.00040190
Epoch [110/300], Train Loss: 0.000407
Validation Loss: 0.00039956
Epoch [111/300], Train Loss: 0.000406
Validation Loss: 0.00039742
Epoch [112/300], Train Loss: 0.000403
Validation Loss: 0.00039807
Epoch [113/300], Train Loss: 0.000404
Validation Loss: 0.00039739
Epoch [114/300], Train Loss: 0.000403
Validation Loss: 0.00039716
Epoch [115/300], Train Loss: 0.000402
Validation Loss: 0.00039598
Epoch [116/300], Train Loss: 0.000402
Validation Loss: 0.00040166
Epoch [117/300], Train Loss: 0.000402
Validation Loss: 0.00040003
Epoch [118/300], Train Loss: 0.000403
Validation Loss: 0.00039515
Epoch [119/300], Train Loss: 0.000399
Validation Loss: 0.00039254
Epoch [120/300], Train Loss: 0.000399
Validation Loss: 0.00039411
Epoch [121/300], Train Loss: 0.000396
Validation Loss: 0.00038893
Epoch [122/300], Train Loss: 0.000396
Validation Loss: 0.00039312
Epoch [123/300], Train Loss: 0.000395
Validation Loss: 0.00039172
Epoch [124/300], Train Loss: 0.000395
Validation Loss: 0.00038674
Epoch [125/300], Train Loss: 0.000393
Validation Loss: 0.00039049
Epoch [126/300], Train Loss: 0.000393
Validation Loss: 0.00038773
Epoch [127/300], Train Loss: 0.000393
Validation Loss: 0.00038615
Epoch [128/300], Train Loss: 0.000391
Validation Loss: 0.00038515
Epoch [129/300], Train Loss: 0.000393
Validation Loss: 0.00038676
Epoch [130/300], Train Loss: 0.000391
Validation Loss: 0.00038790
Epoch [131/300], Train Loss: 0.000391
Validation Loss: 0.00038884
Epoch [132/300], Train Loss: 0.000390
Validation Loss: 0.00038137
Epoch [133/300], Train Loss: 0.000390
Validation Loss: 0.00038221
Epoch [134/300], Train Loss: 0.000388
Validation Loss: 0.00038716
Epoch [135/300], Train Loss: 0.000387
Validation Loss: 0.00038220
Epoch [136/300], Train Loss: 0.000386
Validation Loss: 0.00038143
Epoch [137/300], Train Loss: 0.000386
Validation Loss: 0.00038316
Epoch [138/300], Train Loss: 0.000386
Validation Loss: 0.00037860
Epoch [139/300], Train Loss: 0.000384
Validation Loss: 0.00037793
Epoch [140/300], Train Loss: 0.000384
Validation Loss: 0.00037785
Epoch [141/300], Train Loss: 0.000383
Validation Loss: 0.00037746
Epoch [142/300], Train Loss: 0.000382
Validation Loss: 0.00037611
Epoch [143/300], Train Loss: 0.000381
Validation Loss: 0.00037815
Epoch [144/300], Train Loss: 0.000381
Validation Loss: 0.00037496
Epoch [145/300], Train Loss: 0.000381
Validation Loss: 0.00037851
Epoch [146/300], Train Loss: 0.000380
Validation Loss: 0.00037665
Epoch [147/300], Train Loss: 0.000380
Validation Loss: 0.00037220
Epoch [148/300], Train Loss: 0.000379
Validation Loss: 0.00038144
Epoch [149/300], Train Loss: 0.000378
Validation Loss: 0.00037279
Epoch [150/300], Train Loss: 0.000378
Validation Loss: 0.00037223
Epoch [151/300], Train Loss: 0.000378
Validation Loss: 0.00037198
Epoch [152/300], Train Loss: 0.000378
Validation Loss: 0.00037027
Epoch [153/300], Train Loss: 0.000376
Validation Loss: 0.00037049
Epoch [154/300], Train Loss: 0.000375
Validation Loss: 0.00036892
Epoch [155/300], Train Loss: 0.000375
Validation Loss: 0.00037215
Epoch [156/300], Train Loss: 0.000375
Validation Loss: 0.00036812
Epoch [157/300], Train Loss: 0.000374
Validation Loss: 0.00036886
Epoch [158/300], Train Loss: 0.000373
Validation Loss: 0.00036680
Epoch [159/300], Train Loss: 0.000374
Validation Loss: 0.00036811
Epoch [160/300], Train Loss: 0.000372
Validation Loss: 0.00036531
Epoch [161/300], Train Loss: 0.000371
Validation Loss: 0.00036518
Epoch [162/300], Train Loss: 0.000371
Validation Loss: 0.00036692
Epoch [163/300], Train Loss: 0.000372
Validation Loss: 0.00036437
Epoch [164/300], Train Loss: 0.000370
Validation Loss: 0.00036559
Epoch [165/300], Train Loss: 0.000370
Validation Loss: 0.00036709
Epoch [166/300], Train Loss: 0.000370
Validation Loss: 0.00036333
Epoch [167/300], Train Loss: 0.000370
Validation Loss: 0.00036257
Epoch [168/300], Train Loss: 0.000368
Validation Loss: 0.00036134
Epoch [169/300], Train Loss: 0.000367
Validation Loss: 0.00036142
Epoch [170/300], Train Loss: 0.000367
Validation Loss: 0.00036342
Epoch [171/300], Train Loss: 0.000368
Validation Loss: 0.00036118
Epoch [172/300], Train Loss: 0.000367
Validation Loss: 0.00036357
Epoch [173/300], Train Loss: 0.000366
Validation Loss: 0.00036204
Epoch [174/300], Train Loss: 0.000365
Validation Loss: 0.00035836
Epoch [175/300], Train Loss: 0.000365
Validation Loss: 0.00035863
Epoch [176/300], Train Loss: 0.000366
Validation Loss: 0.00035759
Epoch [177/300], Train Loss: 0.000365
Validation Loss: 0.00035847
Epoch [178/300], Train Loss: 0.000364
Validation Loss: 0.00035884
Epoch [179/300], Train Loss: 0.000364
Validation Loss: 0.00035693
Epoch [180/300], Train Loss: 0.000364
Validation Loss: 0.00035572
Epoch [181/300], Train Loss: 0.000364
Validation Loss: 0.00035582
Epoch [182/300], Train Loss: 0.000362
Validation Loss: 0.00035397
Epoch [183/300], Train Loss: 0.000362
Validation Loss: 0.00035872
Epoch [184/300], Train Loss: 0.000361
Validation Loss: 0.00035294
Epoch [185/300], Train Loss: 0.000362
Validation Loss: 0.00035351
Epoch [186/300], Train Loss: 0.000361
Validation Loss: 0.00035226
Epoch [187/300], Train Loss: 0.000361
Validation Loss: 0.00035438
Epoch [188/300], Train Loss: 0.000359
Validation Loss: 0.00035083
Epoch [189/300], Train Loss: 0.000359
Validation Loss: 0.00035150
Epoch [190/300], Train Loss: 0.000359
Validation Loss: 0.00035302
Epoch [191/300], Train Loss: 0.000359
Validation Loss: 0.00035054
Epoch [192/300], Train Loss: 0.000358
Validation Loss: 0.00035203
Epoch [193/300], Train Loss: 0.000358
Validation Loss: 0.00035105
Epoch [194/300], Train Loss: 0.000357
Validation Loss: 0.00034931
Epoch [195/300], Train Loss: 0.000357
Validation Loss: 0.00034752
Epoch [196/300], Train Loss: 0.000356
Validation Loss: 0.00034773
Epoch [197/300], Train Loss: 0.000356
Validation Loss: 0.00034738
Epoch [198/300], Train Loss: 0.000355
Validation Loss: 0.00034857
Epoch [199/300], Train Loss: 0.000355
Validation Loss: 0.00034707
Epoch [200/300], Train Loss: 0.000355
Validation Loss: 0.00034989
Epoch [201/300], Train Loss: 0.000355
Validation Loss: 0.00034596
Epoch [202/300], Train Loss: 0.000355
Validation Loss: 0.00034655
Epoch [203/300], Train Loss: 0.000354
Validation Loss: 0.00034427
Epoch [204/300], Train Loss: 0.000353
Validation Loss: 0.00034857
Epoch [205/300], Train Loss: 0.000353
Validation Loss: 0.00034425
Epoch [206/300], Train Loss: 0.000353
Validation Loss: 0.00034253
Epoch [207/300], Train Loss: 0.000352
Validation Loss: 0.00034284
Epoch [208/300], Train Loss: 0.000352
Validation Loss: 0.00034165
Epoch [209/300], Train Loss: 0.000351
Validation Loss: 0.00034121
Epoch [210/300], Train Loss: 0.000351
Validation Loss: 0.00034061
Epoch [211/300], Train Loss: 0.000350
Validation Loss: 0.00034190
Epoch [212/300], Train Loss: 0.000350
Validation Loss: 0.00034121
Epoch [213/300], Train Loss: 0.000349
Validation Loss: 0.00033929
Epoch [214/300], Train Loss: 0.000349
Validation Loss: 0.00034154
Epoch [215/300], Train Loss: 0.000349
Validation Loss: 0.00033875
Epoch [216/300], Train Loss: 0.000349
Validation Loss: 0.00033873
Epoch [217/300], Train Loss: 0.000348
Validation Loss: 0.00033977
Epoch [218/300], Train Loss: 0.000348
Validation Loss: 0.00034025
Epoch [219/300], Train Loss: 0.000348
Validation Loss: 0.00033856
Epoch [220/300], Train Loss: 0.000349
Validation Loss: 0.00033904
Epoch [221/300], Train Loss: 0.000347
Validation Loss: 0.00033625
Epoch [222/300], Train Loss: 0.000346
Validation Loss: 0.00033849
Epoch [223/300], Train Loss: 0.000346
Validation Loss: 0.00033708
Epoch [224/300], Train Loss: 0.000345
Validation Loss: 0.00033684
Epoch [225/300], Train Loss: 0.000345
Validation Loss: 0.00033460
Epoch [226/300], Train Loss: 0.000345
Validation Loss: 0.00033828
Epoch [227/300], Train Loss: 0.000345
Validation Loss: 0.00033352
Epoch [228/300], Train Loss: 0.000344
Validation Loss: 0.00033755
Epoch [229/300], Train Loss: 0.000344
Validation Loss: 0.00033518
Epoch [230/300], Train Loss: 0.000343
Validation Loss: 0.00033434
Epoch [231/300], Train Loss: 0.000343
Validation Loss: 0.00033552
Epoch [232/300], Train Loss: 0.000343
Validation Loss: 0.00033203
Epoch [233/300], Train Loss: 0.000342
Validation Loss: 0.00033209
Epoch [234/300], Train Loss: 0.000342
Validation Loss: 0.00033214
Epoch [235/300], Train Loss: 0.000342
Validation Loss: 0.00033232
Epoch [236/300], Train Loss: 0.000341
Validation Loss: 0.00033206
Epoch [237/300], Train Loss: 0.000342
Validation Loss: 0.00033120
Epoch [238/300], Train Loss: 0.000342
Validation Loss: 0.00033119
Epoch [239/300], Train Loss: 0.000342
Validation Loss: 0.00033076
Epoch [240/300], Train Loss: 0.000341
Validation Loss: 0.00033629
Epoch [241/300], Train Loss: 0.000340
Validation Loss: 0.00033150
Epoch [242/300], Train Loss: 0.000340
Validation Loss: 0.00033166
Epoch [243/300], Train Loss: 0.000339
Validation Loss: 0.00033136
Epoch [244/300], Train Loss: 0.000339
Validation Loss: 0.00032934
Epoch [245/300], Train Loss: 0.000339
Validation Loss: 0.00033218
Epoch [246/300], Train Loss: 0.000339
Validation Loss: 0.00032838
Epoch [247/300], Train Loss: 0.000338
Validation Loss: 0.00032771
Epoch [248/300], Train Loss: 0.000338
Validation Loss: 0.00033110
Epoch [249/300], Train Loss: 0.000338
Validation Loss: 0.00032783
Epoch [250/300], Train Loss: 0.000338
Validation Loss: 0.00032629
Epoch [251/300], Train Loss: 0.000338
Validation Loss: 0.00033152
Epoch [252/300], Train Loss: 0.000337
Validation Loss: 0.00032796
Epoch [253/300], Train Loss: 0.000337
Validation Loss: 0.00032801
Epoch [254/300], Train Loss: 0.000337
Validation Loss: 0.00032797
Epoch [255/300], Train Loss: 0.000336
Validation Loss: 0.00032970
Epoch [256/300], Train Loss: 0.000336
Validation Loss: 0.00032583
Epoch [257/300], Train Loss: 0.000336
Validation Loss: 0.00032676
Epoch [258/300], Train Loss: 0.000336
Validation Loss: 0.00032480
Epoch [259/300], Train Loss: 0.000335
Validation Loss: 0.00032690
Epoch [260/300], Train Loss: 0.000335
Validation Loss: 0.00032521
Epoch [261/300], Train Loss: 0.000335
Validation Loss: 0.00032676
Epoch [262/300], Train Loss: 0.000334
Validation Loss: 0.00032402
Epoch [263/300], Train Loss: 0.000335
Validation Loss: 0.00032478
Epoch [264/300], Train Loss: 0.000334
Validation Loss: 0.00032365
Epoch [265/300], Train Loss: 0.000334
Validation Loss: 0.00032550
Epoch [266/300], Train Loss: 0.000334
Validation Loss: 0.00032317
Epoch [267/300], Train Loss: 0.000333
Validation Loss: 0.00032233
Epoch [268/300], Train Loss: 0.000334
Validation Loss: 0.00032522
Epoch [269/300], Train Loss: 0.000333
Validation Loss: 0.00032391
Epoch [270/300], Train Loss: 0.000334
Validation Loss: 0.00032926
Epoch [271/300], Train Loss: 0.000334
Validation Loss: 0.00032543
Epoch [272/300], Train Loss: 0.000333
Validation Loss: 0.00032206
Epoch [273/300], Train Loss: 0.000332
Validation Loss: 0.00032098
Epoch [274/300], Train Loss: 0.000332
Validation Loss: 0.00032177
Epoch [275/300], Train Loss: 0.000332
Validation Loss: 0.00032069
Epoch [276/300], Train Loss: 0.000332
Validation Loss: 0.00032058
Epoch [277/300], Train Loss: 0.000331
Validation Loss: 0.00032156
Epoch [278/300], Train Loss: 0.000331
Validation Loss: 0.00032499
Epoch [279/300], Train Loss: 0.000331
Validation Loss: 0.00032233
Epoch [280/300], Train Loss: 0.000331
Validation Loss: 0.00032315
Epoch [281/300], Train Loss: 0.000331
Validation Loss: 0.00032037
Epoch [282/300], Train Loss: 0.000331
Validation Loss: 0.00032066
Epoch [283/300], Train Loss: 0.000330
Validation Loss: 0.00031980
Epoch [284/300], Train Loss: 0.000330
Validation Loss: 0.00032202
Epoch [285/300], Train Loss: 0.000330
Validation Loss: 0.00032043
Epoch [286/300], Train Loss: 0.000329
Validation Loss: 0.00032011
Epoch [287/300], Train Loss: 0.000329
Validation Loss: 0.00031985
Epoch [288/300], Train Loss: 0.000329
Validation Loss: 0.00032069
Epoch [289/300], Train Loss: 0.000330
Validation Loss: 0.00031937
Epoch [290/300], Train Loss: 0.000329
Validation Loss: 0.00031972
Epoch [291/300], Train Loss: 0.000329
Validation Loss: 0.00031937
Epoch [292/300], Train Loss: 0.000329
Validation Loss: 0.00031768
Epoch [293/300], Train Loss: 0.000328
Validation Loss: 0.00031812
Epoch [294/300], Train Loss: 0.000328
Validation Loss: 0.00031837
Epoch [295/300], Train Loss: 0.000328
Validation Loss: 0.00031894
Epoch [296/300], Train Loss: 0.000327
Validation Loss: 0.00031697
Epoch [297/300], Train Loss: 0.000328
Validation Loss: 0.00031723
Epoch [298/300], Train Loss: 0.000328
Validation Loss: 0.00031587
Epoch [299/300], Train Loss: 0.000327
Validation Loss: 0.00031660
Epoch [300/300], Train Loss: 0.000327
Validation Loss: 0.00031613

Evaluating model for: Fridge
Run 9/72 completed in 5808.04 seconds with: {'MAE': np.float32(23.541006), 'MSE': np.float32(1082.5719), 'RMSE': np.float32(32.902462), 'SAE': np.float32(0.011564055), 'NDE': np.float32(0.5685436)}

Run 10/72: hidden=128, seq_len=360, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000745
Validation Loss: 0.00066809
Epoch [2/300], Train Loss: 0.000675
Validation Loss: 0.00066503
Epoch [3/300], Train Loss: 0.000669
Validation Loss: 0.00065778
Epoch [4/300], Train Loss: 0.000655
Validation Loss: 0.00064104
Epoch [5/300], Train Loss: 0.000634
Validation Loss: 0.00061722
Epoch [6/300], Train Loss: 0.000623
Validation Loss: 0.00061369
Epoch [7/300], Train Loss: 0.000617
Validation Loss: 0.00061016
Epoch [8/300], Train Loss: 0.000614
Validation Loss: 0.00060789
Epoch [9/300], Train Loss: 0.000613
Validation Loss: 0.00060843
Epoch [10/300], Train Loss: 0.000610
Validation Loss: 0.00060339
Epoch [11/300], Train Loss: 0.000608
Validation Loss: 0.00060468
Epoch [12/300], Train Loss: 0.000607
Validation Loss: 0.00060093
Epoch [13/300], Train Loss: 0.000606
Validation Loss: 0.00059974
Epoch [14/300], Train Loss: 0.000605
Validation Loss: 0.00059898
Epoch [15/300], Train Loss: 0.000603
Validation Loss: 0.00059803
Epoch [16/300], Train Loss: 0.000602
Validation Loss: 0.00059666
Epoch [17/300], Train Loss: 0.000601
Validation Loss: 0.00059626
Epoch [18/300], Train Loss: 0.000600
Validation Loss: 0.00059645
Epoch [19/300], Train Loss: 0.000599
Validation Loss: 0.00059372
Epoch [20/300], Train Loss: 0.000598
Validation Loss: 0.00059514
Epoch [21/300], Train Loss: 0.000596
Validation Loss: 0.00059234
Epoch [22/300], Train Loss: 0.000596
Validation Loss: 0.00059159
Epoch [23/300], Train Loss: 0.000595
Validation Loss: 0.00059265
Epoch [24/300], Train Loss: 0.000595
Validation Loss: 0.00058941
Epoch [25/300], Train Loss: 0.000593
Validation Loss: 0.00058999
Epoch [26/300], Train Loss: 0.000592
Validation Loss: 0.00058848
Epoch [27/300], Train Loss: 0.000591
Validation Loss: 0.00058855
Epoch [28/300], Train Loss: 0.000591
Validation Loss: 0.00058719
Epoch [29/300], Train Loss: 0.000590
Validation Loss: 0.00058608
Epoch [30/300], Train Loss: 0.000589
Validation Loss: 0.00058497
Epoch [31/300], Train Loss: 0.000588
Validation Loss: 0.00058905
Epoch [32/300], Train Loss: 0.000587
Validation Loss: 0.00058357
Epoch [33/300], Train Loss: 0.000587
Validation Loss: 0.00058274
Epoch [34/300], Train Loss: 0.000585
Validation Loss: 0.00058186
Epoch [35/300], Train Loss: 0.000584
Validation Loss: 0.00058368
Epoch [36/300], Train Loss: 0.000584
Validation Loss: 0.00058007
Epoch [37/300], Train Loss: 0.000582
Validation Loss: 0.00057817
Epoch [38/300], Train Loss: 0.000581
Validation Loss: 0.00057694
Epoch [39/300], Train Loss: 0.000579
Validation Loss: 0.00057622
Epoch [40/300], Train Loss: 0.000577
Validation Loss: 0.00057328
Epoch [41/300], Train Loss: 0.000576
Validation Loss: 0.00057203
Epoch [42/300], Train Loss: 0.000575
Validation Loss: 0.00057381
Epoch [43/300], Train Loss: 0.000574
Validation Loss: 0.00056827
Epoch [44/300], Train Loss: 0.000570
Validation Loss: 0.00056813
Epoch [45/300], Train Loss: 0.000567
Validation Loss: 0.00056525
Epoch [46/300], Train Loss: 0.000566
Validation Loss: 0.00056110
Epoch [47/300], Train Loss: 0.000563
Validation Loss: 0.00055961
Epoch [48/300], Train Loss: 0.000561
Validation Loss: 0.00055846
Epoch [49/300], Train Loss: 0.000560
Validation Loss: 0.00055507
Epoch [50/300], Train Loss: 0.000560
Validation Loss: 0.00055287
Epoch [51/300], Train Loss: 0.000557
Validation Loss: 0.00055486
Epoch [52/300], Train Loss: 0.000555
Validation Loss: 0.00055417
Epoch [53/300], Train Loss: 0.000553
Validation Loss: 0.00054794
Epoch [54/300], Train Loss: 0.000552
Validation Loss: 0.00054546
Epoch [55/300], Train Loss: 0.000550
Validation Loss: 0.00054256
Epoch [56/300], Train Loss: 0.000547
Validation Loss: 0.00053966
Epoch [57/300], Train Loss: 0.000545
Validation Loss: 0.00053889
Epoch [58/300], Train Loss: 0.000542
Validation Loss: 0.00053326
Epoch [59/300], Train Loss: 0.000538
Validation Loss: 0.00052729
Epoch [60/300], Train Loss: 0.000535
Validation Loss: 0.00052421
Epoch [61/300], Train Loss: 0.000531
Validation Loss: 0.00051935
Epoch [62/300], Train Loss: 0.000527
Validation Loss: 0.00051698
Epoch [63/300], Train Loss: 0.000525
Validation Loss: 0.00051473
Epoch [64/300], Train Loss: 0.000521
Validation Loss: 0.00051585
Epoch [65/300], Train Loss: 0.000518
Validation Loss: 0.00050522
Epoch [66/300], Train Loss: 0.000510
Validation Loss: 0.00050280
Epoch [67/300], Train Loss: 0.000507
Validation Loss: 0.00049533
Epoch [68/300], Train Loss: 0.000498
Validation Loss: 0.00049147
Epoch [69/300], Train Loss: 0.000492
Validation Loss: 0.00049762
Epoch [70/300], Train Loss: 0.000494
Validation Loss: 0.00049290
Epoch [71/300], Train Loss: 0.000488
Validation Loss: 0.00049174
Epoch [72/300], Train Loss: 0.000486
Validation Loss: 0.00047096
Epoch [73/300], Train Loss: 0.000480
Validation Loss: 0.00047645
Epoch [74/300], Train Loss: 0.000476
Validation Loss: 0.00047866
Epoch [75/300], Train Loss: 0.000473
Validation Loss: 0.00046408
Epoch [76/300], Train Loss: 0.000478
Validation Loss: 0.00048425
Epoch [77/300], Train Loss: 0.000473
Validation Loss: 0.00048491
Epoch [78/300], Train Loss: 0.000474
Validation Loss: 0.00046106
Epoch [79/300], Train Loss: 0.000473
Validation Loss: 0.00046240
Epoch [80/300], Train Loss: 0.000469
Validation Loss: 0.00046790
Epoch [81/300], Train Loss: 0.000472
Validation Loss: 0.00046167
Epoch [82/300], Train Loss: 0.000465
Validation Loss: 0.00045742
Epoch [83/300], Train Loss: 0.000464
Validation Loss: 0.00045566
Epoch [84/300], Train Loss: 0.000465
Validation Loss: 0.00045449
Epoch [85/300], Train Loss: 0.000458
Validation Loss: 0.00048795
Epoch [86/300], Train Loss: 0.000460
Validation Loss: 0.00044789
Epoch [87/300], Train Loss: 0.000455
Validation Loss: 0.00045003
Epoch [88/300], Train Loss: 0.000457
Validation Loss: 0.00045234
Epoch [89/300], Train Loss: 0.000452
Validation Loss: 0.00045047
Epoch [90/300], Train Loss: 0.000462
Validation Loss: 0.00044339
Epoch [91/300], Train Loss: 0.000450
Validation Loss: 0.00044326
Epoch [92/300], Train Loss: 0.000450
Validation Loss: 0.00045343
Epoch [93/300], Train Loss: 0.000458
Validation Loss: 0.00046519
Epoch [94/300], Train Loss: 0.000449
Validation Loss: 0.00044022
Epoch [95/300], Train Loss: 0.000448
Validation Loss: 0.00045395
Epoch [96/300], Train Loss: 0.000448
Validation Loss: 0.00043958
Epoch [97/300], Train Loss: 0.000446
Validation Loss: 0.00043923
Epoch [98/300], Train Loss: 0.000441
Validation Loss: 0.00043414
Epoch [99/300], Train Loss: 0.000442
Validation Loss: 0.00043804
Epoch [100/300], Train Loss: 0.000439
Validation Loss: 0.00043150
Epoch [101/300], Train Loss: 0.000437
Validation Loss: 0.00043176
Epoch [102/300], Train Loss: 0.000440
Validation Loss: 0.00043107
Epoch [103/300], Train Loss: 0.000436
Validation Loss: 0.00042904
Epoch [104/300], Train Loss: 0.000436
Validation Loss: 0.00042985
Epoch [105/300], Train Loss: 0.000433
Validation Loss: 0.00042806
Epoch [106/300], Train Loss: 0.000432
Validation Loss: 0.00043039
Epoch [107/300], Train Loss: 0.000432
Validation Loss: 0.00042718
Epoch [108/300], Train Loss: 0.000430
Validation Loss: 0.00042400
Epoch [109/300], Train Loss: 0.000429
Validation Loss: 0.00042338
Epoch [110/300], Train Loss: 0.000428
Validation Loss: 0.00042161
Epoch [111/300], Train Loss: 0.000427
Validation Loss: 0.00042105
Epoch [112/300], Train Loss: 0.000427
Validation Loss: 0.00042143
Epoch [113/300], Train Loss: 0.000425
Validation Loss: 0.00042056
Epoch [114/300], Train Loss: 0.000425
Validation Loss: 0.00041965
Epoch [115/300], Train Loss: 0.000426
Validation Loss: 0.00042021
Epoch [116/300], Train Loss: 0.000426
Validation Loss: 0.00041742
Epoch [117/300], Train Loss: 0.000423
Validation Loss: 0.00041667
Epoch [118/300], Train Loss: 0.000422
Validation Loss: 0.00041772
Epoch [119/300], Train Loss: 0.000421
Validation Loss: 0.00041430
Epoch [120/300], Train Loss: 0.000422
Validation Loss: 0.00041761
Epoch [121/300], Train Loss: 0.000421
Validation Loss: 0.00041687
Epoch [122/300], Train Loss: 0.000420
Validation Loss: 0.00041241
Epoch [123/300], Train Loss: 0.000418
Validation Loss: 0.00041425
Epoch [124/300], Train Loss: 0.000418
Validation Loss: 0.00041088
Epoch [125/300], Train Loss: 0.000419
Validation Loss: 0.00041124
Epoch [126/300], Train Loss: 0.000415
Validation Loss: 0.00041129
Epoch [127/300], Train Loss: 0.000415
Validation Loss: 0.00041004
Epoch [128/300], Train Loss: 0.000415
Validation Loss: 0.00040838
Epoch [129/300], Train Loss: 0.000414
Validation Loss: 0.00040821
Epoch [130/300], Train Loss: 0.000413
Validation Loss: 0.00041186
Epoch [131/300], Train Loss: 0.000414
Validation Loss: 0.00040575
Epoch [132/300], Train Loss: 0.000413
Validation Loss: 0.00041071
Epoch [133/300], Train Loss: 0.000411
Validation Loss: 0.00040566
Epoch [134/300], Train Loss: 0.000410
Validation Loss: 0.00040468
Epoch [135/300], Train Loss: 0.000410
Validation Loss: 0.00040472
Epoch [136/300], Train Loss: 0.000409
Validation Loss: 0.00040770
Epoch [137/300], Train Loss: 0.000409
Validation Loss: 0.00040830
Epoch [138/300], Train Loss: 0.000408
Validation Loss: 0.00040153
Epoch [139/300], Train Loss: 0.000408
Validation Loss: 0.00040022
Epoch [140/300], Train Loss: 0.000406
Validation Loss: 0.00040046
Epoch [141/300], Train Loss: 0.000408
Validation Loss: 0.00040093
Epoch [142/300], Train Loss: 0.000406
Validation Loss: 0.00039976
Epoch [143/300], Train Loss: 0.000405
Validation Loss: 0.00040525
Epoch [144/300], Train Loss: 0.000406
Validation Loss: 0.00039971
Epoch [145/300], Train Loss: 0.000406
Validation Loss: 0.00039650
Epoch [146/300], Train Loss: 0.000404
Validation Loss: 0.00039861
Epoch [147/300], Train Loss: 0.000405
Validation Loss: 0.00039593
Epoch [148/300], Train Loss: 0.000403
Validation Loss: 0.00039868
Epoch [149/300], Train Loss: 0.000404
Validation Loss: 0.00039621
Epoch [150/300], Train Loss: 0.000402
Validation Loss: 0.00039436
Epoch [151/300], Train Loss: 0.000401
Validation Loss: 0.00039600
Epoch [152/300], Train Loss: 0.000402
Validation Loss: 0.00039619
Epoch [153/300], Train Loss: 0.000404
Validation Loss: 0.00039454
Epoch [154/300], Train Loss: 0.000399
Validation Loss: 0.00039718
Epoch [155/300], Train Loss: 0.000401
Validation Loss: 0.00039293
Epoch [156/300], Train Loss: 0.000402
Validation Loss: 0.00039141
Epoch [157/300], Train Loss: 0.000398
Validation Loss: 0.00039264
Epoch [158/300], Train Loss: 0.000399
Validation Loss: 0.00039099
Epoch [159/300], Train Loss: 0.000399
Validation Loss: 0.00039577
Epoch [160/300], Train Loss: 0.000397
Validation Loss: 0.00038868
Epoch [161/300], Train Loss: 0.000397
Validation Loss: 0.00038915
Epoch [162/300], Train Loss: 0.000398
Validation Loss: 0.00038993
Epoch [163/300], Train Loss: 0.000395
Validation Loss: 0.00038784
Epoch [164/300], Train Loss: 0.000395
Validation Loss: 0.00039195
Epoch [165/300], Train Loss: 0.000394
Validation Loss: 0.00038860
Epoch [166/300], Train Loss: 0.000394
Validation Loss: 0.00038704
Epoch [167/300], Train Loss: 0.000397
Validation Loss: 0.00040460
Epoch [168/300], Train Loss: 0.000397
Validation Loss: 0.00038613
Epoch [169/300], Train Loss: 0.000392
Validation Loss: 0.00038497
Epoch [170/300], Train Loss: 0.000394
Validation Loss: 0.00039065
Epoch [171/300], Train Loss: 0.000393
Validation Loss: 0.00038666
Epoch [172/300], Train Loss: 0.000393
Validation Loss: 0.00038478
Epoch [173/300], Train Loss: 0.000392
Validation Loss: 0.00038943
Epoch [174/300], Train Loss: 0.000391
Validation Loss: 0.00038356
Epoch [175/300], Train Loss: 0.000391
Validation Loss: 0.00038335
Epoch [176/300], Train Loss: 0.000390
Validation Loss: 0.00038206
Epoch [177/300], Train Loss: 0.000390
Validation Loss: 0.00038411
Epoch [178/300], Train Loss: 0.000390
Validation Loss: 0.00038420
Epoch [179/300], Train Loss: 0.000390
Validation Loss: 0.00038242
Epoch [180/300], Train Loss: 0.000391
Validation Loss: 0.00038247
Epoch [181/300], Train Loss: 0.000389
Validation Loss: 0.00038370
Epoch [182/300], Train Loss: 0.000388
Validation Loss: 0.00038108
Epoch [183/300], Train Loss: 0.000387
Validation Loss: 0.00038076
Epoch [184/300], Train Loss: 0.000389
Validation Loss: 0.00038967
Epoch [185/300], Train Loss: 0.000387
Validation Loss: 0.00037955
Epoch [186/300], Train Loss: 0.000386
Validation Loss: 0.00038005
Epoch [187/300], Train Loss: 0.000387
Validation Loss: 0.00037979
Epoch [188/300], Train Loss: 0.000386
Validation Loss: 0.00038007
Epoch [189/300], Train Loss: 0.000386
Validation Loss: 0.00038022
Epoch [190/300], Train Loss: 0.000385
Validation Loss: 0.00037795
Epoch [191/300], Train Loss: 0.000385
Validation Loss: 0.00037759
Epoch [192/300], Train Loss: 0.000384
Validation Loss: 0.00037870
Epoch [193/300], Train Loss: 0.000385
Validation Loss: 0.00037691
Epoch [194/300], Train Loss: 0.000385
Validation Loss: 0.00038144
Epoch [195/300], Train Loss: 0.000383
Validation Loss: 0.00037520
Epoch [196/300], Train Loss: 0.000383
Validation Loss: 0.00037684
Epoch [197/300], Train Loss: 0.000383
Validation Loss: 0.00038032
Epoch [198/300], Train Loss: 0.000383
Validation Loss: 0.00037668
Epoch [199/300], Train Loss: 0.000383
Validation Loss: 0.00037537
Epoch [200/300], Train Loss: 0.000383
Validation Loss: 0.00037678
Epoch [201/300], Train Loss: 0.000381
Validation Loss: 0.00037832
Epoch [202/300], Train Loss: 0.000382
Validation Loss: 0.00037608
Epoch [203/300], Train Loss: 0.000382
Validation Loss: 0.00037347
Epoch [204/300], Train Loss: 0.000381
Validation Loss: 0.00038276
Epoch [205/300], Train Loss: 0.000381
Validation Loss: 0.00037292
Epoch [206/300], Train Loss: 0.000380
Validation Loss: 0.00037337
Epoch [207/300], Train Loss: 0.000380
Validation Loss: 0.00037372
Epoch [208/300], Train Loss: 0.000380
Validation Loss: 0.00037185
Epoch [209/300], Train Loss: 0.000379
Validation Loss: 0.00037233
Epoch [210/300], Train Loss: 0.000378
Validation Loss: 0.00037118
Epoch [211/300], Train Loss: 0.000379
Validation Loss: 0.00037270
Epoch [212/300], Train Loss: 0.000379
Validation Loss: 0.00037009
Epoch [213/300], Train Loss: 0.000377
Validation Loss: 0.00037100
Epoch [214/300], Train Loss: 0.000377
Validation Loss: 0.00037037
Epoch [215/300], Train Loss: 0.000376
Validation Loss: 0.00037006
Epoch [216/300], Train Loss: 0.000377
Validation Loss: 0.00037018
Epoch [217/300], Train Loss: 0.000377
Validation Loss: 0.00037235
Epoch [218/300], Train Loss: 0.000377
Validation Loss: 0.00037494
Epoch [219/300], Train Loss: 0.000375
Validation Loss: 0.00037291
Epoch [220/300], Train Loss: 0.000376
Validation Loss: 0.00036964
Epoch [221/300], Train Loss: 0.000376
Validation Loss: 0.00036790
Epoch [222/300], Train Loss: 0.000375
Validation Loss: 0.00037126
Epoch [223/300], Train Loss: 0.000375
Validation Loss: 0.00036952
Epoch [224/300], Train Loss: 0.000374
Validation Loss: 0.00036695
Epoch [225/300], Train Loss: 0.000374
Validation Loss: 0.00036679
Epoch [226/300], Train Loss: 0.000374
Validation Loss: 0.00036799
Epoch [227/300], Train Loss: 0.000375
Validation Loss: 0.00036535
Epoch [228/300], Train Loss: 0.000373
Validation Loss: 0.00036542
Epoch [229/300], Train Loss: 0.000373
Validation Loss: 0.00036685
Epoch [230/300], Train Loss: 0.000372
Validation Loss: 0.00036551
Epoch [231/300], Train Loss: 0.000372
Validation Loss: 0.00036997
Epoch [232/300], Train Loss: 0.000372
Validation Loss: 0.00036511
Epoch [233/300], Train Loss: 0.000372
Validation Loss: 0.00036518
Epoch [234/300], Train Loss: 0.000372
Validation Loss: 0.00036654
Epoch [235/300], Train Loss: 0.000371
Validation Loss: 0.00036407
Epoch [236/300], Train Loss: 0.000372
Validation Loss: 0.00036633
Epoch [237/300], Train Loss: 0.000372
Validation Loss: 0.00036428
Epoch [238/300], Train Loss: 0.000370
Validation Loss: 0.00036349
Epoch [239/300], Train Loss: 0.000371
Validation Loss: 0.00036801
Epoch [240/300], Train Loss: 0.000371
Validation Loss: 0.00036346
Epoch [241/300], Train Loss: 0.000370
Validation Loss: 0.00036777
Epoch [242/300], Train Loss: 0.000370
Validation Loss: 0.00036466
Epoch [243/300], Train Loss: 0.000369
Validation Loss: 0.00036127
Epoch [244/300], Train Loss: 0.000369
Validation Loss: 0.00036415
Epoch [245/300], Train Loss: 0.000369
Validation Loss: 0.00036194
Epoch [246/300], Train Loss: 0.000368
Validation Loss: 0.00036177
Epoch [247/300], Train Loss: 0.000368
Validation Loss: 0.00036146
Epoch [248/300], Train Loss: 0.000371
Validation Loss: 0.00036296
Epoch [249/300], Train Loss: 0.000368
Validation Loss: 0.00036041
Epoch [250/300], Train Loss: 0.000367
Validation Loss: 0.00035971
Epoch [251/300], Train Loss: 0.000368
Validation Loss: 0.00036034
Epoch [252/300], Train Loss: 0.000368
Validation Loss: 0.00036381
Epoch [253/300], Train Loss: 0.000366
Validation Loss: 0.00036033
Epoch [254/300], Train Loss: 0.000367
Validation Loss: 0.00036094
Epoch [255/300], Train Loss: 0.000367
Validation Loss: 0.00035879
Epoch [256/300], Train Loss: 0.000366
Validation Loss: 0.00036029
Epoch [257/300], Train Loss: 0.000366
Validation Loss: 0.00035975
Epoch [258/300], Train Loss: 0.000366
Validation Loss: 0.00035818
Epoch [259/300], Train Loss: 0.000365
Validation Loss: 0.00035854
Epoch [260/300], Train Loss: 0.000365
Validation Loss: 0.00035791
Epoch [261/300], Train Loss: 0.000365
Validation Loss: 0.00035725
Epoch [262/300], Train Loss: 0.000365
Validation Loss: 0.00035624
Epoch [263/300], Train Loss: 0.000364
Validation Loss: 0.00035877
Epoch [264/300], Train Loss: 0.000364
Validation Loss: 0.00035645
Epoch [265/300], Train Loss: 0.000364
Validation Loss: 0.00035888
Epoch [266/300], Train Loss: 0.000364
Validation Loss: 0.00036293
Epoch [267/300], Train Loss: 0.000364
Validation Loss: 0.00035658
Epoch [268/300], Train Loss: 0.000363
Validation Loss: 0.00035690
Epoch [269/300], Train Loss: 0.000363
Validation Loss: 0.00035959
Epoch [270/300], Train Loss: 0.000364
Validation Loss: 0.00035519
Epoch [271/300], Train Loss: 0.000363
Validation Loss: 0.00036298
Epoch [272/300], Train Loss: 0.000362
Validation Loss: 0.00035493
Epoch [273/300], Train Loss: 0.000362
Validation Loss: 0.00035560
Epoch [274/300], Train Loss: 0.000362
Validation Loss: 0.00035396
Epoch [275/300], Train Loss: 0.000362
Validation Loss: 0.00035344
Epoch [276/300], Train Loss: 0.000362
Validation Loss: 0.00035468
Epoch [277/300], Train Loss: 0.000363
Validation Loss: 0.00035397
Epoch [278/300], Train Loss: 0.000361
Validation Loss: 0.00035318
Epoch [279/300], Train Loss: 0.000361
Validation Loss: 0.00035802
Epoch [280/300], Train Loss: 0.000361
Validation Loss: 0.00035301
Epoch [281/300], Train Loss: 0.000362
Validation Loss: 0.00036290
Epoch [282/300], Train Loss: 0.000361
Validation Loss: 0.00035281
Epoch [283/300], Train Loss: 0.000360
Validation Loss: 0.00035235
Epoch [284/300], Train Loss: 0.000360
Validation Loss: 0.00035225
Epoch [285/300], Train Loss: 0.000360
Validation Loss: 0.00035238
Epoch [286/300], Train Loss: 0.000359
Validation Loss: 0.00035545
Epoch [287/300], Train Loss: 0.000360
Validation Loss: 0.00035383
Epoch [288/300], Train Loss: 0.000359
Validation Loss: 0.00035878
Epoch [289/300], Train Loss: 0.000360
Validation Loss: 0.00035290
Epoch [290/300], Train Loss: 0.000359
Validation Loss: 0.00035092
Epoch [291/300], Train Loss: 0.000359
Validation Loss: 0.00035057
Epoch [292/300], Train Loss: 0.000359
Validation Loss: 0.00035679
Epoch [293/300], Train Loss: 0.000359
Validation Loss: 0.00035055
Epoch [294/300], Train Loss: 0.000358
Validation Loss: 0.00035091
Epoch [295/300], Train Loss: 0.000358
Validation Loss: 0.00034904
Epoch [296/300], Train Loss: 0.000358
Validation Loss: 0.00034889
Epoch [297/300], Train Loss: 0.000358
Validation Loss: 0.00034936
Epoch [298/300], Train Loss: 0.000358
Validation Loss: 0.00034902
Epoch [299/300], Train Loss: 0.000358
Validation Loss: 0.00034915
Epoch [300/300], Train Loss: 0.000358
Validation Loss: 0.00034985

Evaluating model for: Fridge
Run 10/72 completed in 5883.83 seconds with: {'MAE': np.float32(24.77575), 'MSE': np.float32(1192.0994), 'RMSE': np.float32(34.52679), 'SAE': np.float32(0.008957816), 'NDE': np.float32(0.5966115)}

Run 11/72: hidden=128, seq_len=360, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000702
Validation Loss: 0.00066532
Epoch [2/300], Train Loss: 0.000668
Validation Loss: 0.00066444
Epoch [3/300], Train Loss: 0.000665
Validation Loss: 0.00066072
Epoch [4/300], Train Loss: 0.000653
Validation Loss: 0.00064387
Epoch [5/300], Train Loss: 0.000628
Validation Loss: 0.00061752
Epoch [6/300], Train Loss: 0.000618
Validation Loss: 0.00061164
Epoch [7/300], Train Loss: 0.000611
Validation Loss: 0.00060570
Epoch [8/300], Train Loss: 0.000608
Validation Loss: 0.00060530
Epoch [9/300], Train Loss: 0.000606
Validation Loss: 0.00060314
Epoch [10/300], Train Loss: 0.000604
Validation Loss: 0.00060028
Epoch [11/300], Train Loss: 0.000602
Validation Loss: 0.00060093
Epoch [12/300], Train Loss: 0.000600
Validation Loss: 0.00059570
Epoch [13/300], Train Loss: 0.000599
Validation Loss: 0.00059483
Epoch [14/300], Train Loss: 0.000597
Validation Loss: 0.00059385
Epoch [15/300], Train Loss: 0.000596
Validation Loss: 0.00059350
Epoch [16/300], Train Loss: 0.000594
Validation Loss: 0.00059256
Epoch [17/300], Train Loss: 0.000593
Validation Loss: 0.00058948
Epoch [18/300], Train Loss: 0.000591
Validation Loss: 0.00058824
Epoch [19/300], Train Loss: 0.000590
Validation Loss: 0.00058538
Epoch [20/300], Train Loss: 0.000588
Validation Loss: 0.00058432
Epoch [21/300], Train Loss: 0.000587
Validation Loss: 0.00058423
Epoch [22/300], Train Loss: 0.000585
Validation Loss: 0.00058157
Epoch [23/300], Train Loss: 0.000583
Validation Loss: 0.00058137
Epoch [24/300], Train Loss: 0.000582
Validation Loss: 0.00057751
Epoch [25/300], Train Loss: 0.000580
Validation Loss: 0.00057931
Epoch [26/300], Train Loss: 0.000578
Validation Loss: 0.00057308
Epoch [27/300], Train Loss: 0.000575
Validation Loss: 0.00057233
Epoch [28/300], Train Loss: 0.000575
Validation Loss: 0.00058056
Epoch [29/300], Train Loss: 0.000573
Validation Loss: 0.00056885
Epoch [30/300], Train Loss: 0.000569
Validation Loss: 0.00056455
Epoch [31/300], Train Loss: 0.000567
Validation Loss: 0.00056627
Epoch [32/300], Train Loss: 0.000564
Validation Loss: 0.00055999
Epoch [33/300], Train Loss: 0.000562
Validation Loss: 0.00056399
Epoch [34/300], Train Loss: 0.000558
Validation Loss: 0.00055480
Epoch [35/300], Train Loss: 0.000557
Validation Loss: 0.00055388
Epoch [36/300], Train Loss: 0.000552
Validation Loss: 0.00054558
Epoch [37/300], Train Loss: 0.000548
Validation Loss: 0.00054079
Epoch [38/300], Train Loss: 0.000540
Validation Loss: 0.00053217
Epoch [39/300], Train Loss: 0.000532
Validation Loss: 0.00052092
Epoch [40/300], Train Loss: 0.000523
Validation Loss: 0.00051193
Epoch [41/300], Train Loss: 0.000511
Validation Loss: 0.00051013
Epoch [42/300], Train Loss: 0.000502
Validation Loss: 0.00048806
Epoch [43/300], Train Loss: 0.000488
Validation Loss: 0.00047570
Epoch [44/300], Train Loss: 0.000482
Validation Loss: 0.00047145
Epoch [45/300], Train Loss: 0.000477
Validation Loss: 0.00048013
Epoch [46/300], Train Loss: 0.000469
Validation Loss: 0.00046553
Epoch [47/300], Train Loss: 0.000465
Validation Loss: 0.00045567
Epoch [48/300], Train Loss: 0.000462
Validation Loss: 0.00045925
Epoch [49/300], Train Loss: 0.000459
Validation Loss: 0.00045134
Epoch [50/300], Train Loss: 0.000458
Validation Loss: 0.00045481
Epoch [51/300], Train Loss: 0.000456
Validation Loss: 0.00044693
Epoch [52/300], Train Loss: 0.000451
Validation Loss: 0.00044403
Epoch [53/300], Train Loss: 0.000450
Validation Loss: 0.00045005
Epoch [54/300], Train Loss: 0.000445
Validation Loss: 0.00043898
Epoch [55/300], Train Loss: 0.000444
Validation Loss: 0.00044081
Epoch [56/300], Train Loss: 0.000441
Validation Loss: 0.00043557
Epoch [57/300], Train Loss: 0.000440
Validation Loss: 0.00044078
Epoch [58/300], Train Loss: 0.000440
Validation Loss: 0.00043499
Epoch [59/300], Train Loss: 0.000435
Validation Loss: 0.00043088
Epoch [60/300], Train Loss: 0.000435
Validation Loss: 0.00043711
Epoch [61/300], Train Loss: 0.000434
Validation Loss: 0.00042795
Epoch [62/300], Train Loss: 0.000433
Validation Loss: 0.00042650
Epoch [63/300], Train Loss: 0.000433
Validation Loss: 0.00044713
Epoch [64/300], Train Loss: 0.000431
Validation Loss: 0.00042376
Epoch [65/300], Train Loss: 0.000427
Validation Loss: 0.00042088
Epoch [66/300], Train Loss: 0.000428
Validation Loss: 0.00042045
Epoch [67/300], Train Loss: 0.000430
Validation Loss: 0.00042816
Epoch [68/300], Train Loss: 0.000425
Validation Loss: 0.00042108
Epoch [69/300], Train Loss: 0.000423
Validation Loss: 0.00042686
Epoch [70/300], Train Loss: 0.000425
Validation Loss: 0.00042128
Epoch [71/300], Train Loss: 0.000423
Validation Loss: 0.00042237
Epoch [72/300], Train Loss: 0.000422
Validation Loss: 0.00043159
Epoch [73/300], Train Loss: 0.000421
Validation Loss: 0.00041348
Epoch [74/300], Train Loss: 0.000420
Validation Loss: 0.00041919
Epoch [75/300], Train Loss: 0.000419
Validation Loss: 0.00041582
Epoch [76/300], Train Loss: 0.000420
Validation Loss: 0.00041147
Epoch [77/300], Train Loss: 0.000415
Validation Loss: 0.00041257
Epoch [78/300], Train Loss: 0.000415
Validation Loss: 0.00040882
Epoch [79/300], Train Loss: 0.000415
Validation Loss: 0.00040716
Epoch [80/300], Train Loss: 0.000412
Validation Loss: 0.00041024
Epoch [81/300], Train Loss: 0.000412
Validation Loss: 0.00040712
Epoch [82/300], Train Loss: 0.000411
Validation Loss: 0.00041163
Epoch [83/300], Train Loss: 0.000411
Validation Loss: 0.00040342
Epoch [84/300], Train Loss: 0.000410
Validation Loss: 0.00040683
Epoch [85/300], Train Loss: 0.000408
Validation Loss: 0.00040170
Epoch [86/300], Train Loss: 0.000411
Validation Loss: 0.00040230
Epoch [87/300], Train Loss: 0.000407
Validation Loss: 0.00039833
Epoch [88/300], Train Loss: 0.000410
Validation Loss: 0.00039989
Epoch [89/300], Train Loss: 0.000410
Validation Loss: 0.00040014
Epoch [90/300], Train Loss: 0.000405
Validation Loss: 0.00039605
Epoch [91/300], Train Loss: 0.000405
Validation Loss: 0.00040221
Epoch [92/300], Train Loss: 0.000405
Validation Loss: 0.00040484
Epoch [93/300], Train Loss: 0.000405
Validation Loss: 0.00039900
Epoch [94/300], Train Loss: 0.000403
Validation Loss: 0.00039634
Epoch [95/300], Train Loss: 0.000401
Validation Loss: 0.00040002
Epoch [96/300], Train Loss: 0.000402
Validation Loss: 0.00039602
Epoch [97/300], Train Loss: 0.000401
Validation Loss: 0.00039793
Epoch [98/300], Train Loss: 0.000401
Validation Loss: 0.00040495
Epoch [99/300], Train Loss: 0.000403
Validation Loss: 0.00040141
Epoch [100/300], Train Loss: 0.000399
Validation Loss: 0.00039444
Epoch [101/300], Train Loss: 0.000398
Validation Loss: 0.00039120
Epoch [102/300], Train Loss: 0.000398
Validation Loss: 0.00039100
Epoch [103/300], Train Loss: 0.000397
Validation Loss: 0.00039343
Epoch [104/300], Train Loss: 0.000397
Validation Loss: 0.00039269
Epoch [105/300], Train Loss: 0.000396
Validation Loss: 0.00039087
Epoch [106/300], Train Loss: 0.000395
Validation Loss: 0.00038857
Epoch [107/300], Train Loss: 0.000396
Validation Loss: 0.00039443
Epoch [108/300], Train Loss: 0.000395
Validation Loss: 0.00038721
Epoch [109/300], Train Loss: 0.000394
Validation Loss: 0.00038871
Epoch [110/300], Train Loss: 0.000393
Validation Loss: 0.00038874
Epoch [111/300], Train Loss: 0.000393
Validation Loss: 0.00038797
Epoch [112/300], Train Loss: 0.000392
Validation Loss: 0.00038663
Epoch [113/300], Train Loss: 0.000393
Validation Loss: 0.00039223
Epoch [114/300], Train Loss: 0.000396
Validation Loss: 0.00038830
Epoch [115/300], Train Loss: 0.000397
Validation Loss: 0.00042207
Epoch [116/300], Train Loss: 0.000405
Validation Loss: 0.00038833
Epoch [117/300], Train Loss: 0.000390
Validation Loss: 0.00038722
Epoch [118/300], Train Loss: 0.000392
Validation Loss: 0.00039085
Epoch [119/300], Train Loss: 0.000390
Validation Loss: 0.00038300
Epoch [120/300], Train Loss: 0.000389
Validation Loss: 0.00038567
Epoch [121/300], Train Loss: 0.000390
Validation Loss: 0.00038702
Epoch [122/300], Train Loss: 0.000388
Validation Loss: 0.00038310
Epoch [123/300], Train Loss: 0.000388
Validation Loss: 0.00038448
Epoch [124/300], Train Loss: 0.000388
Validation Loss: 0.00038183
Epoch [125/300], Train Loss: 0.000389
Validation Loss: 0.00038169
Epoch [126/300], Train Loss: 0.000386
Validation Loss: 0.00038080
Epoch [127/300], Train Loss: 0.000388
Validation Loss: 0.00038416
Epoch [128/300], Train Loss: 0.000386
Validation Loss: 0.00038151
Epoch [129/300], Train Loss: 0.000386
Validation Loss: 0.00038281
Epoch [130/300], Train Loss: 0.000386
Validation Loss: 0.00038089
Epoch [131/300], Train Loss: 0.000386
Validation Loss: 0.00038868
Epoch [132/300], Train Loss: 0.000385
Validation Loss: 0.00038175
Epoch [133/300], Train Loss: 0.000456
Validation Loss: 0.00065094
Epoch [134/300], Train Loss: 0.000606
Validation Loss: 0.00055453
Epoch [135/300], Train Loss: 0.000550
Validation Loss: 0.00051623
Epoch [136/300], Train Loss: 0.000517
Validation Loss: 0.00048786
Early stopping triggered

Evaluating model for: Fridge
Run 11/72 completed in 2770.49 seconds with: {'MAE': np.float32(31.805178), 'MSE': np.float32(1630.6622), 'RMSE': np.float32(40.38146), 'SAE': np.float32(0.02410788), 'NDE': np.float32(0.69777834)}

Run 12/72: hidden=128, seq_len=360, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.001297
Validation Loss: 0.00068601
Epoch [2/300], Train Loss: 0.000713
Validation Loss: 0.00068497
Epoch [3/300], Train Loss: 0.000709
Validation Loss: 0.00068331
Epoch [4/300], Train Loss: 0.000704
Validation Loss: 0.00068179
Epoch [5/300], Train Loss: 0.000699
Validation Loss: 0.00067942
Epoch [6/300], Train Loss: 0.000693
Validation Loss: 0.00067689
Epoch [7/300], Train Loss: 0.000684
Validation Loss: 0.00065923
Epoch [8/300], Train Loss: 0.000660
Validation Loss: 0.00064508
Epoch [9/300], Train Loss: 0.000649
Validation Loss: 0.00063302
Epoch [10/300], Train Loss: 0.000642
Validation Loss: 0.00062820
Epoch [11/300], Train Loss: 0.000637
Validation Loss: 0.00062605
Epoch [12/300], Train Loss: 0.000633
Validation Loss: 0.00061793
Epoch [13/300], Train Loss: 0.000630
Validation Loss: 0.00061591
Epoch [14/300], Train Loss: 0.000628
Validation Loss: 0.00061311
Epoch [15/300], Train Loss: 0.000625
Validation Loss: 0.00061160
Epoch [16/300], Train Loss: 0.000623
Validation Loss: 0.00060870
Epoch [17/300], Train Loss: 0.000621
Validation Loss: 0.00060640
Epoch [18/300], Train Loss: 0.000619
Validation Loss: 0.00060541
Epoch [19/300], Train Loss: 0.000617
Validation Loss: 0.00060444
Epoch [20/300], Train Loss: 0.000616
Validation Loss: 0.00060954
Epoch [21/300], Train Loss: 0.000614
Validation Loss: 0.00060038
Epoch [22/300], Train Loss: 0.000611
Validation Loss: 0.00059821
Epoch [23/300], Train Loss: 0.000606
Validation Loss: 0.00059863
Epoch [24/300], Train Loss: 0.000602
Validation Loss: 0.00059478
Epoch [25/300], Train Loss: 0.000596
Validation Loss: 0.00059011
Epoch [26/300], Train Loss: 0.000592
Validation Loss: 0.00059008
Epoch [27/300], Train Loss: 0.000617
Validation Loss: 0.00061551
Epoch [28/300], Train Loss: 0.000619
Validation Loss: 0.00060395
Epoch [29/300], Train Loss: 0.000613
Validation Loss: 0.00060060
Epoch [30/300], Train Loss: 0.000610
Validation Loss: 0.00059835
Epoch [31/300], Train Loss: 0.000608
Validation Loss: 0.00060029
Epoch [32/300], Train Loss: 0.000606
Validation Loss: 0.00059585
Epoch [33/300], Train Loss: 0.000604
Validation Loss: 0.00059445
Epoch [34/300], Train Loss: 0.000602
Validation Loss: 0.00059376
Epoch [35/300], Train Loss: 0.000597
Validation Loss: 0.00058672
Epoch [36/300], Train Loss: 0.000604
Validation Loss: 0.00060530
Epoch [37/300], Train Loss: 0.000607
Validation Loss: 0.00060044
Epoch [38/300], Train Loss: 0.000603
Validation Loss: 0.00059529
Epoch [39/300], Train Loss: 0.000601
Validation Loss: 0.00059415
Epoch [40/300], Train Loss: 0.000600
Validation Loss: 0.00059326
Epoch [41/300], Train Loss: 0.000599
Validation Loss: 0.00059247
Epoch [42/300], Train Loss: 0.000598
Validation Loss: 0.00059366
Epoch [43/300], Train Loss: 0.000597
Validation Loss: 0.00059099
Epoch [44/300], Train Loss: 0.000593
Validation Loss: 0.00058550
Epoch [45/300], Train Loss: 0.000589
Validation Loss: 0.00057766
Epoch [46/300], Train Loss: 0.000591
Validation Loss: 0.00060204
Epoch [47/300], Train Loss: 0.000600
Validation Loss: 0.00058904
Epoch [48/300], Train Loss: 0.000584
Validation Loss: 0.00059103
Epoch [49/300], Train Loss: 0.000600
Validation Loss: 0.00057029
Epoch [50/300], Train Loss: 0.000587
Validation Loss: 0.00060611
Epoch [51/300], Train Loss: 0.000584
Validation Loss: 0.00055860
Epoch [52/300], Train Loss: 0.000561
Validation Loss: 0.00055799
Epoch [53/300], Train Loss: 0.000564
Validation Loss: 0.00056105
Epoch [54/300], Train Loss: 0.000562
Validation Loss: 0.00060855
Epoch [55/300], Train Loss: 0.000575
Validation Loss: 0.00057552
Epoch [56/300], Train Loss: 0.000564
Validation Loss: 0.00057010
Epoch [57/300], Train Loss: 0.000557
Validation Loss: 0.00055099
Epoch [58/300], Train Loss: 0.000552
Validation Loss: 0.00055054
Epoch [59/300], Train Loss: 0.000556
Validation Loss: 0.00054846
Epoch [60/300], Train Loss: 0.000601
Validation Loss: 0.00061531
Epoch [61/300], Train Loss: 0.000608
Validation Loss: 0.00059750
Epoch [62/300], Train Loss: 0.000601
Validation Loss: 0.00059452
Epoch [63/300], Train Loss: 0.000599
Validation Loss: 0.00059248
Epoch [64/300], Train Loss: 0.000590
Validation Loss: 0.00059424
Epoch [65/300], Train Loss: 0.000556
Validation Loss: 0.00055166
Epoch [66/300], Train Loss: 0.000545
Validation Loss: 0.00054232
Epoch [67/300], Train Loss: 0.000544
Validation Loss: 0.00053696
Epoch [68/300], Train Loss: 0.000549
Validation Loss: 0.00054485
Epoch [69/300], Train Loss: 0.000547
Validation Loss: 0.00054005
Epoch [70/300], Train Loss: 0.000543
Validation Loss: 0.00053892
Epoch [71/300], Train Loss: 0.000544
Validation Loss: 0.00053853
Epoch [72/300], Train Loss: 0.000548
Validation Loss: 0.00054700
Epoch [73/300], Train Loss: 0.000605
Validation Loss: 0.00058745
Epoch [74/300], Train Loss: 0.000592
Validation Loss: 0.00057512
Epoch [75/300], Train Loss: 0.000584
Validation Loss: 0.00057163
Epoch [76/300], Train Loss: 0.000581
Validation Loss: 0.00056989
Epoch [77/300], Train Loss: 0.000579
Validation Loss: 0.00056744
Early stopping triggered

Evaluating model for: Fridge
Run 12/72 completed in 1646.53 seconds with: {'MAE': np.float32(36.267353), 'MSE': np.float32(1944.2635), 'RMSE': np.float32(44.093803), 'SAE': np.float32(0.018486103), 'NDE': np.float32(0.76192653)}

Run 13/72: hidden=128, seq_len=360, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000681
Validation Loss: 0.00066000
Epoch [2/300], Train Loss: 0.000665
Validation Loss: 0.00065514
Epoch [3/300], Train Loss: 0.000660
Validation Loss: 0.00065052
Epoch [4/300], Train Loss: 0.000654
Validation Loss: 0.00064394
Epoch [5/300], Train Loss: 0.000648
Validation Loss: 0.00063440
Epoch [6/300], Train Loss: 0.000640
Validation Loss: 0.00062563
Epoch [7/300], Train Loss: 0.000632
Validation Loss: 0.00061624
Epoch [8/300], Train Loss: 0.000622
Validation Loss: 0.00061064
Epoch [9/300], Train Loss: 0.000613
Validation Loss: 0.00060139
Epoch [10/300], Train Loss: 0.000608
Validation Loss: 0.00060792
Epoch [11/300], Train Loss: 0.000608
Validation Loss: 0.00059727
Epoch [12/300], Train Loss: 0.000606
Validation Loss: 0.00059779
Epoch [13/300], Train Loss: 0.000604
Validation Loss: 0.00059704
Epoch [14/300], Train Loss: 0.000603
Validation Loss: 0.00059610
Epoch [15/300], Train Loss: 0.000604
Validation Loss: 0.00059377
Epoch [16/300], Train Loss: 0.000601
Validation Loss: 0.00059485
Epoch [17/300], Train Loss: 0.000601
Validation Loss: 0.00059387
Epoch [18/300], Train Loss: 0.000600
Validation Loss: 0.00059319
Epoch [19/300], Train Loss: 0.000600
Validation Loss: 0.00059466
Epoch [20/300], Train Loss: 0.000600
Validation Loss: 0.00059627
Epoch [21/300], Train Loss: 0.000599
Validation Loss: 0.00059117
Epoch [22/300], Train Loss: 0.000598
Validation Loss: 0.00059301
Epoch [23/300], Train Loss: 0.000598
Validation Loss: 0.00059277
Epoch [24/300], Train Loss: 0.000596
Validation Loss: 0.00059117
Epoch [25/300], Train Loss: 0.000596
Validation Loss: 0.00059215
Epoch [26/300], Train Loss: 0.000597
Validation Loss: 0.00059215
Epoch [27/300], Train Loss: 0.000596
Validation Loss: 0.00059220
Epoch [28/300], Train Loss: 0.000595
Validation Loss: 0.00058774
Epoch [29/300], Train Loss: 0.000594
Validation Loss: 0.00058886
Epoch [30/300], Train Loss: 0.000595
Validation Loss: 0.00058773
Epoch [31/300], Train Loss: 0.000593
Validation Loss: 0.00058868
Epoch [32/300], Train Loss: 0.000592
Validation Loss: 0.00058594
Epoch [33/300], Train Loss: 0.000592
Validation Loss: 0.00058626
Epoch [34/300], Train Loss: 0.000591
Validation Loss: 0.00058475
Epoch [35/300], Train Loss: 0.000590
Validation Loss: 0.00058476
Epoch [36/300], Train Loss: 0.000590
Validation Loss: 0.00058394
Epoch [37/300], Train Loss: 0.000589
Validation Loss: 0.00058348
Epoch [38/300], Train Loss: 0.000589
Validation Loss: 0.00058148
Epoch [39/300], Train Loss: 0.000588
Validation Loss: 0.00058143
Epoch [40/300], Train Loss: 0.000588
Validation Loss: 0.00058317
Epoch [41/300], Train Loss: 0.000588
Validation Loss: 0.00058219
Epoch [42/300], Train Loss: 0.000587
Validation Loss: 0.00058108
Epoch [43/300], Train Loss: 0.000586
Validation Loss: 0.00057851
Epoch [44/300], Train Loss: 0.000587
Validation Loss: 0.00057912
Epoch [45/300], Train Loss: 0.000585
Validation Loss: 0.00057824
Epoch [46/300], Train Loss: 0.000584
Validation Loss: 0.00057697
Epoch [47/300], Train Loss: 0.000584
Validation Loss: 0.00057511
Epoch [48/300], Train Loss: 0.000583
Validation Loss: 0.00057653
Epoch [49/300], Train Loss: 0.000582
Validation Loss: 0.00057704
Epoch [50/300], Train Loss: 0.000581
Validation Loss: 0.00057400
Epoch [51/300], Train Loss: 0.000581
Validation Loss: 0.00057516
Epoch [52/300], Train Loss: 0.000580
Validation Loss: 0.00057193
Epoch [53/300], Train Loss: 0.000579
Validation Loss: 0.00057178
Epoch [54/300], Train Loss: 0.000579
Validation Loss: 0.00057363
Epoch [55/300], Train Loss: 0.000579
Validation Loss: 0.00057188
Epoch [56/300], Train Loss: 0.000578
Validation Loss: 0.00057090
Epoch [57/300], Train Loss: 0.000578
Validation Loss: 0.00056908
Epoch [58/300], Train Loss: 0.000577
Validation Loss: 0.00057062
Epoch [59/300], Train Loss: 0.000577
Validation Loss: 0.00056691
Epoch [60/300], Train Loss: 0.000575
Validation Loss: 0.00056567
Epoch [61/300], Train Loss: 0.000574
Validation Loss: 0.00056582
Epoch [62/300], Train Loss: 0.000574
Validation Loss: 0.00056436
Epoch [63/300], Train Loss: 0.000574
Validation Loss: 0.00056494
Epoch [64/300], Train Loss: 0.000573
Validation Loss: 0.00056443
Epoch [65/300], Train Loss: 0.000573
Validation Loss: 0.00056403
Epoch [66/300], Train Loss: 0.000572
Validation Loss: 0.00056304
Epoch [67/300], Train Loss: 0.000572
Validation Loss: 0.00056310
Epoch [68/300], Train Loss: 0.000570
Validation Loss: 0.00056164
Epoch [69/300], Train Loss: 0.000570
Validation Loss: 0.00056150
Epoch [70/300], Train Loss: 0.000569
Validation Loss: 0.00055787
Epoch [71/300], Train Loss: 0.000569
Validation Loss: 0.00056101
Epoch [72/300], Train Loss: 0.000567
Validation Loss: 0.00055712
Epoch [73/300], Train Loss: 0.000567
Validation Loss: 0.00055593
Epoch [74/300], Train Loss: 0.000567
Validation Loss: 0.00055617
Epoch [75/300], Train Loss: 0.000567
Validation Loss: 0.00055622
Epoch [76/300], Train Loss: 0.000565
Validation Loss: 0.00055464
Epoch [77/300], Train Loss: 0.000564
Validation Loss: 0.00055470
Epoch [78/300], Train Loss: 0.000563
Validation Loss: 0.00055160
Epoch [79/300], Train Loss: 0.000563
Validation Loss: 0.00055040
Epoch [80/300], Train Loss: 0.000561
Validation Loss: 0.00055269
Epoch [81/300], Train Loss: 0.000562
Validation Loss: 0.00055090
Epoch [82/300], Train Loss: 0.000560
Validation Loss: 0.00055002
Epoch [83/300], Train Loss: 0.000561
Validation Loss: 0.00054825
Epoch [84/300], Train Loss: 0.000558
Validation Loss: 0.00054954
Epoch [85/300], Train Loss: 0.000558
Validation Loss: 0.00054710
Epoch [86/300], Train Loss: 0.000556
Validation Loss: 0.00054826
Epoch [87/300], Train Loss: 0.000557
Validation Loss: 0.00054548
Epoch [88/300], Train Loss: 0.000556
Validation Loss: 0.00054527
Epoch [89/300], Train Loss: 0.000555
Validation Loss: 0.00054720
Epoch [90/300], Train Loss: 0.000554
Validation Loss: 0.00054433
Epoch [91/300], Train Loss: 0.000553
Validation Loss: 0.00054280
Epoch [92/300], Train Loss: 0.000552
Validation Loss: 0.00054172
Epoch [93/300], Train Loss: 0.000550
Validation Loss: 0.00054349
Epoch [94/300], Train Loss: 0.000550
Validation Loss: 0.00054087
Epoch [95/300], Train Loss: 0.000549
Validation Loss: 0.00053925
Epoch [96/300], Train Loss: 0.000549
Validation Loss: 0.00053785
Epoch [97/300], Train Loss: 0.000546
Validation Loss: 0.00053971
Epoch [98/300], Train Loss: 0.000546
Validation Loss: 0.00053472
Epoch [99/300], Train Loss: 0.000545
Validation Loss: 0.00053615
Epoch [100/300], Train Loss: 0.000543
Validation Loss: 0.00053356
Epoch [101/300], Train Loss: 0.000543
Validation Loss: 0.00053576
Epoch [102/300], Train Loss: 0.000543
Validation Loss: 0.00053327
Epoch [103/300], Train Loss: 0.000540
Validation Loss: 0.00053063
Epoch [104/300], Train Loss: 0.000539
Validation Loss: 0.00052848
Epoch [105/300], Train Loss: 0.000540
Validation Loss: 0.00052530
Epoch [106/300], Train Loss: 0.000538
Validation Loss: 0.00052656
Epoch [107/300], Train Loss: 0.000537
Validation Loss: 0.00052488
Epoch [108/300], Train Loss: 0.000535
Validation Loss: 0.00052536
Epoch [109/300], Train Loss: 0.000534
Validation Loss: 0.00052410
Epoch [110/300], Train Loss: 0.000533
Validation Loss: 0.00052060
Epoch [111/300], Train Loss: 0.000530
Validation Loss: 0.00052009
Epoch [112/300], Train Loss: 0.000529
Validation Loss: 0.00051735
Epoch [113/300], Train Loss: 0.000529
Validation Loss: 0.00051728
Epoch [114/300], Train Loss: 0.000527
Validation Loss: 0.00051719
Epoch [115/300], Train Loss: 0.000527
Validation Loss: 0.00051496
Epoch [116/300], Train Loss: 0.000524
Validation Loss: 0.00051200
Epoch [117/300], Train Loss: 0.000527
Validation Loss: 0.00051718
Epoch [118/300], Train Loss: 0.000525
Validation Loss: 0.00050852
Epoch [119/300], Train Loss: 0.000524
Validation Loss: 0.00050618
Epoch [120/300], Train Loss: 0.000522
Validation Loss: 0.00050716
Epoch [121/300], Train Loss: 0.000522
Validation Loss: 0.00050649
Epoch [122/300], Train Loss: 0.000520
Validation Loss: 0.00050295
Epoch [123/300], Train Loss: 0.000519
Validation Loss: 0.00050880
Epoch [124/300], Train Loss: 0.000518
Validation Loss: 0.00050169
Epoch [125/300], Train Loss: 0.000517
Validation Loss: 0.00050158
Epoch [126/300], Train Loss: 0.000515
Validation Loss: 0.00050054
Epoch [127/300], Train Loss: 0.000517
Validation Loss: 0.00049723
Epoch [128/300], Train Loss: 0.000515
Validation Loss: 0.00049803
Epoch [129/300], Train Loss: 0.000513
Validation Loss: 0.00049668
Epoch [130/300], Train Loss: 0.000511
Validation Loss: 0.00049624
Epoch [131/300], Train Loss: 0.000511
Validation Loss: 0.00049156
Epoch [132/300], Train Loss: 0.000510
Validation Loss: 0.00049368
Epoch [133/300], Train Loss: 0.000509
Validation Loss: 0.00049794
Epoch [134/300], Train Loss: 0.000511
Validation Loss: 0.00048802
Epoch [135/300], Train Loss: 0.000505
Validation Loss: 0.00048490
Epoch [136/300], Train Loss: 0.000506
Validation Loss: 0.00048644
Epoch [137/300], Train Loss: 0.000504
Validation Loss: 0.00048575
Epoch [138/300], Train Loss: 0.000503
Validation Loss: 0.00048157
Epoch [139/300], Train Loss: 0.000503
Validation Loss: 0.00048277
Epoch [140/300], Train Loss: 0.000502
Validation Loss: 0.00048067
Epoch [141/300], Train Loss: 0.000501
Validation Loss: 0.00048089
Epoch [142/300], Train Loss: 0.000501
Validation Loss: 0.00047968
Epoch [143/300], Train Loss: 0.000499
Validation Loss: 0.00047897
Epoch [144/300], Train Loss: 0.000499
Validation Loss: 0.00048012
Epoch [145/300], Train Loss: 0.000497
Validation Loss: 0.00047963
Epoch [146/300], Train Loss: 0.000497
Validation Loss: 0.00049138
Epoch [147/300], Train Loss: 0.000498
Validation Loss: 0.00047995
Epoch [148/300], Train Loss: 0.000496
Validation Loss: 0.00047914
Epoch [149/300], Train Loss: 0.000496
Validation Loss: 0.00048186
Epoch [150/300], Train Loss: 0.000496
Validation Loss: 0.00047933
Epoch [151/300], Train Loss: 0.000493
Validation Loss: 0.00047816
Epoch [152/300], Train Loss: 0.000493
Validation Loss: 0.00047518
Epoch [153/300], Train Loss: 0.000492
Validation Loss: 0.00047376
Epoch [154/300], Train Loss: 0.000492
Validation Loss: 0.00047445
Epoch [155/300], Train Loss: 0.000491
Validation Loss: 0.00047857
Epoch [156/300], Train Loss: 0.000492
Validation Loss: 0.00047468
Epoch [157/300], Train Loss: 0.000491
Validation Loss: 0.00047619
Epoch [158/300], Train Loss: 0.000490
Validation Loss: 0.00047382
Epoch [159/300], Train Loss: 0.000489
Validation Loss: 0.00047212
Epoch [160/300], Train Loss: 0.000488
Validation Loss: 0.00047219
Epoch [161/300], Train Loss: 0.000489
Validation Loss: 0.00047206
Epoch [162/300], Train Loss: 0.000488
Validation Loss: 0.00047281
Epoch [163/300], Train Loss: 0.000487
Validation Loss: 0.00046947
Epoch [164/300], Train Loss: 0.000486
Validation Loss: 0.00047032
Epoch [165/300], Train Loss: 0.000486
Validation Loss: 0.00046938
Epoch [166/300], Train Loss: 0.000485
Validation Loss: 0.00047123
Epoch [167/300], Train Loss: 0.000485
Validation Loss: 0.00047144
Epoch [168/300], Train Loss: 0.000485
Validation Loss: 0.00046861
Epoch [169/300], Train Loss: 0.000483
Validation Loss: 0.00046814
Epoch [170/300], Train Loss: 0.000484
Validation Loss: 0.00046880
Epoch [171/300], Train Loss: 0.000485
Validation Loss: 0.00047110
Epoch [172/300], Train Loss: 0.000483
Validation Loss: 0.00046800
Epoch [173/300], Train Loss: 0.000482
Validation Loss: 0.00046641
Epoch [174/300], Train Loss: 0.000483
Validation Loss: 0.00046775
Epoch [175/300], Train Loss: 0.000483
Validation Loss: 0.00046770
Epoch [176/300], Train Loss: 0.000481
Validation Loss: 0.00046502
Epoch [177/300], Train Loss: 0.000480
Validation Loss: 0.00046518
Epoch [178/300], Train Loss: 0.000480
Validation Loss: 0.00046435
Epoch [179/300], Train Loss: 0.000480
Validation Loss: 0.00046791
Epoch [180/300], Train Loss: 0.000481
Validation Loss: 0.00046554
Epoch [181/300], Train Loss: 0.000479
Validation Loss: 0.00046284
Epoch [182/300], Train Loss: 0.000478
Validation Loss: 0.00046625
Epoch [183/300], Train Loss: 0.000478
Validation Loss: 0.00046540
Epoch [184/300], Train Loss: 0.000478
Validation Loss: 0.00046270
Epoch [185/300], Train Loss: 0.000477
Validation Loss: 0.00046525
Epoch [186/300], Train Loss: 0.000477
Validation Loss: 0.00046391
Epoch [187/300], Train Loss: 0.000476
Validation Loss: 0.00046219
Epoch [188/300], Train Loss: 0.000476
Validation Loss: 0.00046040
Epoch [189/300], Train Loss: 0.000475
Validation Loss: 0.00046413
Epoch [190/300], Train Loss: 0.000476
Validation Loss: 0.00046053
Epoch [191/300], Train Loss: 0.000475
Validation Loss: 0.00046062
Epoch [192/300], Train Loss: 0.000477
Validation Loss: 0.00046325
Epoch [193/300], Train Loss: 0.000476
Validation Loss: 0.00046028
Epoch [194/300], Train Loss: 0.000473
Validation Loss: 0.00045881
Epoch [195/300], Train Loss: 0.000473
Validation Loss: 0.00046022
Epoch [196/300], Train Loss: 0.000473
Validation Loss: 0.00045876
Epoch [197/300], Train Loss: 0.000472
Validation Loss: 0.00045880
Epoch [198/300], Train Loss: 0.000474
Validation Loss: 0.00045806
Epoch [199/300], Train Loss: 0.000472
Validation Loss: 0.00045834
Epoch [200/300], Train Loss: 0.000471
Validation Loss: 0.00045763
Epoch [201/300], Train Loss: 0.000472
Validation Loss: 0.00045846
Epoch [202/300], Train Loss: 0.000471
Validation Loss: 0.00045754
Epoch [203/300], Train Loss: 0.000470
Validation Loss: 0.00045727
Epoch [204/300], Train Loss: 0.000470
Validation Loss: 0.00045705
Epoch [205/300], Train Loss: 0.000469
Validation Loss: 0.00045590
Epoch [206/300], Train Loss: 0.000470
Validation Loss: 0.00045555
Epoch [207/300], Train Loss: 0.000468
Validation Loss: 0.00045549
Epoch [208/300], Train Loss: 0.000468
Validation Loss: 0.00045460
Epoch [209/300], Train Loss: 0.000467
Validation Loss: 0.00045489
Epoch [210/300], Train Loss: 0.000469
Validation Loss: 0.00045468
Epoch [211/300], Train Loss: 0.000467
Validation Loss: 0.00045532
Epoch [212/300], Train Loss: 0.000467
Validation Loss: 0.00045475
Epoch [213/300], Train Loss: 0.000467
Validation Loss: 0.00045457
Epoch [214/300], Train Loss: 0.000466
Validation Loss: 0.00045248
Epoch [215/300], Train Loss: 0.000465
Validation Loss: 0.00045436
Epoch [216/300], Train Loss: 0.000467
Validation Loss: 0.00045370
Epoch [217/300], Train Loss: 0.000465
Validation Loss: 0.00045276
Epoch [218/300], Train Loss: 0.000466
Validation Loss: 0.00045168
Epoch [219/300], Train Loss: 0.000464
Validation Loss: 0.00045117
Epoch [220/300], Train Loss: 0.000465
Validation Loss: 0.00045069
Epoch [221/300], Train Loss: 0.000465
Validation Loss: 0.00045008
Epoch [222/300], Train Loss: 0.000465
Validation Loss: 0.00045144
Epoch [223/300], Train Loss: 0.000465
Validation Loss: 0.00045214
Epoch [224/300], Train Loss: 0.000462
Validation Loss: 0.00044993
Epoch [225/300], Train Loss: 0.000463
Validation Loss: 0.00044948
Epoch [226/300], Train Loss: 0.000462
Validation Loss: 0.00044891
Epoch [227/300], Train Loss: 0.000462
Validation Loss: 0.00045041
Epoch [228/300], Train Loss: 0.000462
Validation Loss: 0.00044683
Epoch [229/300], Train Loss: 0.000462
Validation Loss: 0.00044753
Epoch [230/300], Train Loss: 0.000463
Validation Loss: 0.00044689
Epoch [231/300], Train Loss: 0.000461
Validation Loss: 0.00044584
Epoch [232/300], Train Loss: 0.000461
Validation Loss: 0.00044603
Epoch [233/300], Train Loss: 0.000460
Validation Loss: 0.00044751
Epoch [234/300], Train Loss: 0.000459
Validation Loss: 0.00044637
Epoch [235/300], Train Loss: 0.000459
Validation Loss: 0.00044617
Epoch [236/300], Train Loss: 0.000458
Validation Loss: 0.00044459
Epoch [237/300], Train Loss: 0.000459
Validation Loss: 0.00044585
Epoch [238/300], Train Loss: 0.000457
Validation Loss: 0.00044434
Epoch [239/300], Train Loss: 0.000459
Validation Loss: 0.00044610
Epoch [240/300], Train Loss: 0.000458
Validation Loss: 0.00044429
Epoch [241/300], Train Loss: 0.000457
Validation Loss: 0.00044540
Epoch [242/300], Train Loss: 0.000457
Validation Loss: 0.00044548
Epoch [243/300], Train Loss: 0.000457
Validation Loss: 0.00044387
Epoch [244/300], Train Loss: 0.000455
Validation Loss: 0.00044435
Epoch [245/300], Train Loss: 0.000455
Validation Loss: 0.00044188
Epoch [246/300], Train Loss: 0.000455
Validation Loss: 0.00044507
Epoch [247/300], Train Loss: 0.000456
Validation Loss: 0.00044244
Epoch [248/300], Train Loss: 0.000455
Validation Loss: 0.00044233
Epoch [249/300], Train Loss: 0.000454
Validation Loss: 0.00044296
Epoch [250/300], Train Loss: 0.000455
Validation Loss: 0.00044252
Epoch [251/300], Train Loss: 0.000454
Validation Loss: 0.00044176
Epoch [252/300], Train Loss: 0.000455
Validation Loss: 0.00044299
Epoch [253/300], Train Loss: 0.000454
Validation Loss: 0.00044236
Epoch [254/300], Train Loss: 0.000454
Validation Loss: 0.00044204
Epoch [255/300], Train Loss: 0.000453
Validation Loss: 0.00044122
Epoch [256/300], Train Loss: 0.000453
Validation Loss: 0.00044066
Epoch [257/300], Train Loss: 0.000459
Validation Loss: 0.00043975
Epoch [258/300], Train Loss: 0.000452
Validation Loss: 0.00043999
Epoch [259/300], Train Loss: 0.000451
Validation Loss: 0.00043901
Epoch [260/300], Train Loss: 0.000452
Validation Loss: 0.00043947
Epoch [261/300], Train Loss: 0.000451
Validation Loss: 0.00043844
Epoch [262/300], Train Loss: 0.000451
Validation Loss: 0.00043912
Epoch [263/300], Train Loss: 0.000451
Validation Loss: 0.00043812
Epoch [264/300], Train Loss: 0.000451
Validation Loss: 0.00043866
Epoch [265/300], Train Loss: 0.000451
Validation Loss: 0.00043898
Epoch [266/300], Train Loss: 0.000450
Validation Loss: 0.00043673
Epoch [267/300], Train Loss: 0.000450
Validation Loss: 0.00043672
Epoch [268/300], Train Loss: 0.000451
Validation Loss: 0.00043655
Epoch [269/300], Train Loss: 0.000452
Validation Loss: 0.00043904
Epoch [270/300], Train Loss: 0.000450
Validation Loss: 0.00043801
Epoch [271/300], Train Loss: 0.000449
Validation Loss: 0.00043665
Epoch [272/300], Train Loss: 0.000449
Validation Loss: 0.00043665
Epoch [273/300], Train Loss: 0.000448
Validation Loss: 0.00043622
Epoch [274/300], Train Loss: 0.000448
Validation Loss: 0.00043492
Epoch [275/300], Train Loss: 0.000449
Validation Loss: 0.00043529
Epoch [276/300], Train Loss: 0.000448
Validation Loss: 0.00043460
Epoch [277/300], Train Loss: 0.000449
Validation Loss: 0.00043597
Epoch [278/300], Train Loss: 0.000450
Validation Loss: 0.00043610
Epoch [279/300], Train Loss: 0.000457
Validation Loss: 0.00045315
Epoch [280/300], Train Loss: 0.000451
Validation Loss: 0.00043486
Epoch [281/300], Train Loss: 0.000447
Validation Loss: 0.00043563
Epoch [282/300], Train Loss: 0.000447
Validation Loss: 0.00043670
Epoch [283/300], Train Loss: 0.000446
Validation Loss: 0.00043337
Epoch [284/300], Train Loss: 0.000447
Validation Loss: 0.00043319
Epoch [285/300], Train Loss: 0.000457
Validation Loss: 0.00044031
Epoch [286/300], Train Loss: 0.000450
Validation Loss: 0.00043461
Epoch [287/300], Train Loss: 0.000446
Validation Loss: 0.00043348
Epoch [288/300], Train Loss: 0.000445
Validation Loss: 0.00043394
Epoch [289/300], Train Loss: 0.000446
Validation Loss: 0.00043274
Epoch [290/300], Train Loss: 0.000445
Validation Loss: 0.00043285
Epoch [291/300], Train Loss: 0.000451
Validation Loss: 0.00044049
Epoch [292/300], Train Loss: 0.000448
Validation Loss: 0.00043316
Epoch [293/300], Train Loss: 0.000445
Validation Loss: 0.00043314
Epoch [294/300], Train Loss: 0.000445
Validation Loss: 0.00043282
Epoch [295/300], Train Loss: 0.000445
Validation Loss: 0.00043402
Epoch [296/300], Train Loss: 0.000445
Validation Loss: 0.00043175
Epoch [297/300], Train Loss: 0.000445
Validation Loss: 0.00043324
Epoch [298/300], Train Loss: 0.000444
Validation Loss: 0.00043163
Epoch [299/300], Train Loss: 0.000444
Validation Loss: 0.00043226
Epoch [300/300], Train Loss: 0.000444
Validation Loss: 0.00043083

Evaluating model for: Fridge
Run 13/72 completed in 2857.40 seconds with: {'MAE': np.float32(29.567368), 'MSE': np.float32(1476.6472), 'RMSE': np.float32(38.427166), 'SAE': np.float32(0.017121056), 'NDE': np.float32(0.6698169)}

Run 14/72: hidden=128, seq_len=360, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000669
Validation Loss: 0.00065787
Epoch [2/300], Train Loss: 0.000660
Validation Loss: 0.00065240
Epoch [3/300], Train Loss: 0.000654
Validation Loss: 0.00064698
Epoch [4/300], Train Loss: 0.000648
Validation Loss: 0.00063950
Epoch [5/300], Train Loss: 0.000642
Validation Loss: 0.00062761
Epoch [6/300], Train Loss: 0.000631
Validation Loss: 0.00061605
Epoch [7/300], Train Loss: 0.000619
Validation Loss: 0.00060683
Epoch [8/300], Train Loss: 0.000611
Validation Loss: 0.00060354
Epoch [9/300], Train Loss: 0.000608
Validation Loss: 0.00059637
Epoch [10/300], Train Loss: 0.000605
Validation Loss: 0.00060661
Epoch [11/300], Train Loss: 0.000604
Validation Loss: 0.00059550
Epoch [12/300], Train Loss: 0.000605
Validation Loss: 0.00059599
Epoch [13/300], Train Loss: 0.000602
Validation Loss: 0.00059465
Epoch [14/300], Train Loss: 0.000602
Validation Loss: 0.00059537
Epoch [15/300], Train Loss: 0.000600
Validation Loss: 0.00059088
Epoch [16/300], Train Loss: 0.000599
Validation Loss: 0.00059164
Epoch [17/300], Train Loss: 0.000599
Validation Loss: 0.00059301
Epoch [18/300], Train Loss: 0.000598
Validation Loss: 0.00059105
Epoch [19/300], Train Loss: 0.000598
Validation Loss: 0.00059336
Epoch [20/300], Train Loss: 0.000598
Validation Loss: 0.00059221
Epoch [21/300], Train Loss: 0.000597
Validation Loss: 0.00058945
Epoch [22/300], Train Loss: 0.000598
Validation Loss: 0.00059240
Epoch [23/300], Train Loss: 0.000595
Validation Loss: 0.00059182
Epoch [24/300], Train Loss: 0.000595
Validation Loss: 0.00058995
Epoch [25/300], Train Loss: 0.000595
Validation Loss: 0.00059226
Epoch [26/300], Train Loss: 0.000597
Validation Loss: 0.00059182
Epoch [27/300], Train Loss: 0.000595
Validation Loss: 0.00059381
Epoch [28/300], Train Loss: 0.000594
Validation Loss: 0.00058823
Epoch [29/300], Train Loss: 0.000593
Validation Loss: 0.00058693
Epoch [30/300], Train Loss: 0.000592
Validation Loss: 0.00058595
Epoch [31/300], Train Loss: 0.000591
Validation Loss: 0.00058749
Epoch [32/300], Train Loss: 0.000589
Validation Loss: 0.00058397
Epoch [33/300], Train Loss: 0.000590
Validation Loss: 0.00058254
Epoch [34/300], Train Loss: 0.000588
Validation Loss: 0.00058219
Epoch [35/300], Train Loss: 0.000588
Validation Loss: 0.00058227
Epoch [36/300], Train Loss: 0.000588
Validation Loss: 0.00058160
Epoch [37/300], Train Loss: 0.000587
Validation Loss: 0.00058230
Epoch [38/300], Train Loss: 0.000586
Validation Loss: 0.00057971
Epoch [39/300], Train Loss: 0.000584
Validation Loss: 0.00057834
Epoch [40/300], Train Loss: 0.000584
Validation Loss: 0.00058433
Epoch [41/300], Train Loss: 0.000585
Validation Loss: 0.00058144
Epoch [42/300], Train Loss: 0.000583
Validation Loss: 0.00057970
Epoch [43/300], Train Loss: 0.000582
Validation Loss: 0.00057473
Epoch [44/300], Train Loss: 0.000583
Validation Loss: 0.00057718
Epoch [45/300], Train Loss: 0.000580
Validation Loss: 0.00057426
Epoch [46/300], Train Loss: 0.000579
Validation Loss: 0.00057348
Epoch [47/300], Train Loss: 0.000578
Validation Loss: 0.00057024
Epoch [48/300], Train Loss: 0.000578
Validation Loss: 0.00057488
Epoch [49/300], Train Loss: 0.000576
Validation Loss: 0.00057122
Epoch [50/300], Train Loss: 0.000576
Validation Loss: 0.00057046
Epoch [51/300], Train Loss: 0.000575
Validation Loss: 0.00057254
Epoch [52/300], Train Loss: 0.000574
Validation Loss: 0.00056698
Epoch [53/300], Train Loss: 0.000572
Validation Loss: 0.00056604
Epoch [54/300], Train Loss: 0.000572
Validation Loss: 0.00056407
Epoch [55/300], Train Loss: 0.000574
Validation Loss: 0.00056625
Epoch [56/300], Train Loss: 0.000571
Validation Loss: 0.00056225
Epoch [57/300], Train Loss: 0.000571
Validation Loss: 0.00056521
Epoch [58/300], Train Loss: 0.000569
Validation Loss: 0.00056131
Epoch [59/300], Train Loss: 0.000568
Validation Loss: 0.00056038
Epoch [60/300], Train Loss: 0.000566
Validation Loss: 0.00055779
Epoch [61/300], Train Loss: 0.000567
Validation Loss: 0.00055964
Epoch [62/300], Train Loss: 0.000564
Validation Loss: 0.00055697
Epoch [63/300], Train Loss: 0.000563
Validation Loss: 0.00055847
Epoch [64/300], Train Loss: 0.000562
Validation Loss: 0.00055494
Epoch [65/300], Train Loss: 0.000561
Validation Loss: 0.00055612
Epoch [66/300], Train Loss: 0.000562
Validation Loss: 0.00055265
Epoch [67/300], Train Loss: 0.000559
Validation Loss: 0.00055318
Epoch [68/300], Train Loss: 0.000558
Validation Loss: 0.00055484
Epoch [69/300], Train Loss: 0.000559
Validation Loss: 0.00055487
Epoch [70/300], Train Loss: 0.000556
Validation Loss: 0.00054881
Epoch [71/300], Train Loss: 0.000557
Validation Loss: 0.00055411
Epoch [72/300], Train Loss: 0.000556
Validation Loss: 0.00054830
Epoch [73/300], Train Loss: 0.000555
Validation Loss: 0.00054513
Epoch [74/300], Train Loss: 0.000553
Validation Loss: 0.00054476
Epoch [75/300], Train Loss: 0.000551
Validation Loss: 0.00054375
Epoch [76/300], Train Loss: 0.000552
Validation Loss: 0.00054341
Epoch [77/300], Train Loss: 0.000549
Validation Loss: 0.00054845
Epoch [78/300], Train Loss: 0.000548
Validation Loss: 0.00054242
Epoch [79/300], Train Loss: 0.000548
Validation Loss: 0.00054179
Epoch [80/300], Train Loss: 0.000547
Validation Loss: 0.00054482
Epoch [81/300], Train Loss: 0.000545
Validation Loss: 0.00054115
Epoch [82/300], Train Loss: 0.000542
Validation Loss: 0.00053325
Epoch [83/300], Train Loss: 0.000541
Validation Loss: 0.00053388
Epoch [84/300], Train Loss: 0.000539
Validation Loss: 0.00053265
Epoch [85/300], Train Loss: 0.000541
Validation Loss: 0.00052892
Epoch [86/300], Train Loss: 0.000536
Validation Loss: 0.00052947
Epoch [87/300], Train Loss: 0.000537
Validation Loss: 0.00052532
Epoch [88/300], Train Loss: 0.000534
Validation Loss: 0.00052421
Epoch [89/300], Train Loss: 0.000532
Validation Loss: 0.00052670
Epoch [90/300], Train Loss: 0.000529
Validation Loss: 0.00051977
Epoch [91/300], Train Loss: 0.000529
Validation Loss: 0.00051727
Epoch [92/300], Train Loss: 0.000527
Validation Loss: 0.00052100
Epoch [93/300], Train Loss: 0.000526
Validation Loss: 0.00051455
Epoch [94/300], Train Loss: 0.000529
Validation Loss: 0.00051921
Epoch [95/300], Train Loss: 0.000523
Validation Loss: 0.00051056
Epoch [96/300], Train Loss: 0.000522
Validation Loss: 0.00050994
Epoch [97/300], Train Loss: 0.000520
Validation Loss: 0.00050921
Epoch [98/300], Train Loss: 0.000523
Validation Loss: 0.00050651
Epoch [99/300], Train Loss: 0.000519
Validation Loss: 0.00050278
Epoch [100/300], Train Loss: 0.000515
Validation Loss: 0.00050234
Epoch [101/300], Train Loss: 0.000514
Validation Loss: 0.00050779
Epoch [102/300], Train Loss: 0.000515
Validation Loss: 0.00050212
Epoch [103/300], Train Loss: 0.000513
Validation Loss: 0.00049898
Epoch [104/300], Train Loss: 0.000512
Validation Loss: 0.00050266
Epoch [105/300], Train Loss: 0.000513
Validation Loss: 0.00049625
Epoch [106/300], Train Loss: 0.000507
Validation Loss: 0.00049658
Epoch [107/300], Train Loss: 0.000507
Validation Loss: 0.00049331
Epoch [108/300], Train Loss: 0.000506
Validation Loss: 0.00049761
Epoch [109/300], Train Loss: 0.000533
Validation Loss: 0.00061126
Epoch [110/300], Train Loss: 0.000601
Validation Loss: 0.00056297
Epoch [111/300], Train Loss: 0.000574
Validation Loss: 0.00055217
Epoch [112/300], Train Loss: 0.000563
Validation Loss: 0.00054227
Epoch [113/300], Train Loss: 0.000554
Validation Loss: 0.00053819
Epoch [114/300], Train Loss: 0.000549
Validation Loss: 0.00053442
Epoch [115/300], Train Loss: 0.000544
Validation Loss: 0.00053327
Epoch [116/300], Train Loss: 0.000539
Validation Loss: 0.00052546
Epoch [117/300], Train Loss: 0.000536
Validation Loss: 0.00052493
Early stopping triggered

Evaluating model for: Fridge
Run 14/72 completed in 1159.57 seconds with: {'MAE': np.float32(33.14839), 'MSE': np.float32(1773.6187), 'RMSE': np.float32(42.114353), 'SAE': np.float32(0.06208708), 'NDE': np.float32(0.7340875)}

Run 15/72: hidden=128, seq_len=360, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.001632
Validation Loss: 0.00071122
Epoch [2/300], Train Loss: 0.000712
Validation Loss: 0.00067885
Epoch [3/300], Train Loss: 0.000704
Validation Loss: 0.00067802
Epoch [4/300], Train Loss: 0.000702
Validation Loss: 0.00067727
Epoch [5/300], Train Loss: 0.000701
Validation Loss: 0.00067602
Epoch [6/300], Train Loss: 0.000698
Validation Loss: 0.00067485
Epoch [7/300], Train Loss: 0.000695
Validation Loss: 0.00067409
Epoch [8/300], Train Loss: 0.000692
Validation Loss: 0.00067167
Epoch [9/300], Train Loss: 0.000690
Validation Loss: 0.00067000
Epoch [10/300], Train Loss: 0.000686
Validation Loss: 0.00066855
Epoch [11/300], Train Loss: 0.000682
Validation Loss: 0.00066291
Epoch [12/300], Train Loss: 0.000678
Validation Loss: 0.00065465
Epoch [13/300], Train Loss: 0.000670
Validation Loss: 0.00064345
Epoch [14/300], Train Loss: 0.000658
Validation Loss: 0.00063316
Epoch [15/300], Train Loss: 0.000651
Validation Loss: 0.00062485
Epoch [16/300], Train Loss: 0.000645
Validation Loss: 0.00062075
Epoch [17/300], Train Loss: 0.000641
Validation Loss: 0.00061912
Epoch [18/300], Train Loss: 0.000637
Validation Loss: 0.00061653
Epoch [19/300], Train Loss: 0.000635
Validation Loss: 0.00061695
Epoch [20/300], Train Loss: 0.000633
Validation Loss: 0.00061480
Epoch [21/300], Train Loss: 0.000630
Validation Loss: 0.00060897
Epoch [22/300], Train Loss: 0.000628
Validation Loss: 0.00060988
Epoch [23/300], Train Loss: 0.000626
Validation Loss: 0.00060985
Epoch [24/300], Train Loss: 0.000624
Validation Loss: 0.00060532
Epoch [25/300], Train Loss: 0.000623
Validation Loss: 0.00060483
Epoch [26/300], Train Loss: 0.000623
Validation Loss: 0.00060661
Epoch [27/300], Train Loss: 0.000622
Validation Loss: 0.00060774
Epoch [28/300], Train Loss: 0.000620
Validation Loss: 0.00060101
Epoch [29/300], Train Loss: 0.000619
Validation Loss: 0.00060255
Epoch [30/300], Train Loss: 0.000619
Validation Loss: 0.00060129
Epoch [31/300], Train Loss: 0.000617
Validation Loss: 0.00060006
Epoch [32/300], Train Loss: 0.000615
Validation Loss: 0.00059868
Epoch [33/300], Train Loss: 0.000616
Validation Loss: 0.00059703
Epoch [34/300], Train Loss: 0.000615
Validation Loss: 0.00059701
Epoch [35/300], Train Loss: 0.000614
Validation Loss: 0.00059828
Epoch [36/300], Train Loss: 0.000613
Validation Loss: 0.00059711
Epoch [37/300], Train Loss: 0.000613
Validation Loss: 0.00059810
Epoch [38/300], Train Loss: 0.000612
Validation Loss: 0.00059530
Epoch [39/300], Train Loss: 0.000611
Validation Loss: 0.00059557
Epoch [40/300], Train Loss: 0.000612
Validation Loss: 0.00059986
Epoch [41/300], Train Loss: 0.000612
Validation Loss: 0.00060098
Epoch [42/300], Train Loss: 0.000610
Validation Loss: 0.00059392
Epoch [43/300], Train Loss: 0.000609
Validation Loss: 0.00059333
Epoch [44/300], Train Loss: 0.000610
Validation Loss: 0.00059492
Epoch [45/300], Train Loss: 0.000609
Validation Loss: 0.00059309
Epoch [46/300], Train Loss: 0.000608
Validation Loss: 0.00059322
Epoch [47/300], Train Loss: 0.000608
Validation Loss: 0.00059234
Epoch [48/300], Train Loss: 0.000608
Validation Loss: 0.00059195
Epoch [49/300], Train Loss: 0.000607
Validation Loss: 0.00059310
Epoch [50/300], Train Loss: 0.000606
Validation Loss: 0.00059248
Epoch [51/300], Train Loss: 0.000607
Validation Loss: 0.00059211
Epoch [52/300], Train Loss: 0.000605
Validation Loss: 0.00059132
Epoch [53/300], Train Loss: 0.000605
Validation Loss: 0.00059130
Epoch [54/300], Train Loss: 0.000605
Validation Loss: 0.00058942
Epoch [55/300], Train Loss: 0.000605
Validation Loss: 0.00059222
Epoch [56/300], Train Loss: 0.000604
Validation Loss: 0.00059008
Epoch [57/300], Train Loss: 0.000604
Validation Loss: 0.00059081
Epoch [58/300], Train Loss: 0.000604
Validation Loss: 0.00059195
Epoch [59/300], Train Loss: 0.000603
Validation Loss: 0.00058722
Epoch [60/300], Train Loss: 0.000602
Validation Loss: 0.00058663
Epoch [61/300], Train Loss: 0.000602
Validation Loss: 0.00058792
Epoch [62/300], Train Loss: 0.000601
Validation Loss: 0.00058706
Epoch [63/300], Train Loss: 0.000602
Validation Loss: 0.00058802
Epoch [64/300], Train Loss: 0.000601
Validation Loss: 0.00058644
Epoch [65/300], Train Loss: 0.000600
Validation Loss: 0.00058860
Epoch [66/300], Train Loss: 0.000600
Validation Loss: 0.00058624
Epoch [67/300], Train Loss: 0.000600
Validation Loss: 0.00058888
Epoch [68/300], Train Loss: 0.000599
Validation Loss: 0.00058556
Epoch [69/300], Train Loss: 0.000599
Validation Loss: 0.00058571
Epoch [70/300], Train Loss: 0.000598
Validation Loss: 0.00058400
Epoch [71/300], Train Loss: 0.000599
Validation Loss: 0.00058668
Epoch [72/300], Train Loss: 0.000598
Validation Loss: 0.00058461
Epoch [73/300], Train Loss: 0.000597
Validation Loss: 0.00058261
Epoch [74/300], Train Loss: 0.000597
Validation Loss: 0.00058460
Epoch [75/300], Train Loss: 0.000598
Validation Loss: 0.00058350
Epoch [76/300], Train Loss: 0.000596
Validation Loss: 0.00058311
Epoch [77/300], Train Loss: 0.000596
Validation Loss: 0.00058240
Epoch [78/300], Train Loss: 0.000595
Validation Loss: 0.00058108
Epoch [79/300], Train Loss: 0.000596
Validation Loss: 0.00058198
Epoch [80/300], Train Loss: 0.000594
Validation Loss: 0.00058318
Epoch [81/300], Train Loss: 0.000595
Validation Loss: 0.00058326
Epoch [82/300], Train Loss: 0.000594
Validation Loss: 0.00058202
Epoch [83/300], Train Loss: 0.000595
Validation Loss: 0.00058066
Epoch [84/300], Train Loss: 0.000593
Validation Loss: 0.00057987
Epoch [85/300], Train Loss: 0.000593
Validation Loss: 0.00057993
Epoch [86/300], Train Loss: 0.000592
Validation Loss: 0.00057891
Epoch [87/300], Train Loss: 0.000592
Validation Loss: 0.00057874
Epoch [88/300], Train Loss: 0.000591
Validation Loss: 0.00057994
Epoch [89/300], Train Loss: 0.000592
Validation Loss: 0.00058084
Epoch [90/300], Train Loss: 0.000591
Validation Loss: 0.00058084
Epoch [91/300], Train Loss: 0.000591
Validation Loss: 0.00058023
Epoch [92/300], Train Loss: 0.000591
Validation Loss: 0.00057930
Epoch [93/300], Train Loss: 0.000590
Validation Loss: 0.00057799
Epoch [94/300], Train Loss: 0.000590
Validation Loss: 0.00057695
Epoch [95/300], Train Loss: 0.000590
Validation Loss: 0.00057725
Epoch [96/300], Train Loss: 0.000590
Validation Loss: 0.00057582
Epoch [97/300], Train Loss: 0.000589
Validation Loss: 0.00057770
Epoch [98/300], Train Loss: 0.000588
Validation Loss: 0.00057633
Epoch [99/300], Train Loss: 0.000588
Validation Loss: 0.00057646
Epoch [100/300], Train Loss: 0.000587
Validation Loss: 0.00057646
Epoch [101/300], Train Loss: 0.000587
Validation Loss: 0.00057617
Epoch [102/300], Train Loss: 0.000587
Validation Loss: 0.00057491
Epoch [103/300], Train Loss: 0.000586
Validation Loss: 0.00057570
Epoch [104/300], Train Loss: 0.000585
Validation Loss: 0.00057334
Epoch [105/300], Train Loss: 0.000584
Validation Loss: 0.00057302
Epoch [106/300], Train Loss: 0.000584
Validation Loss: 0.00057314
Epoch [107/300], Train Loss: 0.000582
Validation Loss: 0.00057186
Epoch [108/300], Train Loss: 0.000582
Validation Loss: 0.00057234
Epoch [109/300], Train Loss: 0.000582
Validation Loss: 0.00057241
Epoch [110/300], Train Loss: 0.000579
Validation Loss: 0.00056864
Epoch [111/300], Train Loss: 0.000576
Validation Loss: 0.00056479
Epoch [112/300], Train Loss: 0.000570
Validation Loss: 0.00055412
Epoch [113/300], Train Loss: 0.000593
Validation Loss: 0.00060419
Epoch [114/300], Train Loss: 0.000612
Validation Loss: 0.00059315
Epoch [115/300], Train Loss: 0.000603
Validation Loss: 0.00058704
Epoch [116/300], Train Loss: 0.000594
Validation Loss: 0.00058079
Epoch [117/300], Train Loss: 0.000585
Validation Loss: 0.00057199
Epoch [118/300], Train Loss: 0.000570
Validation Loss: 0.00054986
Epoch [119/300], Train Loss: 0.000555
Validation Loss: 0.00054753
Epoch [120/300], Train Loss: 0.000555
Validation Loss: 0.00054413
Epoch [121/300], Train Loss: 0.000554
Validation Loss: 0.00054695
Epoch [122/300], Train Loss: 0.000550
Validation Loss: 0.00053896
Epoch [123/300], Train Loss: 0.000553
Validation Loss: 0.00055259
Epoch [124/300], Train Loss: 0.000550
Validation Loss: 0.00053920
Epoch [125/300], Train Loss: 0.000548
Validation Loss: 0.00053876
Epoch [126/300], Train Loss: 0.000546
Validation Loss: 0.00053806
Epoch [127/300], Train Loss: 0.000544
Validation Loss: 0.00053837
Epoch [128/300], Train Loss: 0.000546
Validation Loss: 0.00054286
Epoch [129/300], Train Loss: 0.000547
Validation Loss: 0.00053541
Epoch [130/300], Train Loss: 0.000544
Validation Loss: 0.00053572
Epoch [131/300], Train Loss: 0.000544
Validation Loss: 0.00053365
Epoch [132/300], Train Loss: 0.000543
Validation Loss: 0.00053629
Epoch [133/300], Train Loss: 0.000541
Validation Loss: 0.00053547
Epoch [134/300], Train Loss: 0.000547
Validation Loss: 0.00057442
Epoch [135/300], Train Loss: 0.000551
Validation Loss: 0.00054384
Epoch [136/300], Train Loss: 0.000547
Validation Loss: 0.00053206
Epoch [137/300], Train Loss: 0.000556
Validation Loss: 0.00055256
Epoch [138/300], Train Loss: 0.000546
Validation Loss: 0.00053257
Epoch [139/300], Train Loss: 0.000543
Validation Loss: 0.00053892
Epoch [140/300], Train Loss: 0.000570
Validation Loss: 0.00055203
Epoch [141/300], Train Loss: 0.000547
Validation Loss: 0.00052866
Epoch [142/300], Train Loss: 0.000540
Validation Loss: 0.00053018
Epoch [143/300], Train Loss: 0.000545
Validation Loss: 0.00054075
Epoch [144/300], Train Loss: 0.000545
Validation Loss: 0.00053762
Epoch [145/300], Train Loss: 0.000543
Validation Loss: 0.00053566
Epoch [146/300], Train Loss: 0.000546
Validation Loss: 0.00053816
Epoch [147/300], Train Loss: 0.000545
Validation Loss: 0.00054178
Epoch [148/300], Train Loss: 0.000544
Validation Loss: 0.00053367
Epoch [149/300], Train Loss: 0.000546
Validation Loss: 0.00061110
Epoch [150/300], Train Loss: 0.000586
Validation Loss: 0.00055223
Epoch [151/300], Train Loss: 0.000550
Validation Loss: 0.00053633
Early stopping triggered

Evaluating model for: Fridge
Run 15/72 completed in 1542.49 seconds with: {'MAE': np.float32(33.824997), 'MSE': np.float32(1794.6677), 'RMSE': np.float32(42.363518), 'SAE': np.float32(0.042533085), 'NDE': np.float32(0.73843074)}

Run 16/72: hidden=128, seq_len=360, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000701
Validation Loss: 0.00066201
Epoch [2/300], Train Loss: 0.000665
Validation Loss: 0.00066110
Epoch [3/300], Train Loss: 0.000663
Validation Loss: 0.00066045
Epoch [4/300], Train Loss: 0.000663
Validation Loss: 0.00066091
Epoch [5/300], Train Loss: 0.000662
Validation Loss: 0.00066013
Epoch [6/300], Train Loss: 0.000661
Validation Loss: 0.00065988
Epoch [7/300], Train Loss: 0.000660
Validation Loss: 0.00065950
Epoch [8/300], Train Loss: 0.000659
Validation Loss: 0.00065741
Epoch [9/300], Train Loss: 0.000655
Validation Loss: 0.00064301
Epoch [10/300], Train Loss: 0.000636
Validation Loss: 0.00062600
Epoch [11/300], Train Loss: 0.000622
Validation Loss: 0.00061332
Epoch [12/300], Train Loss: 0.000618
Validation Loss: 0.00060963
Epoch [13/300], Train Loss: 0.000614
Validation Loss: 0.00061027
Epoch [14/300], Train Loss: 0.000612
Validation Loss: 0.00060594
Epoch [15/300], Train Loss: 0.000614
Validation Loss: 0.00060407
Epoch [16/300], Train Loss: 0.000610
Validation Loss: 0.00060081
Epoch [17/300], Train Loss: 0.000609
Validation Loss: 0.00060237
Epoch [18/300], Train Loss: 0.000608
Validation Loss: 0.00060055
Epoch [19/300], Train Loss: 0.000608
Validation Loss: 0.00060186
Epoch [20/300], Train Loss: 0.000608
Validation Loss: 0.00060083
Epoch [21/300], Train Loss: 0.000606
Validation Loss: 0.00059914
Epoch [22/300], Train Loss: 0.000606
Validation Loss: 0.00060142
Epoch [23/300], Train Loss: 0.000606
Validation Loss: 0.00060047
Epoch [24/300], Train Loss: 0.000604
Validation Loss: 0.00059900
Epoch [25/300], Train Loss: 0.000604
Validation Loss: 0.00059977
Epoch [26/300], Train Loss: 0.000605
Validation Loss: 0.00060129
Epoch [27/300], Train Loss: 0.000605
Validation Loss: 0.00060493
Epoch [28/300], Train Loss: 0.000604
Validation Loss: 0.00059671
Epoch [29/300], Train Loss: 0.000604
Validation Loss: 0.00059676
Epoch [30/300], Train Loss: 0.000604
Validation Loss: 0.00059798
Epoch [31/300], Train Loss: 0.000602
Validation Loss: 0.00059749
Epoch [32/300], Train Loss: 0.000601
Validation Loss: 0.00059568
Epoch [33/300], Train Loss: 0.000602
Validation Loss: 0.00059428
Epoch [34/300], Train Loss: 0.000601
Validation Loss: 0.00059498
Epoch [35/300], Train Loss: 0.000601
Validation Loss: 0.00059587
Epoch [36/300], Train Loss: 0.000601
Validation Loss: 0.00059614
Epoch [37/300], Train Loss: 0.000601
Validation Loss: 0.00059639
Epoch [38/300], Train Loss: 0.000600
Validation Loss: 0.00059294
Epoch [39/300], Train Loss: 0.000600
Validation Loss: 0.00059308
Epoch [40/300], Train Loss: 0.000600
Validation Loss: 0.00059650
Epoch [41/300], Train Loss: 0.000601
Validation Loss: 0.00059498
Epoch [42/300], Train Loss: 0.000599
Validation Loss: 0.00059226
Epoch [43/300], Train Loss: 0.000598
Validation Loss: 0.00059120
Epoch [44/300], Train Loss: 0.000599
Validation Loss: 0.00059167
Epoch [45/300], Train Loss: 0.000598
Validation Loss: 0.00059390
Epoch [46/300], Train Loss: 0.000597
Validation Loss: 0.00059142
Epoch [47/300], Train Loss: 0.000597
Validation Loss: 0.00058952
Epoch [48/300], Train Loss: 0.000597
Validation Loss: 0.00058931
Epoch [49/300], Train Loss: 0.000596
Validation Loss: 0.00059167
Epoch [50/300], Train Loss: 0.000595
Validation Loss: 0.00058889
Epoch [51/300], Train Loss: 0.000596
Validation Loss: 0.00059032
Epoch [52/300], Train Loss: 0.000594
Validation Loss: 0.00058752
Epoch [53/300], Train Loss: 0.000594
Validation Loss: 0.00058773
Epoch [54/300], Train Loss: 0.000594
Validation Loss: 0.00058658
Epoch [55/300], Train Loss: 0.000594
Validation Loss: 0.00058749
Epoch [56/300], Train Loss: 0.000593
Validation Loss: 0.00058460
Epoch [57/300], Train Loss: 0.000592
Validation Loss: 0.00058415
Epoch [58/300], Train Loss: 0.000592
Validation Loss: 0.00058607
Epoch [59/300], Train Loss: 0.000591
Validation Loss: 0.00058298
Epoch [60/300], Train Loss: 0.000590
Validation Loss: 0.00058033
Epoch [61/300], Train Loss: 0.000589
Validation Loss: 0.00058047
Epoch [62/300], Train Loss: 0.000587
Validation Loss: 0.00057997
Epoch [63/300], Train Loss: 0.000587
Validation Loss: 0.00057758
Epoch [64/300], Train Loss: 0.000587
Validation Loss: 0.00057593
Epoch [65/300], Train Loss: 0.000585
Validation Loss: 0.00057706
Epoch [66/300], Train Loss: 0.000584
Validation Loss: 0.00057524
Epoch [67/300], Train Loss: 0.000584
Validation Loss: 0.00057397
Epoch [68/300], Train Loss: 0.000581
Validation Loss: 0.00057419
Epoch [69/300], Train Loss: 0.000580
Validation Loss: 0.00056982
Epoch [70/300], Train Loss: 0.000577
Validation Loss: 0.00056396
Epoch [71/300], Train Loss: 0.000577
Validation Loss: 0.00056577
Epoch [72/300], Train Loss: 0.000574
Validation Loss: 0.00056313
Epoch [73/300], Train Loss: 0.000572
Validation Loss: 0.00055905
Epoch [74/300], Train Loss: 0.000571
Validation Loss: 0.00055930
Epoch [75/300], Train Loss: 0.000571
Validation Loss: 0.00056226
Epoch [76/300], Train Loss: 0.000570
Validation Loss: 0.00055721
Epoch [77/300], Train Loss: 0.000567
Validation Loss: 0.00055475
Epoch [78/300], Train Loss: 0.000565
Validation Loss: 0.00055535
Epoch [79/300], Train Loss: 0.000566
Validation Loss: 0.00055232
Epoch [80/300], Train Loss: 0.000563
Validation Loss: 0.00055462
Epoch [81/300], Train Loss: 0.000563
Validation Loss: 0.00055143
Epoch [82/300], Train Loss: 0.000558
Validation Loss: 0.00055040
Epoch [83/300], Train Loss: 0.000559
Validation Loss: 0.00054609
Epoch [84/300], Train Loss: 0.000556
Validation Loss: 0.00054696
Epoch [85/300], Train Loss: 0.000554
Validation Loss: 0.00054494
Epoch [86/300], Train Loss: 0.000551
Validation Loss: 0.00054309
Epoch [87/300], Train Loss: 0.000550
Validation Loss: 0.00053940
Epoch [88/300], Train Loss: 0.000547
Validation Loss: 0.00054174
Epoch [89/300], Train Loss: 0.000547
Validation Loss: 0.00054132
Epoch [90/300], Train Loss: 0.000545
Validation Loss: 0.00054120
Epoch [91/300], Train Loss: 0.000544
Validation Loss: 0.00053886
Epoch [92/300], Train Loss: 0.000543
Validation Loss: 0.00053974
Epoch [93/300], Train Loss: 0.000542
Validation Loss: 0.00053493
Epoch [94/300], Train Loss: 0.000541
Validation Loss: 0.00053632
Epoch [95/300], Train Loss: 0.000540
Validation Loss: 0.00053280
Epoch [96/300], Train Loss: 0.000539
Validation Loss: 0.00053238
Epoch [97/300], Train Loss: 0.000539
Validation Loss: 0.00053376
Epoch [98/300], Train Loss: 0.000537
Validation Loss: 0.00052932
Epoch [99/300], Train Loss: 0.000538
Validation Loss: 0.00052733
Epoch [100/300], Train Loss: 0.000536
Validation Loss: 0.00053169
Epoch [101/300], Train Loss: 0.000534
Validation Loss: 0.00052960
Epoch [102/300], Train Loss: 0.000534
Validation Loss: 0.00052665
Epoch [103/300], Train Loss: 0.000532
Validation Loss: 0.00052668
Epoch [104/300], Train Loss: 0.000531
Validation Loss: 0.00052501
Epoch [105/300], Train Loss: 0.000532
Validation Loss: 0.00052636
Epoch [106/300], Train Loss: 0.000529
Validation Loss: 0.00052551
Epoch [107/300], Train Loss: 0.000529
Validation Loss: 0.00052044
Epoch [108/300], Train Loss: 0.000528
Validation Loss: 0.00051861
Epoch [109/300], Train Loss: 0.000526
Validation Loss: 0.00053038
Epoch [110/300], Train Loss: 0.000525
Validation Loss: 0.00051879
Epoch [111/300], Train Loss: 0.000524
Validation Loss: 0.00051765
Epoch [112/300], Train Loss: 0.000522
Validation Loss: 0.00051545
Epoch [113/300], Train Loss: 0.000521
Validation Loss: 0.00051352
Epoch [114/300], Train Loss: 0.000521
Validation Loss: 0.00051382
Epoch [115/300], Train Loss: 0.000520
Validation Loss: 0.00051543
Epoch [116/300], Train Loss: 0.000519
Validation Loss: 0.00051153
Epoch [117/300], Train Loss: 0.000520
Validation Loss: 0.00051228
Epoch [118/300], Train Loss: 0.000519
Validation Loss: 0.00050830
Epoch [119/300], Train Loss: 0.000516
Validation Loss: 0.00051048
Epoch [120/300], Train Loss: 0.000514
Validation Loss: 0.00051580
Epoch [121/300], Train Loss: 0.000513
Validation Loss: 0.00050652
Epoch [122/300], Train Loss: 0.000513
Validation Loss: 0.00050464
Epoch [123/300], Train Loss: 0.000514
Validation Loss: 0.00050788
Epoch [124/300], Train Loss: 0.000510
Validation Loss: 0.00050535
Epoch [125/300], Train Loss: 0.000511
Validation Loss: 0.00050555
Epoch [126/300], Train Loss: 0.000509
Validation Loss: 0.00050353
Epoch [127/300], Train Loss: 0.000511
Validation Loss: 0.00050169
Epoch [128/300], Train Loss: 0.000508
Validation Loss: 0.00050475
Epoch [129/300], Train Loss: 0.000508
Validation Loss: 0.00049708
Epoch [130/300], Train Loss: 0.000506
Validation Loss: 0.00049509
Epoch [131/300], Train Loss: 0.000506
Validation Loss: 0.00049605
Epoch [132/300], Train Loss: 0.000503
Validation Loss: 0.00049852
Epoch [133/300], Train Loss: 0.000509
Validation Loss: 0.00049286
Epoch [134/300], Train Loss: 0.000502
Validation Loss: 0.00049271
Epoch [135/300], Train Loss: 0.000501
Validation Loss: 0.00049207
Epoch [136/300], Train Loss: 0.000502
Validation Loss: 0.00049311
Epoch [137/300], Train Loss: 0.000501
Validation Loss: 0.00049312
Epoch [138/300], Train Loss: 0.000502
Validation Loss: 0.00049505
Epoch [139/300], Train Loss: 0.000500
Validation Loss: 0.00048985
Epoch [140/300], Train Loss: 0.000499
Validation Loss: 0.00049370
Epoch [141/300], Train Loss: 0.000499
Validation Loss: 0.00048891
Epoch [142/300], Train Loss: 0.000499
Validation Loss: 0.00048918
Epoch [143/300], Train Loss: 0.000500
Validation Loss: 0.00048718
Epoch [144/300], Train Loss: 0.000498
Validation Loss: 0.00048996
Epoch [145/300], Train Loss: 0.000496
Validation Loss: 0.00048774
Epoch [146/300], Train Loss: 0.000497
Validation Loss: 0.00048858
Epoch [147/300], Train Loss: 0.000496
Validation Loss: 0.00048414
Epoch [148/300], Train Loss: 0.000498
Validation Loss: 0.00048583
Epoch [149/300], Train Loss: 0.000560
Validation Loss: 0.00052680
Epoch [150/300], Train Loss: 0.000530
Validation Loss: 0.00051261
Epoch [151/300], Train Loss: 0.000502
Validation Loss: 0.00048407
Epoch [152/300], Train Loss: 0.000494
Validation Loss: 0.00048385
Epoch [153/300], Train Loss: 0.000492
Validation Loss: 0.00049390
Epoch [154/300], Train Loss: 0.000502
Validation Loss: 0.00049382
Epoch [155/300], Train Loss: 0.000495
Validation Loss: 0.00048255
Epoch [156/300], Train Loss: 0.000493
Validation Loss: 0.00048429
Epoch [157/300], Train Loss: 0.000490
Validation Loss: 0.00048281
Epoch [158/300], Train Loss: 0.000494
Validation Loss: 0.00048387
Epoch [159/300], Train Loss: 0.000492
Validation Loss: 0.00047985
Epoch [160/300], Train Loss: 0.000491
Validation Loss: 0.00048483
Epoch [161/300], Train Loss: 0.000498
Validation Loss: 0.00048644
Epoch [162/300], Train Loss: 0.000494
Validation Loss: 0.00048118
Epoch [163/300], Train Loss: 0.000492
Validation Loss: 0.00048145
Epoch [164/300], Train Loss: 0.000488
Validation Loss: 0.00047911
Epoch [165/300], Train Loss: 0.000489
Validation Loss: 0.00047953
Epoch [166/300], Train Loss: 0.000489
Validation Loss: 0.00048666
Epoch [167/300], Train Loss: 0.000489
Validation Loss: 0.00047972
Epoch [168/300], Train Loss: 0.000487
Validation Loss: 0.00047649
Epoch [169/300], Train Loss: 0.000487
Validation Loss: 0.00047805
Epoch [170/300], Train Loss: 0.000488
Validation Loss: 0.00047782
Epoch [171/300], Train Loss: 0.000486
Validation Loss: 0.00047610
Epoch [172/300], Train Loss: 0.000486
Validation Loss: 0.00047686
Epoch [173/300], Train Loss: 0.000485
Validation Loss: 0.00047744
Epoch [174/300], Train Loss: 0.000485
Validation Loss: 0.00047480
Epoch [175/300], Train Loss: 0.000620
Validation Loss: 0.00056278
Epoch [176/300], Train Loss: 0.000556
Validation Loss: 0.00053513
Epoch [177/300], Train Loss: 0.000541
Validation Loss: 0.00052779
Epoch [178/300], Train Loss: 0.000536
Validation Loss: 0.00052526
Epoch [179/300], Train Loss: 0.000533
Validation Loss: 0.00052423
Epoch [180/300], Train Loss: 0.000530
Validation Loss: 0.00052234
Epoch [181/300], Train Loss: 0.000529
Validation Loss: 0.00052165
Epoch [182/300], Train Loss: 0.000528
Validation Loss: 0.00052216
Epoch [183/300], Train Loss: 0.000527
Validation Loss: 0.00052019
Epoch [184/300], Train Loss: 0.000525
Validation Loss: 0.00052014
Early stopping triggered

Evaluating model for: Fridge
Run 16/72 completed in 1930.50 seconds with: {'MAE': np.float32(33.069065), 'MSE': np.float32(1751.0853), 'RMSE': np.float32(41.84597), 'SAE': np.float32(0.029603068), 'NDE': np.float32(0.7294096)}

Run 17/72: hidden=128, seq_len=720, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.001304
Validation Loss: 0.00068685
Epoch [2/300], Train Loss: 0.000678
Validation Loss: 0.00066106
Epoch [3/300], Train Loss: 0.000665
Validation Loss: 0.00066015
Epoch [4/300], Train Loss: 0.000664
Validation Loss: 0.00065927
Epoch [5/300], Train Loss: 0.000664
Validation Loss: 0.00065819
Epoch [6/300], Train Loss: 0.000666
Validation Loss: 0.00065694
Epoch [7/300], Train Loss: 0.000652
Validation Loss: 0.00065544
Epoch [8/300], Train Loss: 0.000657
Validation Loss: 0.00065402
Epoch [9/300], Train Loss: 0.000654
Validation Loss: 0.00065236
Epoch [10/300], Train Loss: 0.000653
Validation Loss: 0.00065053
Epoch [11/300], Train Loss: 0.000650
Validation Loss: 0.00064798
Epoch [12/300], Train Loss: 0.000650
Validation Loss: 0.00064559
Epoch [13/300], Train Loss: 0.000650
Validation Loss: 0.00064199
Epoch [14/300], Train Loss: 0.000644
Validation Loss: 0.00063812
Epoch [15/300], Train Loss: 0.000645
Validation Loss: 0.00063523
Epoch [16/300], Train Loss: 0.000636
Validation Loss: 0.00063001
Epoch [17/300], Train Loss: 0.000634
Validation Loss: 0.00062583
Epoch [18/300], Train Loss: 0.000619
Validation Loss: 0.00062044
Epoch [19/300], Train Loss: 0.000618
Validation Loss: 0.00061445
Epoch [20/300], Train Loss: 0.000611
Validation Loss: 0.00061185
Epoch [21/300], Train Loss: 0.000612
Validation Loss: 0.00060905
Epoch [22/300], Train Loss: 0.000612
Validation Loss: 0.00060821
Epoch [23/300], Train Loss: 0.000619
Validation Loss: 0.00060627
Epoch [24/300], Train Loss: 0.000613
Validation Loss: 0.00060489
Epoch [25/300], Train Loss: 0.000609
Validation Loss: 0.00060657
Epoch [26/300], Train Loss: 0.000608
Validation Loss: 0.00060305
Epoch [27/300], Train Loss: 0.000610
Validation Loss: 0.00060420
Epoch [28/300], Train Loss: 0.000611
Validation Loss: 0.00060150
Epoch [29/300], Train Loss: 0.000607
Validation Loss: 0.00060037
Epoch [30/300], Train Loss: 0.000610
Validation Loss: 0.00060021
Epoch [31/300], Train Loss: 0.000610
Validation Loss: 0.00059941
Epoch [32/300], Train Loss: 0.000605
Validation Loss: 0.00060439
Epoch [33/300], Train Loss: 0.000607
Validation Loss: 0.00059878
Epoch [34/300], Train Loss: 0.000605
Validation Loss: 0.00059973
Epoch [35/300], Train Loss: 0.000606
Validation Loss: 0.00059851
Epoch [36/300], Train Loss: 0.000609
Validation Loss: 0.00060087
Epoch [37/300], Train Loss: 0.000601
Validation Loss: 0.00059982
Epoch [38/300], Train Loss: 0.000605
Validation Loss: 0.00059753
Epoch [39/300], Train Loss: 0.000602
Validation Loss: 0.00059635
Epoch [40/300], Train Loss: 0.000598
Validation Loss: 0.00059790
Epoch [41/300], Train Loss: 0.000601
Validation Loss: 0.00059590
Epoch [42/300], Train Loss: 0.000601
Validation Loss: 0.00059530
Epoch [43/300], Train Loss: 0.000597
Validation Loss: 0.00059464
Epoch [44/300], Train Loss: 0.000598
Validation Loss: 0.00059452
Epoch [45/300], Train Loss: 0.000600
Validation Loss: 0.00059425
Epoch [46/300], Train Loss: 0.000596
Validation Loss: 0.00059366
Epoch [47/300], Train Loss: 0.000595
Validation Loss: 0.00059347
Epoch [48/300], Train Loss: 0.000601
Validation Loss: 0.00059318
Epoch [49/300], Train Loss: 0.000594
Validation Loss: 0.00059229
Epoch [50/300], Train Loss: 0.000593
Validation Loss: 0.00059574
Epoch [51/300], Train Loss: 0.000592
Validation Loss: 0.00059136
Epoch [52/300], Train Loss: 0.000595
Validation Loss: 0.00059132
Epoch [53/300], Train Loss: 0.000595
Validation Loss: 0.00059036
Epoch [54/300], Train Loss: 0.000587
Validation Loss: 0.00059050
Epoch [55/300], Train Loss: 0.000592
Validation Loss: 0.00059005
Epoch [56/300], Train Loss: 0.000590
Validation Loss: 0.00059197
Epoch [57/300], Train Loss: 0.000585
Validation Loss: 0.00059005
Epoch [58/300], Train Loss: 0.000596
Validation Loss: 0.00058872
Epoch [59/300], Train Loss: 0.000593
Validation Loss: 0.00058925
Epoch [60/300], Train Loss: 0.000596
Validation Loss: 0.00058805
Epoch [61/300], Train Loss: 0.000592
Validation Loss: 0.00058881
Epoch [62/300], Train Loss: 0.000592
Validation Loss: 0.00058792
Epoch [63/300], Train Loss: 0.000591
Validation Loss: 0.00058683
Epoch [64/300], Train Loss: 0.000588
Validation Loss: 0.00058705
Epoch [65/300], Train Loss: 0.000593
Validation Loss: 0.00058599
Epoch [66/300], Train Loss: 0.000589
Validation Loss: 0.00058528
Epoch [67/300], Train Loss: 0.000590
Validation Loss: 0.00058533
Epoch [68/300], Train Loss: 0.000587
Validation Loss: 0.00058525
Epoch [69/300], Train Loss: 0.000590
Validation Loss: 0.00058420
Epoch [70/300], Train Loss: 0.000584
Validation Loss: 0.00058424
Epoch [71/300], Train Loss: 0.000583
Validation Loss: 0.00058489
Epoch [72/300], Train Loss: 0.000589
Validation Loss: 0.00058481
Epoch [73/300], Train Loss: 0.000589
Validation Loss: 0.00058351
Epoch [74/300], Train Loss: 0.000590
Validation Loss: 0.00058382
Epoch [75/300], Train Loss: 0.000592
Validation Loss: 0.00058229
Epoch [76/300], Train Loss: 0.000582
Validation Loss: 0.00058197
Epoch [77/300], Train Loss: 0.000586
Validation Loss: 0.00058467
Epoch [78/300], Train Loss: 0.000582
Validation Loss: 0.00058123
Epoch [79/300], Train Loss: 0.000583
Validation Loss: 0.00058073
Epoch [80/300], Train Loss: 0.000584
Validation Loss: 0.00058192
Epoch [81/300], Train Loss: 0.000582
Validation Loss: 0.00058184
Epoch [82/300], Train Loss: 0.000587
Validation Loss: 0.00058034
Epoch [83/300], Train Loss: 0.000583
Validation Loss: 0.00058146
Epoch [84/300], Train Loss: 0.000581
Validation Loss: 0.00057878
Epoch [85/300], Train Loss: 0.000579
Validation Loss: 0.00057964
Epoch [86/300], Train Loss: 0.000582
Validation Loss: 0.00057803
Epoch [87/300], Train Loss: 0.000584
Validation Loss: 0.00057792
Epoch [88/300], Train Loss: 0.000581
Validation Loss: 0.00057727
Epoch [89/300], Train Loss: 0.000574
Validation Loss: 0.00057701
Epoch [90/300], Train Loss: 0.000583
Validation Loss: 0.00057788
Epoch [91/300], Train Loss: 0.000580
Validation Loss: 0.00057692
Epoch [92/300], Train Loss: 0.000583
Validation Loss: 0.00057719
Epoch [93/300], Train Loss: 0.000579
Validation Loss: 0.00057512
Epoch [94/300], Train Loss: 0.000582
Validation Loss: 0.00057576
Epoch [95/300], Train Loss: 0.000578
Validation Loss: 0.00057523
Epoch [96/300], Train Loss: 0.000574
Validation Loss: 0.00057302
Epoch [97/300], Train Loss: 0.000580
Validation Loss: 0.00057242
Epoch [98/300], Train Loss: 0.000574
Validation Loss: 0.00057243
Epoch [99/300], Train Loss: 0.000578
Validation Loss: 0.00057821
Epoch [100/300], Train Loss: 0.000579
Validation Loss: 0.00057073
Epoch [101/300], Train Loss: 0.000578
Validation Loss: 0.00056868
Epoch [102/300], Train Loss: 0.000576
Validation Loss: 0.00056773
Epoch [103/300], Train Loss: 0.000572
Validation Loss: 0.00056759
Epoch [104/300], Train Loss: 0.000570
Validation Loss: 0.00056576
Epoch [105/300], Train Loss: 0.000572
Validation Loss: 0.00056967
Epoch [106/300], Train Loss: 0.000574
Validation Loss: 0.00056445
Epoch [107/300], Train Loss: 0.000570
Validation Loss: 0.00056532
Epoch [108/300], Train Loss: 0.000569
Validation Loss: 0.00056011
Epoch [109/300], Train Loss: 0.000569
Validation Loss: 0.00055811
Epoch [110/300], Train Loss: 0.000572
Validation Loss: 0.00055616
Epoch [111/300], Train Loss: 0.000566
Validation Loss: 0.00055916
Epoch [112/300], Train Loss: 0.000559
Validation Loss: 0.00055451
Epoch [113/300], Train Loss: 0.000564
Validation Loss: 0.00055332
Epoch [114/300], Train Loss: 0.000562
Validation Loss: 0.00054767
Epoch [115/300], Train Loss: 0.000558
Validation Loss: 0.00053890
Epoch [116/300], Train Loss: 0.000550
Validation Loss: 0.00053729
Epoch [117/300], Train Loss: 0.000552
Validation Loss: 0.00053226
Epoch [118/300], Train Loss: 0.000547
Validation Loss: 0.00052786
Epoch [119/300], Train Loss: 0.000555
Validation Loss: 0.00052532
Epoch [120/300], Train Loss: 0.000546
Validation Loss: 0.00052744
Epoch [121/300], Train Loss: 0.000544
Validation Loss: 0.00055202
Epoch [122/300], Train Loss: 0.000577
Validation Loss: 0.00052981
Epoch [123/300], Train Loss: 0.000546
Validation Loss: 0.00053011
Epoch [124/300], Train Loss: 0.000545
Validation Loss: 0.00052334
Epoch [125/300], Train Loss: 0.000541
Validation Loss: 0.00052223
Epoch [126/300], Train Loss: 0.000538
Validation Loss: 0.00052875
Epoch [127/300], Train Loss: 0.000536
Validation Loss: 0.00052873
Epoch [128/300], Train Loss: 0.000590
Validation Loss: 0.00059324
Epoch [129/300], Train Loss: 0.000597
Validation Loss: 0.00058379
Epoch [130/300], Train Loss: 0.000595
Validation Loss: 0.00058016
Epoch [131/300], Train Loss: 0.000591
Validation Loss: 0.00057843
Epoch [132/300], Train Loss: 0.000588
Validation Loss: 0.00057711
Epoch [133/300], Train Loss: 0.000587
Validation Loss: 0.00057502
Epoch [134/300], Train Loss: 0.000585
Validation Loss: 0.00057420
Epoch [135/300], Train Loss: 0.000581
Validation Loss: 0.00057141
Early stopping triggered

Evaluating model for: Fridge
Run 17/72 completed in 1386.95 seconds with: {'MAE': np.float32(36.643623), 'MSE': np.float32(1939.4144), 'RMSE': np.float32(44.038784), 'SAE': np.float32(0.014420471), 'NDE': np.float32(0.76104355)}

Run 18/72: hidden=128, seq_len=720, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000668
Validation Loss: 0.00065963
Epoch [2/300], Train Loss: 0.000665
Validation Loss: 0.00065668
Epoch [3/300], Train Loss: 0.000653
Validation Loss: 0.00064982
Epoch [4/300], Train Loss: 0.000645
Validation Loss: 0.00064211
Epoch [5/300], Train Loss: 0.000638
Validation Loss: 0.00063309
Epoch [6/300], Train Loss: 0.000631
Validation Loss: 0.00061919
Epoch [7/300], Train Loss: 0.000606
Validation Loss: 0.00060891
Epoch [8/300], Train Loss: 0.000611
Validation Loss: 0.00060836
Epoch [9/300], Train Loss: 0.000605
Validation Loss: 0.00060577
Epoch [10/300], Train Loss: 0.000606
Validation Loss: 0.00060348
Epoch [11/300], Train Loss: 0.000601
Validation Loss: 0.00060428
Epoch [12/300], Train Loss: 0.000607
Validation Loss: 0.00060140
Epoch [13/300], Train Loss: 0.000606
Validation Loss: 0.00059983
Epoch [14/300], Train Loss: 0.000605
Validation Loss: 0.00060100
Epoch [15/300], Train Loss: 0.000607
Validation Loss: 0.00060833
Epoch [16/300], Train Loss: 0.000605
Validation Loss: 0.00060290
Epoch [17/300], Train Loss: 0.000604
Validation Loss: 0.00059861
Epoch [18/300], Train Loss: 0.000594
Validation Loss: 0.00059929
Epoch [19/300], Train Loss: 0.000598
Validation Loss: 0.00059980
Epoch [20/300], Train Loss: 0.000594
Validation Loss: 0.00059712
Epoch [21/300], Train Loss: 0.000597
Validation Loss: 0.00059725
Epoch [22/300], Train Loss: 0.000598
Validation Loss: 0.00059818
Epoch [23/300], Train Loss: 0.000605
Validation Loss: 0.00059675
Epoch [24/300], Train Loss: 0.000600
Validation Loss: 0.00059954
Epoch [25/300], Train Loss: 0.000598
Validation Loss: 0.00059611
Epoch [26/300], Train Loss: 0.000596
Validation Loss: 0.00059607
Epoch [27/300], Train Loss: 0.000600
Validation Loss: 0.00059838
Epoch [28/300], Train Loss: 0.000601
Validation Loss: 0.00059454
Epoch [29/300], Train Loss: 0.000597
Validation Loss: 0.00059228
Epoch [30/300], Train Loss: 0.000600
Validation Loss: 0.00059248
Epoch [31/300], Train Loss: 0.000599
Validation Loss: 0.00059118
Epoch [32/300], Train Loss: 0.000594
Validation Loss: 0.00060426
Epoch [33/300], Train Loss: 0.000598
Validation Loss: 0.00059075
Epoch [34/300], Train Loss: 0.000596
Validation Loss: 0.00059016
Epoch [35/300], Train Loss: 0.000595
Validation Loss: 0.00059001
Epoch [36/300], Train Loss: 0.000598
Validation Loss: 0.00059093
Epoch [37/300], Train Loss: 0.000591
Validation Loss: 0.00058910
Epoch [38/300], Train Loss: 0.000595
Validation Loss: 0.00058849
Epoch [39/300], Train Loss: 0.000593
Validation Loss: 0.00058987
Epoch [40/300], Train Loss: 0.000589
Validation Loss: 0.00058696
Epoch [41/300], Train Loss: 0.000591
Validation Loss: 0.00058656
Epoch [42/300], Train Loss: 0.000590
Validation Loss: 0.00058485
Epoch [43/300], Train Loss: 0.000585
Validation Loss: 0.00058472
Epoch [44/300], Train Loss: 0.000587
Validation Loss: 0.00058220
Epoch [45/300], Train Loss: 0.000587
Validation Loss: 0.00058995
Epoch [46/300], Train Loss: 0.000587
Validation Loss: 0.00058092
Epoch [47/300], Train Loss: 0.000582
Validation Loss: 0.00057948
Epoch [48/300], Train Loss: 0.000588
Validation Loss: 0.00057892
Epoch [49/300], Train Loss: 0.000580
Validation Loss: 0.00057790
Epoch [50/300], Train Loss: 0.000580
Validation Loss: 0.00058821
Epoch [51/300], Train Loss: 0.000581
Validation Loss: 0.00057704
Epoch [52/300], Train Loss: 0.000581
Validation Loss: 0.00057645
Epoch [53/300], Train Loss: 0.000582
Validation Loss: 0.00057668
Epoch [54/300], Train Loss: 0.000574
Validation Loss: 0.00057573
Epoch [55/300], Train Loss: 0.000578
Validation Loss: 0.00057432
Epoch [56/300], Train Loss: 0.000576
Validation Loss: 0.00057506
Epoch [57/300], Train Loss: 0.000572
Validation Loss: 0.00057472
Epoch [58/300], Train Loss: 0.000581
Validation Loss: 0.00057236
Epoch [59/300], Train Loss: 0.000578
Validation Loss: 0.00057207
Epoch [60/300], Train Loss: 0.000582
Validation Loss: 0.00057328
Epoch [61/300], Train Loss: 0.000577
Validation Loss: 0.00057141
Epoch [62/300], Train Loss: 0.000576
Validation Loss: 0.00056997
Epoch [63/300], Train Loss: 0.000576
Validation Loss: 0.00057070
Epoch [64/300], Train Loss: 0.000573
Validation Loss: 0.00056922
Epoch [65/300], Train Loss: 0.000577
Validation Loss: 0.00056868
Epoch [66/300], Train Loss: 0.000573
Validation Loss: 0.00056967
Epoch [67/300], Train Loss: 0.000575
Validation Loss: 0.00056785
Epoch [68/300], Train Loss: 0.000573
Validation Loss: 0.00056670
Epoch [69/300], Train Loss: 0.000576
Validation Loss: 0.00056576
Epoch [70/300], Train Loss: 0.000568
Validation Loss: 0.00056506
Epoch [71/300], Train Loss: 0.000568
Validation Loss: 0.00057342
Epoch [72/300], Train Loss: 0.000575
Validation Loss: 0.00056523
Epoch [73/300], Train Loss: 0.000574
Validation Loss: 0.00056388
Epoch [74/300], Train Loss: 0.000573
Validation Loss: 0.00056621
Epoch [75/300], Train Loss: 0.000576
Validation Loss: 0.00056359
Epoch [76/300], Train Loss: 0.000566
Validation Loss: 0.00056169
Epoch [77/300], Train Loss: 0.000567
Validation Loss: 0.00056684
Epoch [78/300], Train Loss: 0.000566
Validation Loss: 0.00055876
Epoch [79/300], Train Loss: 0.000564
Validation Loss: 0.00055980
Epoch [80/300], Train Loss: 0.000565
Validation Loss: 0.00055986
Epoch [81/300], Train Loss: 0.000562
Validation Loss: 0.00055777
Epoch [82/300], Train Loss: 0.000567
Validation Loss: 0.00055720
Epoch [83/300], Train Loss: 0.000563
Validation Loss: 0.00055553
Epoch [84/300], Train Loss: 0.000558
Validation Loss: 0.00055203
Epoch [85/300], Train Loss: 0.000557
Validation Loss: 0.00055265
Epoch [86/300], Train Loss: 0.000559
Validation Loss: 0.00055252
Epoch [87/300], Train Loss: 0.000562
Validation Loss: 0.00055160
Epoch [88/300], Train Loss: 0.000559
Validation Loss: 0.00054969
Epoch [89/300], Train Loss: 0.000550
Validation Loss: 0.00054982
Epoch [90/300], Train Loss: 0.000558
Validation Loss: 0.00054894
Epoch [91/300], Train Loss: 0.000555
Validation Loss: 0.00054794
Epoch [92/300], Train Loss: 0.000558
Validation Loss: 0.00054773
Epoch [93/300], Train Loss: 0.000556
Validation Loss: 0.00054886
Epoch [94/300], Train Loss: 0.000556
Validation Loss: 0.00054488
Epoch [95/300], Train Loss: 0.000553
Validation Loss: 0.00054684
Epoch [96/300], Train Loss: 0.000549
Validation Loss: 0.00054305
Epoch [97/300], Train Loss: 0.000553
Validation Loss: 0.00054302
Epoch [98/300], Train Loss: 0.000549
Validation Loss: 0.00054170
Epoch [99/300], Train Loss: 0.000551
Validation Loss: 0.00054099
Epoch [100/300], Train Loss: 0.000551
Validation Loss: 0.00054294
Epoch [101/300], Train Loss: 0.000550
Validation Loss: 0.00053977
Epoch [102/300], Train Loss: 0.000553
Validation Loss: 0.00054417
Epoch [103/300], Train Loss: 0.000548
Validation Loss: 0.00053927
Epoch [104/300], Train Loss: 0.000545
Validation Loss: 0.00053684
Epoch [105/300], Train Loss: 0.000548
Validation Loss: 0.00053893
Epoch [106/300], Train Loss: 0.000550
Validation Loss: 0.00053875
Epoch [107/300], Train Loss: 0.000548
Validation Loss: 0.00053704
Epoch [108/300], Train Loss: 0.000543
Validation Loss: 0.00053519
Epoch [109/300], Train Loss: 0.000547
Validation Loss: 0.00053512
Epoch [110/300], Train Loss: 0.000549
Validation Loss: 0.00053388
Epoch [111/300], Train Loss: 0.000546
Validation Loss: 0.00053511
Epoch [112/300], Train Loss: 0.000540
Validation Loss: 0.00053332
Epoch [113/300], Train Loss: 0.000547
Validation Loss: 0.00053226
Epoch [114/300], Train Loss: 0.000547
Validation Loss: 0.00053208
Epoch [115/300], Train Loss: 0.000543
Validation Loss: 0.00053096
Epoch [116/300], Train Loss: 0.000541
Validation Loss: 0.00053173
Epoch [117/300], Train Loss: 0.000540
Validation Loss: 0.00052910
Epoch [118/300], Train Loss: 0.000539
Validation Loss: 0.00052936
Epoch [119/300], Train Loss: 0.000540
Validation Loss: 0.00052700
Epoch [120/300], Train Loss: 0.000538
Validation Loss: 0.00052742
Epoch [121/300], Train Loss: 0.000541
Validation Loss: 0.00052518
Epoch [122/300], Train Loss: 0.000544
Validation Loss: 0.00052684
Epoch [123/300], Train Loss: 0.000539
Validation Loss: 0.00052193
Epoch [124/300], Train Loss: 0.000538
Validation Loss: 0.00052004
Epoch [125/300], Train Loss: 0.000535
Validation Loss: 0.00052194
Epoch [126/300], Train Loss: 0.000535
Validation Loss: 0.00052208
Epoch [127/300], Train Loss: 0.000532
Validation Loss: 0.00052425
Epoch [128/300], Train Loss: 0.000530
Validation Loss: 0.00051351
Epoch [129/300], Train Loss: 0.000526
Validation Loss: 0.00050960
Epoch [130/300], Train Loss: 0.000527
Validation Loss: 0.00050531
Epoch [131/300], Train Loss: 0.000526
Validation Loss: 0.00050741
Epoch [132/300], Train Loss: 0.000521
Validation Loss: 0.00050039
Epoch [133/300], Train Loss: 0.000518
Validation Loss: 0.00049846
Epoch [134/300], Train Loss: 0.000515
Validation Loss: 0.00049450
Epoch [135/300], Train Loss: 0.000509
Validation Loss: 0.00049744
Epoch [136/300], Train Loss: 0.000511
Validation Loss: 0.00049396
Epoch [137/300], Train Loss: 0.000508
Validation Loss: 0.00049166
Epoch [138/300], Train Loss: 0.000503
Validation Loss: 0.00049134
Epoch [139/300], Train Loss: 0.000506
Validation Loss: 0.00048705
Epoch [140/300], Train Loss: 0.000499
Validation Loss: 0.00048657
Epoch [141/300], Train Loss: 0.000506
Validation Loss: 0.00048461
Epoch [142/300], Train Loss: 0.000504
Validation Loss: 0.00048910
Epoch [143/300], Train Loss: 0.000504
Validation Loss: 0.00048550
Epoch [144/300], Train Loss: 0.000500
Validation Loss: 0.00048376
Epoch [145/300], Train Loss: 0.000503
Validation Loss: 0.00048972
Epoch [146/300], Train Loss: 0.000496
Validation Loss: 0.00048201
Epoch [147/300], Train Loss: 0.000500
Validation Loss: 0.00048554
Epoch [148/300], Train Loss: 0.000501
Validation Loss: 0.00048199
Epoch [149/300], Train Loss: 0.000503
Validation Loss: 0.00048530
Epoch [150/300], Train Loss: 0.000497
Validation Loss: 0.00048287
Epoch [151/300], Train Loss: 0.000501
Validation Loss: 0.00048353
Epoch [152/300], Train Loss: 0.000498
Validation Loss: 0.00048084
Epoch [153/300], Train Loss: 0.000496
Validation Loss: 0.00048016
Epoch [154/300], Train Loss: 0.000492
Validation Loss: 0.00048171
Epoch [155/300], Train Loss: 0.000501
Validation Loss: 0.00048839
Epoch [156/300], Train Loss: 0.000503
Validation Loss: 0.00047844
Epoch [157/300], Train Loss: 0.000500
Validation Loss: 0.00047950
Epoch [158/300], Train Loss: 0.000492
Validation Loss: 0.00048116
Epoch [159/300], Train Loss: 0.000497
Validation Loss: 0.00047900
Epoch [160/300], Train Loss: 0.000490
Validation Loss: 0.00047805
Epoch [161/300], Train Loss: 0.000490
Validation Loss: 0.00047889
Epoch [162/300], Train Loss: 0.000489
Validation Loss: 0.00047659
Epoch [163/300], Train Loss: 0.000489
Validation Loss: 0.00047681
Epoch [164/300], Train Loss: 0.000491
Validation Loss: 0.00047645
Epoch [165/300], Train Loss: 0.000496
Validation Loss: 0.00048592
Epoch [166/300], Train Loss: 0.000499
Validation Loss: 0.00047878
Epoch [167/300], Train Loss: 0.000491
Validation Loss: 0.00047515
Epoch [168/300], Train Loss: 0.000492
Validation Loss: 0.00047623
Epoch [169/300], Train Loss: 0.000494
Validation Loss: 0.00047599
Epoch [170/300], Train Loss: 0.000491
Validation Loss: 0.00047534
Epoch [171/300], Train Loss: 0.000485
Validation Loss: 0.00047528
Epoch [172/300], Train Loss: 0.000487
Validation Loss: 0.00047418
Epoch [173/300], Train Loss: 0.000491
Validation Loss: 0.00047391
Epoch [174/300], Train Loss: 0.000492
Validation Loss: 0.00048718
Epoch [175/300], Train Loss: 0.000494
Validation Loss: 0.00047516
Epoch [176/300], Train Loss: 0.000488
Validation Loss: 0.00047251
Epoch [177/300], Train Loss: 0.000488
Validation Loss: 0.00047328
Epoch [178/300], Train Loss: 0.000486
Validation Loss: 0.00047493
Epoch [179/300], Train Loss: 0.000491
Validation Loss: 0.00047311
Epoch [180/300], Train Loss: 0.000486
Validation Loss: 0.00047373
Epoch [181/300], Train Loss: 0.000485
Validation Loss: 0.00047777
Epoch [182/300], Train Loss: 0.000484
Validation Loss: 0.00047180
Epoch [183/300], Train Loss: 0.000485
Validation Loss: 0.00047150
Epoch [184/300], Train Loss: 0.000481
Validation Loss: 0.00047242
Epoch [185/300], Train Loss: 0.000484
Validation Loss: 0.00047453
Epoch [186/300], Train Loss: 0.000484
Validation Loss: 0.00047378
Epoch [187/300], Train Loss: 0.000487
Validation Loss: 0.00047255
Epoch [188/300], Train Loss: 0.000485
Validation Loss: 0.00047556
Epoch [189/300], Train Loss: 0.000484
Validation Loss: 0.00047162
Epoch [190/300], Train Loss: 0.000484
Validation Loss: 0.00047273
Epoch [191/300], Train Loss: 0.000485
Validation Loss: 0.00047257
Epoch [192/300], Train Loss: 0.000487
Validation Loss: 0.00047057
Epoch [193/300], Train Loss: 0.000484
Validation Loss: 0.00047236
Epoch [194/300], Train Loss: 0.000485
Validation Loss: 0.00046897
Epoch [195/300], Train Loss: 0.000486
Validation Loss: 0.00047268
Epoch [196/300], Train Loss: 0.000483
Validation Loss: 0.00046901
Epoch [197/300], Train Loss: 0.000483
Validation Loss: 0.00046912
Epoch [198/300], Train Loss: 0.000483
Validation Loss: 0.00046893
Epoch [199/300], Train Loss: 0.000481
Validation Loss: 0.00047395
Epoch [200/300], Train Loss: 0.000482
Validation Loss: 0.00046989
Epoch [201/300], Train Loss: 0.000483
Validation Loss: 0.00047993
Epoch [202/300], Train Loss: 0.000490
Validation Loss: 0.00046945
Epoch [203/300], Train Loss: 0.000482
Validation Loss: 0.00047004
Epoch [204/300], Train Loss: 0.000487
Validation Loss: 0.00046812
Epoch [205/300], Train Loss: 0.000483
Validation Loss: 0.00046903
Epoch [206/300], Train Loss: 0.000484
Validation Loss: 0.00046988
Epoch [207/300], Train Loss: 0.000480
Validation Loss: 0.00046899
Epoch [208/300], Train Loss: 0.000485
Validation Loss: 0.00046719
Epoch [209/300], Train Loss: 0.000479
Validation Loss: 0.00046807
Epoch [210/300], Train Loss: 0.000486
Validation Loss: 0.00046679
Epoch [211/300], Train Loss: 0.000481
Validation Loss: 0.00047058
Epoch [212/300], Train Loss: 0.000478
Validation Loss: 0.00046685
Epoch [213/300], Train Loss: 0.000480
Validation Loss: 0.00046623
Epoch [214/300], Train Loss: 0.000480
Validation Loss: 0.00046679
Epoch [215/300], Train Loss: 0.000478
Validation Loss: 0.00046629
Epoch [216/300], Train Loss: 0.000478
Validation Loss: 0.00046530
Epoch [217/300], Train Loss: 0.000479
Validation Loss: 0.00046625
Epoch [218/300], Train Loss: 0.000483
Validation Loss: 0.00046684
Epoch [219/300], Train Loss: 0.000477
Validation Loss: 0.00046740
Epoch [220/300], Train Loss: 0.000480
Validation Loss: 0.00046497
Epoch [221/300], Train Loss: 0.000479
Validation Loss: 0.00046610
Epoch [222/300], Train Loss: 0.000478
Validation Loss: 0.00046528
Epoch [223/300], Train Loss: 0.000485
Validation Loss: 0.00046598
Epoch [224/300], Train Loss: 0.000475
Validation Loss: 0.00046566
Epoch [225/300], Train Loss: 0.000475
Validation Loss: 0.00046531
Epoch [226/300], Train Loss: 0.000474
Validation Loss: 0.00046433
Epoch [227/300], Train Loss: 0.000483
Validation Loss: 0.00047540
Epoch [228/300], Train Loss: 0.000487
Validation Loss: 0.00046504
Epoch [229/300], Train Loss: 0.000480
Validation Loss: 0.00047063
Epoch [230/300], Train Loss: 0.000481
Validation Loss: 0.00046736
Epoch [231/300], Train Loss: 0.000478
Validation Loss: 0.00046347
Epoch [232/300], Train Loss: 0.000476
Validation Loss: 0.00046506
Epoch [233/300], Train Loss: 0.000472
Validation Loss: 0.00046448
Epoch [234/300], Train Loss: 0.000472
Validation Loss: 0.00046289
Epoch [235/300], Train Loss: 0.000472
Validation Loss: 0.00046361
Epoch [236/300], Train Loss: 0.000477
Validation Loss: 0.00046268
Epoch [237/300], Train Loss: 0.000476
Validation Loss: 0.00046258
Epoch [238/300], Train Loss: 0.000474
Validation Loss: 0.00046452
Epoch [239/300], Train Loss: 0.000477
Validation Loss: 0.00046261
Epoch [240/300], Train Loss: 0.000472
Validation Loss: 0.00046259
Epoch [241/300], Train Loss: 0.000472
Validation Loss: 0.00046300
Epoch [242/300], Train Loss: 0.000469
Validation Loss: 0.00046389
Epoch [243/300], Train Loss: 0.000474
Validation Loss: 0.00046144
Epoch [244/300], Train Loss: 0.000473
Validation Loss: 0.00046248
Epoch [245/300], Train Loss: 0.000470
Validation Loss: 0.00046393
Epoch [246/300], Train Loss: 0.000475
Validation Loss: 0.00046214
Epoch [247/300], Train Loss: 0.000476
Validation Loss: 0.00046218
Epoch [248/300], Train Loss: 0.000474
Validation Loss: 0.00046292
Epoch [249/300], Train Loss: 0.000476
Validation Loss: 0.00046038
Epoch [250/300], Train Loss: 0.000474
Validation Loss: 0.00046103
Epoch [251/300], Train Loss: 0.000474
Validation Loss: 0.00046218
Epoch [252/300], Train Loss: 0.000476
Validation Loss: 0.00046242
Epoch [253/300], Train Loss: 0.000469
Validation Loss: 0.00046177
Epoch [254/300], Train Loss: 0.000474
Validation Loss: 0.00046189
Epoch [255/300], Train Loss: 0.000475
Validation Loss: 0.00046253
Epoch [256/300], Train Loss: 0.000478
Validation Loss: 0.00046026
Epoch [257/300], Train Loss: 0.000471
Validation Loss: 0.00046014
Epoch [258/300], Train Loss: 0.000476
Validation Loss: 0.00045965
Epoch [259/300], Train Loss: 0.000473
Validation Loss: 0.00045966
Epoch [260/300], Train Loss: 0.000472
Validation Loss: 0.00046078
Epoch [261/300], Train Loss: 0.000469
Validation Loss: 0.00045981
Epoch [262/300], Train Loss: 0.000470
Validation Loss: 0.00045942
Epoch [263/300], Train Loss: 0.000474
Validation Loss: 0.00045943
Epoch [264/300], Train Loss: 0.000468
Validation Loss: 0.00045870
Epoch [265/300], Train Loss: 0.000472
Validation Loss: 0.00045906
Epoch [266/300], Train Loss: 0.000472
Validation Loss: 0.00045938
Epoch [267/300], Train Loss: 0.000471
Validation Loss: 0.00045971
Epoch [268/300], Train Loss: 0.000471
Validation Loss: 0.00045811
Epoch [269/300], Train Loss: 0.000472
Validation Loss: 0.00045913
Epoch [270/300], Train Loss: 0.000472
Validation Loss: 0.00045941
Epoch [271/300], Train Loss: 0.000471
Validation Loss: 0.00045795
Epoch [272/300], Train Loss: 0.000472
Validation Loss: 0.00045775
Epoch [273/300], Train Loss: 0.000471
Validation Loss: 0.00046192
Epoch [274/300], Train Loss: 0.000473
Validation Loss: 0.00046013
Epoch [275/300], Train Loss: 0.000470
Validation Loss: 0.00045733
Epoch [276/300], Train Loss: 0.000469
Validation Loss: 0.00045726
Epoch [277/300], Train Loss: 0.000469
Validation Loss: 0.00045909
Epoch [278/300], Train Loss: 0.000471
Validation Loss: 0.00045697
Epoch [279/300], Train Loss: 0.000466
Validation Loss: 0.00045737
Epoch [280/300], Train Loss: 0.000468
Validation Loss: 0.00045783
Epoch [281/300], Train Loss: 0.000472
Validation Loss: 0.00045714
Epoch [282/300], Train Loss: 0.000471
Validation Loss: 0.00045622
Epoch [283/300], Train Loss: 0.000469
Validation Loss: 0.00045670
Epoch [284/300], Train Loss: 0.000465
Validation Loss: 0.00045693
Epoch [285/300], Train Loss: 0.000470
Validation Loss: 0.00045688
Epoch [286/300], Train Loss: 0.000474
Validation Loss: 0.00045827
Epoch [287/300], Train Loss: 0.000467
Validation Loss: 0.00046143
Epoch [288/300], Train Loss: 0.000471
Validation Loss: 0.00045565
Epoch [289/300], Train Loss: 0.000471
Validation Loss: 0.00045555
Epoch [290/300], Train Loss: 0.000469
Validation Loss: 0.00045797
Epoch [291/300], Train Loss: 0.000466
Validation Loss: 0.00045970
Epoch [292/300], Train Loss: 0.000467
Validation Loss: 0.00045973
Epoch [293/300], Train Loss: 0.000480
Validation Loss: 0.00045599
Epoch [294/300], Train Loss: 0.000469
Validation Loss: 0.00045539
Epoch [295/300], Train Loss: 0.000471
Validation Loss: 0.00045521
Epoch [296/300], Train Loss: 0.000467
Validation Loss: 0.00045644
Epoch [297/300], Train Loss: 0.000466
Validation Loss: 0.00045559
Epoch [298/300], Train Loss: 0.000467
Validation Loss: 0.00045430
Epoch [299/300], Train Loss: 0.000468
Validation Loss: 0.00045480
Epoch [300/300], Train Loss: 0.000469
Validation Loss: 0.00045443

Evaluating model for: Fridge
Run 18/72 completed in 3238.38 seconds with: {'MAE': np.float32(30.191048), 'MSE': np.float32(1538.1298), 'RMSE': np.float32(39.218998), 'SAE': np.float32(0.0076486217), 'NDE': np.float32(0.67775166)}

Run 19/72: hidden=128, seq_len=720, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000670
Validation Loss: 0.00066152
Epoch [2/300], Train Loss: 0.000669
Validation Loss: 0.00066089
Epoch [3/300], Train Loss: 0.000660
Validation Loss: 0.00066165
Epoch [4/300], Train Loss: 0.000657
Validation Loss: 0.00066114
Epoch [5/300], Train Loss: 0.000657
Validation Loss: 0.00065613
Epoch [6/300], Train Loss: 0.000654
Validation Loss: 0.00064704
Epoch [7/300], Train Loss: 0.000626
Validation Loss: 0.00062704
Epoch [8/300], Train Loss: 0.000621
Validation Loss: 0.00061849
Epoch [9/300], Train Loss: 0.000611
Validation Loss: 0.00061143
Epoch [10/300], Train Loss: 0.000612
Validation Loss: 0.00061300
Epoch [11/300], Train Loss: 0.000608
Validation Loss: 0.00061054
Epoch [12/300], Train Loss: 0.000612
Validation Loss: 0.00060732
Epoch [13/300], Train Loss: 0.000611
Validation Loss: 0.00060533
Epoch [14/300], Train Loss: 0.000610
Validation Loss: 0.00060601
Epoch [15/300], Train Loss: 0.000613
Validation Loss: 0.00060880
Epoch [16/300], Train Loss: 0.000609
Validation Loss: 0.00060598
Epoch [17/300], Train Loss: 0.000609
Validation Loss: 0.00060433
Epoch [18/300], Train Loss: 0.000599
Validation Loss: 0.00060413
Epoch [19/300], Train Loss: 0.000603
Validation Loss: 0.00060404
Epoch [20/300], Train Loss: 0.000599
Validation Loss: 0.00060060
Epoch [21/300], Train Loss: 0.000602
Validation Loss: 0.00060014
Epoch [22/300], Train Loss: 0.000602
Validation Loss: 0.00060034
Epoch [23/300], Train Loss: 0.000610
Validation Loss: 0.00059886
Epoch [24/300], Train Loss: 0.000604
Validation Loss: 0.00060112
Epoch [25/300], Train Loss: 0.000602
Validation Loss: 0.00059813
Epoch [26/300], Train Loss: 0.000600
Validation Loss: 0.00059738
Epoch [27/300], Train Loss: 0.000604
Validation Loss: 0.00060026
Epoch [28/300], Train Loss: 0.000605
Validation Loss: 0.00060015
Epoch [29/300], Train Loss: 0.000602
Validation Loss: 0.00059523
Epoch [30/300], Train Loss: 0.000604
Validation Loss: 0.00059492
Epoch [31/300], Train Loss: 0.000603
Validation Loss: 0.00059413
Epoch [32/300], Train Loss: 0.000599
Validation Loss: 0.00059794
Epoch [33/300], Train Loss: 0.000600
Validation Loss: 0.00059341
Epoch [34/300], Train Loss: 0.000600
Validation Loss: 0.00059280
Epoch [35/300], Train Loss: 0.000599
Validation Loss: 0.00059239
Epoch [36/300], Train Loss: 0.000602
Validation Loss: 0.00059981
Epoch [37/300], Train Loss: 0.000596
Validation Loss: 0.00059377
Epoch [38/300], Train Loss: 0.000599
Validation Loss: 0.00059141
Epoch [39/300], Train Loss: 0.000597
Validation Loss: 0.00059020
Epoch [40/300], Train Loss: 0.000593
Validation Loss: 0.00059035
Epoch [41/300], Train Loss: 0.000596
Validation Loss: 0.00058990
Epoch [42/300], Train Loss: 0.000596
Validation Loss: 0.00059041
Epoch [43/300], Train Loss: 0.000591
Validation Loss: 0.00058869
Epoch [44/300], Train Loss: 0.000593
Validation Loss: 0.00058792
Epoch [45/300], Train Loss: 0.000594
Validation Loss: 0.00058815
Epoch [46/300], Train Loss: 0.000590
Validation Loss: 0.00058748
Epoch [47/300], Train Loss: 0.000589
Validation Loss: 0.00058772
Epoch [48/300], Train Loss: 0.000595
Validation Loss: 0.00058638
Epoch [49/300], Train Loss: 0.000589
Validation Loss: 0.00058645
Epoch [50/300], Train Loss: 0.000588
Validation Loss: 0.00059097
Epoch [51/300], Train Loss: 0.000588
Validation Loss: 0.00058541
Epoch [52/300], Train Loss: 0.000590
Validation Loss: 0.00058649
Epoch [53/300], Train Loss: 0.000590
Validation Loss: 0.00058418
Epoch [54/300], Train Loss: 0.000582
Validation Loss: 0.00058408
Epoch [55/300], Train Loss: 0.000587
Validation Loss: 0.00058443
Epoch [56/300], Train Loss: 0.000584
Validation Loss: 0.00058521
Epoch [57/300], Train Loss: 0.000580
Validation Loss: 0.00058307
Epoch [58/300], Train Loss: 0.000590
Validation Loss: 0.00058154
Epoch [59/300], Train Loss: 0.000587
Validation Loss: 0.00058203
Epoch [60/300], Train Loss: 0.000589
Validation Loss: 0.00058027
Epoch [61/300], Train Loss: 0.000586
Validation Loss: 0.00058171
Epoch [62/300], Train Loss: 0.000586
Validation Loss: 0.00058053
Epoch [63/300], Train Loss: 0.000585
Validation Loss: 0.00057762
Epoch [64/300], Train Loss: 0.000581
Validation Loss: 0.00057833
Epoch [65/300], Train Loss: 0.000586
Validation Loss: 0.00057751
Epoch [66/300], Train Loss: 0.000582
Validation Loss: 0.00057505
Epoch [67/300], Train Loss: 0.000582
Validation Loss: 0.00057607
Epoch [68/300], Train Loss: 0.000579
Validation Loss: 0.00057301
Epoch [69/300], Train Loss: 0.000581
Validation Loss: 0.00057120
Epoch [70/300], Train Loss: 0.000572
Validation Loss: 0.00057023
Epoch [71/300], Train Loss: 0.000571
Validation Loss: 0.00056852
Epoch [72/300], Train Loss: 0.000575
Validation Loss: 0.00056484
Epoch [73/300], Train Loss: 0.000574
Validation Loss: 0.00056680
Epoch [74/300], Train Loss: 0.000574
Validation Loss: 0.00056745
Epoch [75/300], Train Loss: 0.000575
Validation Loss: 0.00056962
Epoch [76/300], Train Loss: 0.000573
Validation Loss: 0.00056690
Epoch [77/300], Train Loss: 0.000568
Validation Loss: 0.00057021
Epoch [78/300], Train Loss: 0.000565
Validation Loss: 0.00056434
Epoch [79/300], Train Loss: 0.000565
Validation Loss: 0.00055960
Epoch [80/300], Train Loss: 0.000566
Validation Loss: 0.00056308
Epoch [81/300], Train Loss: 0.000564
Validation Loss: 0.00056001
Epoch [82/300], Train Loss: 0.000568
Validation Loss: 0.00055819
Epoch [83/300], Train Loss: 0.000564
Validation Loss: 0.00055809
Epoch [84/300], Train Loss: 0.000561
Validation Loss: 0.00055828
Epoch [85/300], Train Loss: 0.000560
Validation Loss: 0.00055707
Epoch [86/300], Train Loss: 0.000562
Validation Loss: 0.00055682
Epoch [87/300], Train Loss: 0.000564
Validation Loss: 0.00055650
Epoch [88/300], Train Loss: 0.000562
Validation Loss: 0.00055466
Epoch [89/300], Train Loss: 0.000554
Validation Loss: 0.00055838
Epoch [90/300], Train Loss: 0.000562
Validation Loss: 0.00055588
Epoch [91/300], Train Loss: 0.000559
Validation Loss: 0.00055423
Epoch [92/300], Train Loss: 0.000561
Validation Loss: 0.00055337
Epoch [93/300], Train Loss: 0.000560
Validation Loss: 0.00055269
Epoch [94/300], Train Loss: 0.000559
Validation Loss: 0.00055193
Epoch [95/300], Train Loss: 0.000558
Validation Loss: 0.00055445
Epoch [96/300], Train Loss: 0.000554
Validation Loss: 0.00055163
Epoch [97/300], Train Loss: 0.000560
Validation Loss: 0.00055106
Epoch [98/300], Train Loss: 0.000554
Validation Loss: 0.00054942
Epoch [99/300], Train Loss: 0.000556
Validation Loss: 0.00054861
Epoch [100/300], Train Loss: 0.000556
Validation Loss: 0.00054966
Epoch [101/300], Train Loss: 0.000554
Validation Loss: 0.00054709
Epoch [102/300], Train Loss: 0.000558
Validation Loss: 0.00055093
Epoch [103/300], Train Loss: 0.000554
Validation Loss: 0.00054853
Epoch [104/300], Train Loss: 0.000551
Validation Loss: 0.00054499
Epoch [105/300], Train Loss: 0.000554
Validation Loss: 0.00054660
Epoch [106/300], Train Loss: 0.000555
Validation Loss: 0.00054409
Epoch [107/300], Train Loss: 0.000553
Validation Loss: 0.00054678
Epoch [108/300], Train Loss: 0.000549
Validation Loss: 0.00054313
Epoch [109/300], Train Loss: 0.000553
Validation Loss: 0.00054459
Epoch [110/300], Train Loss: 0.000556
Validation Loss: 0.00054211
Epoch [111/300], Train Loss: 0.000554
Validation Loss: 0.00054547
Epoch [112/300], Train Loss: 0.000547
Validation Loss: 0.00054154
Epoch [113/300], Train Loss: 0.000553
Validation Loss: 0.00054010
Epoch [114/300], Train Loss: 0.000553
Validation Loss: 0.00054014
Epoch [115/300], Train Loss: 0.000550
Validation Loss: 0.00053916
Epoch [116/300], Train Loss: 0.000548
Validation Loss: 0.00053917
Epoch [117/300], Train Loss: 0.000546
Validation Loss: 0.00053771
Epoch [118/300], Train Loss: 0.000545
Validation Loss: 0.00053737
Epoch [119/300], Train Loss: 0.000547
Validation Loss: 0.00053683
Epoch [120/300], Train Loss: 0.000546
Validation Loss: 0.00053584
Epoch [121/300], Train Loss: 0.000548
Validation Loss: 0.00053470
Epoch [122/300], Train Loss: 0.000549
Validation Loss: 0.00053429
Epoch [123/300], Train Loss: 0.000548
Validation Loss: 0.00053292
Epoch [124/300], Train Loss: 0.000546
Validation Loss: 0.00053264
Epoch [125/300], Train Loss: 0.000544
Validation Loss: 0.00053268
Epoch [126/300], Train Loss: 0.000543
Validation Loss: 0.00053084
Epoch [127/300], Train Loss: 0.000542
Validation Loss: 0.00052983
Epoch [128/300], Train Loss: 0.000539
Validation Loss: 0.00052870
Epoch [129/300], Train Loss: 0.000538
Validation Loss: 0.00052364
Epoch [130/300], Train Loss: 0.000537
Validation Loss: 0.00052004
Epoch [131/300], Train Loss: 0.000535
Validation Loss: 0.00051466
Epoch [132/300], Train Loss: 0.000527
Validation Loss: 0.00050865
Epoch [133/300], Train Loss: 0.000525
Validation Loss: 0.00050460
Epoch [134/300], Train Loss: 0.000519
Validation Loss: 0.00049869
Epoch [135/300], Train Loss: 0.000517
Validation Loss: 0.00050538
Epoch [136/300], Train Loss: 0.000519
Validation Loss: 0.00050124
Epoch [137/300], Train Loss: 0.000519
Validation Loss: 0.00049625
Epoch [138/300], Train Loss: 0.000509
Validation Loss: 0.00052046
Epoch [139/300], Train Loss: 0.000544
Validation Loss: 0.00051024
Epoch [140/300], Train Loss: 0.000520
Validation Loss: 0.00050475
Epoch [141/300], Train Loss: 0.000519
Validation Loss: 0.00050854
Epoch [142/300], Train Loss: 0.000516
Validation Loss: 0.00049806
Epoch [143/300], Train Loss: 0.000512
Validation Loss: 0.00049316
Epoch [144/300], Train Loss: 0.000511
Validation Loss: 0.00049460
Epoch [145/300], Train Loss: 0.000515
Validation Loss: 0.00049412
Epoch [146/300], Train Loss: 0.000508
Validation Loss: 0.00049608
Epoch [147/300], Train Loss: 0.000514
Validation Loss: 0.00049352
Epoch [148/300], Train Loss: 0.000507
Validation Loss: 0.00049601
Epoch [149/300], Train Loss: 0.000515
Validation Loss: 0.00049280
Epoch [150/300], Train Loss: 0.000504
Validation Loss: 0.00048990
Epoch [151/300], Train Loss: 0.000510
Validation Loss: 0.00048893
Epoch [152/300], Train Loss: 0.000504
Validation Loss: 0.00048954
Epoch [153/300], Train Loss: 0.000505
Validation Loss: 0.00049012
Epoch [154/300], Train Loss: 0.000501
Validation Loss: 0.00049015
Epoch [155/300], Train Loss: 0.000507
Validation Loss: 0.00048663
Epoch [156/300], Train Loss: 0.000508
Validation Loss: 0.00048647
Epoch [157/300], Train Loss: 0.000506
Validation Loss: 0.00049299
Epoch [158/300], Train Loss: 0.000511
Validation Loss: 0.00048972
Epoch [159/300], Train Loss: 0.000506
Validation Loss: 0.00048482
Epoch [160/300], Train Loss: 0.000497
Validation Loss: 0.00048591
Epoch [161/300], Train Loss: 0.000499
Validation Loss: 0.00048501
Epoch [162/300], Train Loss: 0.000495
Validation Loss: 0.00048274
Epoch [163/300], Train Loss: 0.000496
Validation Loss: 0.00048491
Epoch [164/300], Train Loss: 0.000497
Validation Loss: 0.00048126
Epoch [165/300], Train Loss: 0.000504
Validation Loss: 0.00048172
Epoch [166/300], Train Loss: 0.000493
Validation Loss: 0.00048026
Epoch [167/300], Train Loss: 0.000493
Validation Loss: 0.00047811
Epoch [168/300], Train Loss: 0.000494
Validation Loss: 0.00047760
Epoch [169/300], Train Loss: 0.000494
Validation Loss: 0.00048937
Epoch [170/300], Train Loss: 0.000524
Validation Loss: 0.00048777
Epoch [171/300], Train Loss: 0.000492
Validation Loss: 0.00047783
Epoch [172/300], Train Loss: 0.000494
Validation Loss: 0.00047633
Epoch [173/300], Train Loss: 0.000500
Validation Loss: 0.00047547
Epoch [174/300], Train Loss: 0.000495
Validation Loss: 0.00047568
Epoch [175/300], Train Loss: 0.000487
Validation Loss: 0.00047423
Epoch [176/300], Train Loss: 0.000489
Validation Loss: 0.00048585
Epoch [177/300], Train Loss: 0.000489
Validation Loss: 0.00047815
Epoch [178/300], Train Loss: 0.000490
Validation Loss: 0.00047444
Epoch [179/300], Train Loss: 0.000496
Validation Loss: 0.00047413
Epoch [180/300], Train Loss: 0.000494
Validation Loss: 0.00048109
Epoch [181/300], Train Loss: 0.000488
Validation Loss: 0.00047275
Epoch [182/300], Train Loss: 0.000484
Validation Loss: 0.00047097
Epoch [183/300], Train Loss: 0.000612
Validation Loss: 0.00059688
Epoch [184/300], Train Loss: 0.000579
Validation Loss: 0.00056388
Epoch [185/300], Train Loss: 0.000566
Validation Loss: 0.00055543
Epoch [186/300], Train Loss: 0.000566
Validation Loss: 0.00055124
Epoch [187/300], Train Loss: 0.000562
Validation Loss: 0.00054760
Epoch [188/300], Train Loss: 0.000554
Validation Loss: 0.00054422
Epoch [189/300], Train Loss: 0.000551
Validation Loss: 0.00054206
Epoch [190/300], Train Loss: 0.000550
Validation Loss: 0.00053978
Epoch [191/300], Train Loss: 0.000551
Validation Loss: 0.00053708
Epoch [192/300], Train Loss: 0.000544
Validation Loss: 0.00053506
Early stopping triggered

Evaluating model for: Fridge
Run 19/72 completed in 2179.30 seconds with: {'MAE': np.float32(34.9558), 'MSE': np.float32(1844.6351), 'RMSE': np.float32(42.949215), 'SAE': np.float32(0.02279323), 'NDE': np.float32(0.7422145)}

Run 20/72: hidden=128, seq_len=720, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000666
Validation Loss: 0.00066168
Epoch [2/300], Train Loss: 0.000666
Validation Loss: 0.00066150
Epoch [3/300], Train Loss: 0.000658
Validation Loss: 0.00066172
Epoch [4/300], Train Loss: 0.000656
Validation Loss: 0.00065982
Epoch [5/300], Train Loss: 0.000654
Validation Loss: 0.00065020
Epoch [6/300], Train Loss: 0.000638
Validation Loss: 0.00062388
Epoch [7/300], Train Loss: 0.000606
Validation Loss: 0.00061000
Epoch [8/300], Train Loss: 0.000611
Validation Loss: 0.00061225
Epoch [9/300], Train Loss: 0.000607
Validation Loss: 0.00060563
Epoch [10/300], Train Loss: 0.000606
Validation Loss: 0.00060663
Epoch [11/300], Train Loss: 0.000603
Validation Loss: 0.00060591
Epoch [12/300], Train Loss: 0.000608
Validation Loss: 0.00060217
Epoch [13/300], Train Loss: 0.000608
Validation Loss: 0.00060126
Epoch [14/300], Train Loss: 0.000607
Validation Loss: 0.00060113
Epoch [15/300], Train Loss: 0.000609
Validation Loss: 0.00060577
Epoch [16/300], Train Loss: 0.000606
Validation Loss: 0.00060174
Epoch [17/300], Train Loss: 0.000605
Validation Loss: 0.00060255
Epoch [18/300], Train Loss: 0.000596
Validation Loss: 0.00060125
Epoch [19/300], Train Loss: 0.000600
Validation Loss: 0.00060007
Epoch [20/300], Train Loss: 0.000596
Validation Loss: 0.00059795
Epoch [21/300], Train Loss: 0.000599
Validation Loss: 0.00059715
Epoch [22/300], Train Loss: 0.000599
Validation Loss: 0.00059724
Epoch [23/300], Train Loss: 0.000607
Validation Loss: 0.00059624
Epoch [24/300], Train Loss: 0.000602
Validation Loss: 0.00060004
Epoch [25/300], Train Loss: 0.000600
Validation Loss: 0.00059567
Epoch [26/300], Train Loss: 0.000598
Validation Loss: 0.00059560
Epoch [27/300], Train Loss: 0.000602
Validation Loss: 0.00059922
Epoch [28/300], Train Loss: 0.000603
Validation Loss: 0.00059749
Epoch [29/300], Train Loss: 0.000599
Validation Loss: 0.00059341
Epoch [30/300], Train Loss: 0.000602
Validation Loss: 0.00059267
Epoch [31/300], Train Loss: 0.000602
Validation Loss: 0.00059251
Epoch [32/300], Train Loss: 0.000597
Validation Loss: 0.00059660
Epoch [33/300], Train Loss: 0.000599
Validation Loss: 0.00059074
Epoch [34/300], Train Loss: 0.000597
Validation Loss: 0.00058996
Epoch [35/300], Train Loss: 0.000596
Validation Loss: 0.00058901
Epoch [36/300], Train Loss: 0.000599
Validation Loss: 0.00059194
Epoch [37/300], Train Loss: 0.000592
Validation Loss: 0.00058842
Epoch [38/300], Train Loss: 0.000595
Validation Loss: 0.00058828
Epoch [39/300], Train Loss: 0.000593
Validation Loss: 0.00058763
Epoch [40/300], Train Loss: 0.000589
Validation Loss: 0.00058547
Epoch [41/300], Train Loss: 0.000592
Validation Loss: 0.00058437
Epoch [42/300], Train Loss: 0.000592
Validation Loss: 0.00058441
Epoch [43/300], Train Loss: 0.000588
Validation Loss: 0.00058783
Epoch [44/300], Train Loss: 0.000588
Validation Loss: 0.00058345
Epoch [45/300], Train Loss: 0.000590
Validation Loss: 0.00058509
Epoch [46/300], Train Loss: 0.000588
Validation Loss: 0.00058150
Epoch [47/300], Train Loss: 0.000584
Validation Loss: 0.00057977
Epoch [48/300], Train Loss: 0.000589
Validation Loss: 0.00057860
Epoch [49/300], Train Loss: 0.000584
Validation Loss: 0.00057753
Epoch [50/300], Train Loss: 0.000583
Validation Loss: 0.00058384
Epoch [51/300], Train Loss: 0.000582
Validation Loss: 0.00057781
Epoch [52/300], Train Loss: 0.000584
Validation Loss: 0.00057717
Epoch [53/300], Train Loss: 0.000586
Validation Loss: 0.00057982
Epoch [54/300], Train Loss: 0.000577
Validation Loss: 0.00057608
Epoch [55/300], Train Loss: 0.000580
Validation Loss: 0.00057493
Epoch [56/300], Train Loss: 0.000577
Validation Loss: 0.00057459
Epoch [57/300], Train Loss: 0.000573
Validation Loss: 0.00057307
Epoch [58/300], Train Loss: 0.000581
Validation Loss: 0.00057034
Epoch [59/300], Train Loss: 0.000578
Validation Loss: 0.00057266
Epoch [60/300], Train Loss: 0.000582
Validation Loss: 0.00057164
Epoch [61/300], Train Loss: 0.000578
Validation Loss: 0.00056964
Epoch [62/300], Train Loss: 0.000576
Validation Loss: 0.00056727
Epoch [63/300], Train Loss: 0.000575
Validation Loss: 0.00056814
Epoch [64/300], Train Loss: 0.000571
Validation Loss: 0.00056713
Epoch [65/300], Train Loss: 0.000574
Validation Loss: 0.00056995
Epoch [66/300], Train Loss: 0.000572
Validation Loss: 0.00056483
Epoch [67/300], Train Loss: 0.000571
Validation Loss: 0.00056434
Epoch [68/300], Train Loss: 0.000569
Validation Loss: 0.00056177
Epoch [69/300], Train Loss: 0.000571
Validation Loss: 0.00055907
Epoch [70/300], Train Loss: 0.000563
Validation Loss: 0.00055929
Epoch [71/300], Train Loss: 0.000563
Validation Loss: 0.00056013
Epoch [72/300], Train Loss: 0.000570
Validation Loss: 0.00055760
Epoch [73/300], Train Loss: 0.000564
Validation Loss: 0.00056774
Epoch [74/300], Train Loss: 0.000568
Validation Loss: 0.00055651
Epoch [75/300], Train Loss: 0.000564
Validation Loss: 0.00055614
Epoch [76/300], Train Loss: 0.000555
Validation Loss: 0.00054942
Epoch [77/300], Train Loss: 0.000557
Validation Loss: 0.00057136
Epoch [78/300], Train Loss: 0.000562
Validation Loss: 0.00055732
Epoch [79/300], Train Loss: 0.000555
Validation Loss: 0.00054365
Epoch [80/300], Train Loss: 0.000552
Validation Loss: 0.00055427
Epoch [81/300], Train Loss: 0.000551
Validation Loss: 0.00053886
Epoch [82/300], Train Loss: 0.000551
Validation Loss: 0.00053576
Epoch [83/300], Train Loss: 0.000546
Validation Loss: 0.00053741
Epoch [84/300], Train Loss: 0.000536
Validation Loss: 0.00052307
Epoch [85/300], Train Loss: 0.000533
Validation Loss: 0.00051721
Epoch [86/300], Train Loss: 0.000534
Validation Loss: 0.00051393
Epoch [87/300], Train Loss: 0.000530
Validation Loss: 0.00051388
Epoch [88/300], Train Loss: 0.000524
Validation Loss: 0.00050746
Epoch [89/300], Train Loss: 0.000513
Validation Loss: 0.00050022
Epoch [90/300], Train Loss: 0.000521
Validation Loss: 0.00053743
Epoch [91/300], Train Loss: 0.000521
Validation Loss: 0.00049831
Epoch [92/300], Train Loss: 0.000513
Validation Loss: 0.00050004
Epoch [93/300], Train Loss: 0.000509
Validation Loss: 0.00048922
Epoch [94/300], Train Loss: 0.000506
Validation Loss: 0.00049609
Epoch [95/300], Train Loss: 0.000505
Validation Loss: 0.00048680
Epoch [96/300], Train Loss: 0.000498
Validation Loss: 0.00048461
Epoch [97/300], Train Loss: 0.000499
Validation Loss: 0.00048516
Epoch [98/300], Train Loss: 0.000494
Validation Loss: 0.00048283
Epoch [99/300], Train Loss: 0.000495
Validation Loss: 0.00047722
Epoch [100/300], Train Loss: 0.000495
Validation Loss: 0.00048043
Epoch [101/300], Train Loss: 0.000493
Validation Loss: 0.00048205
Epoch [102/300], Train Loss: 0.000492
Validation Loss: 0.00047639
Epoch [103/300], Train Loss: 0.000490
Validation Loss: 0.00048378
Epoch [104/300], Train Loss: 0.000491
Validation Loss: 0.00047442
Epoch [105/300], Train Loss: 0.000488
Validation Loss: 0.00046585
Epoch [106/300], Train Loss: 0.000486
Validation Loss: 0.00046658
Epoch [107/300], Train Loss: 0.000483
Validation Loss: 0.00046446
Epoch [108/300], Train Loss: 0.000479
Validation Loss: 0.00046560
Epoch [109/300], Train Loss: 0.000481
Validation Loss: 0.00046424
Epoch [110/300], Train Loss: 0.000482
Validation Loss: 0.00046415
Epoch [111/300], Train Loss: 0.000480
Validation Loss: 0.00046812
Epoch [112/300], Train Loss: 0.000474
Validation Loss: 0.00045995
Epoch [113/300], Train Loss: 0.000480
Validation Loss: 0.00046989
Epoch [114/300], Train Loss: 0.000483
Validation Loss: 0.00047160
Epoch [115/300], Train Loss: 0.000474
Validation Loss: 0.00045590
Epoch [116/300], Train Loss: 0.000473
Validation Loss: 0.00045644
Epoch [117/300], Train Loss: 0.000471
Validation Loss: 0.00046057
Epoch [118/300], Train Loss: 0.000468
Validation Loss: 0.00045788
Epoch [119/300], Train Loss: 0.000470
Validation Loss: 0.00045309
Epoch [120/300], Train Loss: 0.000469
Validation Loss: 0.00046641
Epoch [121/300], Train Loss: 0.000480
Validation Loss: 0.00047442
Epoch [122/300], Train Loss: 0.000477
Validation Loss: 0.00046908
Epoch [123/300], Train Loss: 0.000474
Validation Loss: 0.00045200
Epoch [124/300], Train Loss: 0.000470
Validation Loss: 0.00045247
Epoch [125/300], Train Loss: 0.000464
Validation Loss: 0.00045563
Epoch [126/300], Train Loss: 0.000463
Validation Loss: 0.00046200
Epoch [127/300], Train Loss: 0.000463
Validation Loss: 0.00045069
Epoch [128/300], Train Loss: 0.000464
Validation Loss: 0.00044851
Epoch [129/300], Train Loss: 0.000460
Validation Loss: 0.00044959
Epoch [130/300], Train Loss: 0.000459
Validation Loss: 0.00045123
Epoch [131/300], Train Loss: 0.000464
Validation Loss: 0.00044757
Epoch [132/300], Train Loss: 0.000460
Validation Loss: 0.00044605
Epoch [133/300], Train Loss: 0.000459
Validation Loss: 0.00044572
Epoch [134/300], Train Loss: 0.000456
Validation Loss: 0.00044959
Epoch [135/300], Train Loss: 0.000458
Validation Loss: 0.00044519
Epoch [136/300], Train Loss: 0.000458
Validation Loss: 0.00044753
Epoch [137/300], Train Loss: 0.000456
Validation Loss: 0.00044598
Epoch [138/300], Train Loss: 0.000451
Validation Loss: 0.00045689
Epoch [139/300], Train Loss: 0.000457
Validation Loss: 0.00044239
Epoch [140/300], Train Loss: 0.000451
Validation Loss: 0.00044014
Epoch [141/300], Train Loss: 0.000457
Validation Loss: 0.00044129
Epoch [142/300], Train Loss: 0.000456
Validation Loss: 0.00046423
Epoch [143/300], Train Loss: 0.000462
Validation Loss: 0.00044360
Epoch [144/300], Train Loss: 0.000455
Validation Loss: 0.00044195
Epoch [145/300], Train Loss: 0.000457
Validation Loss: 0.00044121
Epoch [146/300], Train Loss: 0.000449
Validation Loss: 0.00043939
Epoch [147/300], Train Loss: 0.000454
Validation Loss: 0.00045873
Epoch [148/300], Train Loss: 0.000457
Validation Loss: 0.00043642
Epoch [149/300], Train Loss: 0.000450
Validation Loss: 0.00043766
Epoch [150/300], Train Loss: 0.000449
Validation Loss: 0.00043753
Epoch [151/300], Train Loss: 0.000453
Validation Loss: 0.00043731
Epoch [152/300], Train Loss: 0.000449
Validation Loss: 0.00043809
Epoch [153/300], Train Loss: 0.000449
Validation Loss: 0.00043474
Epoch [154/300], Train Loss: 0.000445
Validation Loss: 0.00043988
Epoch [155/300], Train Loss: 0.000451
Validation Loss: 0.00045189
Epoch [156/300], Train Loss: 0.000452
Validation Loss: 0.00043890
Epoch [157/300], Train Loss: 0.000447
Validation Loss: 0.00043155
Epoch [158/300], Train Loss: 0.000444
Validation Loss: 0.00043140
Epoch [159/300], Train Loss: 0.000448
Validation Loss: 0.00049835
Epoch [160/300], Train Loss: 0.000474
Validation Loss: 0.00043408
Epoch [161/300], Train Loss: 0.000444
Validation Loss: 0.00043240
Epoch [162/300], Train Loss: 0.000440
Validation Loss: 0.00043327
Epoch [163/300], Train Loss: 0.000440
Validation Loss: 0.00043470
Epoch [164/300], Train Loss: 0.000441
Validation Loss: 0.00043151
Epoch [165/300], Train Loss: 0.000449
Validation Loss: 0.00043899
Epoch [166/300], Train Loss: 0.000443
Validation Loss: 0.00042883
Epoch [167/300], Train Loss: 0.000443
Validation Loss: 0.00042885
Epoch [168/300], Train Loss: 0.000443
Validation Loss: 0.00043058
Epoch [169/300], Train Loss: 0.000444
Validation Loss: 0.00042688
Epoch [170/300], Train Loss: 0.000441
Validation Loss: 0.00042769
Epoch [171/300], Train Loss: 0.000437
Validation Loss: 0.00042882
Epoch [172/300], Train Loss: 0.000439
Validation Loss: 0.00042807
Epoch [173/300], Train Loss: 0.000439
Validation Loss: 0.00042617
Epoch [174/300], Train Loss: 0.000439
Validation Loss: 0.00045435
Epoch [175/300], Train Loss: 0.000442
Validation Loss: 0.00044310
Epoch [176/300], Train Loss: 0.000442
Validation Loss: 0.00042625
Epoch [177/300], Train Loss: 0.000438
Validation Loss: 0.00042846
Epoch [178/300], Train Loss: 0.000435
Validation Loss: 0.00042583
Epoch [179/300], Train Loss: 0.000439
Validation Loss: 0.00042558
Epoch [180/300], Train Loss: 0.000435
Validation Loss: 0.00042320
Epoch [181/300], Train Loss: 0.000435
Validation Loss: 0.00042360
Epoch [182/300], Train Loss: 0.000435
Validation Loss: 0.00042281
Epoch [183/300], Train Loss: 0.000432
Validation Loss: 0.00042177
Epoch [184/300], Train Loss: 0.000431
Validation Loss: 0.00042186
Epoch [185/300], Train Loss: 0.000434
Validation Loss: 0.00042425
Epoch [186/300], Train Loss: 0.000434
Validation Loss: 0.00042091
Epoch [187/300], Train Loss: 0.000436
Validation Loss: 0.00042104
Epoch [188/300], Train Loss: 0.000434
Validation Loss: 0.00042427
Epoch [189/300], Train Loss: 0.000433
Validation Loss: 0.00043067
Epoch [190/300], Train Loss: 0.000433
Validation Loss: 0.00042202
Epoch [191/300], Train Loss: 0.000434
Validation Loss: 0.00042032
Epoch [192/300], Train Loss: 0.000431
Validation Loss: 0.00043848
Epoch [193/300], Train Loss: 0.000445
Validation Loss: 0.00042059
Epoch [194/300], Train Loss: 0.000434
Validation Loss: 0.00042432
Epoch [195/300], Train Loss: 0.000436
Validation Loss: 0.00041845
Epoch [196/300], Train Loss: 0.000429
Validation Loss: 0.00041987
Epoch [197/300], Train Loss: 0.000431
Validation Loss: 0.00042181
Epoch [198/300], Train Loss: 0.000431
Validation Loss: 0.00042207
Epoch [199/300], Train Loss: 0.000427
Validation Loss: 0.00041837
Epoch [200/300], Train Loss: 0.000429
Validation Loss: 0.00041740
Epoch [201/300], Train Loss: 0.000432
Validation Loss: 0.00041799
Epoch [202/300], Train Loss: 0.000432
Validation Loss: 0.00041712
Epoch [203/300], Train Loss: 0.000431
Validation Loss: 0.00041844
Epoch [204/300], Train Loss: 0.000432
Validation Loss: 0.00042131
Epoch [205/300], Train Loss: 0.000432
Validation Loss: 0.00041645
Epoch [206/300], Train Loss: 0.000432
Validation Loss: 0.00041641
Epoch [207/300], Train Loss: 0.000424
Validation Loss: 0.00041638
Epoch [208/300], Train Loss: 0.000430
Validation Loss: 0.00041454
Epoch [209/300], Train Loss: 0.000425
Validation Loss: 0.00041662
Epoch [210/300], Train Loss: 0.000427
Validation Loss: 0.00041614
Epoch [211/300], Train Loss: 0.000424
Validation Loss: 0.00041816
Epoch [212/300], Train Loss: 0.000424
Validation Loss: 0.00041421
Epoch [213/300], Train Loss: 0.000427
Validation Loss: 0.00043030
Epoch [214/300], Train Loss: 0.000431
Validation Loss: 0.00041593
Epoch [215/300], Train Loss: 0.000424
Validation Loss: 0.00041494
Epoch [216/300], Train Loss: 0.000425
Validation Loss: 0.00041258
Epoch [217/300], Train Loss: 0.000426
Validation Loss: 0.00041209
Epoch [218/300], Train Loss: 0.000431
Validation Loss: 0.00041438
Epoch [219/300], Train Loss: 0.000425
Validation Loss: 0.00041206
Epoch [220/300], Train Loss: 0.000429
Validation Loss: 0.00043416
Epoch [221/300], Train Loss: 0.000435
Validation Loss: 0.00041540
Epoch [222/300], Train Loss: 0.000423
Validation Loss: 0.00041263
Epoch [223/300], Train Loss: 0.000430
Validation Loss: 0.00041081
Epoch [224/300], Train Loss: 0.000421
Validation Loss: 0.00041135
Epoch [225/300], Train Loss: 0.000420
Validation Loss: 0.00041063
Epoch [226/300], Train Loss: 0.000418
Validation Loss: 0.00041313
Epoch [227/300], Train Loss: 0.000427
Validation Loss: 0.00042756
Epoch [228/300], Train Loss: 0.000436
Validation Loss: 0.00041029
Epoch [229/300], Train Loss: 0.000428
Validation Loss: 0.00041310
Epoch [230/300], Train Loss: 0.000422
Validation Loss: 0.00040973
Epoch [231/300], Train Loss: 0.000421
Validation Loss: 0.00040840
Epoch [232/300], Train Loss: 0.000419
Validation Loss: 0.00040930
Epoch [233/300], Train Loss: 0.000418
Validation Loss: 0.00040799
Epoch [234/300], Train Loss: 0.000416
Validation Loss: 0.00040961
Epoch [235/300], Train Loss: 0.000417
Validation Loss: 0.00041089
Epoch [236/300], Train Loss: 0.000425
Validation Loss: 0.00041556
Epoch [237/300], Train Loss: 0.000424
Validation Loss: 0.00040726
Epoch [238/300], Train Loss: 0.000418
Validation Loss: 0.00041127
Epoch [239/300], Train Loss: 0.000424
Validation Loss: 0.00041058
Epoch [240/300], Train Loss: 0.000417
Validation Loss: 0.00040780
Epoch [241/300], Train Loss: 0.000417
Validation Loss: 0.00040678
Epoch [242/300], Train Loss: 0.000414
Validation Loss: 0.00040755
Epoch [243/300], Train Loss: 0.000417
Validation Loss: 0.00040697
Epoch [244/300], Train Loss: 0.000419
Validation Loss: 0.00041785
Epoch [245/300], Train Loss: 0.000417
Validation Loss: 0.00040922
Epoch [246/300], Train Loss: 0.000420
Validation Loss: 0.00041390
Epoch [247/300], Train Loss: 0.000422
Validation Loss: 0.00040668
Epoch [248/300], Train Loss: 0.000417
Validation Loss: 0.00040679
Epoch [249/300], Train Loss: 0.000418
Validation Loss: 0.00040634
Epoch [250/300], Train Loss: 0.000416
Validation Loss: 0.00040521
Epoch [251/300], Train Loss: 0.000416
Validation Loss: 0.00041048
Epoch [252/300], Train Loss: 0.000420
Validation Loss: 0.00040722
Epoch [253/300], Train Loss: 0.000412
Validation Loss: 0.00040481
Epoch [254/300], Train Loss: 0.000416
Validation Loss: 0.00040657
Epoch [255/300], Train Loss: 0.000415
Validation Loss: 0.00040449
Epoch [256/300], Train Loss: 0.000421
Validation Loss: 0.00040555
Epoch [257/300], Train Loss: 0.000415
Validation Loss: 0.00040453
Epoch [258/300], Train Loss: 0.000418
Validation Loss: 0.00041045
Epoch [259/300], Train Loss: 0.000415
Validation Loss: 0.00040567
Epoch [260/300], Train Loss: 0.000416
Validation Loss: 0.00040427
Epoch [261/300], Train Loss: 0.000412
Validation Loss: 0.00040355
Epoch [262/300], Train Loss: 0.000414
Validation Loss: 0.00040374
Epoch [263/300], Train Loss: 0.000418
Validation Loss: 0.00040323
Epoch [264/300], Train Loss: 0.000412
Validation Loss: 0.00040306
Epoch [265/300], Train Loss: 0.000415
Validation Loss: 0.00040332
Epoch [266/300], Train Loss: 0.000417
Validation Loss: 0.00040482
Epoch [267/300], Train Loss: 0.000412
Validation Loss: 0.00040968
Epoch [268/300], Train Loss: 0.000412
Validation Loss: 0.00040222
Epoch [269/300], Train Loss: 0.000416
Validation Loss: 0.00040446
Epoch [270/300], Train Loss: 0.000414
Validation Loss: 0.00041715
Epoch [271/300], Train Loss: 0.000419
Validation Loss: 0.00040467
Epoch [272/300], Train Loss: 0.000416
Validation Loss: 0.00040440
Epoch [273/300], Train Loss: 0.000414
Validation Loss: 0.00040149
Epoch [274/300], Train Loss: 0.000414
Validation Loss: 0.00040152
Epoch [275/300], Train Loss: 0.000415
Validation Loss: 0.00040376
Epoch [276/300], Train Loss: 0.000415
Validation Loss: 0.00040120
Epoch [277/300], Train Loss: 0.000414
Validation Loss: 0.00040590
Epoch [278/300], Train Loss: 0.000415
Validation Loss: 0.00040101
Epoch [279/300], Train Loss: 0.000408
Validation Loss: 0.00040384
Epoch [280/300], Train Loss: 0.000416
Validation Loss: 0.00040226
Epoch [281/300], Train Loss: 0.000414
Validation Loss: 0.00040098
Epoch [282/300], Train Loss: 0.000413
Validation Loss: 0.00040062
Epoch [283/300], Train Loss: 0.000412
Validation Loss: 0.00040035
Epoch [284/300], Train Loss: 0.000409
Validation Loss: 0.00040273
Epoch [285/300], Train Loss: 0.000417
Validation Loss: 0.00039926
Epoch [286/300], Train Loss: 0.000421
Validation Loss: 0.00040268
Epoch [287/300], Train Loss: 0.000409
Validation Loss: 0.00039944
Epoch [288/300], Train Loss: 0.000410
Validation Loss: 0.00040102
Epoch [289/300], Train Loss: 0.000413
Validation Loss: 0.00040060
Epoch [290/300], Train Loss: 0.000413
Validation Loss: 0.00040150
Epoch [291/300], Train Loss: 0.000409
Validation Loss: 0.00040039
Epoch [292/300], Train Loss: 0.000406
Validation Loss: 0.00040963
Epoch [293/300], Train Loss: 0.000420
Validation Loss: 0.00039899
Epoch [294/300], Train Loss: 0.000410
Validation Loss: 0.00040227
Epoch [295/300], Train Loss: 0.000411
Validation Loss: 0.00039923
Epoch [296/300], Train Loss: 0.000410
Validation Loss: 0.00039984
Epoch [297/300], Train Loss: 0.000405
Validation Loss: 0.00040047
Epoch [298/300], Train Loss: 0.000410
Validation Loss: 0.00039849
Epoch [299/300], Train Loss: 0.000409
Validation Loss: 0.00040147
Epoch [300/300], Train Loss: 0.000409
Validation Loss: 0.00040028

Evaluating model for: Fridge
Run 20/72 completed in 3551.13 seconds with: {'MAE': np.float32(26.8265), 'MSE': np.float32(1343.0385), 'RMSE': np.float32(36.647488), 'SAE': np.float32(0.00013705583), 'NDE': np.float32(0.6333128)}

Run 21/72: hidden=128, seq_len=720, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.000873
Validation Loss: 0.00068656
Epoch [2/300], Train Loss: 0.000684
Validation Loss: 0.00067116
Epoch [3/300], Train Loss: 0.000673
Validation Loss: 0.00066721
Epoch [4/300], Train Loss: 0.000673
Validation Loss: 0.00066620
Epoch [5/300], Train Loss: 0.000669
Validation Loss: 0.00066531
Epoch [6/300], Train Loss: 0.000669
Validation Loss: 0.00066452
Epoch [7/300], Train Loss: 0.000665
Validation Loss: 0.00066260
Epoch [8/300], Train Loss: 0.000663
Validation Loss: 0.00066127
Epoch [9/300], Train Loss: 0.000662
Validation Loss: 0.00065996
Epoch [10/300], Train Loss: 0.000654
Validation Loss: 0.00065828
Epoch [11/300], Train Loss: 0.000653
Validation Loss: 0.00065708
Epoch [12/300], Train Loss: 0.000654
Validation Loss: 0.00065460
Epoch [13/300], Train Loss: 0.000647
Validation Loss: 0.00065153
Epoch [14/300], Train Loss: 0.000642
Validation Loss: 0.00064864
Epoch [15/300], Train Loss: 0.000638
Validation Loss: 0.00064612
Epoch [16/300], Train Loss: 0.000631
Validation Loss: 0.00064336
Epoch [17/300], Train Loss: 0.000629
Validation Loss: 0.00063813
Epoch [18/300], Train Loss: 0.000617
Validation Loss: 0.00063334
Epoch [19/300], Train Loss: 0.000621
Validation Loss: 0.00062757
Epoch [20/300], Train Loss: 0.000614
Validation Loss: 0.00062576
Epoch [21/300], Train Loss: 0.000616
Validation Loss: 0.00062403
Epoch [22/300], Train Loss: 0.000614
Validation Loss: 0.00062291
Epoch [23/300], Train Loss: 0.000614
Validation Loss: 0.00062414
Epoch [24/300], Train Loss: 0.000616
Validation Loss: 0.00062196
Epoch [25/300], Train Loss: 0.000610
Validation Loss: 0.00062075
Epoch [26/300], Train Loss: 0.000610
Validation Loss: 0.00062084
Epoch [27/300], Train Loss: 0.000610
Validation Loss: 0.00062352
Epoch [28/300], Train Loss: 0.000610
Validation Loss: 0.00061872
Epoch [29/300], Train Loss: 0.000612
Validation Loss: 0.00061936
Epoch [30/300], Train Loss: 0.000610
Validation Loss: 0.00061868
Epoch [31/300], Train Loss: 0.000607
Validation Loss: 0.00061738
Epoch [32/300], Train Loss: 0.000608
Validation Loss: 0.00061725
Epoch [33/300], Train Loss: 0.000604
Validation Loss: 0.00061642
Epoch [34/300], Train Loss: 0.000607
Validation Loss: 0.00061770
Epoch [35/300], Train Loss: 0.000604
Validation Loss: 0.00061629
Epoch [36/300], Train Loss: 0.000604
Validation Loss: 0.00061568
Epoch [37/300], Train Loss: 0.000605
Validation Loss: 0.00061455
Epoch [38/300], Train Loss: 0.000609
Validation Loss: 0.00061598
Epoch [39/300], Train Loss: 0.000602
Validation Loss: 0.00061543
Epoch [40/300], Train Loss: 0.000601
Validation Loss: 0.00061458
Epoch [41/300], Train Loss: 0.000603
Validation Loss: 0.00061316
Epoch [42/300], Train Loss: 0.000602
Validation Loss: 0.00061381
Epoch [43/300], Train Loss: 0.000602
Validation Loss: 0.00061272
Epoch [44/300], Train Loss: 0.000606
Validation Loss: 0.00061342
Epoch [45/300], Train Loss: 0.000602
Validation Loss: 0.00061323
Epoch [46/300], Train Loss: 0.000598
Validation Loss: 0.00061157
Epoch [47/300], Train Loss: 0.000601
Validation Loss: 0.00061294
Epoch [48/300], Train Loss: 0.000599
Validation Loss: 0.00061694
Epoch [49/300], Train Loss: 0.000600
Validation Loss: 0.00061128
Epoch [50/300], Train Loss: 0.000602
Validation Loss: 0.00061060
Epoch [51/300], Train Loss: 0.000601
Validation Loss: 0.00061116
Epoch [52/300], Train Loss: 0.000601
Validation Loss: 0.00061276
Epoch [53/300], Train Loss: 0.000598
Validation Loss: 0.00061294
Epoch [54/300], Train Loss: 0.000597
Validation Loss: 0.00061285
Epoch [55/300], Train Loss: 0.000599
Validation Loss: 0.00061076
Epoch [56/300], Train Loss: 0.000596
Validation Loss: 0.00061416
Epoch [57/300], Train Loss: 0.000600
Validation Loss: 0.00060947
Epoch [58/300], Train Loss: 0.000599
Validation Loss: 0.00060869
Epoch [59/300], Train Loss: 0.000595
Validation Loss: 0.00061214
Epoch [60/300], Train Loss: 0.000600
Validation Loss: 0.00060822
Epoch [61/300], Train Loss: 0.000595
Validation Loss: 0.00060795
Epoch [62/300], Train Loss: 0.000597
Validation Loss: 0.00061008
Epoch [63/300], Train Loss: 0.000598
Validation Loss: 0.00060987
Epoch [64/300], Train Loss: 0.000595
Validation Loss: 0.00060916
Epoch [65/300], Train Loss: 0.000593
Validation Loss: 0.00060973
Epoch [66/300], Train Loss: 0.000596
Validation Loss: 0.00060675
Epoch [67/300], Train Loss: 0.000597
Validation Loss: 0.00060757
Epoch [68/300], Train Loss: 0.000593
Validation Loss: 0.00060619
Epoch [69/300], Train Loss: 0.000595
Validation Loss: 0.00060599
Epoch [70/300], Train Loss: 0.000592
Validation Loss: 0.00060705
Epoch [71/300], Train Loss: 0.000595
Validation Loss: 0.00060545
Epoch [72/300], Train Loss: 0.000594
Validation Loss: 0.00060511
Epoch [73/300], Train Loss: 0.000595
Validation Loss: 0.00060604
Epoch [74/300], Train Loss: 0.000595
Validation Loss: 0.00060578
Epoch [75/300], Train Loss: 0.000596
Validation Loss: 0.00060489
Epoch [76/300], Train Loss: 0.000592
Validation Loss: 0.00060424
Epoch [77/300], Train Loss: 0.000593
Validation Loss: 0.00060398
Epoch [78/300], Train Loss: 0.000595
Validation Loss: 0.00060327
Epoch [79/300], Train Loss: 0.000592
Validation Loss: 0.00060403
Epoch [80/300], Train Loss: 0.000595
Validation Loss: 0.00060372
Epoch [81/300], Train Loss: 0.000593
Validation Loss: 0.00060342
Epoch [82/300], Train Loss: 0.000590
Validation Loss: 0.00060192
Epoch [83/300], Train Loss: 0.000594
Validation Loss: 0.00060244
Epoch [84/300], Train Loss: 0.000589
Validation Loss: 0.00060288
Epoch [85/300], Train Loss: 0.000592
Validation Loss: 0.00060157
Epoch [86/300], Train Loss: 0.000588
Validation Loss: 0.00060128
Epoch [87/300], Train Loss: 0.000586
Validation Loss: 0.00060107
Epoch [88/300], Train Loss: 0.000591
Validation Loss: 0.00060055
Epoch [89/300], Train Loss: 0.000591
Validation Loss: 0.00060055
Epoch [90/300], Train Loss: 0.000593
Validation Loss: 0.00060129
Epoch [91/300], Train Loss: 0.000592
Validation Loss: 0.00059989
Epoch [92/300], Train Loss: 0.000590
Validation Loss: 0.00060077
Epoch [93/300], Train Loss: 0.000589
Validation Loss: 0.00059921
Epoch [94/300], Train Loss: 0.000584
Validation Loss: 0.00059952
Epoch [95/300], Train Loss: 0.000591
Validation Loss: 0.00059939
Epoch [96/300], Train Loss: 0.000591
Validation Loss: 0.00059836
Epoch [97/300], Train Loss: 0.000591
Validation Loss: 0.00059946
Epoch [98/300], Train Loss: 0.000586
Validation Loss: 0.00060012
Epoch [99/300], Train Loss: 0.000586
Validation Loss: 0.00059788
Epoch [100/300], Train Loss: 0.000587
Validation Loss: 0.00059840
Epoch [101/300], Train Loss: 0.000590
Validation Loss: 0.00060227
Epoch [102/300], Train Loss: 0.000588
Validation Loss: 0.00059686
Epoch [103/300], Train Loss: 0.000586
Validation Loss: 0.00059735
Epoch [104/300], Train Loss: 0.000585
Validation Loss: 0.00059686
Epoch [105/300], Train Loss: 0.000589
Validation Loss: 0.00059658
Epoch [106/300], Train Loss: 0.000584
Validation Loss: 0.00059620
Epoch [107/300], Train Loss: 0.000588
Validation Loss: 0.00059568
Epoch [108/300], Train Loss: 0.000585
Validation Loss: 0.00059572
Epoch [109/300], Train Loss: 0.000580
Validation Loss: 0.00059559
Epoch [110/300], Train Loss: 0.000585
Validation Loss: 0.00059610
Epoch [111/300], Train Loss: 0.000583
Validation Loss: 0.00059509
Epoch [112/300], Train Loss: 0.000587
Validation Loss: 0.00059406
Epoch [113/300], Train Loss: 0.000584
Validation Loss: 0.00059359
Epoch [114/300], Train Loss: 0.000583
Validation Loss: 0.00059363
Epoch [115/300], Train Loss: 0.000582
Validation Loss: 0.00059321
Epoch [116/300], Train Loss: 0.000582
Validation Loss: 0.00059264
Epoch [117/300], Train Loss: 0.000581
Validation Loss: 0.00059242
Epoch [118/300], Train Loss: 0.000583
Validation Loss: 0.00059232
Epoch [119/300], Train Loss: 0.000583
Validation Loss: 0.00059226
Epoch [120/300], Train Loss: 0.000581
Validation Loss: 0.00059272
Epoch [121/300], Train Loss: 0.000579
Validation Loss: 0.00059034
Epoch [122/300], Train Loss: 0.000583
Validation Loss: 0.00059069
Epoch [123/300], Train Loss: 0.000583
Validation Loss: 0.00058966
Epoch [124/300], Train Loss: 0.000581
Validation Loss: 0.00058920
Epoch [125/300], Train Loss: 0.000580
Validation Loss: 0.00058917
Epoch [126/300], Train Loss: 0.000581
Validation Loss: 0.00058877
Epoch [127/300], Train Loss: 0.000580
Validation Loss: 0.00058867
Epoch [128/300], Train Loss: 0.000578
Validation Loss: 0.00058821
Epoch [129/300], Train Loss: 0.000576
Validation Loss: 0.00058707
Epoch [130/300], Train Loss: 0.000578
Validation Loss: 0.00058873
Epoch [131/300], Train Loss: 0.000576
Validation Loss: 0.00058779
Epoch [132/300], Train Loss: 0.000577
Validation Loss: 0.00058616
Epoch [133/300], Train Loss: 0.000575
Validation Loss: 0.00058597
Epoch [134/300], Train Loss: 0.000577
Validation Loss: 0.00058544
Epoch [135/300], Train Loss: 0.000576
Validation Loss: 0.00058424
Epoch [136/300], Train Loss: 0.000579
Validation Loss: 0.00058505
Epoch [137/300], Train Loss: 0.000573
Validation Loss: 0.00058420
Epoch [138/300], Train Loss: 0.000578
Validation Loss: 0.00058273
Epoch [139/300], Train Loss: 0.000574
Validation Loss: 0.00058345
Epoch [140/300], Train Loss: 0.000578
Validation Loss: 0.00058171
Epoch [141/300], Train Loss: 0.000573
Validation Loss: 0.00058137
Epoch [142/300], Train Loss: 0.000574
Validation Loss: 0.00058214
Epoch [143/300], Train Loss: 0.000575
Validation Loss: 0.00058267
Epoch [144/300], Train Loss: 0.000571
Validation Loss: 0.00058257
Epoch [145/300], Train Loss: 0.000575
Validation Loss: 0.00058116
Epoch [146/300], Train Loss: 0.000573
Validation Loss: 0.00057943
Epoch [147/300], Train Loss: 0.000571
Validation Loss: 0.00058104
Epoch [148/300], Train Loss: 0.000575
Validation Loss: 0.00057898
Epoch [149/300], Train Loss: 0.000570
Validation Loss: 0.00057778
Epoch [150/300], Train Loss: 0.000572
Validation Loss: 0.00057690
Epoch [151/300], Train Loss: 0.000569
Validation Loss: 0.00057644
Epoch [152/300], Train Loss: 0.000574
Validation Loss: 0.00057654
Epoch [153/300], Train Loss: 0.000569
Validation Loss: 0.00057592
Epoch [154/300], Train Loss: 0.000568
Validation Loss: 0.00058035
Epoch [155/300], Train Loss: 0.000570
Validation Loss: 0.00057510
Epoch [156/300], Train Loss: 0.000567
Validation Loss: 0.00057390
Epoch [157/300], Train Loss: 0.000569
Validation Loss: 0.00057304
Epoch [158/300], Train Loss: 0.000566
Validation Loss: 0.00057242
Epoch [159/300], Train Loss: 0.000566
Validation Loss: 0.00057189
Epoch [160/300], Train Loss: 0.000564
Validation Loss: 0.00057405
Epoch [161/300], Train Loss: 0.000566
Validation Loss: 0.00057177
Epoch [162/300], Train Loss: 0.000561
Validation Loss: 0.00056984
Epoch [163/300], Train Loss: 0.000567
Validation Loss: 0.00056969
Epoch [164/300], Train Loss: 0.000563
Validation Loss: 0.00056934
Epoch [165/300], Train Loss: 0.000563
Validation Loss: 0.00056874
Epoch [166/300], Train Loss: 0.000563
Validation Loss: 0.00056849
Epoch [167/300], Train Loss: 0.000563
Validation Loss: 0.00056702
Epoch [168/300], Train Loss: 0.000561
Validation Loss: 0.00056933
Epoch [169/300], Train Loss: 0.000565
Validation Loss: 0.00056554
Epoch [170/300], Train Loss: 0.000565
Validation Loss: 0.00056621
Epoch [171/300], Train Loss: 0.000566
Validation Loss: 0.00056693
Epoch [172/300], Train Loss: 0.000558
Validation Loss: 0.00056725
Epoch [173/300], Train Loss: 0.000563
Validation Loss: 0.00056412
Epoch [174/300], Train Loss: 0.000560
Validation Loss: 0.00056351
Epoch [175/300], Train Loss: 0.000561
Validation Loss: 0.00056354
Epoch [176/300], Train Loss: 0.000559
Validation Loss: 0.00056173
Epoch [177/300], Train Loss: 0.000558
Validation Loss: 0.00056119
Epoch [178/300], Train Loss: 0.000562
Validation Loss: 0.00056240
Epoch [179/300], Train Loss: 0.000559
Validation Loss: 0.00056240
Epoch [180/300], Train Loss: 0.000559
Validation Loss: 0.00055918
Epoch [181/300], Train Loss: 0.000557
Validation Loss: 0.00055915
Epoch [182/300], Train Loss: 0.000554
Validation Loss: 0.00056086
Epoch [183/300], Train Loss: 0.000558
Validation Loss: 0.00055724
Epoch [184/300], Train Loss: 0.000553
Validation Loss: 0.00055621
Epoch [185/300], Train Loss: 0.000556
Validation Loss: 0.00055547
Epoch [186/300], Train Loss: 0.000552
Validation Loss: 0.00055509
Epoch [187/300], Train Loss: 0.000550
Validation Loss: 0.00055368
Epoch [188/300], Train Loss: 0.000553
Validation Loss: 0.00055351
Epoch [189/300], Train Loss: 0.000551
Validation Loss: 0.00055320
Epoch [190/300], Train Loss: 0.000553
Validation Loss: 0.00055368
Epoch [191/300], Train Loss: 0.000557
Validation Loss: 0.00055614
Epoch [192/300], Train Loss: 0.000558
Validation Loss: 0.00055148
Epoch [193/300], Train Loss: 0.000552
Validation Loss: 0.00055065
Epoch [194/300], Train Loss: 0.000548
Validation Loss: 0.00055261
Epoch [195/300], Train Loss: 0.000547
Validation Loss: 0.00054896
Epoch [196/300], Train Loss: 0.000551
Validation Loss: 0.00054967
Epoch [197/300], Train Loss: 0.000543
Validation Loss: 0.00054987
Epoch [198/300], Train Loss: 0.000546
Validation Loss: 0.00054771
Epoch [199/300], Train Loss: 0.000544
Validation Loss: 0.00054767
Epoch [200/300], Train Loss: 0.000547
Validation Loss: 0.00054755
Epoch [201/300], Train Loss: 0.000547
Validation Loss: 0.00054681
Epoch [202/300], Train Loss: 0.000548
Validation Loss: 0.00054628
Epoch [203/300], Train Loss: 0.000544
Validation Loss: 0.00054754
Epoch [204/300], Train Loss: 0.000545
Validation Loss: 0.00055611
Epoch [205/300], Train Loss: 0.000550
Validation Loss: 0.00054504
Epoch [206/300], Train Loss: 0.000542
Validation Loss: 0.00054475
Epoch [207/300], Train Loss: 0.000546
Validation Loss: 0.00054532
Epoch [208/300], Train Loss: 0.000544
Validation Loss: 0.00054369
Epoch [209/300], Train Loss: 0.000550
Validation Loss: 0.00054524
Epoch [210/300], Train Loss: 0.000545
Validation Loss: 0.00054747
Epoch [211/300], Train Loss: 0.000547
Validation Loss: 0.00054309
Epoch [212/300], Train Loss: 0.000547
Validation Loss: 0.00054538
Epoch [213/300], Train Loss: 0.000543
Validation Loss: 0.00054520
Epoch [214/300], Train Loss: 0.000545
Validation Loss: 0.00054302
Epoch [215/300], Train Loss: 0.000539
Validation Loss: 0.00054221
Epoch [216/300], Train Loss: 0.000544
Validation Loss: 0.00054362
Epoch [217/300], Train Loss: 0.000541
Validation Loss: 0.00054229
Epoch [218/300], Train Loss: 0.000541
Validation Loss: 0.00054143
Epoch [219/300], Train Loss: 0.000543
Validation Loss: 0.00054208
Epoch [220/300], Train Loss: 0.000540
Validation Loss: 0.00054177
Epoch [221/300], Train Loss: 0.000540
Validation Loss: 0.00054030
Epoch [222/300], Train Loss: 0.000543
Validation Loss: 0.00054083
Epoch [223/300], Train Loss: 0.000540
Validation Loss: 0.00054017
Epoch [224/300], Train Loss: 0.000543
Validation Loss: 0.00054005
Epoch [225/300], Train Loss: 0.000542
Validation Loss: 0.00054010
Epoch [226/300], Train Loss: 0.000544
Validation Loss: 0.00054023
Epoch [227/300], Train Loss: 0.000540
Validation Loss: 0.00053963
Epoch [228/300], Train Loss: 0.000544
Validation Loss: 0.00054045
Epoch [229/300], Train Loss: 0.000538
Validation Loss: 0.00053891
Epoch [230/300], Train Loss: 0.000540
Validation Loss: 0.00053958
Epoch [231/300], Train Loss: 0.000537
Validation Loss: 0.00053887
Epoch [232/300], Train Loss: 0.000538
Validation Loss: 0.00054054
Epoch [233/300], Train Loss: 0.000542
Validation Loss: 0.00053805
Epoch [234/300], Train Loss: 0.000542
Validation Loss: 0.00053853
Epoch [235/300], Train Loss: 0.000537
Validation Loss: 0.00053891
Epoch [236/300], Train Loss: 0.000538
Validation Loss: 0.00053900
Epoch [237/300], Train Loss: 0.000538
Validation Loss: 0.00053682
Epoch [238/300], Train Loss: 0.000538
Validation Loss: 0.00053664
Epoch [239/300], Train Loss: 0.000538
Validation Loss: 0.00053833
Epoch [240/300], Train Loss: 0.000536
Validation Loss: 0.00053869
Epoch [241/300], Train Loss: 0.000541
Validation Loss: 0.00053624
Epoch [242/300], Train Loss: 0.000538
Validation Loss: 0.00053600
Epoch [243/300], Train Loss: 0.000537
Validation Loss: 0.00053580
Epoch [244/300], Train Loss: 0.000539
Validation Loss: 0.00053603
Epoch [245/300], Train Loss: 0.000537
Validation Loss: 0.00053629
Epoch [246/300], Train Loss: 0.000539
Validation Loss: 0.00053584
Epoch [247/300], Train Loss: 0.000537
Validation Loss: 0.00053577
Epoch [248/300], Train Loss: 0.000542
Validation Loss: 0.00053526
Epoch [249/300], Train Loss: 0.000539
Validation Loss: 0.00053512
Epoch [250/300], Train Loss: 0.000534
Validation Loss: 0.00053496
Epoch [251/300], Train Loss: 0.000535
Validation Loss: 0.00053437
Epoch [252/300], Train Loss: 0.000534
Validation Loss: 0.00053429
Epoch [253/300], Train Loss: 0.000536
Validation Loss: 0.00053420
Epoch [254/300], Train Loss: 0.000534
Validation Loss: 0.00053449
Epoch [255/300], Train Loss: 0.000535
Validation Loss: 0.00053391
Epoch [256/300], Train Loss: 0.000539
Validation Loss: 0.00053868
Epoch [257/300], Train Loss: 0.000533
Validation Loss: 0.00053326
Epoch [258/300], Train Loss: 0.000534
Validation Loss: 0.00053478
Epoch [259/300], Train Loss: 0.000536
Validation Loss: 0.00053401
Epoch [260/300], Train Loss: 0.000535
Validation Loss: 0.00053304
Epoch [261/300], Train Loss: 0.000537
Validation Loss: 0.00053311
Epoch [262/300], Train Loss: 0.000533
Validation Loss: 0.00053203
Epoch [263/300], Train Loss: 0.000538
Validation Loss: 0.00053236
Epoch [264/300], Train Loss: 0.000537
Validation Loss: 0.00053241
Epoch [265/300], Train Loss: 0.000534
Validation Loss: 0.00053216
Epoch [266/300], Train Loss: 0.000535
Validation Loss: 0.00053213
Epoch [267/300], Train Loss: 0.000534
Validation Loss: 0.00053150
Epoch [268/300], Train Loss: 0.000536
Validation Loss: 0.00053215
Epoch [269/300], Train Loss: 0.000535
Validation Loss: 0.00053125
Epoch [270/300], Train Loss: 0.000532
Validation Loss: 0.00053316
Epoch [271/300], Train Loss: 0.000531
Validation Loss: 0.00053427
Epoch [272/300], Train Loss: 0.000537
Validation Loss: 0.00053079
Epoch [273/300], Train Loss: 0.000534
Validation Loss: 0.00053247
Epoch [274/300], Train Loss: 0.000533
Validation Loss: 0.00053097
Epoch [275/300], Train Loss: 0.000534
Validation Loss: 0.00053057
Epoch [276/300], Train Loss: 0.000530
Validation Loss: 0.00053124
Epoch [277/300], Train Loss: 0.000529
Validation Loss: 0.00053070
Epoch [278/300], Train Loss: 0.000534
Validation Loss: 0.00053025
Epoch [279/300], Train Loss: 0.000531
Validation Loss: 0.00053105
Epoch [280/300], Train Loss: 0.000535
Validation Loss: 0.00052959
Epoch [281/300], Train Loss: 0.000530
Validation Loss: 0.00052985
Epoch [282/300], Train Loss: 0.000532
Validation Loss: 0.00053326
Epoch [283/300], Train Loss: 0.000532
Validation Loss: 0.00052949
Epoch [284/300], Train Loss: 0.000532
Validation Loss: 0.00052884
Epoch [285/300], Train Loss: 0.000535
Validation Loss: 0.00053001
Epoch [286/300], Train Loss: 0.000533
Validation Loss: 0.00053263
Epoch [287/300], Train Loss: 0.000535
Validation Loss: 0.00052864
Epoch [288/300], Train Loss: 0.000533
Validation Loss: 0.00052878
Epoch [289/300], Train Loss: 0.000532
Validation Loss: 0.00052817
Epoch [290/300], Train Loss: 0.000528
Validation Loss: 0.00052822
Epoch [291/300], Train Loss: 0.000536
Validation Loss: 0.00052842
Epoch [292/300], Train Loss: 0.000534
Validation Loss: 0.00052805
Epoch [293/300], Train Loss: 0.000534
Validation Loss: 0.00052837
Epoch [294/300], Train Loss: 0.000532
Validation Loss: 0.00052812
Epoch [295/300], Train Loss: 0.000529
Validation Loss: 0.00052843
Epoch [296/300], Train Loss: 0.000530
Validation Loss: 0.00052779
Epoch [297/300], Train Loss: 0.000530
Validation Loss: 0.00053098
Epoch [298/300], Train Loss: 0.000532
Validation Loss: 0.00052708
Epoch [299/300], Train Loss: 0.000528
Validation Loss: 0.00052735
Epoch [300/300], Train Loss: 0.000529
Validation Loss: 0.00052702

Evaluating model for: Fridge
Run 21/72 completed in 1572.74 seconds with: {'MAE': np.float32(33.818428), 'MSE': np.float32(1771.5308), 'RMSE': np.float32(42.089558), 'SAE': np.float32(0.02372465), 'NDE': np.float32(0.7293828)}

Run 22/72: hidden=128, seq_len=720, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.001850
Validation Loss: 0.00079875
Epoch [2/300], Train Loss: 0.000715
Validation Loss: 0.00068948
Epoch [3/300], Train Loss: 0.000688
Validation Loss: 0.00067525
Epoch [4/300], Train Loss: 0.000686
Validation Loss: 0.00067279
Epoch [5/300], Train Loss: 0.000683
Validation Loss: 0.00067239
Epoch [6/300], Train Loss: 0.000683
Validation Loss: 0.00067197
Epoch [7/300], Train Loss: 0.000681
Validation Loss: 0.00067151
Epoch [8/300], Train Loss: 0.000681
Validation Loss: 0.00067111
Epoch [9/300], Train Loss: 0.000681
Validation Loss: 0.00067061
Epoch [10/300], Train Loss: 0.000675
Validation Loss: 0.00067021
Epoch [11/300], Train Loss: 0.000676
Validation Loss: 0.00066950
Epoch [12/300], Train Loss: 0.000679
Validation Loss: 0.00066883
Epoch [13/300], Train Loss: 0.000675
Validation Loss: 0.00066800
Epoch [14/300], Train Loss: 0.000673
Validation Loss: 0.00066700
Epoch [15/300], Train Loss: 0.000673
Validation Loss: 0.00066596
Epoch [16/300], Train Loss: 0.000667
Validation Loss: 0.00066470
Epoch [17/300], Train Loss: 0.000668
Validation Loss: 0.00066391
Epoch [18/300], Train Loss: 0.000658
Validation Loss: 0.00066287
Epoch [19/300], Train Loss: 0.000663
Validation Loss: 0.00066189
Epoch [20/300], Train Loss: 0.000655
Validation Loss: 0.00066092
Epoch [21/300], Train Loss: 0.000657
Validation Loss: 0.00065998
Epoch [22/300], Train Loss: 0.000654
Validation Loss: 0.00065829
Epoch [23/300], Train Loss: 0.000653
Validation Loss: 0.00065682
Epoch [24/300], Train Loss: 0.000652
Validation Loss: 0.00065394
Epoch [25/300], Train Loss: 0.000643
Validation Loss: 0.00065131
Epoch [26/300], Train Loss: 0.000643
Validation Loss: 0.00064961
Epoch [27/300], Train Loss: 0.000640
Validation Loss: 0.00064563
Epoch [28/300], Train Loss: 0.000635
Validation Loss: 0.00064022
Epoch [29/300], Train Loss: 0.000633
Validation Loss: 0.00063606
Epoch [30/300], Train Loss: 0.000631
Validation Loss: 0.00063247
Epoch [31/300], Train Loss: 0.000628
Validation Loss: 0.00062942
Epoch [32/300], Train Loss: 0.000627
Validation Loss: 0.00062867
Epoch [33/300], Train Loss: 0.000623
Validation Loss: 0.00062853
Epoch [34/300], Train Loss: 0.000625
Validation Loss: 0.00062808
Epoch [35/300], Train Loss: 0.000622
Validation Loss: 0.00062591
Epoch [36/300], Train Loss: 0.000621
Validation Loss: 0.00062469
Epoch [37/300], Train Loss: 0.000621
Validation Loss: 0.00062268
Epoch [38/300], Train Loss: 0.000624
Validation Loss: 0.00062260
Epoch [39/300], Train Loss: 0.000616
Validation Loss: 0.00062291
Epoch [40/300], Train Loss: 0.000616
Validation Loss: 0.00062089
Epoch [41/300], Train Loss: 0.000618
Validation Loss: 0.00061997
Epoch [42/300], Train Loss: 0.000616
Validation Loss: 0.00061919
Epoch [43/300], Train Loss: 0.000615
Validation Loss: 0.00061870
Epoch [44/300], Train Loss: 0.000618
Validation Loss: 0.00061927
Epoch [45/300], Train Loss: 0.000615
Validation Loss: 0.00061811
Epoch [46/300], Train Loss: 0.000610
Validation Loss: 0.00061799
Epoch [47/300], Train Loss: 0.000614
Validation Loss: 0.00061869
Epoch [48/300], Train Loss: 0.000611
Validation Loss: 0.00062249
Epoch [49/300], Train Loss: 0.000612
Validation Loss: 0.00061613
Epoch [50/300], Train Loss: 0.000615
Validation Loss: 0.00061582
Epoch [51/300], Train Loss: 0.000613
Validation Loss: 0.00061679
Epoch [52/300], Train Loss: 0.000614
Validation Loss: 0.00061746
Epoch [53/300], Train Loss: 0.000609
Validation Loss: 0.00061688
Epoch [54/300], Train Loss: 0.000608
Validation Loss: 0.00061533
Epoch [55/300], Train Loss: 0.000611
Validation Loss: 0.00061571
Epoch [56/300], Train Loss: 0.000607
Validation Loss: 0.00061637
Epoch [57/300], Train Loss: 0.000612
Validation Loss: 0.00061501
Epoch [58/300], Train Loss: 0.000611
Validation Loss: 0.00061383
Epoch [59/300], Train Loss: 0.000607
Validation Loss: 0.00061538
Epoch [60/300], Train Loss: 0.000612
Validation Loss: 0.00061349
Epoch [61/300], Train Loss: 0.000606
Validation Loss: 0.00061291
Epoch [62/300], Train Loss: 0.000609
Validation Loss: 0.00061440
Epoch [63/300], Train Loss: 0.000610
Validation Loss: 0.00061580
Epoch [64/300], Train Loss: 0.000606
Validation Loss: 0.00061543
Epoch [65/300], Train Loss: 0.000605
Validation Loss: 0.00061522
Epoch [66/300], Train Loss: 0.000608
Validation Loss: 0.00061243
Epoch [67/300], Train Loss: 0.000608
Validation Loss: 0.00061269
Epoch [68/300], Train Loss: 0.000604
Validation Loss: 0.00061168
Epoch [69/300], Train Loss: 0.000607
Validation Loss: 0.00061154
Epoch [70/300], Train Loss: 0.000604
Validation Loss: 0.00061173
Epoch [71/300], Train Loss: 0.000606
Validation Loss: 0.00061095
Epoch [72/300], Train Loss: 0.000606
Validation Loss: 0.00061107
Epoch [73/300], Train Loss: 0.000607
Validation Loss: 0.00061188
Epoch [74/300], Train Loss: 0.000608
Validation Loss: 0.00061191
Epoch [75/300], Train Loss: 0.000608
Validation Loss: 0.00061168
Epoch [76/300], Train Loss: 0.000604
Validation Loss: 0.00061013
Epoch [77/300], Train Loss: 0.000605
Validation Loss: 0.00061004
Epoch [78/300], Train Loss: 0.000607
Validation Loss: 0.00060969
Epoch [79/300], Train Loss: 0.000605
Validation Loss: 0.00061123
Epoch [80/300], Train Loss: 0.000607
Validation Loss: 0.00060949
Epoch [81/300], Train Loss: 0.000605
Validation Loss: 0.00060991
Epoch [82/300], Train Loss: 0.000602
Validation Loss: 0.00060906
Epoch [83/300], Train Loss: 0.000606
Validation Loss: 0.00060899
Epoch [84/300], Train Loss: 0.000602
Validation Loss: 0.00060949
Epoch [85/300], Train Loss: 0.000604
Validation Loss: 0.00060908
Epoch [86/300], Train Loss: 0.000601
Validation Loss: 0.00060848
Epoch [87/300], Train Loss: 0.000598
Validation Loss: 0.00060773
Epoch [88/300], Train Loss: 0.000604
Validation Loss: 0.00060760
Epoch [89/300], Train Loss: 0.000603
Validation Loss: 0.00060820
Epoch [90/300], Train Loss: 0.000605
Validation Loss: 0.00060886
Epoch [91/300], Train Loss: 0.000603
Validation Loss: 0.00060730
Epoch [92/300], Train Loss: 0.000602
Validation Loss: 0.00060799
Epoch [93/300], Train Loss: 0.000601
Validation Loss: 0.00060646
Epoch [94/300], Train Loss: 0.000597
Validation Loss: 0.00060689
Epoch [95/300], Train Loss: 0.000603
Validation Loss: 0.00060643
Epoch [96/300], Train Loss: 0.000603
Validation Loss: 0.00060554
Epoch [97/300], Train Loss: 0.000602
Validation Loss: 0.00060574
Epoch [98/300], Train Loss: 0.000597
Validation Loss: 0.00060710
Epoch [99/300], Train Loss: 0.000598
Validation Loss: 0.00060530
Epoch [100/300], Train Loss: 0.000600
Validation Loss: 0.00060680
Epoch [101/300], Train Loss: 0.000603
Validation Loss: 0.00060660
Epoch [102/300], Train Loss: 0.000599
Validation Loss: 0.00060442
Epoch [103/300], Train Loss: 0.000598
Validation Loss: 0.00060456
Epoch [104/300], Train Loss: 0.000598
Validation Loss: 0.00060522
Epoch [105/300], Train Loss: 0.000602
Validation Loss: 0.00060395
Epoch [106/300], Train Loss: 0.000596
Validation Loss: 0.00060388
Epoch [107/300], Train Loss: 0.000601
Validation Loss: 0.00060423
Epoch [108/300], Train Loss: 0.000598
Validation Loss: 0.00060362
Epoch [109/300], Train Loss: 0.000592
Validation Loss: 0.00060339
Epoch [110/300], Train Loss: 0.000598
Validation Loss: 0.00060309
Epoch [111/300], Train Loss: 0.000595
Validation Loss: 0.00060254
Epoch [112/300], Train Loss: 0.000599
Validation Loss: 0.00060393
Epoch [113/300], Train Loss: 0.000597
Validation Loss: 0.00060273
Epoch [114/300], Train Loss: 0.000597
Validation Loss: 0.00060266
Epoch [115/300], Train Loss: 0.000595
Validation Loss: 0.00060298
Epoch [116/300], Train Loss: 0.000595
Validation Loss: 0.00060187
Epoch [117/300], Train Loss: 0.000595
Validation Loss: 0.00060147
Epoch [118/300], Train Loss: 0.000597
Validation Loss: 0.00060358
Epoch [119/300], Train Loss: 0.000597
Validation Loss: 0.00060139
Epoch [120/300], Train Loss: 0.000594
Validation Loss: 0.00060158
Epoch [121/300], Train Loss: 0.000593
Validation Loss: 0.00060095
Epoch [122/300], Train Loss: 0.000598
Validation Loss: 0.00060082
Epoch [123/300], Train Loss: 0.000597
Validation Loss: 0.00060138
Epoch [124/300], Train Loss: 0.000597
Validation Loss: 0.00060101
Epoch [125/300], Train Loss: 0.000594
Validation Loss: 0.00060096
Epoch [126/300], Train Loss: 0.000596
Validation Loss: 0.00060098
Epoch [127/300], Train Loss: 0.000595
Validation Loss: 0.00060012
Epoch [128/300], Train Loss: 0.000594
Validation Loss: 0.00060059
Epoch [129/300], Train Loss: 0.000591
Validation Loss: 0.00060053
Epoch [130/300], Train Loss: 0.000594
Validation Loss: 0.00060054
Epoch [131/300], Train Loss: 0.000592
Validation Loss: 0.00059968
Epoch [132/300], Train Loss: 0.000593
Validation Loss: 0.00059961
Epoch [133/300], Train Loss: 0.000592
Validation Loss: 0.00060065
Epoch [134/300], Train Loss: 0.000594
Validation Loss: 0.00060018
Epoch [135/300], Train Loss: 0.000593
Validation Loss: 0.00059870
Epoch [136/300], Train Loss: 0.000596
Validation Loss: 0.00059988
Epoch [137/300], Train Loss: 0.000590
Validation Loss: 0.00059973
Epoch [138/300], Train Loss: 0.000596
Validation Loss: 0.00059798
Epoch [139/300], Train Loss: 0.000592
Validation Loss: 0.00059871
Epoch [140/300], Train Loss: 0.000595
Validation Loss: 0.00059806
Epoch [141/300], Train Loss: 0.000592
Validation Loss: 0.00059856
Epoch [142/300], Train Loss: 0.000593
Validation Loss: 0.00059776
Epoch [143/300], Train Loss: 0.000593
Validation Loss: 0.00059854
Epoch [144/300], Train Loss: 0.000590
Validation Loss: 0.00059833
Epoch [145/300], Train Loss: 0.000594
Validation Loss: 0.00059763
Epoch [146/300], Train Loss: 0.000591
Validation Loss: 0.00059739
Epoch [147/300], Train Loss: 0.000592
Validation Loss: 0.00059927
Epoch [148/300], Train Loss: 0.000595
Validation Loss: 0.00059678
Epoch [149/300], Train Loss: 0.000591
Validation Loss: 0.00059838
Epoch [150/300], Train Loss: 0.000593
Validation Loss: 0.00059721
Epoch [151/300], Train Loss: 0.000591
Validation Loss: 0.00059723
Epoch [152/300], Train Loss: 0.000596
Validation Loss: 0.00059716
Epoch [153/300], Train Loss: 0.000591
Validation Loss: 0.00059619
Epoch [154/300], Train Loss: 0.000590
Validation Loss: 0.00059712
Epoch [155/300], Train Loss: 0.000591
Validation Loss: 0.00059606
Epoch [156/300], Train Loss: 0.000589
Validation Loss: 0.00059669
Epoch [157/300], Train Loss: 0.000592
Validation Loss: 0.00059621
Epoch [158/300], Train Loss: 0.000587
Validation Loss: 0.00059609
Epoch [159/300], Train Loss: 0.000589
Validation Loss: 0.00059673
Epoch [160/300], Train Loss: 0.000588
Validation Loss: 0.00059665
Epoch [161/300], Train Loss: 0.000590
Validation Loss: 0.00059559
Epoch [162/300], Train Loss: 0.000585
Validation Loss: 0.00059503
Epoch [163/300], Train Loss: 0.000592
Validation Loss: 0.00059554
Epoch [164/300], Train Loss: 0.000588
Validation Loss: 0.00059595
Epoch [165/300], Train Loss: 0.000588
Validation Loss: 0.00059515
Epoch [166/300], Train Loss: 0.000589
Validation Loss: 0.00059422
Epoch [167/300], Train Loss: 0.000588
Validation Loss: 0.00059459
Epoch [168/300], Train Loss: 0.000589
Validation Loss: 0.00059590
Epoch [169/300], Train Loss: 0.000592
Validation Loss: 0.00059491
Epoch [170/300], Train Loss: 0.000591
Validation Loss: 0.00059430
Epoch [171/300], Train Loss: 0.000592
Validation Loss: 0.00059462
Epoch [172/300], Train Loss: 0.000586
Validation Loss: 0.00059383
Epoch [173/300], Train Loss: 0.000592
Validation Loss: 0.00059460
Epoch [174/300], Train Loss: 0.000589
Validation Loss: 0.00059402
Epoch [175/300], Train Loss: 0.000589
Validation Loss: 0.00059302
Epoch [176/300], Train Loss: 0.000589
Validation Loss: 0.00059361
Epoch [177/300], Train Loss: 0.000588
Validation Loss: 0.00059363
Epoch [178/300], Train Loss: 0.000591
Validation Loss: 0.00059297
Epoch [179/300], Train Loss: 0.000588
Validation Loss: 0.00059297
Epoch [180/300], Train Loss: 0.000588
Validation Loss: 0.00059305
Epoch [181/300], Train Loss: 0.000589
Validation Loss: 0.00059306
Epoch [182/300], Train Loss: 0.000585
Validation Loss: 0.00059196
Epoch [183/300], Train Loss: 0.000589
Validation Loss: 0.00059346
Epoch [184/300], Train Loss: 0.000586
Validation Loss: 0.00059222
Epoch [185/300], Train Loss: 0.000589
Validation Loss: 0.00059156
Epoch [186/300], Train Loss: 0.000585
Validation Loss: 0.00059187
Epoch [187/300], Train Loss: 0.000584
Validation Loss: 0.00059163
Epoch [188/300], Train Loss: 0.000587
Validation Loss: 0.00059125
Epoch [189/300], Train Loss: 0.000586
Validation Loss: 0.00059085
Epoch [190/300], Train Loss: 0.000587
Validation Loss: 0.00059131
Epoch [191/300], Train Loss: 0.000590
Validation Loss: 0.00059075
Epoch [192/300], Train Loss: 0.000588
Validation Loss: 0.00059141
Epoch [193/300], Train Loss: 0.000586
Validation Loss: 0.00059107
Epoch [194/300], Train Loss: 0.000583
Validation Loss: 0.00059040
Epoch [195/300], Train Loss: 0.000584
Validation Loss: 0.00059140
Epoch [196/300], Train Loss: 0.000588
Validation Loss: 0.00058971
Epoch [197/300], Train Loss: 0.000581
Validation Loss: 0.00058997
Epoch [198/300], Train Loss: 0.000582
Validation Loss: 0.00058941
Epoch [199/300], Train Loss: 0.000581
Validation Loss: 0.00058935
Epoch [200/300], Train Loss: 0.000586
Validation Loss: 0.00058949
Epoch [201/300], Train Loss: 0.000585
Validation Loss: 0.00058945
Epoch [202/300], Train Loss: 0.000588
Validation Loss: 0.00059021
Epoch [203/300], Train Loss: 0.000582
Validation Loss: 0.00058846
Epoch [204/300], Train Loss: 0.000584
Validation Loss: 0.00058982
Epoch [205/300], Train Loss: 0.000588
Validation Loss: 0.00058896
Epoch [206/300], Train Loss: 0.000581
Validation Loss: 0.00058861
Epoch [207/300], Train Loss: 0.000585
Validation Loss: 0.00058845
Epoch [208/300], Train Loss: 0.000583
Validation Loss: 0.00058777
Epoch [209/300], Train Loss: 0.000586
Validation Loss: 0.00058756
Epoch [210/300], Train Loss: 0.000586
Validation Loss: 0.00058743
Epoch [211/300], Train Loss: 0.000585
Validation Loss: 0.00058763
Epoch [212/300], Train Loss: 0.000586
Validation Loss: 0.00058817
Epoch [213/300], Train Loss: 0.000579
Validation Loss: 0.00058743
Epoch [214/300], Train Loss: 0.000587
Validation Loss: 0.00058732
Epoch [215/300], Train Loss: 0.000581
Validation Loss: 0.00058701
Epoch [216/300], Train Loss: 0.000584
Validation Loss: 0.00058808
Epoch [217/300], Train Loss: 0.000582
Validation Loss: 0.00058677
Epoch [218/300], Train Loss: 0.000582
Validation Loss: 0.00058691
Epoch [219/300], Train Loss: 0.000585
Validation Loss: 0.00058596
Epoch [220/300], Train Loss: 0.000581
Validation Loss: 0.00058647
Epoch [221/300], Train Loss: 0.000583
Validation Loss: 0.00058653
Epoch [222/300], Train Loss: 0.000585
Validation Loss: 0.00058617
Epoch [223/300], Train Loss: 0.000581
Validation Loss: 0.00058591
Epoch [224/300], Train Loss: 0.000585
Validation Loss: 0.00058548
Epoch [225/300], Train Loss: 0.000583
Validation Loss: 0.00058547
Epoch [226/300], Train Loss: 0.000585
Validation Loss: 0.00058613
Epoch [227/300], Train Loss: 0.000583
Validation Loss: 0.00058602
Epoch [228/300], Train Loss: 0.000585
Validation Loss: 0.00058490
Epoch [229/300], Train Loss: 0.000581
Validation Loss: 0.00058459
Epoch [230/300], Train Loss: 0.000583
Validation Loss: 0.00058537
Epoch [231/300], Train Loss: 0.000578
Validation Loss: 0.00058570
Epoch [232/300], Train Loss: 0.000581
Validation Loss: 0.00058448
Epoch [233/300], Train Loss: 0.000583
Validation Loss: 0.00058444
Epoch [234/300], Train Loss: 0.000584
Validation Loss: 0.00058428
Epoch [235/300], Train Loss: 0.000578
Validation Loss: 0.00058383
Epoch [236/300], Train Loss: 0.000581
Validation Loss: 0.00058389
Epoch [237/300], Train Loss: 0.000581
Validation Loss: 0.00058361
Epoch [238/300], Train Loss: 0.000581
Validation Loss: 0.00058370
Epoch [239/300], Train Loss: 0.000581
Validation Loss: 0.00058355
Epoch [240/300], Train Loss: 0.000579
Validation Loss: 0.00058368
Epoch [241/300], Train Loss: 0.000582
Validation Loss: 0.00058331
Epoch [242/300], Train Loss: 0.000582
Validation Loss: 0.00058352
Epoch [243/300], Train Loss: 0.000579
Validation Loss: 0.00058360
Epoch [244/300], Train Loss: 0.000582
Validation Loss: 0.00058303
Epoch [245/300], Train Loss: 0.000580
Validation Loss: 0.00058284
Epoch [246/300], Train Loss: 0.000583
Validation Loss: 0.00058370
Epoch [247/300], Train Loss: 0.000580
Validation Loss: 0.00058300
Epoch [248/300], Train Loss: 0.000584
Validation Loss: 0.00058242
Epoch [249/300], Train Loss: 0.000583
Validation Loss: 0.00058294
Epoch [250/300], Train Loss: 0.000577
Validation Loss: 0.00058210
Epoch [251/300], Train Loss: 0.000579
Validation Loss: 0.00058173
Epoch [252/300], Train Loss: 0.000577
Validation Loss: 0.00058222
Epoch [253/300], Train Loss: 0.000580
Validation Loss: 0.00058231
Epoch [254/300], Train Loss: 0.000577
Validation Loss: 0.00058201
Epoch [255/300], Train Loss: 0.000580
Validation Loss: 0.00058149
Epoch [256/300], Train Loss: 0.000583
Validation Loss: 0.00058223
Epoch [257/300], Train Loss: 0.000577
Validation Loss: 0.00058138
Epoch [258/300], Train Loss: 0.000577
Validation Loss: 0.00058092
Epoch [259/300], Train Loss: 0.000581
Validation Loss: 0.00058212
Epoch [260/300], Train Loss: 0.000580
Validation Loss: 0.00058066
Epoch [261/300], Train Loss: 0.000582
Validation Loss: 0.00058055
Epoch [262/300], Train Loss: 0.000577
Validation Loss: 0.00058047
Epoch [263/300], Train Loss: 0.000581
Validation Loss: 0.00058139
Epoch [264/300], Train Loss: 0.000581
Validation Loss: 0.00058122
Epoch [265/300], Train Loss: 0.000578
Validation Loss: 0.00058053
Epoch [266/300], Train Loss: 0.000580
Validation Loss: 0.00057986
Epoch [267/300], Train Loss: 0.000576
Validation Loss: 0.00058085
Epoch [268/300], Train Loss: 0.000581
Validation Loss: 0.00058084
Epoch [269/300], Train Loss: 0.000578
Validation Loss: 0.00058010
Epoch [270/300], Train Loss: 0.000575
Validation Loss: 0.00057978
Epoch [271/300], Train Loss: 0.000575
Validation Loss: 0.00057953
Epoch [272/300], Train Loss: 0.000581
Validation Loss: 0.00058021
Epoch [273/300], Train Loss: 0.000579
Validation Loss: 0.00057972
Epoch [274/300], Train Loss: 0.000577
Validation Loss: 0.00057962
Epoch [275/300], Train Loss: 0.000580
Validation Loss: 0.00057964
Epoch [276/300], Train Loss: 0.000577
Validation Loss: 0.00057998
Epoch [277/300], Train Loss: 0.000575
Validation Loss: 0.00057882
Epoch [278/300], Train Loss: 0.000578
Validation Loss: 0.00057917
Epoch [279/300], Train Loss: 0.000576
Validation Loss: 0.00057921
Epoch [280/300], Train Loss: 0.000578
Validation Loss: 0.00057926
Epoch [281/300], Train Loss: 0.000576
Validation Loss: 0.00057866
Epoch [282/300], Train Loss: 0.000579
Validation Loss: 0.00057885
Epoch [283/300], Train Loss: 0.000578
Validation Loss: 0.00057849
Epoch [284/300], Train Loss: 0.000577
Validation Loss: 0.00057826
Epoch [285/300], Train Loss: 0.000581
Validation Loss: 0.00057814
Epoch [286/300], Train Loss: 0.000578
Validation Loss: 0.00057866
Epoch [287/300], Train Loss: 0.000579
Validation Loss: 0.00057834
Epoch [288/300], Train Loss: 0.000578
Validation Loss: 0.00057796
Epoch [289/300], Train Loss: 0.000578
Validation Loss: 0.00057754
Epoch [290/300], Train Loss: 0.000573
Validation Loss: 0.00057787
Epoch [291/300], Train Loss: 0.000581
Validation Loss: 0.00057770
Epoch [292/300], Train Loss: 0.000580
Validation Loss: 0.00057763
Epoch [293/300], Train Loss: 0.000581
Validation Loss: 0.00057775
Epoch [294/300], Train Loss: 0.000578
Validation Loss: 0.00057746
Epoch [295/300], Train Loss: 0.000574
Validation Loss: 0.00057785
Epoch [296/300], Train Loss: 0.000576
Validation Loss: 0.00057752
Epoch [297/300], Train Loss: 0.000576
Validation Loss: 0.00057810
Epoch [298/300], Train Loss: 0.000576
Validation Loss: 0.00057774
Epoch [299/300], Train Loss: 0.000575
Validation Loss: 0.00057794
Epoch [300/300], Train Loss: 0.000575
Validation Loss: 0.00057742

Evaluating model for: Fridge
Run 22/72 completed in 1603.14 seconds with: {'MAE': np.float32(36.17397), 'MSE': np.float32(1945.8877), 'RMSE': np.float32(44.112217), 'SAE': np.float32(0.03165188), 'NDE': np.float32(0.764434)}

Run 23/72: hidden=128, seq_len=720, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.001257
Validation Loss: 0.00067060
Epoch [2/300], Train Loss: 0.000688
Validation Loss: 0.00066948
Epoch [3/300], Train Loss: 0.000675
Validation Loss: 0.00066930
Epoch [4/300], Train Loss: 0.000675
Validation Loss: 0.00066915
Epoch [5/300], Train Loss: 0.000672
Validation Loss: 0.00066901
Epoch [6/300], Train Loss: 0.000673
Validation Loss: 0.00066895
Epoch [7/300], Train Loss: 0.000671
Validation Loss: 0.00066871
Epoch [8/300], Train Loss: 0.000671
Validation Loss: 0.00066849
Epoch [9/300], Train Loss: 0.000672
Validation Loss: 0.00066824
Epoch [10/300], Train Loss: 0.000665
Validation Loss: 0.00066805
Epoch [11/300], Train Loss: 0.000667
Validation Loss: 0.00066754
Epoch [12/300], Train Loss: 0.000671
Validation Loss: 0.00066701
Epoch [13/300], Train Loss: 0.000666
Validation Loss: 0.00066644
Epoch [14/300], Train Loss: 0.000665
Validation Loss: 0.00066554
Epoch [15/300], Train Loss: 0.000665
Validation Loss: 0.00066446
Epoch [16/300], Train Loss: 0.000658
Validation Loss: 0.00066266
Epoch [17/300], Train Loss: 0.000659
Validation Loss: 0.00066140
Epoch [18/300], Train Loss: 0.000647
Validation Loss: 0.00065802
Epoch [19/300], Train Loss: 0.000647
Validation Loss: 0.00065382
Epoch [20/300], Train Loss: 0.000633
Validation Loss: 0.00063853
Epoch [21/300], Train Loss: 0.000626
Validation Loss: 0.00063190
Epoch [22/300], Train Loss: 0.000621
Validation Loss: 0.00062613
Epoch [23/300], Train Loss: 0.000619
Validation Loss: 0.00062582
Epoch [24/300], Train Loss: 0.000620
Validation Loss: 0.00062370
Epoch [25/300], Train Loss: 0.000613
Validation Loss: 0.00062250
Epoch [26/300], Train Loss: 0.000614
Validation Loss: 0.00062275
Epoch [27/300], Train Loss: 0.000614
Validation Loss: 0.00062159
Epoch [28/300], Train Loss: 0.000612
Validation Loss: 0.00062006
Epoch [29/300], Train Loss: 0.000615
Validation Loss: 0.00062123
Epoch [30/300], Train Loss: 0.000615
Validation Loss: 0.00061987
Epoch [31/300], Train Loss: 0.000610
Validation Loss: 0.00061889
Epoch [32/300], Train Loss: 0.000612
Validation Loss: 0.00061856
Epoch [33/300], Train Loss: 0.000608
Validation Loss: 0.00061795
Epoch [34/300], Train Loss: 0.000610
Validation Loss: 0.00061930
Epoch [35/300], Train Loss: 0.000608
Validation Loss: 0.00061735
Epoch [36/300], Train Loss: 0.000608
Validation Loss: 0.00061744
Epoch [37/300], Train Loss: 0.000608
Validation Loss: 0.00061592
Epoch [38/300], Train Loss: 0.000612
Validation Loss: 0.00061674
Epoch [39/300], Train Loss: 0.000605
Validation Loss: 0.00061619
Epoch [40/300], Train Loss: 0.000606
Validation Loss: 0.00061557
Epoch [41/300], Train Loss: 0.000607
Validation Loss: 0.00061472
Epoch [42/300], Train Loss: 0.000606
Validation Loss: 0.00061477
Epoch [43/300], Train Loss: 0.000606
Validation Loss: 0.00061475
Epoch [44/300], Train Loss: 0.000610
Validation Loss: 0.00061516
Epoch [45/300], Train Loss: 0.000606
Validation Loss: 0.00061496
Epoch [46/300], Train Loss: 0.000602
Validation Loss: 0.00061351
Epoch [47/300], Train Loss: 0.000605
Validation Loss: 0.00061451
Epoch [48/300], Train Loss: 0.000603
Validation Loss: 0.00061863
Epoch [49/300], Train Loss: 0.000604
Validation Loss: 0.00061316
Epoch [50/300], Train Loss: 0.000607
Validation Loss: 0.00061291
Epoch [51/300], Train Loss: 0.000605
Validation Loss: 0.00061313
Epoch [52/300], Train Loss: 0.000606
Validation Loss: 0.00061326
Epoch [53/300], Train Loss: 0.000602
Validation Loss: 0.00061370
Epoch [54/300], Train Loss: 0.000602
Validation Loss: 0.00061248
Epoch [55/300], Train Loss: 0.000604
Validation Loss: 0.00061260
Epoch [56/300], Train Loss: 0.000600
Validation Loss: 0.00061235
Epoch [57/300], Train Loss: 0.000604
Validation Loss: 0.00061110
Epoch [58/300], Train Loss: 0.000603
Validation Loss: 0.00061118
Epoch [59/300], Train Loss: 0.000600
Validation Loss: 0.00061129
Epoch [60/300], Train Loss: 0.000605
Validation Loss: 0.00061081
Epoch [61/300], Train Loss: 0.000599
Validation Loss: 0.00061044
Epoch [62/300], Train Loss: 0.000602
Validation Loss: 0.00061053
Epoch [63/300], Train Loss: 0.000604
Validation Loss: 0.00061213
Epoch [64/300], Train Loss: 0.000600
Validation Loss: 0.00061218
Epoch [65/300], Train Loss: 0.000598
Validation Loss: 0.00061177
Epoch [66/300], Train Loss: 0.000601
Validation Loss: 0.00061007
Epoch [67/300], Train Loss: 0.000601
Validation Loss: 0.00060935
Epoch [68/300], Train Loss: 0.000598
Validation Loss: 0.00060916
Epoch [69/300], Train Loss: 0.000601
Validation Loss: 0.00060917
Epoch [70/300], Train Loss: 0.000598
Validation Loss: 0.00060950
Epoch [71/300], Train Loss: 0.000600
Validation Loss: 0.00060813
Epoch [72/300], Train Loss: 0.000600
Validation Loss: 0.00060817
Epoch [73/300], Train Loss: 0.000601
Validation Loss: 0.00060854
Epoch [74/300], Train Loss: 0.000601
Validation Loss: 0.00060958
Epoch [75/300], Train Loss: 0.000601
Validation Loss: 0.00060903
Epoch [76/300], Train Loss: 0.000598
Validation Loss: 0.00060721
Epoch [77/300], Train Loss: 0.000599
Validation Loss: 0.00060740
Epoch [78/300], Train Loss: 0.000600
Validation Loss: 0.00060677
Epoch [79/300], Train Loss: 0.000599
Validation Loss: 0.00060709
Epoch [80/300], Train Loss: 0.000601
Validation Loss: 0.00060788
Epoch [81/300], Train Loss: 0.000600
Validation Loss: 0.00060680
Epoch [82/300], Train Loss: 0.000596
Validation Loss: 0.00060589
Epoch [83/300], Train Loss: 0.000600
Validation Loss: 0.00060620
Epoch [84/300], Train Loss: 0.000595
Validation Loss: 0.00060671
Epoch [85/300], Train Loss: 0.000598
Validation Loss: 0.00060586
Epoch [86/300], Train Loss: 0.000595
Validation Loss: 0.00060603
Epoch [87/300], Train Loss: 0.000592
Validation Loss: 0.00060486
Epoch [88/300], Train Loss: 0.000598
Validation Loss: 0.00060488
Epoch [89/300], Train Loss: 0.000597
Validation Loss: 0.00060475
Epoch [90/300], Train Loss: 0.000599
Validation Loss: 0.00060612
Epoch [91/300], Train Loss: 0.000598
Validation Loss: 0.00060529
Epoch [92/300], Train Loss: 0.000596
Validation Loss: 0.00060478
Epoch [93/300], Train Loss: 0.000595
Validation Loss: 0.00060339
Epoch [94/300], Train Loss: 0.000591
Validation Loss: 0.00060418
Epoch [95/300], Train Loss: 0.000598
Validation Loss: 0.00060406
Epoch [96/300], Train Loss: 0.000597
Validation Loss: 0.00060355
Epoch [97/300], Train Loss: 0.000597
Validation Loss: 0.00060290
Epoch [98/300], Train Loss: 0.000592
Validation Loss: 0.00060420
Epoch [99/300], Train Loss: 0.000593
Validation Loss: 0.00060264
Epoch [100/300], Train Loss: 0.000595
Validation Loss: 0.00060447
Epoch [101/300], Train Loss: 0.000598
Validation Loss: 0.00060296
Epoch [102/300], Train Loss: 0.000594
Validation Loss: 0.00060270
Epoch [103/300], Train Loss: 0.000593
Validation Loss: 0.00060201
Epoch [104/300], Train Loss: 0.000593
Validation Loss: 0.00060320
Epoch [105/300], Train Loss: 0.000597
Validation Loss: 0.00060327
Epoch [106/300], Train Loss: 0.000591
Validation Loss: 0.00060263
Epoch [107/300], Train Loss: 0.000596
Validation Loss: 0.00060193
Epoch [108/300], Train Loss: 0.000593
Validation Loss: 0.00060096
Epoch [109/300], Train Loss: 0.000587
Validation Loss: 0.00060080
Epoch [110/300], Train Loss: 0.000594
Validation Loss: 0.00060042
Epoch [111/300], Train Loss: 0.000591
Validation Loss: 0.00059993
Epoch [112/300], Train Loss: 0.000594
Validation Loss: 0.00060092
Epoch [113/300], Train Loss: 0.000592
Validation Loss: 0.00059988
Epoch [114/300], Train Loss: 0.000591
Validation Loss: 0.00059972
Epoch [115/300], Train Loss: 0.000590
Validation Loss: 0.00059970
Epoch [116/300], Train Loss: 0.000589
Validation Loss: 0.00059853
Epoch [117/300], Train Loss: 0.000589
Validation Loss: 0.00059818
Epoch [118/300], Train Loss: 0.000592
Validation Loss: 0.00060090
Epoch [119/300], Train Loss: 0.000591
Validation Loss: 0.00059709
Epoch [120/300], Train Loss: 0.000589
Validation Loss: 0.00059727
Epoch [121/300], Train Loss: 0.000587
Validation Loss: 0.00059664
Epoch [122/300], Train Loss: 0.000592
Validation Loss: 0.00059611
Epoch [123/300], Train Loss: 0.000592
Validation Loss: 0.00059571
Epoch [124/300], Train Loss: 0.000591
Validation Loss: 0.00059631
Epoch [125/300], Train Loss: 0.000588
Validation Loss: 0.00059676
Epoch [126/300], Train Loss: 0.000590
Validation Loss: 0.00059535
Epoch [127/300], Train Loss: 0.000588
Validation Loss: 0.00059430
Epoch [128/300], Train Loss: 0.000587
Validation Loss: 0.00059414
Epoch [129/300], Train Loss: 0.000584
Validation Loss: 0.00059496
Epoch [130/300], Train Loss: 0.000587
Validation Loss: 0.00059412
Epoch [131/300], Train Loss: 0.000586
Validation Loss: 0.00059261
Epoch [132/300], Train Loss: 0.000587
Validation Loss: 0.00059304
Epoch [133/300], Train Loss: 0.000584
Validation Loss: 0.00059421
Epoch [134/300], Train Loss: 0.000587
Validation Loss: 0.00059290
Epoch [135/300], Train Loss: 0.000586
Validation Loss: 0.00059149
Epoch [136/300], Train Loss: 0.000590
Validation Loss: 0.00059228
Epoch [137/300], Train Loss: 0.000582
Validation Loss: 0.00059196
Epoch [138/300], Train Loss: 0.000588
Validation Loss: 0.00059036
Epoch [139/300], Train Loss: 0.000584
Validation Loss: 0.00059066
Epoch [140/300], Train Loss: 0.000587
Validation Loss: 0.00059001
Epoch [141/300], Train Loss: 0.000584
Validation Loss: 0.00059036
Epoch [142/300], Train Loss: 0.000584
Validation Loss: 0.00058942
Epoch [143/300], Train Loss: 0.000585
Validation Loss: 0.00058999
Epoch [144/300], Train Loss: 0.000582
Validation Loss: 0.00058931
Epoch [145/300], Train Loss: 0.000585
Validation Loss: 0.00058852
Epoch [146/300], Train Loss: 0.000584
Validation Loss: 0.00058772
Epoch [147/300], Train Loss: 0.000583
Validation Loss: 0.00059059
Epoch [148/300], Train Loss: 0.000586
Validation Loss: 0.00058784
Epoch [149/300], Train Loss: 0.000581
Validation Loss: 0.00058864
Epoch [150/300], Train Loss: 0.000584
Validation Loss: 0.00058760
Epoch [151/300], Train Loss: 0.000582
Validation Loss: 0.00058684
Epoch [152/300], Train Loss: 0.000586
Validation Loss: 0.00058685
Epoch [153/300], Train Loss: 0.000581
Validation Loss: 0.00058591
Epoch [154/300], Train Loss: 0.000582
Validation Loss: 0.00058627
Epoch [155/300], Train Loss: 0.000582
Validation Loss: 0.00058527
Epoch [156/300], Train Loss: 0.000580
Validation Loss: 0.00058544
Epoch [157/300], Train Loss: 0.000582
Validation Loss: 0.00058514
Epoch [158/300], Train Loss: 0.000578
Validation Loss: 0.00058597
Epoch [159/300], Train Loss: 0.000580
Validation Loss: 0.00058641
Epoch [160/300], Train Loss: 0.000578
Validation Loss: 0.00058542
Epoch [161/300], Train Loss: 0.000579
Validation Loss: 0.00058467
Epoch [162/300], Train Loss: 0.000575
Validation Loss: 0.00058337
Epoch [163/300], Train Loss: 0.000581
Validation Loss: 0.00058352
Epoch [164/300], Train Loss: 0.000577
Validation Loss: 0.00058435
Epoch [165/300], Train Loss: 0.000578
Validation Loss: 0.00058314
Epoch [166/300], Train Loss: 0.000579
Validation Loss: 0.00058275
Epoch [167/300], Train Loss: 0.000578
Validation Loss: 0.00058204
Epoch [168/300], Train Loss: 0.000577
Validation Loss: 0.00058471
Epoch [169/300], Train Loss: 0.000582
Validation Loss: 0.00058163
Epoch [170/300], Train Loss: 0.000579
Validation Loss: 0.00058234
Epoch [171/300], Train Loss: 0.000581
Validation Loss: 0.00058186
Epoch [172/300], Train Loss: 0.000575
Validation Loss: 0.00058168
Epoch [173/300], Train Loss: 0.000580
Validation Loss: 0.00058104
Epoch [174/300], Train Loss: 0.000576
Validation Loss: 0.00058102
Epoch [175/300], Train Loss: 0.000578
Validation Loss: 0.00057956
Epoch [176/300], Train Loss: 0.000576
Validation Loss: 0.00058056
Epoch [177/300], Train Loss: 0.000577
Validation Loss: 0.00057949
Epoch [178/300], Train Loss: 0.000578
Validation Loss: 0.00057915
Epoch [179/300], Train Loss: 0.000576
Validation Loss: 0.00057911
Epoch [180/300], Train Loss: 0.000576
Validation Loss: 0.00057886
Epoch [181/300], Train Loss: 0.000576
Validation Loss: 0.00057888
Epoch [182/300], Train Loss: 0.000574
Validation Loss: 0.00057770
Epoch [183/300], Train Loss: 0.000576
Validation Loss: 0.00058030
Epoch [184/300], Train Loss: 0.000574
Validation Loss: 0.00057756
Epoch [185/300], Train Loss: 0.000577
Validation Loss: 0.00057687
Epoch [186/300], Train Loss: 0.000572
Validation Loss: 0.00057672
Epoch [187/300], Train Loss: 0.000570
Validation Loss: 0.00057678
Epoch [188/300], Train Loss: 0.000573
Validation Loss: 0.00057601
Epoch [189/300], Train Loss: 0.000572
Validation Loss: 0.00057589
Epoch [190/300], Train Loss: 0.000573
Validation Loss: 0.00057596
Epoch [191/300], Train Loss: 0.000575
Validation Loss: 0.00057476
Epoch [192/300], Train Loss: 0.000575
Validation Loss: 0.00057550
Epoch [193/300], Train Loss: 0.000571
Validation Loss: 0.00057447
Epoch [194/300], Train Loss: 0.000570
Validation Loss: 0.00057411
Epoch [195/300], Train Loss: 0.000569
Validation Loss: 0.00057498
Epoch [196/300], Train Loss: 0.000574
Validation Loss: 0.00057356
Epoch [197/300], Train Loss: 0.000566
Validation Loss: 0.00057338
Epoch [198/300], Train Loss: 0.000569
Validation Loss: 0.00057327
Epoch [199/300], Train Loss: 0.000566
Validation Loss: 0.00057257
Epoch [200/300], Train Loss: 0.000571
Validation Loss: 0.00057361
Epoch [201/300], Train Loss: 0.000570
Validation Loss: 0.00057286
Epoch [202/300], Train Loss: 0.000573
Validation Loss: 0.00057248
Epoch [203/300], Train Loss: 0.000568
Validation Loss: 0.00057171
Epoch [204/300], Train Loss: 0.000568
Validation Loss: 0.00057319
Epoch [205/300], Train Loss: 0.000573
Validation Loss: 0.00057131
Epoch [206/300], Train Loss: 0.000566
Validation Loss: 0.00057084
Epoch [207/300], Train Loss: 0.000569
Validation Loss: 0.00057077
Epoch [208/300], Train Loss: 0.000567
Validation Loss: 0.00057015
Epoch [209/300], Train Loss: 0.000571
Validation Loss: 0.00056958
Epoch [210/300], Train Loss: 0.000570
Validation Loss: 0.00057154
Epoch [211/300], Train Loss: 0.000569
Validation Loss: 0.00056985
Epoch [212/300], Train Loss: 0.000571
Validation Loss: 0.00056952
Epoch [213/300], Train Loss: 0.000564
Validation Loss: 0.00056944
Epoch [214/300], Train Loss: 0.000571
Validation Loss: 0.00056912
Epoch [215/300], Train Loss: 0.000565
Validation Loss: 0.00056842
Epoch [216/300], Train Loss: 0.000568
Validation Loss: 0.00056972
Epoch [217/300], Train Loss: 0.000565
Validation Loss: 0.00056826
Epoch [218/300], Train Loss: 0.000566
Validation Loss: 0.00056803
Epoch [219/300], Train Loss: 0.000566
Validation Loss: 0.00056771
Epoch [220/300], Train Loss: 0.000563
Validation Loss: 0.00056841
Epoch [221/300], Train Loss: 0.000565
Validation Loss: 0.00056676
Epoch [222/300], Train Loss: 0.000569
Validation Loss: 0.00056684
Epoch [223/300], Train Loss: 0.000564
Validation Loss: 0.00056658
Epoch [224/300], Train Loss: 0.000567
Validation Loss: 0.00056615
Epoch [225/300], Train Loss: 0.000566
Validation Loss: 0.00056612
Epoch [226/300], Train Loss: 0.000568
Validation Loss: 0.00056615
Epoch [227/300], Train Loss: 0.000566
Validation Loss: 0.00056610
Epoch [228/300], Train Loss: 0.000568
Validation Loss: 0.00056559
Epoch [229/300], Train Loss: 0.000564
Validation Loss: 0.00056543
Epoch [230/300], Train Loss: 0.000566
Validation Loss: 0.00056526
Epoch [231/300], Train Loss: 0.000561
Validation Loss: 0.00056506
Epoch [232/300], Train Loss: 0.000563
Validation Loss: 0.00056582
Epoch [233/300], Train Loss: 0.000567
Validation Loss: 0.00056554
Epoch [234/300], Train Loss: 0.000565
Validation Loss: 0.00056522
Epoch [235/300], Train Loss: 0.000561
Validation Loss: 0.00056481
Epoch [236/300], Train Loss: 0.000563
Validation Loss: 0.00056461
Epoch [237/300], Train Loss: 0.000562
Validation Loss: 0.00056478
Epoch [238/300], Train Loss: 0.000563
Validation Loss: 0.00056467
Epoch [239/300], Train Loss: 0.000563
Validation Loss: 0.00056356
Epoch [240/300], Train Loss: 0.000562
Validation Loss: 0.00056348
Epoch [241/300], Train Loss: 0.000564
Validation Loss: 0.00056401
Epoch [242/300], Train Loss: 0.000564
Validation Loss: 0.00056301
Epoch [243/300], Train Loss: 0.000560
Validation Loss: 0.00056283
Epoch [244/300], Train Loss: 0.000563
Validation Loss: 0.00056324
Epoch [245/300], Train Loss: 0.000562
Validation Loss: 0.00056248
Epoch [246/300], Train Loss: 0.000564
Validation Loss: 0.00056291
Epoch [247/300], Train Loss: 0.000562
Validation Loss: 0.00056226
Epoch [248/300], Train Loss: 0.000566
Validation Loss: 0.00056222
Epoch [249/300], Train Loss: 0.000562
Validation Loss: 0.00056246
Epoch [250/300], Train Loss: 0.000558
Validation Loss: 0.00056166
Epoch [251/300], Train Loss: 0.000560
Validation Loss: 0.00056189
Epoch [252/300], Train Loss: 0.000558
Validation Loss: 0.00056124
Epoch [253/300], Train Loss: 0.000560
Validation Loss: 0.00056115
Epoch [254/300], Train Loss: 0.000557
Validation Loss: 0.00056088
Epoch [255/300], Train Loss: 0.000562
Validation Loss: 0.00056069
Epoch [256/300], Train Loss: 0.000562
Validation Loss: 0.00056076
Epoch [257/300], Train Loss: 0.000557
Validation Loss: 0.00056048
Epoch [258/300], Train Loss: 0.000559
Validation Loss: 0.00056040
Epoch [259/300], Train Loss: 0.000561
Validation Loss: 0.00056033
Epoch [260/300], Train Loss: 0.000560
Validation Loss: 0.00055973
Epoch [261/300], Train Loss: 0.000561
Validation Loss: 0.00055915
Epoch [262/300], Train Loss: 0.000557
Validation Loss: 0.00055936
Epoch [263/300], Train Loss: 0.000561
Validation Loss: 0.00056061
Epoch [264/300], Train Loss: 0.000560
Validation Loss: 0.00055866
Epoch [265/300], Train Loss: 0.000558
Validation Loss: 0.00055846
Epoch [266/300], Train Loss: 0.000560
Validation Loss: 0.00055934
Epoch [267/300], Train Loss: 0.000555
Validation Loss: 0.00056003
Epoch [268/300], Train Loss: 0.000560
Validation Loss: 0.00055861
Epoch [269/300], Train Loss: 0.000558
Validation Loss: 0.00055833
Epoch [270/300], Train Loss: 0.000555
Validation Loss: 0.00055783
Epoch [271/300], Train Loss: 0.000554
Validation Loss: 0.00055735
Epoch [272/300], Train Loss: 0.000559
Validation Loss: 0.00055731
Epoch [273/300], Train Loss: 0.000557
Validation Loss: 0.00055695
Epoch [274/300], Train Loss: 0.000556
Validation Loss: 0.00055715
Epoch [275/300], Train Loss: 0.000559
Validation Loss: 0.00055880
Epoch [276/300], Train Loss: 0.000554
Validation Loss: 0.00055699
Epoch [277/300], Train Loss: 0.000554
Validation Loss: 0.00055606
Epoch [278/300], Train Loss: 0.000557
Validation Loss: 0.00055573
Epoch [279/300], Train Loss: 0.000554
Validation Loss: 0.00055614
Epoch [280/300], Train Loss: 0.000557
Validation Loss: 0.00055559
Epoch [281/300], Train Loss: 0.000554
Validation Loss: 0.00055570
Epoch [282/300], Train Loss: 0.000555
Validation Loss: 0.00055677
Epoch [283/300], Train Loss: 0.000554
Validation Loss: 0.00055453
Epoch [284/300], Train Loss: 0.000553
Validation Loss: 0.00055417
Epoch [285/300], Train Loss: 0.000557
Validation Loss: 0.00055500
Epoch [286/300], Train Loss: 0.000555
Validation Loss: 0.00055437
Epoch [287/300], Train Loss: 0.000556
Validation Loss: 0.00055422
Epoch [288/300], Train Loss: 0.000554
Validation Loss: 0.00055381
Epoch [289/300], Train Loss: 0.000554
Validation Loss: 0.00055465
Epoch [290/300], Train Loss: 0.000550
Validation Loss: 0.00055340
Epoch [291/300], Train Loss: 0.000556
Validation Loss: 0.00055310
Epoch [292/300], Train Loss: 0.000556
Validation Loss: 0.00055248
Epoch [293/300], Train Loss: 0.000557
Validation Loss: 0.00055262
Epoch [294/300], Train Loss: 0.000554
Validation Loss: 0.00055250
Epoch [295/300], Train Loss: 0.000551
Validation Loss: 0.00055308
Epoch [296/300], Train Loss: 0.000551
Validation Loss: 0.00055235
Epoch [297/300], Train Loss: 0.000553
Validation Loss: 0.00055218
Epoch [298/300], Train Loss: 0.000550
Validation Loss: 0.00055210
Epoch [299/300], Train Loss: 0.000550
Validation Loss: 0.00055195
Epoch [300/300], Train Loss: 0.000551
Validation Loss: 0.00055197

Evaluating model for: Fridge
Run 23/72 completed in 1660.14 seconds with: {'MAE': np.float32(34.677994), 'MSE': np.float32(1851.5814), 'RMSE': np.float32(43.030006), 'SAE': np.float32(0.03221719), 'NDE': np.float32(0.74568015)}

Run 24/72: hidden=128, seq_len=720, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.000984
Validation Loss: 0.00069455
Epoch [2/300], Train Loss: 0.000695
Validation Loss: 0.00067234
Epoch [3/300], Train Loss: 0.000679
Validation Loss: 0.00066855
Epoch [4/300], Train Loss: 0.000679
Validation Loss: 0.00066843
Epoch [5/300], Train Loss: 0.000677
Validation Loss: 0.00066824
Epoch [6/300], Train Loss: 0.000676
Validation Loss: 0.00066834
Epoch [7/300], Train Loss: 0.000674
Validation Loss: 0.00066817
Epoch [8/300], Train Loss: 0.000673
Validation Loss: 0.00066812
Epoch [9/300], Train Loss: 0.000674
Validation Loss: 0.00066817
Epoch [10/300], Train Loss: 0.000667
Validation Loss: 0.00066812
Epoch [11/300], Train Loss: 0.000668
Validation Loss: 0.00066801
Epoch [12/300], Train Loss: 0.000672
Validation Loss: 0.00066797
Epoch [13/300], Train Loss: 0.000668
Validation Loss: 0.00066812
Epoch [14/300], Train Loss: 0.000667
Validation Loss: 0.00066815
Epoch [15/300], Train Loss: 0.000668
Validation Loss: 0.00066800
Epoch [16/300], Train Loss: 0.000663
Validation Loss: 0.00066831
Epoch [17/300], Train Loss: 0.000666
Validation Loss: 0.00066893
Epoch [18/300], Train Loss: 0.000658
Validation Loss: 0.00066774
Epoch [19/300], Train Loss: 0.000664
Validation Loss: 0.00066737
Epoch [20/300], Train Loss: 0.000658
Validation Loss: 0.00066694
Epoch [21/300], Train Loss: 0.000661
Validation Loss: 0.00066696
Epoch [22/300], Train Loss: 0.000660
Validation Loss: 0.00066545
Epoch [23/300], Train Loss: 0.000659
Validation Loss: 0.00066284
Epoch [24/300], Train Loss: 0.000656
Validation Loss: 0.00065752
Epoch [25/300], Train Loss: 0.000640
Validation Loss: 0.00065314
Epoch [26/300], Train Loss: 0.000634
Validation Loss: 0.00064483
Epoch [27/300], Train Loss: 0.000630
Validation Loss: 0.00064367
Epoch [28/300], Train Loss: 0.000628
Validation Loss: 0.00064025
Epoch [29/300], Train Loss: 0.000623
Validation Loss: 0.00063164
Epoch [30/300], Train Loss: 0.000621
Validation Loss: 0.00062962
Epoch [31/300], Train Loss: 0.000617
Validation Loss: 0.00062711
Epoch [32/300], Train Loss: 0.000617
Validation Loss: 0.00062563
Epoch [33/300], Train Loss: 0.000611
Validation Loss: 0.00062385
Epoch [34/300], Train Loss: 0.000613
Validation Loss: 0.00062504
Epoch [35/300], Train Loss: 0.000612
Validation Loss: 0.00062285
Epoch [36/300], Train Loss: 0.000610
Validation Loss: 0.00062052
Epoch [37/300], Train Loss: 0.000611
Validation Loss: 0.00061957
Epoch [38/300], Train Loss: 0.000615
Validation Loss: 0.00061911
Epoch [39/300], Train Loss: 0.000607
Validation Loss: 0.00062071
Epoch [40/300], Train Loss: 0.000607
Validation Loss: 0.00061852
Epoch [41/300], Train Loss: 0.000609
Validation Loss: 0.00061767
Epoch [42/300], Train Loss: 0.000607
Validation Loss: 0.00061789
Epoch [43/300], Train Loss: 0.000607
Validation Loss: 0.00061707
Epoch [44/300], Train Loss: 0.000610
Validation Loss: 0.00061784
Epoch [45/300], Train Loss: 0.000607
Validation Loss: 0.00061742
Epoch [46/300], Train Loss: 0.000603
Validation Loss: 0.00061599
Epoch [47/300], Train Loss: 0.000606
Validation Loss: 0.00061806
Epoch [48/300], Train Loss: 0.000604
Validation Loss: 0.00062317
Epoch [49/300], Train Loss: 0.000605
Validation Loss: 0.00061517
Epoch [50/300], Train Loss: 0.000607
Validation Loss: 0.00061507
Epoch [51/300], Train Loss: 0.000606
Validation Loss: 0.00061545
Epoch [52/300], Train Loss: 0.000606
Validation Loss: 0.00061728
Epoch [53/300], Train Loss: 0.000603
Validation Loss: 0.00061697
Epoch [54/300], Train Loss: 0.000602
Validation Loss: 0.00061936
Epoch [55/300], Train Loss: 0.000604
Validation Loss: 0.00061520
Epoch [56/300], Train Loss: 0.000601
Validation Loss: 0.00061819
Epoch [57/300], Train Loss: 0.000605
Validation Loss: 0.00061246
Epoch [58/300], Train Loss: 0.000604
Validation Loss: 0.00061282
Epoch [59/300], Train Loss: 0.000600
Validation Loss: 0.00061577
Epoch [60/300], Train Loss: 0.000605
Validation Loss: 0.00061228
Epoch [61/300], Train Loss: 0.000600
Validation Loss: 0.00061214
Epoch [62/300], Train Loss: 0.000602
Validation Loss: 0.00061248
Epoch [63/300], Train Loss: 0.000604
Validation Loss: 0.00061549
Epoch [64/300], Train Loss: 0.000600
Validation Loss: 0.00061302
Epoch [65/300], Train Loss: 0.000599
Validation Loss: 0.00061451
Epoch [66/300], Train Loss: 0.000602
Validation Loss: 0.00061220
Epoch [67/300], Train Loss: 0.000602
Validation Loss: 0.00061102
Epoch [68/300], Train Loss: 0.000598
Validation Loss: 0.00061043
Epoch [69/300], Train Loss: 0.000601
Validation Loss: 0.00060969
Epoch [70/300], Train Loss: 0.000598
Validation Loss: 0.00061089
Epoch [71/300], Train Loss: 0.000600
Validation Loss: 0.00060929
Epoch [72/300], Train Loss: 0.000600
Validation Loss: 0.00060885
Epoch [73/300], Train Loss: 0.000600
Validation Loss: 0.00060914
Epoch [74/300], Train Loss: 0.000601
Validation Loss: 0.00060920
Epoch [75/300], Train Loss: 0.000600
Validation Loss: 0.00061090
Epoch [76/300], Train Loss: 0.000598
Validation Loss: 0.00060756
Epoch [77/300], Train Loss: 0.000598
Validation Loss: 0.00060650
Epoch [78/300], Train Loss: 0.000600
Validation Loss: 0.00060620
Epoch [79/300], Train Loss: 0.000597
Validation Loss: 0.00060798
Epoch [80/300], Train Loss: 0.000599
Validation Loss: 0.00060597
Epoch [81/300], Train Loss: 0.000598
Validation Loss: 0.00060522
Epoch [82/300], Train Loss: 0.000594
Validation Loss: 0.00060467
Epoch [83/300], Train Loss: 0.000598
Validation Loss: 0.00060442
Epoch [84/300], Train Loss: 0.000592
Validation Loss: 0.00060345
Epoch [85/300], Train Loss: 0.000595
Validation Loss: 0.00060510
Epoch [86/300], Train Loss: 0.000591
Validation Loss: 0.00060200
Epoch [87/300], Train Loss: 0.000589
Validation Loss: 0.00060258
Epoch [88/300], Train Loss: 0.000595
Validation Loss: 0.00060149
Epoch [89/300], Train Loss: 0.000591
Validation Loss: 0.00059860
Epoch [90/300], Train Loss: 0.000593
Validation Loss: 0.00059886
Epoch [91/300], Train Loss: 0.000591
Validation Loss: 0.00059770
Epoch [92/300], Train Loss: 0.000589
Validation Loss: 0.00059626
Epoch [93/300], Train Loss: 0.000587
Validation Loss: 0.00059674
Epoch [94/300], Train Loss: 0.000583
Validation Loss: 0.00059536
Epoch [95/300], Train Loss: 0.000588
Validation Loss: 0.00059623
Epoch [96/300], Train Loss: 0.000588
Validation Loss: 0.00059461
Epoch [97/300], Train Loss: 0.000588
Validation Loss: 0.00059241
Epoch [98/300], Train Loss: 0.000582
Validation Loss: 0.00059526
Epoch [99/300], Train Loss: 0.000583
Validation Loss: 0.00059261
Epoch [100/300], Train Loss: 0.000587
Validation Loss: 0.00059454
Epoch [101/300], Train Loss: 0.000587
Validation Loss: 0.00059243
Epoch [102/300], Train Loss: 0.000586
Validation Loss: 0.00059681
Epoch [103/300], Train Loss: 0.000583
Validation Loss: 0.00059165
Epoch [104/300], Train Loss: 0.000582
Validation Loss: 0.00059099
Epoch [105/300], Train Loss: 0.000584
Validation Loss: 0.00059091
Epoch [106/300], Train Loss: 0.000580
Validation Loss: 0.00058945
Epoch [107/300], Train Loss: 0.000583
Validation Loss: 0.00059167
Epoch [108/300], Train Loss: 0.000581
Validation Loss: 0.00059024
Epoch [109/300], Train Loss: 0.000576
Validation Loss: 0.00058918
Epoch [110/300], Train Loss: 0.000581
Validation Loss: 0.00058907
Epoch [111/300], Train Loss: 0.000580
Validation Loss: 0.00058975
Epoch [112/300], Train Loss: 0.000583
Validation Loss: 0.00058912
Epoch [113/300], Train Loss: 0.000579
Validation Loss: 0.00058865
Epoch [114/300], Train Loss: 0.000579
Validation Loss: 0.00058875
Epoch [115/300], Train Loss: 0.000580
Validation Loss: 0.00058892
Epoch [116/300], Train Loss: 0.000578
Validation Loss: 0.00059149
Epoch [117/300], Train Loss: 0.000578
Validation Loss: 0.00059034
Epoch [118/300], Train Loss: 0.000582
Validation Loss: 0.00058965
Epoch [119/300], Train Loss: 0.000582
Validation Loss: 0.00058775
Epoch [120/300], Train Loss: 0.000577
Validation Loss: 0.00058777
Epoch [121/300], Train Loss: 0.000577
Validation Loss: 0.00058768
Epoch [122/300], Train Loss: 0.000582
Validation Loss: 0.00058904
Epoch [123/300], Train Loss: 0.000582
Validation Loss: 0.00058731
Epoch [124/300], Train Loss: 0.000580
Validation Loss: 0.00058767
Epoch [125/300], Train Loss: 0.000577
Validation Loss: 0.00058892
Epoch [126/300], Train Loss: 0.000580
Validation Loss: 0.00058686
Epoch [127/300], Train Loss: 0.000577
Validation Loss: 0.00058655
Epoch [128/300], Train Loss: 0.000576
Validation Loss: 0.00058764
Epoch [129/300], Train Loss: 0.000574
Validation Loss: 0.00058678
Epoch [130/300], Train Loss: 0.000578
Validation Loss: 0.00058762
Epoch [131/300], Train Loss: 0.000575
Validation Loss: 0.00058573
Epoch [132/300], Train Loss: 0.000576
Validation Loss: 0.00058597
Epoch [133/300], Train Loss: 0.000575
Validation Loss: 0.00058594
Epoch [134/300], Train Loss: 0.000577
Validation Loss: 0.00058660
Epoch [135/300], Train Loss: 0.000576
Validation Loss: 0.00058616
Epoch [136/300], Train Loss: 0.000581
Validation Loss: 0.00058533
Epoch [137/300], Train Loss: 0.000573
Validation Loss: 0.00058578
Epoch [138/300], Train Loss: 0.000579
Validation Loss: 0.00058480
Epoch [139/300], Train Loss: 0.000576
Validation Loss: 0.00058483
Epoch [140/300], Train Loss: 0.000578
Validation Loss: 0.00058467
Epoch [141/300], Train Loss: 0.000576
Validation Loss: 0.00058470
Epoch [142/300], Train Loss: 0.000574
Validation Loss: 0.00058524
Epoch [143/300], Train Loss: 0.000578
Validation Loss: 0.00058520
Epoch [144/300], Train Loss: 0.000574
Validation Loss: 0.00058525
Epoch [145/300], Train Loss: 0.000576
Validation Loss: 0.00058435
Epoch [146/300], Train Loss: 0.000576
Validation Loss: 0.00058400
Epoch [147/300], Train Loss: 0.000575
Validation Loss: 0.00058475
Epoch [148/300], Train Loss: 0.000577
Validation Loss: 0.00058414
Epoch [149/300], Train Loss: 0.000574
Validation Loss: 0.00058378
Epoch [150/300], Train Loss: 0.000575
Validation Loss: 0.00058328
Epoch [151/300], Train Loss: 0.000572
Validation Loss: 0.00058500
Epoch [152/300], Train Loss: 0.000579
Validation Loss: 0.00058345
Epoch [153/300], Train Loss: 0.000572
Validation Loss: 0.00058332
Epoch [154/300], Train Loss: 0.000574
Validation Loss: 0.00058404
Epoch [155/300], Train Loss: 0.000576
Validation Loss: 0.00058309
Epoch [156/300], Train Loss: 0.000574
Validation Loss: 0.00058522
Epoch [157/300], Train Loss: 0.000575
Validation Loss: 0.00058293
Epoch [158/300], Train Loss: 0.000574
Validation Loss: 0.00058210
Epoch [159/300], Train Loss: 0.000575
Validation Loss: 0.00058277
Epoch [160/300], Train Loss: 0.000571
Validation Loss: 0.00058211
Epoch [161/300], Train Loss: 0.000572
Validation Loss: 0.00058225
Epoch [162/300], Train Loss: 0.000569
Validation Loss: 0.00058207
Epoch [163/300], Train Loss: 0.000574
Validation Loss: 0.00058224
Epoch [164/300], Train Loss: 0.000572
Validation Loss: 0.00058201
Epoch [165/300], Train Loss: 0.000573
Validation Loss: 0.00058228
Epoch [166/300], Train Loss: 0.000574
Validation Loss: 0.00058166
Epoch [167/300], Train Loss: 0.000573
Validation Loss: 0.00058236
Epoch [168/300], Train Loss: 0.000572
Validation Loss: 0.00058183
Epoch [169/300], Train Loss: 0.000577
Validation Loss: 0.00058067
Epoch [170/300], Train Loss: 0.000573
Validation Loss: 0.00058117
Epoch [171/300], Train Loss: 0.000576
Validation Loss: 0.00058088
Epoch [172/300], Train Loss: 0.000569
Validation Loss: 0.00058089
Epoch [173/300], Train Loss: 0.000573
Validation Loss: 0.00058119
Epoch [174/300], Train Loss: 0.000572
Validation Loss: 0.00058090
Epoch [175/300], Train Loss: 0.000573
Validation Loss: 0.00058033
Epoch [176/300], Train Loss: 0.000572
Validation Loss: 0.00057993
Epoch [177/300], Train Loss: 0.000573
Validation Loss: 0.00058027
Epoch [178/300], Train Loss: 0.000572
Validation Loss: 0.00058035
Epoch [179/300], Train Loss: 0.000571
Validation Loss: 0.00057998
Epoch [180/300], Train Loss: 0.000571
Validation Loss: 0.00057979
Epoch [181/300], Train Loss: 0.000571
Validation Loss: 0.00057951
Epoch [182/300], Train Loss: 0.000570
Validation Loss: 0.00058013
Epoch [183/300], Train Loss: 0.000569
Validation Loss: 0.00057864
Epoch [184/300], Train Loss: 0.000570
Validation Loss: 0.00057913
Epoch [185/300], Train Loss: 0.000574
Validation Loss: 0.00057744
Epoch [186/300], Train Loss: 0.000568
Validation Loss: 0.00057833
Epoch [187/300], Train Loss: 0.000567
Validation Loss: 0.00057717
Epoch [188/300], Train Loss: 0.000568
Validation Loss: 0.00057737
Epoch [189/300], Train Loss: 0.000568
Validation Loss: 0.00057942
Epoch [190/300], Train Loss: 0.000567
Validation Loss: 0.00057750
Epoch [191/300], Train Loss: 0.000571
Validation Loss: 0.00057585
Epoch [192/300], Train Loss: 0.000568
Validation Loss: 0.00057934
Epoch [193/300], Train Loss: 0.000568
Validation Loss: 0.00057536
Epoch [194/300], Train Loss: 0.000565
Validation Loss: 0.00057523
Epoch [195/300], Train Loss: 0.000564
Validation Loss: 0.00057453
Epoch [196/300], Train Loss: 0.000570
Validation Loss: 0.00057479
Epoch [197/300], Train Loss: 0.000562
Validation Loss: 0.00057498
Epoch [198/300], Train Loss: 0.000564
Validation Loss: 0.00057504
Epoch [199/300], Train Loss: 0.000562
Validation Loss: 0.00057528
Epoch [200/300], Train Loss: 0.000567
Validation Loss: 0.00057500
Epoch [201/300], Train Loss: 0.000565
Validation Loss: 0.00057505
Epoch [202/300], Train Loss: 0.000568
Validation Loss: 0.00057542
Epoch [203/300], Train Loss: 0.000563
Validation Loss: 0.00057474
Epoch [204/300], Train Loss: 0.000565
Validation Loss: 0.00057468
Epoch [205/300], Train Loss: 0.000568
Validation Loss: 0.00057419
Epoch [206/300], Train Loss: 0.000564
Validation Loss: 0.00057335
Epoch [207/300], Train Loss: 0.000566
Validation Loss: 0.00057407
Epoch [208/300], Train Loss: 0.000564
Validation Loss: 0.00057474
Epoch [209/300], Train Loss: 0.000567
Validation Loss: 0.00057391
Epoch [210/300], Train Loss: 0.000568
Validation Loss: 0.00057493
Epoch [211/300], Train Loss: 0.000565
Validation Loss: 0.00057441
Epoch [212/300], Train Loss: 0.000568
Validation Loss: 0.00057545
Epoch [213/300], Train Loss: 0.000562
Validation Loss: 0.00057425
Epoch [214/300], Train Loss: 0.000569
Validation Loss: 0.00057367
Epoch [215/300], Train Loss: 0.000564
Validation Loss: 0.00057382
Epoch [216/300], Train Loss: 0.000566
Validation Loss: 0.00057556
Early stopping triggered

Evaluating model for: Fridge
Run 24/72 completed in 1313.56 seconds with: {'MAE': np.float32(36.464767), 'MSE': np.float32(1945.6877), 'RMSE': np.float32(44.10995), 'SAE': np.float32(0.022517996), 'NDE': np.float32(0.7643948)}

Run 25/72: hidden=256, seq_len=120, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000795
Validation Loss: 0.00065507
Epoch [2/300], Train Loss: 0.000664
Validation Loss: 0.00062545
Epoch [3/300], Train Loss: 0.000636
Validation Loss: 0.00060957
Epoch [4/300], Train Loss: 0.000624
Validation Loss: 0.00059847
Epoch [5/300], Train Loss: 0.000614
Validation Loss: 0.00059217
Epoch [6/300], Train Loss: 0.000608
Validation Loss: 0.00058683
Epoch [7/300], Train Loss: 0.000602
Validation Loss: 0.00058306
Epoch [8/300], Train Loss: 0.000598
Validation Loss: 0.00058205
Epoch [9/300], Train Loss: 0.000594
Validation Loss: 0.00057364
Epoch [10/300], Train Loss: 0.000590
Validation Loss: 0.00057109
Epoch [11/300], Train Loss: 0.000588
Validation Loss: 0.00057057
Epoch [12/300], Train Loss: 0.000583
Validation Loss: 0.00056432
Epoch [13/300], Train Loss: 0.000580
Validation Loss: 0.00055867
Epoch [14/300], Train Loss: 0.000575
Validation Loss: 0.00055924
Epoch [15/300], Train Loss: 0.000570
Validation Loss: 0.00054833
Epoch [16/300], Train Loss: 0.000564
Validation Loss: 0.00054113
Epoch [17/300], Train Loss: 0.000555
Validation Loss: 0.00053660
Epoch [18/300], Train Loss: 0.000548
Validation Loss: 0.00052770
Epoch [19/300], Train Loss: 0.000541
Validation Loss: 0.00052415
Epoch [20/300], Train Loss: 0.000535
Validation Loss: 0.00051756
Epoch [21/300], Train Loss: 0.000528
Validation Loss: 0.00052041
Epoch [22/300], Train Loss: 0.000522
Validation Loss: 0.00050460
Epoch [23/300], Train Loss: 0.000517
Validation Loss: 0.00049965
Epoch [24/300], Train Loss: 0.000508
Validation Loss: 0.00048862
Epoch [25/300], Train Loss: 0.000501
Validation Loss: 0.00049043
Epoch [26/300], Train Loss: 0.000494
Validation Loss: 0.00047347
Epoch [27/300], Train Loss: 0.000486
Validation Loss: 0.00046891
Epoch [28/300], Train Loss: 0.000477
Validation Loss: 0.00046024
Epoch [29/300], Train Loss: 0.000471
Validation Loss: 0.00045707
Epoch [30/300], Train Loss: 0.000464
Validation Loss: 0.00043937
Epoch [31/300], Train Loss: 0.000454
Validation Loss: 0.00043422
Epoch [32/300], Train Loss: 0.000448
Validation Loss: 0.00043375
Epoch [33/300], Train Loss: 0.000441
Validation Loss: 0.00042022
Epoch [34/300], Train Loss: 0.000434
Validation Loss: 0.00042811
Epoch [35/300], Train Loss: 0.000429
Validation Loss: 0.00042187
Epoch [36/300], Train Loss: 0.000421
Validation Loss: 0.00040621
Epoch [37/300], Train Loss: 0.000416
Validation Loss: 0.00040213
Epoch [38/300], Train Loss: 0.000410
Validation Loss: 0.00039454
Epoch [39/300], Train Loss: 0.000404
Validation Loss: 0.00039283
Epoch [40/300], Train Loss: 0.000400
Validation Loss: 0.00038803
Epoch [41/300], Train Loss: 0.000393
Validation Loss: 0.00037652
Epoch [42/300], Train Loss: 0.000390
Validation Loss: 0.00037938
Epoch [43/300], Train Loss: 0.000381
Validation Loss: 0.00037635
Epoch [44/300], Train Loss: 0.000377
Validation Loss: 0.00036279
Epoch [45/300], Train Loss: 0.000369
Validation Loss: 0.00036165
Epoch [46/300], Train Loss: 0.000360
Validation Loss: 0.00035529
Epoch [47/300], Train Loss: 0.000355
Validation Loss: 0.00035595
Epoch [48/300], Train Loss: 0.000354
Validation Loss: 0.00034708
Epoch [49/300], Train Loss: 0.000347
Validation Loss: 0.00033636
Epoch [50/300], Train Loss: 0.000342
Validation Loss: 0.00032865
Epoch [51/300], Train Loss: 0.000338
Validation Loss: 0.00033110
Epoch [52/300], Train Loss: 0.000330
Validation Loss: 0.00031977
Epoch [53/300], Train Loss: 0.000325
Validation Loss: 0.00031376
Epoch [54/300], Train Loss: 0.000325
Validation Loss: 0.00031361
Epoch [55/300], Train Loss: 0.000320
Validation Loss: 0.00031048
Epoch [56/300], Train Loss: 0.000314
Validation Loss: 0.00030608
Epoch [57/300], Train Loss: 0.000309
Validation Loss: 0.00030949
Epoch [58/300], Train Loss: 0.000311
Validation Loss: 0.00030100
Epoch [59/300], Train Loss: 0.000304
Validation Loss: 0.00028994
Epoch [60/300], Train Loss: 0.000300
Validation Loss: 0.00028705
Epoch [61/300], Train Loss: 0.000301
Validation Loss: 0.00029531
Epoch [62/300], Train Loss: 0.000293
Validation Loss: 0.00029404
Epoch [63/300], Train Loss: 0.000293
Validation Loss: 0.00027980
Epoch [64/300], Train Loss: 0.000288
Validation Loss: 0.00028277
Epoch [65/300], Train Loss: 0.000286
Validation Loss: 0.00027189
Epoch [66/300], Train Loss: 0.000284
Validation Loss: 0.00026975
Epoch [67/300], Train Loss: 0.000286
Validation Loss: 0.00027322
Epoch [68/300], Train Loss: 0.000282
Validation Loss: 0.00027763
Epoch [69/300], Train Loss: 0.000278
Validation Loss: 0.00026768
Epoch [70/300], Train Loss: 0.000280
Validation Loss: 0.00026416
Epoch [71/300], Train Loss: 0.000274
Validation Loss: 0.00027145
Epoch [72/300], Train Loss: 0.000273
Validation Loss: 0.00025835
Epoch [73/300], Train Loss: 0.000270
Validation Loss: 0.00026556
Epoch [74/300], Train Loss: 0.000272
Validation Loss: 0.00025546
Epoch [75/300], Train Loss: 0.000269
Validation Loss: 0.00025325
Epoch [76/300], Train Loss: 0.000267
Validation Loss: 0.00025758
Epoch [77/300], Train Loss: 0.000264
Validation Loss: 0.00025182
Epoch [78/300], Train Loss: 0.000263
Validation Loss: 0.00024934
Epoch [79/300], Train Loss: 0.000263
Validation Loss: 0.00024865
Epoch [80/300], Train Loss: 0.000259
Validation Loss: 0.00024517
Epoch [81/300], Train Loss: 0.000259
Validation Loss: 0.00024896
Epoch [82/300], Train Loss: 0.000258
Validation Loss: 0.00025667
Epoch [83/300], Train Loss: 0.000519
Validation Loss: 0.00051836
Epoch [84/300], Train Loss: 0.000539
Validation Loss: 0.00048139
Epoch [85/300], Train Loss: 0.000502
Validation Loss: 0.00046103
Epoch [86/300], Train Loss: 0.000477
Validation Loss: 0.00043638
Epoch [87/300], Train Loss: 0.000454
Validation Loss: 0.00041726
Epoch [88/300], Train Loss: 0.000434
Validation Loss: 0.00039750
Epoch [89/300], Train Loss: 0.000414
Validation Loss: 0.00038601
Epoch [90/300], Train Loss: 0.000400
Validation Loss: 0.00036864
Early stopping triggered

Evaluating model for: Fridge
Run 25/72 completed in 5323.64 seconds with: {'MAE': np.float32(27.08019), 'MSE': np.float32(1230.5187), 'RMSE': np.float32(35.07875), 'SAE': np.float32(0.03707409), 'NDE': np.float32(0.60772085)}

Run 26/72: hidden=256, seq_len=120, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000663
Validation Loss: 0.00062539
Epoch [2/300], Train Loss: 0.000620
Validation Loss: 0.00059859
Epoch [3/300], Train Loss: 0.000609
Validation Loss: 0.00059369
Epoch [4/300], Train Loss: 0.000605
Validation Loss: 0.00058948
Epoch [5/300], Train Loss: 0.000601
Validation Loss: 0.00058707
Epoch [6/300], Train Loss: 0.000597
Validation Loss: 0.00058315
Epoch [7/300], Train Loss: 0.000594
Validation Loss: 0.00058118
Epoch [8/300], Train Loss: 0.000592
Validation Loss: 0.00057822
Epoch [9/300], Train Loss: 0.000588
Validation Loss: 0.00057386
Epoch [10/300], Train Loss: 0.000585
Validation Loss: 0.00057111
Epoch [11/300], Train Loss: 0.000583
Validation Loss: 0.00056983
Epoch [12/300], Train Loss: 0.000578
Validation Loss: 0.00056495
Epoch [13/300], Train Loss: 0.000574
Validation Loss: 0.00055737
Epoch [14/300], Train Loss: 0.000568
Validation Loss: 0.00055277
Epoch [15/300], Train Loss: 0.000563
Validation Loss: 0.00054749
Epoch [16/300], Train Loss: 0.000556
Validation Loss: 0.00053641
Epoch [17/300], Train Loss: 0.000548
Validation Loss: 0.00053201
Epoch [18/300], Train Loss: 0.000542
Validation Loss: 0.00052463
Epoch [19/300], Train Loss: 0.000533
Validation Loss: 0.00051785
Epoch [20/300], Train Loss: 0.000526
Validation Loss: 0.00051122
Epoch [21/300], Train Loss: 0.000517
Validation Loss: 0.00051090
Epoch [22/300], Train Loss: 0.000509
Validation Loss: 0.00049988
Epoch [23/300], Train Loss: 0.000502
Validation Loss: 0.00048577
Epoch [24/300], Train Loss: 0.000495
Validation Loss: 0.00047854
Epoch [25/300], Train Loss: 0.000489
Validation Loss: 0.00048175
Epoch [26/300], Train Loss: 0.000483
Validation Loss: 0.00046322
Epoch [27/300], Train Loss: 0.000474
Validation Loss: 0.00045521
Epoch [28/300], Train Loss: 0.000464
Validation Loss: 0.00045154
Epoch [29/300], Train Loss: 0.000453
Validation Loss: 0.00045749
Epoch [30/300], Train Loss: 0.000439
Validation Loss: 0.00041187
Epoch [31/300], Train Loss: 0.000423
Validation Loss: 0.00039598
Epoch [32/300], Train Loss: 0.000410
Validation Loss: 0.00039042
Epoch [33/300], Train Loss: 0.000396
Validation Loss: 0.00037654
Epoch [34/300], Train Loss: 0.000385
Validation Loss: 0.00037208
Epoch [35/300], Train Loss: 0.000379
Validation Loss: 0.00036373
Epoch [36/300], Train Loss: 0.000373
Validation Loss: 0.00035285
Epoch [37/300], Train Loss: 0.000370
Validation Loss: 0.00035004
Epoch [38/300], Train Loss: 0.000359
Validation Loss: 0.00035163
Epoch [39/300], Train Loss: 0.000355
Validation Loss: 0.00037157
Epoch [40/300], Train Loss: 0.000353
Validation Loss: 0.00033450
Epoch [41/300], Train Loss: 0.000346
Validation Loss: 0.00032814
Epoch [42/300], Train Loss: 0.000342
Validation Loss: 0.00032621
Epoch [43/300], Train Loss: 0.000337
Validation Loss: 0.00035622
Epoch [44/300], Train Loss: 0.000334
Validation Loss: 0.00033038
Epoch [45/300], Train Loss: 0.000333
Validation Loss: 0.00033456
Epoch [46/300], Train Loss: 0.000324
Validation Loss: 0.00034535
Epoch [47/300], Train Loss: 0.000328
Validation Loss: 0.00031525
Epoch [48/300], Train Loss: 0.000323
Validation Loss: 0.00031156
Epoch [49/300], Train Loss: 0.000316
Validation Loss: 0.00031063
Epoch [50/300], Train Loss: 0.000317
Validation Loss: 0.00030524
Epoch [51/300], Train Loss: 0.000312
Validation Loss: 0.00030674
Epoch [52/300], Train Loss: 0.000310
Validation Loss: 0.00030815
Epoch [53/300], Train Loss: 0.000313
Validation Loss: 0.00029753
Epoch [54/300], Train Loss: 0.000308
Validation Loss: 0.00031281
Epoch [55/300], Train Loss: 0.000306
Validation Loss: 0.00031054
Epoch [56/300], Train Loss: 0.000303
Validation Loss: 0.00029405
Epoch [57/300], Train Loss: 0.000385
Validation Loss: 0.00037949
Epoch [58/300], Train Loss: 0.000344
Validation Loss: 0.00031515
Epoch [59/300], Train Loss: 0.000313
Validation Loss: 0.00029333
Epoch [60/300], Train Loss: 0.000301
Validation Loss: 0.00030308
Epoch [61/300], Train Loss: 0.000300
Validation Loss: 0.00028939
Epoch [62/300], Train Loss: 0.000294
Validation Loss: 0.00029436
Epoch [63/300], Train Loss: 0.000295
Validation Loss: 0.00028389
Epoch [64/300], Train Loss: 0.000293
Validation Loss: 0.00028568
Epoch [65/300], Train Loss: 0.000294
Validation Loss: 0.00027769
Epoch [66/300], Train Loss: 0.000295
Validation Loss: 0.00028406
Epoch [67/300], Train Loss: 0.000287
Validation Loss: 0.00027659
Epoch [68/300], Train Loss: 0.000287
Validation Loss: 0.00027714
Epoch [69/300], Train Loss: 0.000287
Validation Loss: 0.00028782
Epoch [70/300], Train Loss: 0.000284
Validation Loss: 0.00027639
Epoch [71/300], Train Loss: 0.000279
Validation Loss: 0.00029448
Epoch [72/300], Train Loss: 0.000281
Validation Loss: 0.00027294
Epoch [73/300], Train Loss: 0.000289
Validation Loss: 0.00028469
Epoch [74/300], Train Loss: 0.000279
Validation Loss: 0.00027338
Epoch [75/300], Train Loss: 0.000277
Validation Loss: 0.00026919
Epoch [76/300], Train Loss: 0.000275
Validation Loss: 0.00028104
Epoch [77/300], Train Loss: 0.000309
Validation Loss: 0.00027309
Epoch [78/300], Train Loss: 0.000276
Validation Loss: 0.00026800
Epoch [79/300], Train Loss: 0.000276
Validation Loss: 0.00026990
Epoch [80/300], Train Loss: 0.000281
Validation Loss: 0.00026834
Epoch [81/300], Train Loss: 0.000271
Validation Loss: 0.00026767
Epoch [82/300], Train Loss: 0.000327
Validation Loss: 0.00027601
Epoch [83/300], Train Loss: 0.000272
Validation Loss: 0.00026247
Epoch [84/300], Train Loss: 0.000271
Validation Loss: 0.00026737
Epoch [85/300], Train Loss: 0.000266
Validation Loss: 0.00026160
Epoch [86/300], Train Loss: 0.000268
Validation Loss: 0.00026187
Epoch [87/300], Train Loss: 0.000266
Validation Loss: 0.00027351
Epoch [88/300], Train Loss: 0.000265
Validation Loss: 0.00025799
Epoch [89/300], Train Loss: 0.000268
Validation Loss: 0.00026081
Epoch [90/300], Train Loss: 0.000262
Validation Loss: 0.00025787
Epoch [91/300], Train Loss: 0.000263
Validation Loss: 0.00025532
Epoch [92/300], Train Loss: 0.000263
Validation Loss: 0.00025637
Epoch [93/300], Train Loss: 0.000263
Validation Loss: 0.00026014
Epoch [94/300], Train Loss: 0.000258
Validation Loss: 0.00025209
Epoch [95/300], Train Loss: 0.000259
Validation Loss: 0.00025710
Epoch [96/300], Train Loss: 0.000261
Validation Loss: 0.00025201
Epoch [97/300], Train Loss: 0.000255
Validation Loss: 0.00025263
Epoch [98/300], Train Loss: 0.000259
Validation Loss: 0.00025588
Epoch [99/300], Train Loss: 0.000253
Validation Loss: 0.00024826
Epoch [100/300], Train Loss: 0.000259
Validation Loss: 0.00025714
Epoch [101/300], Train Loss: 0.000253
Validation Loss: 0.00025924
Epoch [102/300], Train Loss: 0.000252
Validation Loss: 0.00024484
Epoch [103/300], Train Loss: 0.000250
Validation Loss: 0.00024374
Epoch [104/300], Train Loss: 0.000250
Validation Loss: 0.00024608
Epoch [105/300], Train Loss: 0.000256
Validation Loss: 0.00025628
Epoch [106/300], Train Loss: 0.000247
Validation Loss: 0.00024770
Epoch [107/300], Train Loss: 0.000247
Validation Loss: 0.00024670
Epoch [108/300], Train Loss: 0.000249
Validation Loss: 0.00024640
Epoch [109/300], Train Loss: 0.000249
Validation Loss: 0.00023952
Epoch [110/300], Train Loss: 0.000247
Validation Loss: 0.00024310
Epoch [111/300], Train Loss: 0.000244
Validation Loss: 0.00024097
Epoch [112/300], Train Loss: 0.000243
Validation Loss: 0.00023940
Epoch [113/300], Train Loss: 0.000243
Validation Loss: 0.00024000
Epoch [114/300], Train Loss: 0.000241
Validation Loss: 0.00023937
Epoch [115/300], Train Loss: 0.000243
Validation Loss: 0.00024218
Epoch [116/300], Train Loss: 0.000244
Validation Loss: 0.00024305
Epoch [117/300], Train Loss: 0.000242
Validation Loss: 0.00023557
Epoch [118/300], Train Loss: 0.000239
Validation Loss: 0.00023393
Epoch [119/300], Train Loss: 0.000240
Validation Loss: 0.00023702
Epoch [120/300], Train Loss: 0.000241
Validation Loss: 0.00026812
Epoch [121/300], Train Loss: 0.000238
Validation Loss: 0.00023128
Epoch [122/300], Train Loss: 0.000235
Validation Loss: 0.00024336
Epoch [123/300], Train Loss: 0.000238
Validation Loss: 0.00023342
Epoch [124/300], Train Loss: 0.000233
Validation Loss: 0.00023658
Epoch [125/300], Train Loss: 0.000232
Validation Loss: 0.00022895
Epoch [126/300], Train Loss: 0.000232
Validation Loss: 0.00022765
Epoch [127/300], Train Loss: 0.000231
Validation Loss: 0.00022517
Epoch [128/300], Train Loss: 0.000229
Validation Loss: 0.00023372
Epoch [129/300], Train Loss: 0.000233
Validation Loss: 0.00022686
Epoch [130/300], Train Loss: 0.000230
Validation Loss: 0.00022468
Epoch [131/300], Train Loss: 0.000228
Validation Loss: 0.00022391
Epoch [132/300], Train Loss: 0.000231
Validation Loss: 0.00022934
Epoch [133/300], Train Loss: 0.000233
Validation Loss: 0.00022560
Epoch [134/300], Train Loss: 0.000229
Validation Loss: 0.00022275
Epoch [135/300], Train Loss: 0.000234
Validation Loss: 0.00022136
Epoch [136/300], Train Loss: 0.000225
Validation Loss: 0.00022030
Epoch [137/300], Train Loss: 0.000244
Validation Loss: 0.00022809
Epoch [138/300], Train Loss: 0.000225
Validation Loss: 0.00022238
Epoch [139/300], Train Loss: 0.000225
Validation Loss: 0.00022235
Epoch [140/300], Train Loss: 0.000222
Validation Loss: 0.00021914
Epoch [141/300], Train Loss: 0.000223
Validation Loss: 0.00022126
Epoch [142/300], Train Loss: 0.000222
Validation Loss: 0.00022675
Epoch [143/300], Train Loss: 0.000229
Validation Loss: 0.00021927
Epoch [144/300], Train Loss: 0.000221
Validation Loss: 0.00021598
Epoch [145/300], Train Loss: 0.000222
Validation Loss: 0.00022037
Epoch [146/300], Train Loss: 0.000222
Validation Loss: 0.00021993
Epoch [147/300], Train Loss: 0.000238
Validation Loss: 0.00021994
Epoch [148/300], Train Loss: 0.000219
Validation Loss: 0.00022084
Epoch [149/300], Train Loss: 0.000220
Validation Loss: 0.00021728
Epoch [150/300], Train Loss: 0.000220
Validation Loss: 0.00022144
Epoch [151/300], Train Loss: 0.000220
Validation Loss: 0.00022158
Epoch [152/300], Train Loss: 0.000218
Validation Loss: 0.00021696
Epoch [153/300], Train Loss: 0.000216
Validation Loss: 0.00022294
Epoch [154/300], Train Loss: 0.000218
Validation Loss: 0.00021645
Early stopping triggered

Evaluating model for: Fridge
Run 26/72 completed in 9934.85 seconds with: {'MAE': np.float32(16.906137), 'MSE': np.float32(695.7573), 'RMSE': np.float32(26.377213), 'SAE': np.float32(0.008793762), 'NDE': np.float32(0.45697102)}

Run 27/72: hidden=256, seq_len=120, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000652
Validation Loss: 0.00060959
Epoch [2/300], Train Loss: 0.000616
Validation Loss: 0.00059750
Epoch [3/300], Train Loss: 0.000608
Validation Loss: 0.00059176
Epoch [4/300], Train Loss: 0.000604
Validation Loss: 0.00058801
Epoch [5/300], Train Loss: 0.000599
Validation Loss: 0.00058528
Epoch [6/300], Train Loss: 0.000596
Validation Loss: 0.00058226
Epoch [7/300], Train Loss: 0.000592
Validation Loss: 0.00057933
Epoch [8/300], Train Loss: 0.000588
Validation Loss: 0.00058623
Epoch [9/300], Train Loss: 0.000584
Validation Loss: 0.00057023
Epoch [10/300], Train Loss: 0.000574
Validation Loss: 0.00055728
Epoch [11/300], Train Loss: 0.000564
Validation Loss: 0.00054494
Epoch [12/300], Train Loss: 0.000552
Validation Loss: 0.00053113
Epoch [13/300], Train Loss: 0.000538
Validation Loss: 0.00051564
Epoch [14/300], Train Loss: 0.000522
Validation Loss: 0.00050428
Epoch [15/300], Train Loss: 0.000511
Validation Loss: 0.00049108
Epoch [16/300], Train Loss: 0.000499
Validation Loss: 0.00049086
Epoch [17/300], Train Loss: 0.000486
Validation Loss: 0.00047671
Epoch [18/300], Train Loss: 0.000474
Validation Loss: 0.00046529
Epoch [19/300], Train Loss: 0.000466
Validation Loss: 0.00045279
Epoch [20/300], Train Loss: 0.000460
Validation Loss: 0.00044968
Epoch [21/300], Train Loss: 0.000454
Validation Loss: 0.00046161
Epoch [22/300], Train Loss: 0.000444
Validation Loss: 0.00043249
Epoch [23/300], Train Loss: 0.000437
Validation Loss: 0.00042487
Epoch [24/300], Train Loss: 0.000432
Validation Loss: 0.00042024
Epoch [25/300], Train Loss: 0.000424
Validation Loss: 0.00045441
Epoch [26/300], Train Loss: 0.000423
Validation Loss: 0.00041451
Epoch [27/300], Train Loss: 0.000416
Validation Loss: 0.00042017
Epoch [28/300], Train Loss: 0.000415
Validation Loss: 0.00040802
Epoch [29/300], Train Loss: 0.000408
Validation Loss: 0.00047179
Epoch [30/300], Train Loss: 0.000413
Validation Loss: 0.00041760
Epoch [31/300], Train Loss: 0.000403
Validation Loss: 0.00039748
Epoch [32/300], Train Loss: 0.000401
Validation Loss: 0.00039737
Epoch [33/300], Train Loss: 0.000397
Validation Loss: 0.00039992
Epoch [34/300], Train Loss: 0.000396
Validation Loss: 0.00039420
Epoch [35/300], Train Loss: 0.000394
Validation Loss: 0.00039026
Epoch [36/300], Train Loss: 0.000391
Validation Loss: 0.00038802
Epoch [37/300], Train Loss: 0.000390
Validation Loss: 0.00039633
Epoch [38/300], Train Loss: 0.000388
Validation Loss: 0.00038761
Epoch [39/300], Train Loss: 0.000386
Validation Loss: 0.00039862
Epoch [40/300], Train Loss: 0.000384
Validation Loss: 0.00038172
Epoch [41/300], Train Loss: 0.000381
Validation Loss: 0.00037997
Epoch [42/300], Train Loss: 0.000380
Validation Loss: 0.00038372
Epoch [43/300], Train Loss: 0.000376
Validation Loss: 0.00037072
Epoch [44/300], Train Loss: 0.000372
Validation Loss: 0.00037168
Epoch [45/300], Train Loss: 0.000372
Validation Loss: 0.00037521
Epoch [46/300], Train Loss: 0.000366
Validation Loss: 0.00037003
Epoch [47/300], Train Loss: 0.000367
Validation Loss: 0.00036451
Epoch [48/300], Train Loss: 0.000364
Validation Loss: 0.00036790
Epoch [49/300], Train Loss: 0.000361
Validation Loss: 0.00036492
Epoch [50/300], Train Loss: 0.000360
Validation Loss: 0.00036266
Epoch [51/300], Train Loss: 0.000357
Validation Loss: 0.00035990
Epoch [52/300], Train Loss: 0.000357
Validation Loss: 0.00035529
Epoch [53/300], Train Loss: 0.000351
Validation Loss: 0.00035677
Epoch [54/300], Train Loss: 0.000353
Validation Loss: 0.00035087
Epoch [55/300], Train Loss: 0.000349
Validation Loss: 0.00035580
Epoch [56/300], Train Loss: 0.000345
Validation Loss: 0.00036191
Epoch [57/300], Train Loss: 0.000343
Validation Loss: 0.00035493
Epoch [58/300], Train Loss: 0.000339
Validation Loss: 0.00033520
Epoch [59/300], Train Loss: 0.000337
Validation Loss: 0.00032983
Epoch [60/300], Train Loss: 0.000322
Validation Loss: 0.00033164
Epoch [61/300], Train Loss: 0.000319
Validation Loss: 0.00032120
Epoch [62/300], Train Loss: 0.000311
Validation Loss: 0.00030825
Epoch [63/300], Train Loss: 0.000305
Validation Loss: 0.00029991
Epoch [64/300], Train Loss: 0.000301
Validation Loss: 0.00029871
Epoch [65/300], Train Loss: 0.000302
Validation Loss: 0.00029130
Epoch [66/300], Train Loss: 0.000296
Validation Loss: 0.00029124
Epoch [67/300], Train Loss: 0.000291
Validation Loss: 0.00028916
Epoch [68/300], Train Loss: 0.000288
Validation Loss: 0.00028020
Epoch [69/300], Train Loss: 0.000284
Validation Loss: 0.00027972
Epoch [70/300], Train Loss: 0.000280
Validation Loss: 0.00028282
Epoch [71/300], Train Loss: 0.000281
Validation Loss: 0.00029222
Epoch [72/300], Train Loss: 0.000278
Validation Loss: 0.00028211
Epoch [73/300], Train Loss: 0.000275
Validation Loss: 0.00027129
Epoch [74/300], Train Loss: 0.000275
Validation Loss: 0.00027530
Epoch [75/300], Train Loss: 0.000271
Validation Loss: 0.00026877
Epoch [76/300], Train Loss: 0.000271
Validation Loss: 0.00028092
Epoch [77/300], Train Loss: 0.000269
Validation Loss: 0.00026066
Epoch [78/300], Train Loss: 0.000268
Validation Loss: 0.00026115
Epoch [79/300], Train Loss: 0.000264
Validation Loss: 0.00025556
Epoch [80/300], Train Loss: 0.000264
Validation Loss: 0.00025847
Epoch [81/300], Train Loss: 0.000262
Validation Loss: 0.00026293
Epoch [82/300], Train Loss: 0.000263
Validation Loss: 0.00025781
Epoch [83/300], Train Loss: 0.000258
Validation Loss: 0.00025164
Epoch [84/300], Train Loss: 0.000257
Validation Loss: 0.00026154
Epoch [85/300], Train Loss: 0.000260
Validation Loss: 0.00025746
Epoch [86/300], Train Loss: 0.000270
Validation Loss: 0.00024953
Epoch [87/300], Train Loss: 0.000264
Validation Loss: 0.00030649
Epoch [88/300], Train Loss: 0.000260
Validation Loss: 0.00025312
Epoch [89/300], Train Loss: 0.000252
Validation Loss: 0.00024654
Epoch [90/300], Train Loss: 0.000248
Validation Loss: 0.00024224
Epoch [91/300], Train Loss: 0.000255
Validation Loss: 0.00024363
Epoch [92/300], Train Loss: 0.000250
Validation Loss: 0.00024067
Epoch [93/300], Train Loss: 0.000245
Validation Loss: 0.00024234
Epoch [94/300], Train Loss: 0.000246
Validation Loss: 0.00024003
Epoch [95/300], Train Loss: 0.000242
Validation Loss: 0.00024106
Epoch [96/300], Train Loss: 0.000251
Validation Loss: 0.00024100
Epoch [97/300], Train Loss: 0.000242
Validation Loss: 0.00023751
Epoch [98/300], Train Loss: 0.000245
Validation Loss: 0.00023794
Epoch [99/300], Train Loss: 0.000239
Validation Loss: 0.00023328
Epoch [100/300], Train Loss: 0.000249
Validation Loss: 0.00023529
Epoch [101/300], Train Loss: 0.000248
Validation Loss: 0.00025326
Epoch [102/300], Train Loss: 0.000311
Validation Loss: 0.00025958
Epoch [103/300], Train Loss: 0.000257
Validation Loss: 0.00024963
Epoch [104/300], Train Loss: 0.000244
Validation Loss: 0.00023554
Epoch [105/300], Train Loss: 0.000240
Validation Loss: 0.00023479
Epoch [106/300], Train Loss: 0.000236
Validation Loss: 0.00022921
Epoch [107/300], Train Loss: 0.000234
Validation Loss: 0.00022646
Epoch [108/300], Train Loss: 0.000233
Validation Loss: 0.00023145
Epoch [109/300], Train Loss: 0.000235
Validation Loss: 0.00022595
Epoch [110/300], Train Loss: 0.000230
Validation Loss: 0.00022932
Epoch [111/300], Train Loss: 0.000229
Validation Loss: 0.00022301
Epoch [112/300], Train Loss: 0.000234
Validation Loss: 0.00022755
Epoch [113/300], Train Loss: 0.000226
Validation Loss: 0.00022483
Epoch [114/300], Train Loss: 0.000227
Validation Loss: 0.00025831
Epoch [115/300], Train Loss: 0.000245
Validation Loss: 0.00024720
Epoch [116/300], Train Loss: 0.000231
Validation Loss: 0.00022386
Epoch [117/300], Train Loss: 0.000234
Validation Loss: 0.00022949
Epoch [118/300], Train Loss: 0.000226
Validation Loss: 0.00022675
Epoch [119/300], Train Loss: 0.000225
Validation Loss: 0.00021979
Epoch [120/300], Train Loss: 0.000224
Validation Loss: 0.00021796
Epoch [121/300], Train Loss: 0.000223
Validation Loss: 0.00022678
Epoch [122/300], Train Loss: 0.000234
Validation Loss: 0.00035915
Epoch [123/300], Train Loss: 0.000247
Validation Loss: 0.00021904
Epoch [124/300], Train Loss: 0.000224
Validation Loss: 0.00022363
Epoch [125/300], Train Loss: 0.000220
Validation Loss: 0.00021806
Epoch [126/300], Train Loss: 0.000218
Validation Loss: 0.00021484
Epoch [127/300], Train Loss: 0.000222
Validation Loss: 0.00021617
Epoch [128/300], Train Loss: 0.000216
Validation Loss: 0.00021474
Epoch [129/300], Train Loss: 0.000219
Validation Loss: 0.00021519
Epoch [130/300], Train Loss: 0.000218
Validation Loss: 0.00024191
Epoch [131/300], Train Loss: 0.000232
Validation Loss: 0.00021642
Epoch [132/300], Train Loss: 0.000221
Validation Loss: 0.00021404
Epoch [133/300], Train Loss: 0.000214
Validation Loss: 0.00021165
Epoch [134/300], Train Loss: 0.000218
Validation Loss: 0.00021626
Epoch [135/300], Train Loss: 0.000212
Validation Loss: 0.00021014
Epoch [136/300], Train Loss: 0.000236
Validation Loss: 0.00025919
Epoch [137/300], Train Loss: 0.000224
Validation Loss: 0.00021221
Epoch [138/300], Train Loss: 0.000213
Validation Loss: 0.00020979
Epoch [139/300], Train Loss: 0.000210
Validation Loss: 0.00020857
Epoch [140/300], Train Loss: 0.000212
Validation Loss: 0.00021018
Epoch [141/300], Train Loss: 0.000213
Validation Loss: 0.00021105
Epoch [142/300], Train Loss: 0.000212
Validation Loss: 0.00021715
Epoch [143/300], Train Loss: 0.000209
Validation Loss: 0.00021021
Epoch [144/300], Train Loss: 0.000212
Validation Loss: 0.00021241
Epoch [145/300], Train Loss: 0.000217
Validation Loss: 0.00020689
Epoch [146/300], Train Loss: 0.000207
Validation Loss: 0.00020433
Epoch [147/300], Train Loss: 0.000207
Validation Loss: 0.00020221
Epoch [148/300], Train Loss: 0.000210
Validation Loss: 0.00020688
Epoch [149/300], Train Loss: 0.000209
Validation Loss: 0.00021053
Epoch [150/300], Train Loss: 0.000207
Validation Loss: 0.00021664
Epoch [151/300], Train Loss: 0.000208
Validation Loss: 0.00020394
Epoch [152/300], Train Loss: 0.000204
Validation Loss: 0.00020199
Epoch [153/300], Train Loss: 0.000205
Validation Loss: 0.00020172
Epoch [154/300], Train Loss: 0.000204
Validation Loss: 0.00020503
Epoch [155/300], Train Loss: 0.000202
Validation Loss: 0.00020915
Epoch [156/300], Train Loss: 0.000203
Validation Loss: 0.00020383
Epoch [157/300], Train Loss: 0.000202
Validation Loss: 0.00020244
Epoch [158/300], Train Loss: 0.000212
Validation Loss: 0.00020684
Epoch [159/300], Train Loss: 0.000206
Validation Loss: 0.00020115
Epoch [160/300], Train Loss: 0.000205
Validation Loss: 0.00020025
Epoch [161/300], Train Loss: 0.000200
Validation Loss: 0.00019701
Epoch [162/300], Train Loss: 0.000199
Validation Loss: 0.00019802
Epoch [163/300], Train Loss: 0.000199
Validation Loss: 0.00019675
Epoch [164/300], Train Loss: 0.000198
Validation Loss: 0.00019621
Epoch [165/300], Train Loss: 0.000205
Validation Loss: 0.00019777
Epoch [166/300], Train Loss: 0.000200
Validation Loss: 0.00020020
Epoch [167/300], Train Loss: 0.000198
Validation Loss: 0.00019984
Epoch [168/300], Train Loss: 0.000201
Validation Loss: 0.00020081
Epoch [169/300], Train Loss: 0.000200
Validation Loss: 0.00021127
Epoch [170/300], Train Loss: 0.000196
Validation Loss: 0.00019347
Epoch [171/300], Train Loss: 0.000194
Validation Loss: 0.00019299
Epoch [172/300], Train Loss: 0.000198
Validation Loss: 0.00019236
Epoch [173/300], Train Loss: 0.000198
Validation Loss: 0.00019350
Epoch [174/300], Train Loss: 0.000196
Validation Loss: 0.00019512
Epoch [175/300], Train Loss: 0.000204
Validation Loss: 0.00019435
Epoch [176/300], Train Loss: 0.000194
Validation Loss: 0.00019935
Epoch [177/300], Train Loss: 0.000193
Validation Loss: 0.00019199
Epoch [178/300], Train Loss: 0.000191
Validation Loss: 0.00019175
Epoch [179/300], Train Loss: 0.000191
Validation Loss: 0.00019057
Epoch [180/300], Train Loss: 0.000192
Validation Loss: 0.00019301
Epoch [181/300], Train Loss: 0.000190
Validation Loss: 0.00019510
Epoch [182/300], Train Loss: 0.000190
Validation Loss: 0.00018867
Epoch [183/300], Train Loss: 0.000198
Validation Loss: 0.00019863
Epoch [184/300], Train Loss: 0.000214
Validation Loss: 0.00019003
Epoch [185/300], Train Loss: 0.000191
Validation Loss: 0.00018983
Epoch [186/300], Train Loss: 0.000189
Validation Loss: 0.00019159
Epoch [187/300], Train Loss: 0.000188
Validation Loss: 0.00019064
Epoch [188/300], Train Loss: 0.000191
Validation Loss: 0.00018807
Epoch [189/300], Train Loss: 0.000187
Validation Loss: 0.00019498
Epoch [190/300], Train Loss: 0.000189
Validation Loss: 0.00019216
Epoch [191/300], Train Loss: 0.000190
Validation Loss: 0.00018677
Epoch [192/300], Train Loss: 0.000187
Validation Loss: 0.00020570
Epoch [193/300], Train Loss: 0.000189
Validation Loss: 0.00018631
Epoch [194/300], Train Loss: 0.000189
Validation Loss: 0.00018632
Epoch [195/300], Train Loss: 0.000190
Validation Loss: 0.00020228
Epoch [196/300], Train Loss: 0.000185
Validation Loss: 0.00018491
Epoch [197/300], Train Loss: 0.000198
Validation Loss: 0.00018663
Epoch [198/300], Train Loss: 0.000184
Validation Loss: 0.00018746
Epoch [199/300], Train Loss: 0.000183
Validation Loss: 0.00018493
Epoch [200/300], Train Loss: 0.000189
Validation Loss: 0.00018723
Epoch [201/300], Train Loss: 0.000183
Validation Loss: 0.00018376
Epoch [202/300], Train Loss: 0.000184
Validation Loss: 0.00018511
Epoch [203/300], Train Loss: 0.000183
Validation Loss: 0.00018456
Epoch [204/300], Train Loss: 0.000186
Validation Loss: 0.00018567
Epoch [205/300], Train Loss: 0.000183
Validation Loss: 0.00018162
Epoch [206/300], Train Loss: 0.000182
Validation Loss: 0.00018503
Epoch [207/300], Train Loss: 0.000182
Validation Loss: 0.00018426
Epoch [208/300], Train Loss: 0.000182
Validation Loss: 0.00018522
Epoch [209/300], Train Loss: 0.000181
Validation Loss: 0.00018706
Epoch [210/300], Train Loss: 0.000182
Validation Loss: 0.00018181
Epoch [211/300], Train Loss: 0.000181
Validation Loss: 0.00017959
Epoch [212/300], Train Loss: 0.000181
Validation Loss: 0.00017957
Epoch [213/300], Train Loss: 0.000179
Validation Loss: 0.00018175
Epoch [214/300], Train Loss: 0.000183
Validation Loss: 0.00017847
Epoch [215/300], Train Loss: 0.000179
Validation Loss: 0.00018504
Epoch [216/300], Train Loss: 0.000179
Validation Loss: 0.00018207
Epoch [217/300], Train Loss: 0.000180
Validation Loss: 0.00018073
Epoch [218/300], Train Loss: 0.000181
Validation Loss: 0.00019391
Epoch [219/300], Train Loss: 0.000181
Validation Loss: 0.00018006
Epoch [220/300], Train Loss: 0.000177
Validation Loss: 0.00018134
Epoch [221/300], Train Loss: 0.000177
Validation Loss: 0.00017926
Epoch [222/300], Train Loss: 0.000177
Validation Loss: 0.00017928
Epoch [223/300], Train Loss: 0.000180
Validation Loss: 0.00018625
Epoch [224/300], Train Loss: 0.000177
Validation Loss: 0.00018067
Early stopping triggered

Evaluating model for: Fridge
Run 27/72 completed in 14702.19 seconds with: {'MAE': np.float32(14.61409), 'MSE': np.float32(584.6141), 'RMSE': np.float32(24.178793), 'SAE': np.float32(0.0071728905), 'NDE': np.float32(0.41888463)}

Run 28/72: hidden=256, seq_len=120, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000717
Validation Loss: 0.00065543
Epoch [2/300], Train Loss: 0.000653
Validation Loss: 0.00061149
Epoch [3/300], Train Loss: 0.000621
Validation Loss: 0.00059775
Epoch [4/300], Train Loss: 0.000611
Validation Loss: 0.00059242
Epoch [5/300], Train Loss: 0.000605
Validation Loss: 0.00058856
Epoch [6/300], Train Loss: 0.000602
Validation Loss: 0.00058631
Epoch [7/300], Train Loss: 0.000599
Validation Loss: 0.00058490
Epoch [8/300], Train Loss: 0.000597
Validation Loss: 0.00059156
Epoch [9/300], Train Loss: 0.000595
Validation Loss: 0.00057862
Epoch [10/300], Train Loss: 0.000588
Validation Loss: 0.00056906
Epoch [11/300], Train Loss: 0.000577
Validation Loss: 0.00056083
Epoch [12/300], Train Loss: 0.000569
Validation Loss: 0.00055059
Epoch [13/300], Train Loss: 0.000557
Validation Loss: 0.00053295
Epoch [14/300], Train Loss: 0.000540
Validation Loss: 0.00052398
Epoch [15/300], Train Loss: 0.000532
Validation Loss: 0.00052031
Epoch [16/300], Train Loss: 0.000523
Validation Loss: 0.00052082
Epoch [17/300], Train Loss: 0.000517
Validation Loss: 0.00050582
Epoch [18/300], Train Loss: 0.000512
Validation Loss: 0.00049891
Epoch [19/300], Train Loss: 0.000506
Validation Loss: 0.00049205
Epoch [20/300], Train Loss: 0.000498
Validation Loss: 0.00049090
Epoch [21/300], Train Loss: 0.000491
Validation Loss: 0.00048417
Epoch [22/300], Train Loss: 0.000483
Validation Loss: 0.00047831
Epoch [23/300], Train Loss: 0.000476
Validation Loss: 0.00046383
Epoch [24/300], Train Loss: 0.000467
Validation Loss: 0.00045019
Epoch [25/300], Train Loss: 0.000459
Validation Loss: 0.00046955
Epoch [26/300], Train Loss: 0.000453
Validation Loss: 0.00045986
Epoch [27/300], Train Loss: 0.000446
Validation Loss: 0.00043698
Epoch [28/300], Train Loss: 0.000436
Validation Loss: 0.00043150
Epoch [29/300], Train Loss: 0.000428
Validation Loss: 0.00044085
Epoch [30/300], Train Loss: 0.000422
Validation Loss: 0.00040985
Epoch [31/300], Train Loss: 0.000408
Validation Loss: 0.00041244
Epoch [32/300], Train Loss: 0.000400
Validation Loss: 0.00039121
Epoch [33/300], Train Loss: 0.000396
Validation Loss: 0.00041007
Epoch [34/300], Train Loss: 0.000388
Validation Loss: 0.00038464
Epoch [35/300], Train Loss: 0.000382
Validation Loss: 0.00038607
Epoch [36/300], Train Loss: 0.000378
Validation Loss: 0.00040154
Epoch [37/300], Train Loss: 0.000373
Validation Loss: 0.00035684
Epoch [38/300], Train Loss: 0.000369
Validation Loss: 0.00037636
Epoch [39/300], Train Loss: 0.000360
Validation Loss: 0.00035954
Epoch [40/300], Train Loss: 0.000355
Validation Loss: 0.00035609
Epoch [41/300], Train Loss: 0.000353
Validation Loss: 0.00034148
Epoch [42/300], Train Loss: 0.000344
Validation Loss: 0.00033007
Epoch [43/300], Train Loss: 0.000325
Validation Loss: 0.00030519
Epoch [44/300], Train Loss: 0.000311
Validation Loss: 0.00031243
Epoch [45/300], Train Loss: 0.000303
Validation Loss: 0.00031358
Epoch [46/300], Train Loss: 0.000295
Validation Loss: 0.00028368
Epoch [47/300], Train Loss: 0.000313
Validation Loss: 0.00049432
Epoch [48/300], Train Loss: 0.000399
Validation Loss: 0.00034947
Epoch [49/300], Train Loss: 0.000341
Validation Loss: 0.00032005
Epoch [50/300], Train Loss: 0.000314
Validation Loss: 0.00030364
Epoch [51/300], Train Loss: 0.000300
Validation Loss: 0.00028275
Epoch [52/300], Train Loss: 0.000307
Validation Loss: 0.00030278
Epoch [53/300], Train Loss: 0.000295
Validation Loss: 0.00028178
Epoch [54/300], Train Loss: 0.000285
Validation Loss: 0.00031390
Epoch [55/300], Train Loss: 0.000281
Validation Loss: 0.00027499
Epoch [56/300], Train Loss: 0.000273
Validation Loss: 0.00026633
Epoch [57/300], Train Loss: 0.000269
Validation Loss: 0.00026777
Epoch [58/300], Train Loss: 0.000266
Validation Loss: 0.00025454
Epoch [59/300], Train Loss: 0.000264
Validation Loss: 0.00025034
Epoch [60/300], Train Loss: 0.000259
Validation Loss: 0.00024492
Epoch [61/300], Train Loss: 0.000261
Validation Loss: 0.00024694
Epoch [62/300], Train Loss: 0.000255
Validation Loss: 0.00024468
Epoch [63/300], Train Loss: 0.000254
Validation Loss: 0.00024063
Epoch [64/300], Train Loss: 0.000254
Validation Loss: 0.00025846
Epoch [65/300], Train Loss: 0.000251
Validation Loss: 0.00026810
Epoch [66/300], Train Loss: 0.000249
Validation Loss: 0.00025454
Epoch [67/300], Train Loss: 0.000253
Validation Loss: 0.00024211
Epoch [68/300], Train Loss: 0.000244
Validation Loss: 0.00023193
Epoch [69/300], Train Loss: 0.000255
Validation Loss: 0.00025865
Epoch [70/300], Train Loss: 0.000248
Validation Loss: 0.00023316
Epoch [71/300], Train Loss: 0.000238
Validation Loss: 0.00024797
Epoch [72/300], Train Loss: 0.000242
Validation Loss: 0.00024369
Epoch [73/300], Train Loss: 0.000239
Validation Loss: 0.00026551
Epoch [74/300], Train Loss: 0.000239
Validation Loss: 0.00023670
Epoch [75/300], Train Loss: 0.000235
Validation Loss: 0.00024570
Epoch [76/300], Train Loss: 0.000237
Validation Loss: 0.00022494
Epoch [77/300], Train Loss: 0.000239
Validation Loss: 0.00022487
Epoch [78/300], Train Loss: 0.000231
Validation Loss: 0.00022492
Epoch [79/300], Train Loss: 0.000229
Validation Loss: 0.00022973
Epoch [80/300], Train Loss: 0.000232
Validation Loss: 0.00022439
Epoch [81/300], Train Loss: 0.000231
Validation Loss: 0.00024114
Epoch [82/300], Train Loss: 0.000228
Validation Loss: 0.00022165
Epoch [83/300], Train Loss: 0.000226
Validation Loss: 0.00022283
Epoch [84/300], Train Loss: 0.000228
Validation Loss: 0.00022219
Epoch [85/300], Train Loss: 0.000229
Validation Loss: 0.00022018
Epoch [86/300], Train Loss: 0.000224
Validation Loss: 0.00025473
Epoch [87/300], Train Loss: 0.000228
Validation Loss: 0.00022542
Epoch [88/300], Train Loss: 0.000222
Validation Loss: 0.00021545
Epoch [89/300], Train Loss: 0.000221
Validation Loss: 0.00026200
Epoch [90/300], Train Loss: 0.000224
Validation Loss: 0.00022542
Epoch [91/300], Train Loss: 0.000217
Validation Loss: 0.00021335
Epoch [92/300], Train Loss: 0.000222
Validation Loss: 0.00021278
Epoch [93/300], Train Loss: 0.000218
Validation Loss: 0.00021117
Epoch [94/300], Train Loss: 0.000218
Validation Loss: 0.00021117
Epoch [95/300], Train Loss: 0.000281
Validation Loss: 0.00040421
Epoch [96/300], Train Loss: 0.000331
Validation Loss: 0.00028624
Epoch [97/300], Train Loss: 0.000284
Validation Loss: 0.00026829
Epoch [98/300], Train Loss: 0.000270
Validation Loss: 0.00026538
Epoch [99/300], Train Loss: 0.000261
Validation Loss: 0.00026387
Epoch [100/300], Train Loss: 0.000260
Validation Loss: 0.00024488
Epoch [101/300], Train Loss: 0.000254
Validation Loss: 0.00024243
Epoch [102/300], Train Loss: 0.000250
Validation Loss: 0.00024677
Epoch [103/300], Train Loss: 0.000255
Validation Loss: 0.00023667
Epoch [104/300], Train Loss: 0.000243
Validation Loss: 0.00024116
Early stopping triggered

Evaluating model for: Fridge
Run 28/72 completed in 7500.67 seconds with: {'MAE': np.float32(18.657066), 'MSE': np.float32(809.80414), 'RMSE': np.float32(28.457058), 'SAE': np.float32(0.04603), 'NDE': np.float32(0.49300316)}

Run 29/72: hidden=256, seq_len=120, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000659
Validation Loss: 0.00066428
Epoch [2/300], Train Loss: 0.000640
Validation Loss: 0.00064465
Epoch [3/300], Train Loss: 0.000622
Validation Loss: 0.00062418
Epoch [4/300], Train Loss: 0.000611
Validation Loss: 0.00061803
Epoch [5/300], Train Loss: 0.000605
Validation Loss: 0.00061063
Epoch [6/300], Train Loss: 0.000601
Validation Loss: 0.00060430
Epoch [7/300], Train Loss: 0.000598
Validation Loss: 0.00060038
Epoch [8/300], Train Loss: 0.000598
Validation Loss: 0.00059504
Epoch [9/300], Train Loss: 0.000593
Validation Loss: 0.00059117
Epoch [10/300], Train Loss: 0.000589
Validation Loss: 0.00058737
Epoch [11/300], Train Loss: 0.000588
Validation Loss: 0.00058482
Epoch [12/300], Train Loss: 0.000585
Validation Loss: 0.00058494
Epoch [13/300], Train Loss: 0.000584
Validation Loss: 0.00058266
Epoch [14/300], Train Loss: 0.000581
Validation Loss: 0.00058048
Epoch [15/300], Train Loss: 0.000580
Validation Loss: 0.00057949
Epoch [16/300], Train Loss: 0.000577
Validation Loss: 0.00057448
Epoch [17/300], Train Loss: 0.000576
Validation Loss: 0.00057315
Epoch [18/300], Train Loss: 0.000574
Validation Loss: 0.00057144
Epoch [19/300], Train Loss: 0.000572
Validation Loss: 0.00057223
Epoch [20/300], Train Loss: 0.000571
Validation Loss: 0.00056598
Epoch [21/300], Train Loss: 0.000569
Validation Loss: 0.00056719
Epoch [22/300], Train Loss: 0.000567
Validation Loss: 0.00056042
Epoch [23/300], Train Loss: 0.000565
Validation Loss: 0.00055906
Epoch [24/300], Train Loss: 0.000563
Validation Loss: 0.00055540
Epoch [25/300], Train Loss: 0.000564
Validation Loss: 0.00055878
Epoch [26/300], Train Loss: 0.000558
Validation Loss: 0.00055334
Epoch [27/300], Train Loss: 0.000554
Validation Loss: 0.00055134
Epoch [28/300], Train Loss: 0.000554
Validation Loss: 0.00055776
Epoch [29/300], Train Loss: 0.000550
Validation Loss: 0.00054352
Epoch [30/300], Train Loss: 0.000548
Validation Loss: 0.00054260
Epoch [31/300], Train Loss: 0.000544
Validation Loss: 0.00053609
Epoch [32/300], Train Loss: 0.000541
Validation Loss: 0.00053498
Epoch [33/300], Train Loss: 0.000540
Validation Loss: 0.00053752
Epoch [34/300], Train Loss: 0.000537
Validation Loss: 0.00053229
Epoch [35/300], Train Loss: 0.000535
Validation Loss: 0.00052595
Epoch [36/300], Train Loss: 0.000532
Validation Loss: 0.00052346
Epoch [37/300], Train Loss: 0.000528
Validation Loss: 0.00053682
Epoch [38/300], Train Loss: 0.000528
Validation Loss: 0.00051853
Epoch [39/300], Train Loss: 0.000524
Validation Loss: 0.00051632
Epoch [40/300], Train Loss: 0.000522
Validation Loss: 0.00051464
Epoch [41/300], Train Loss: 0.000519
Validation Loss: 0.00051400
Epoch [42/300], Train Loss: 0.000514
Validation Loss: 0.00051063
Epoch [43/300], Train Loss: 0.000511
Validation Loss: 0.00050260
Epoch [44/300], Train Loss: 0.000507
Validation Loss: 0.00051383
Epoch [45/300], Train Loss: 0.000505
Validation Loss: 0.00050193
Epoch [46/300], Train Loss: 0.000499
Validation Loss: 0.00049306
Epoch [47/300], Train Loss: 0.000498
Validation Loss: 0.00049454
Epoch [48/300], Train Loss: 0.000496
Validation Loss: 0.00049574
Epoch [49/300], Train Loss: 0.000492
Validation Loss: 0.00048281
Epoch [50/300], Train Loss: 0.000490
Validation Loss: 0.00048965
Epoch [51/300], Train Loss: 0.000485
Validation Loss: 0.00048284
Epoch [52/300], Train Loss: 0.000484
Validation Loss: 0.00048114
Epoch [53/300], Train Loss: 0.000482
Validation Loss: 0.00047612
Epoch [54/300], Train Loss: 0.000481
Validation Loss: 0.00048638
Epoch [55/300], Train Loss: 0.000476
Validation Loss: 0.00047281
Epoch [56/300], Train Loss: 0.000472
Validation Loss: 0.00047364
Epoch [57/300], Train Loss: 0.000470
Validation Loss: 0.00046327
Epoch [58/300], Train Loss: 0.000467
Validation Loss: 0.00046614
Epoch [59/300], Train Loss: 0.000461
Validation Loss: 0.00045545
Epoch [60/300], Train Loss: 0.000458
Validation Loss: 0.00045040
Epoch [61/300], Train Loss: 0.000451
Validation Loss: 0.00043828
Epoch [62/300], Train Loss: 0.000448
Validation Loss: 0.00043797
Epoch [63/300], Train Loss: 0.000439
Validation Loss: 0.00042990
Epoch [64/300], Train Loss: 0.000435
Validation Loss: 0.00046574
Epoch [65/300], Train Loss: 0.000432
Validation Loss: 0.00044123
Epoch [66/300], Train Loss: 0.000428
Validation Loss: 0.00043075
Epoch [67/300], Train Loss: 0.000423
Validation Loss: 0.00040692
Epoch [68/300], Train Loss: 0.000419
Validation Loss: 0.00039795
Epoch [69/300], Train Loss: 0.000412
Validation Loss: 0.00039913
Epoch [70/300], Train Loss: 0.000408
Validation Loss: 0.00039181
Epoch [71/300], Train Loss: 0.000404
Validation Loss: 0.00038639
Epoch [72/300], Train Loss: 0.000405
Validation Loss: 0.00040346
Epoch [73/300], Train Loss: 0.000398
Validation Loss: 0.00039232
Epoch [74/300], Train Loss: 0.000394
Validation Loss: 0.00037681
Epoch [75/300], Train Loss: 0.000397
Validation Loss: 0.00038474
Epoch [76/300], Train Loss: 0.000388
Validation Loss: 0.00037480
Epoch [77/300], Train Loss: 0.000383
Validation Loss: 0.00037834
Epoch [78/300], Train Loss: 0.000386
Validation Loss: 0.00036843
Epoch [79/300], Train Loss: 0.000381
Validation Loss: 0.00036314
Epoch [80/300], Train Loss: 0.000376
Validation Loss: 0.00036864
Epoch [81/300], Train Loss: 0.000377
Validation Loss: 0.00035811
Epoch [82/300], Train Loss: 0.000381
Validation Loss: 0.00036531
Epoch [83/300], Train Loss: 0.000371
Validation Loss: 0.00035831
Epoch [84/300], Train Loss: 0.000369
Validation Loss: 0.00036546
Epoch [85/300], Train Loss: 0.000371
Validation Loss: 0.00035324
Epoch [86/300], Train Loss: 0.000364
Validation Loss: 0.00035228
Epoch [87/300], Train Loss: 0.000368
Validation Loss: 0.00036454
Epoch [88/300], Train Loss: 0.000366
Validation Loss: 0.00035863
Epoch [89/300], Train Loss: 0.000357
Validation Loss: 0.00034721
Epoch [90/300], Train Loss: 0.000358
Validation Loss: 0.00035474
Epoch [91/300], Train Loss: 0.000357
Validation Loss: 0.00034876
Epoch [92/300], Train Loss: 0.000355
Validation Loss: 0.00034913
Epoch [93/300], Train Loss: 0.000353
Validation Loss: 0.00034186
Epoch [94/300], Train Loss: 0.000349
Validation Loss: 0.00033838
Epoch [95/300], Train Loss: 0.000347
Validation Loss: 0.00033778
Epoch [96/300], Train Loss: 0.000351
Validation Loss: 0.00034864
Epoch [97/300], Train Loss: 0.000346
Validation Loss: 0.00033843
Epoch [98/300], Train Loss: 0.000346
Validation Loss: 0.00033556
Epoch [99/300], Train Loss: 0.000343
Validation Loss: 0.00033102
Epoch [100/300], Train Loss: 0.000340
Validation Loss: 0.00033531
Epoch [101/300], Train Loss: 0.000348
Validation Loss: 0.00035961
Epoch [102/300], Train Loss: 0.000339
Validation Loss: 0.00032769
Epoch [103/300], Train Loss: 0.000340
Validation Loss: 0.00032121
Epoch [104/300], Train Loss: 0.000335
Validation Loss: 0.00033295
Epoch [105/300], Train Loss: 0.000341
Validation Loss: 0.00033710
Epoch [106/300], Train Loss: 0.000333
Validation Loss: 0.00033148
Epoch [107/300], Train Loss: 0.000331
Validation Loss: 0.00032463
Epoch [108/300], Train Loss: 0.000331
Validation Loss: 0.00033586
Epoch [109/300], Train Loss: 0.000333
Validation Loss: 0.00033723
Epoch [110/300], Train Loss: 0.000327
Validation Loss: 0.00031455
Epoch [111/300], Train Loss: 0.000328
Validation Loss: 0.00035411
Epoch [112/300], Train Loss: 0.000326
Validation Loss: 0.00031279
Epoch [113/300], Train Loss: 0.000325
Validation Loss: 0.00031456
Epoch [114/300], Train Loss: 0.000322
Validation Loss: 0.00031180
Epoch [115/300], Train Loss: 0.000326
Validation Loss: 0.00032404
Epoch [116/300], Train Loss: 0.000322
Validation Loss: 0.00034575
Epoch [117/300], Train Loss: 0.000324
Validation Loss: 0.00031363
Epoch [118/300], Train Loss: 0.000318
Validation Loss: 0.00031181
Epoch [119/300], Train Loss: 0.000317
Validation Loss: 0.00030659
Epoch [120/300], Train Loss: 0.000316
Validation Loss: 0.00030540
Epoch [121/300], Train Loss: 0.000315
Validation Loss: 0.00030348
Epoch [122/300], Train Loss: 0.000313
Validation Loss: 0.00031019
Epoch [123/300], Train Loss: 0.000315
Validation Loss: 0.00032740
Epoch [124/300], Train Loss: 0.000314
Validation Loss: 0.00031588
Epoch [125/300], Train Loss: 0.000312
Validation Loss: 0.00030262
Epoch [126/300], Train Loss: 0.000311
Validation Loss: 0.00031310
Epoch [127/300], Train Loss: 0.000318
Validation Loss: 0.00030248
Epoch [128/300], Train Loss: 0.000310
Validation Loss: 0.00030251
Epoch [129/300], Train Loss: 0.000308
Validation Loss: 0.00029926
Epoch [130/300], Train Loss: 0.000308
Validation Loss: 0.00030312
Epoch [131/300], Train Loss: 0.000306
Validation Loss: 0.00029427
Epoch [132/300], Train Loss: 0.000305
Validation Loss: 0.00029727
Epoch [133/300], Train Loss: 0.000304
Validation Loss: 0.00029679
Epoch [134/300], Train Loss: 0.000307
Validation Loss: 0.00029665
Epoch [135/300], Train Loss: 0.000305
Validation Loss: 0.00031048
Epoch [136/300], Train Loss: 0.000307
Validation Loss: 0.00029282
Epoch [137/300], Train Loss: 0.000304
Validation Loss: 0.00034113
Epoch [138/300], Train Loss: 0.000306
Validation Loss: 0.00030460
Epoch [139/300], Train Loss: 0.000302
Validation Loss: 0.00030177
Epoch [140/300], Train Loss: 0.000298
Validation Loss: 0.00032364
Epoch [141/300], Train Loss: 0.000301
Validation Loss: 0.00029256
Epoch [142/300], Train Loss: 0.000300
Validation Loss: 0.00029784
Epoch [143/300], Train Loss: 0.000299
Validation Loss: 0.00029475
Epoch [144/300], Train Loss: 0.000301
Validation Loss: 0.00029584
Epoch [145/300], Train Loss: 0.000300
Validation Loss: 0.00029252
Epoch [146/300], Train Loss: 0.000297
Validation Loss: 0.00029253
Epoch [147/300], Train Loss: 0.000297
Validation Loss: 0.00028669
Epoch [148/300], Train Loss: 0.000298
Validation Loss: 0.00028737
Epoch [149/300], Train Loss: 0.000295
Validation Loss: 0.00028483
Epoch [150/300], Train Loss: 0.000293
Validation Loss: 0.00029009
Epoch [151/300], Train Loss: 0.000296
Validation Loss: 0.00028658
Epoch [152/300], Train Loss: 0.000297
Validation Loss: 0.00028422
Epoch [153/300], Train Loss: 0.000293
Validation Loss: 0.00028915
Epoch [154/300], Train Loss: 0.000292
Validation Loss: 0.00030900
Epoch [155/300], Train Loss: 0.000293
Validation Loss: 0.00028899
Epoch [156/300], Train Loss: 0.000294
Validation Loss: 0.00030136
Epoch [157/300], Train Loss: 0.000291
Validation Loss: 0.00029332
Epoch [158/300], Train Loss: 0.000293
Validation Loss: 0.00028890
Epoch [159/300], Train Loss: 0.000289
Validation Loss: 0.00028276
Epoch [160/300], Train Loss: 0.000290
Validation Loss: 0.00027955
Epoch [161/300], Train Loss: 0.000290
Validation Loss: 0.00028127
Epoch [162/300], Train Loss: 0.000286
Validation Loss: 0.00028473
Epoch [163/300], Train Loss: 0.000287
Validation Loss: 0.00028355
Epoch [164/300], Train Loss: 0.000291
Validation Loss: 0.00028363
Epoch [165/300], Train Loss: 0.000285
Validation Loss: 0.00029733
Epoch [166/300], Train Loss: 0.000289
Validation Loss: 0.00027999
Epoch [167/300], Train Loss: 0.000283
Validation Loss: 0.00029007
Epoch [168/300], Train Loss: 0.000285
Validation Loss: 0.00027356
Epoch [169/300], Train Loss: 0.000285
Validation Loss: 0.00027728
Epoch [170/300], Train Loss: 0.000284
Validation Loss: 0.00027670
Epoch [171/300], Train Loss: 0.000284
Validation Loss: 0.00028374
Epoch [172/300], Train Loss: 0.000283
Validation Loss: 0.00027205
Epoch [173/300], Train Loss: 0.000282
Validation Loss: 0.00027590
Epoch [174/300], Train Loss: 0.000282
Validation Loss: 0.00027655
Epoch [175/300], Train Loss: 0.000281
Validation Loss: 0.00027490
Epoch [176/300], Train Loss: 0.000279
Validation Loss: 0.00027083
Epoch [177/300], Train Loss: 0.000278
Validation Loss: 0.00027427
Epoch [178/300], Train Loss: 0.000283
Validation Loss: 0.00027168
Epoch [179/300], Train Loss: 0.000278
Validation Loss: 0.00027788
Epoch [180/300], Train Loss: 0.000280
Validation Loss: 0.00026963
Epoch [181/300], Train Loss: 0.000279
Validation Loss: 0.00026961
Epoch [182/300], Train Loss: 0.000277
Validation Loss: 0.00028541
Epoch [183/300], Train Loss: 0.000279
Validation Loss: 0.00028895
Epoch [184/300], Train Loss: 0.000278
Validation Loss: 0.00027103
Epoch [185/300], Train Loss: 0.000275
Validation Loss: 0.00027001
Epoch [186/300], Train Loss: 0.000277
Validation Loss: 0.00026835
Epoch [187/300], Train Loss: 0.000275
Validation Loss: 0.00027710
Epoch [188/300], Train Loss: 0.000278
Validation Loss: 0.00026868
Epoch [189/300], Train Loss: 0.000275
Validation Loss: 0.00026925
Epoch [190/300], Train Loss: 0.000274
Validation Loss: 0.00026974
Epoch [191/300], Train Loss: 0.000276
Validation Loss: 0.00027518
Epoch [192/300], Train Loss: 0.000273
Validation Loss: 0.00026843
Epoch [193/300], Train Loss: 0.000275
Validation Loss: 0.00026551
Epoch [194/300], Train Loss: 0.000274
Validation Loss: 0.00026812
Epoch [195/300], Train Loss: 0.000272
Validation Loss: 0.00026429
Epoch [196/300], Train Loss: 0.000273
Validation Loss: 0.00026951
Epoch [197/300], Train Loss: 0.000272
Validation Loss: 0.00026396
Epoch [198/300], Train Loss: 0.000270
Validation Loss: 0.00027457
Epoch [199/300], Train Loss: 0.000274
Validation Loss: 0.00027042
Epoch [200/300], Train Loss: 0.000271
Validation Loss: 0.00026390
Epoch [201/300], Train Loss: 0.000270
Validation Loss: 0.00026959
Epoch [202/300], Train Loss: 0.000270
Validation Loss: 0.00026145
Epoch [203/300], Train Loss: 0.000269
Validation Loss: 0.00026533
Epoch [204/300], Train Loss: 0.000275
Validation Loss: 0.00026697
Epoch [205/300], Train Loss: 0.000270
Validation Loss: 0.00026412
Epoch [206/300], Train Loss: 0.000270
Validation Loss: 0.00030718
Epoch [207/300], Train Loss: 0.000270
Validation Loss: 0.00027792
Epoch [208/300], Train Loss: 0.000269
Validation Loss: 0.00026150
Epoch [209/300], Train Loss: 0.000267
Validation Loss: 0.00026229
Epoch [210/300], Train Loss: 0.000271
Validation Loss: 0.00026143
Epoch [211/300], Train Loss: 0.000269
Validation Loss: 0.00026226
Epoch [212/300], Train Loss: 0.000267
Validation Loss: 0.00025931
Epoch [213/300], Train Loss: 0.000267
Validation Loss: 0.00025852
Epoch [214/300], Train Loss: 0.000265
Validation Loss: 0.00025788
Epoch [215/300], Train Loss: 0.000267
Validation Loss: 0.00025835
Epoch [216/300], Train Loss: 0.000267
Validation Loss: 0.00026373
Epoch [217/300], Train Loss: 0.000265
Validation Loss: 0.00026081
Epoch [218/300], Train Loss: 0.000266
Validation Loss: 0.00026095
Epoch [219/300], Train Loss: 0.000265
Validation Loss: 0.00026399
Epoch [220/300], Train Loss: 0.000265
Validation Loss: 0.00026174
Epoch [221/300], Train Loss: 0.000265
Validation Loss: 0.00025689
Epoch [222/300], Train Loss: 0.000264
Validation Loss: 0.00026221
Epoch [223/300], Train Loss: 0.000264
Validation Loss: 0.00026246
Epoch [224/300], Train Loss: 0.000265
Validation Loss: 0.00025815
Epoch [225/300], Train Loss: 0.000264
Validation Loss: 0.00025647
Epoch [226/300], Train Loss: 0.000264
Validation Loss: 0.00025672
Epoch [227/300], Train Loss: 0.000263
Validation Loss: 0.00026403
Epoch [228/300], Train Loss: 0.000262
Validation Loss: 0.00025734
Epoch [229/300], Train Loss: 0.000263
Validation Loss: 0.00026380
Epoch [230/300], Train Loss: 0.000262
Validation Loss: 0.00025649
Epoch [231/300], Train Loss: 0.000261
Validation Loss: 0.00025403
Epoch [232/300], Train Loss: 0.000263
Validation Loss: 0.00025981
Epoch [233/300], Train Loss: 0.000265
Validation Loss: 0.00025562
Epoch [234/300], Train Loss: 0.000261
Validation Loss: 0.00025894
Epoch [235/300], Train Loss: 0.000262
Validation Loss: 0.00028015
Epoch [236/300], Train Loss: 0.000264
Validation Loss: 0.00026696
Epoch [237/300], Train Loss: 0.000261
Validation Loss: 0.00025846
Epoch [238/300], Train Loss: 0.000261
Validation Loss: 0.00025570
Epoch [239/300], Train Loss: 0.000260
Validation Loss: 0.00025374
Epoch [240/300], Train Loss: 0.000260
Validation Loss: 0.00025423
Epoch [241/300], Train Loss: 0.000259
Validation Loss: 0.00025789
Epoch [242/300], Train Loss: 0.000261
Validation Loss: 0.00025293
Epoch [243/300], Train Loss: 0.000257
Validation Loss: 0.00025398
Epoch [244/300], Train Loss: 0.000258
Validation Loss: 0.00025725
Epoch [245/300], Train Loss: 0.000259
Validation Loss: 0.00026307
Epoch [246/300], Train Loss: 0.000260
Validation Loss: 0.00025139
Epoch [247/300], Train Loss: 0.000258
Validation Loss: 0.00025318
Epoch [248/300], Train Loss: 0.000259
Validation Loss: 0.00025311
Epoch [249/300], Train Loss: 0.000258
Validation Loss: 0.00025735
Epoch [250/300], Train Loss: 0.000259
Validation Loss: 0.00025215
Epoch [251/300], Train Loss: 0.000257
Validation Loss: 0.00025062
Epoch [252/300], Train Loss: 0.000257
Validation Loss: 0.00025089
Epoch [253/300], Train Loss: 0.000256
Validation Loss: 0.00025104
Epoch [254/300], Train Loss: 0.000259
Validation Loss: 0.00025625
Epoch [255/300], Train Loss: 0.000256
Validation Loss: 0.00025319
Epoch [256/300], Train Loss: 0.000255
Validation Loss: 0.00024920
Epoch [257/300], Train Loss: 0.000255
Validation Loss: 0.00024933
Epoch [258/300], Train Loss: 0.000258
Validation Loss: 0.00025144
Epoch [259/300], Train Loss: 0.000256
Validation Loss: 0.00025342
Epoch [260/300], Train Loss: 0.000255
Validation Loss: 0.00025123
Epoch [261/300], Train Loss: 0.000256
Validation Loss: 0.00024980
Epoch [262/300], Train Loss: 0.000256
Validation Loss: 0.00024831
Epoch [263/300], Train Loss: 0.000254
Validation Loss: 0.00024788
Epoch [264/300], Train Loss: 0.000255
Validation Loss: 0.00025467
Epoch [265/300], Train Loss: 0.000255
Validation Loss: 0.00024957
Epoch [266/300], Train Loss: 0.000256
Validation Loss: 0.00025084
Epoch [267/300], Train Loss: 0.000254
Validation Loss: 0.00024950
Epoch [268/300], Train Loss: 0.000254
Validation Loss: 0.00024819
Epoch [269/300], Train Loss: 0.000255
Validation Loss: 0.00024798
Epoch [270/300], Train Loss: 0.000254
Validation Loss: 0.00025089
Epoch [271/300], Train Loss: 0.000254
Validation Loss: 0.00025516
Epoch [272/300], Train Loss: 0.000253
Validation Loss: 0.00025459
Epoch [273/300], Train Loss: 0.000253
Validation Loss: 0.00024922
Early stopping triggered

Evaluating model for: Fridge
Run 29/72 completed in 8043.40 seconds with: {'MAE': np.float32(19.329546), 'MSE': np.float32(804.50415), 'RMSE': np.float32(28.363783), 'SAE': np.float32(0.022626359), 'NDE': np.float32(0.4879377)}

Run 30/72: hidden=256, seq_len=120, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000843
Validation Loss: 0.00070927
Epoch [2/300], Train Loss: 0.000683
Validation Loss: 0.00069942
Epoch [3/300], Train Loss: 0.000669
Validation Loss: 0.00066858
Epoch [4/300], Train Loss: 0.000649
Validation Loss: 0.00064607
Epoch [5/300], Train Loss: 0.000626
Validation Loss: 0.00062839
Epoch [6/300], Train Loss: 0.000617
Validation Loss: 0.00062065
Epoch [7/300], Train Loss: 0.000614
Validation Loss: 0.00062020
Epoch [8/300], Train Loss: 0.000612
Validation Loss: 0.00061528
Epoch [9/300], Train Loss: 0.000608
Validation Loss: 0.00061277
Epoch [10/300], Train Loss: 0.000606
Validation Loss: 0.00060903
Epoch [11/300], Train Loss: 0.000603
Validation Loss: 0.00061155
Epoch [12/300], Train Loss: 0.000602
Validation Loss: 0.00060523
Epoch [13/300], Train Loss: 0.000601
Validation Loss: 0.00060331
Epoch [14/300], Train Loss: 0.000598
Validation Loss: 0.00060809
Epoch [15/300], Train Loss: 0.000597
Validation Loss: 0.00061129
Epoch [16/300], Train Loss: 0.000595
Validation Loss: 0.00060315
Epoch [17/300], Train Loss: 0.000595
Validation Loss: 0.00059849
Epoch [18/300], Train Loss: 0.000594
Validation Loss: 0.00060146
Epoch [19/300], Train Loss: 0.000592
Validation Loss: 0.00059759
Epoch [20/300], Train Loss: 0.000591
Validation Loss: 0.00059851
Epoch [21/300], Train Loss: 0.000591
Validation Loss: 0.00059995
Epoch [22/300], Train Loss: 0.000589
Validation Loss: 0.00059024
Epoch [23/300], Train Loss: 0.000590
Validation Loss: 0.00059334
Epoch [24/300], Train Loss: 0.000587
Validation Loss: 0.00059043
Epoch [25/300], Train Loss: 0.000589
Validation Loss: 0.00059337
Epoch [26/300], Train Loss: 0.000586
Validation Loss: 0.00058712
Epoch [27/300], Train Loss: 0.000583
Validation Loss: 0.00058492
Epoch [28/300], Train Loss: 0.000582
Validation Loss: 0.00059128
Epoch [29/300], Train Loss: 0.000580
Validation Loss: 0.00058351
Epoch [30/300], Train Loss: 0.000580
Validation Loss: 0.00057886
Epoch [31/300], Train Loss: 0.000578
Validation Loss: 0.00057747
Epoch [32/300], Train Loss: 0.000575
Validation Loss: 0.00057167
Epoch [33/300], Train Loss: 0.000571
Validation Loss: 0.00057359
Epoch [34/300], Train Loss: 0.000568
Validation Loss: 0.00056701
Epoch [35/300], Train Loss: 0.000566
Validation Loss: 0.00055985
Epoch [36/300], Train Loss: 0.000563
Validation Loss: 0.00055574
Epoch [37/300], Train Loss: 0.000559
Validation Loss: 0.00055995
Epoch [38/300], Train Loss: 0.000558
Validation Loss: 0.00054917
Epoch [39/300], Train Loss: 0.000554
Validation Loss: 0.00054367
Epoch [40/300], Train Loss: 0.000553
Validation Loss: 0.00054207
Epoch [41/300], Train Loss: 0.000549
Validation Loss: 0.00054050
Epoch [42/300], Train Loss: 0.000543
Validation Loss: 0.00053795
Epoch [43/300], Train Loss: 0.000538
Validation Loss: 0.00052694
Epoch [44/300], Train Loss: 0.000534
Validation Loss: 0.00052534
Epoch [45/300], Train Loss: 0.000530
Validation Loss: 0.00051917
Epoch [46/300], Train Loss: 0.000523
Validation Loss: 0.00051199
Epoch [47/300], Train Loss: 0.000518
Validation Loss: 0.00051410
Epoch [48/300], Train Loss: 0.000511
Validation Loss: 0.00049452
Epoch [49/300], Train Loss: 0.000505
Validation Loss: 0.00048467
Epoch [50/300], Train Loss: 0.000501
Validation Loss: 0.00048893
Epoch [51/300], Train Loss: 0.000495
Validation Loss: 0.00048015
Epoch [52/300], Train Loss: 0.000489
Validation Loss: 0.00047271
Epoch [53/300], Train Loss: 0.000485
Validation Loss: 0.00046839
Epoch [54/300], Train Loss: 0.000483
Validation Loss: 0.00046500
Epoch [55/300], Train Loss: 0.000480
Validation Loss: 0.00047445
Epoch [56/300], Train Loss: 0.000474
Validation Loss: 0.00046370
Epoch [57/300], Train Loss: 0.000470
Validation Loss: 0.00045559
Epoch [58/300], Train Loss: 0.000467
Validation Loss: 0.00045678
Epoch [59/300], Train Loss: 0.000465
Validation Loss: 0.00044770
Epoch [60/300], Train Loss: 0.000462
Validation Loss: 0.00045184
Epoch [61/300], Train Loss: 0.000458
Validation Loss: 0.00044033
Epoch [62/300], Train Loss: 0.000461
Validation Loss: 0.00044262
Epoch [63/300], Train Loss: 0.000450
Validation Loss: 0.00043757
Epoch [64/300], Train Loss: 0.000452
Validation Loss: 0.00043505
Epoch [65/300], Train Loss: 0.000446
Validation Loss: 0.00042651
Epoch [66/300], Train Loss: 0.000444
Validation Loss: 0.00043030
Epoch [67/300], Train Loss: 0.000448
Validation Loss: 0.00042881
Epoch [68/300], Train Loss: 0.000439
Validation Loss: 0.00042032
Epoch [69/300], Train Loss: 0.000439
Validation Loss: 0.00044799
Epoch [70/300], Train Loss: 0.000438
Validation Loss: 0.00041496
Epoch [71/300], Train Loss: 0.000435
Validation Loss: 0.00042142
Epoch [72/300], Train Loss: 0.000433
Validation Loss: 0.00041062
Epoch [73/300], Train Loss: 0.000429
Validation Loss: 0.00041237
Epoch [74/300], Train Loss: 0.000429
Validation Loss: 0.00040727
Epoch [75/300], Train Loss: 0.000423
Validation Loss: 0.00040480
Epoch [76/300], Train Loss: 0.000420
Validation Loss: 0.00040683
Epoch [77/300], Train Loss: 0.000419
Validation Loss: 0.00041725
Epoch [78/300], Train Loss: 0.000420
Validation Loss: 0.00041313
Epoch [79/300], Train Loss: 0.000414
Validation Loss: 0.00040320
Epoch [80/300], Train Loss: 0.000413
Validation Loss: 0.00040424
Epoch [81/300], Train Loss: 0.000413
Validation Loss: 0.00041170
Epoch [82/300], Train Loss: 0.000415
Validation Loss: 0.00039222
Epoch [83/300], Train Loss: 0.000410
Validation Loss: 0.00040469
Epoch [84/300], Train Loss: 0.000411
Validation Loss: 0.00039266
Epoch [85/300], Train Loss: 0.000406
Validation Loss: 0.00039966
Epoch [86/300], Train Loss: 0.000405
Validation Loss: 0.00038890
Epoch [87/300], Train Loss: 0.000408
Validation Loss: 0.00039752
Epoch [88/300], Train Loss: 0.000405
Validation Loss: 0.00041206
Epoch [89/300], Train Loss: 0.000401
Validation Loss: 0.00039934
Epoch [90/300], Train Loss: 0.000401
Validation Loss: 0.00038683
Epoch [91/300], Train Loss: 0.000395
Validation Loss: 0.00038265
Epoch [92/300], Train Loss: 0.000395
Validation Loss: 0.00038328
Epoch [93/300], Train Loss: 0.000395
Validation Loss: 0.00038178
Epoch [94/300], Train Loss: 0.000392
Validation Loss: 0.00037571
Epoch [95/300], Train Loss: 0.000392
Validation Loss: 0.00037905
Epoch [96/300], Train Loss: 0.000392
Validation Loss: 0.00037732
Epoch [97/300], Train Loss: 0.000390
Validation Loss: 0.00037205
Epoch [98/300], Train Loss: 0.000389
Validation Loss: 0.00038014
Epoch [99/300], Train Loss: 0.000387
Validation Loss: 0.00037151
Epoch [100/300], Train Loss: 0.000386
Validation Loss: 0.00036645
Epoch [101/300], Train Loss: 0.000385
Validation Loss: 0.00038806
Epoch [102/300], Train Loss: 0.000385
Validation Loss: 0.00037172
Epoch [103/300], Train Loss: 0.000383
Validation Loss: 0.00036371
Epoch [104/300], Train Loss: 0.000378
Validation Loss: 0.00036251
Epoch [105/300], Train Loss: 0.000379
Validation Loss: 0.00036347
Epoch [106/300], Train Loss: 0.000376
Validation Loss: 0.00036315
Epoch [107/300], Train Loss: 0.000376
Validation Loss: 0.00035342
Epoch [108/300], Train Loss: 0.000373
Validation Loss: 0.00035664
Epoch [109/300], Train Loss: 0.000376
Validation Loss: 0.00037464
Epoch [110/300], Train Loss: 0.000370
Validation Loss: 0.00034937
Epoch [111/300], Train Loss: 0.000366
Validation Loss: 0.00038504
Epoch [112/300], Train Loss: 0.000368
Validation Loss: 0.00034899
Epoch [113/300], Train Loss: 0.000365
Validation Loss: 0.00035101
Epoch [114/300], Train Loss: 0.000364
Validation Loss: 0.00034410
Epoch [115/300], Train Loss: 0.000362
Validation Loss: 0.00034373
Epoch [116/300], Train Loss: 0.000363
Validation Loss: 0.00037711
Epoch [117/300], Train Loss: 0.000358
Validation Loss: 0.00034225
Epoch [118/300], Train Loss: 0.000356
Validation Loss: 0.00033767
Epoch [119/300], Train Loss: 0.000355
Validation Loss: 0.00034145
Epoch [120/300], Train Loss: 0.000355
Validation Loss: 0.00033855
Epoch [121/300], Train Loss: 0.000352
Validation Loss: 0.00033431
Epoch [122/300], Train Loss: 0.000351
Validation Loss: 0.00033452
Epoch [123/300], Train Loss: 0.000355
Validation Loss: 0.00033030
Epoch [124/300], Train Loss: 0.000352
Validation Loss: 0.00033736
Epoch [125/300], Train Loss: 0.000351
Validation Loss: 0.00034270
Epoch [126/300], Train Loss: 0.000347
Validation Loss: 0.00032843
Epoch [127/300], Train Loss: 0.000348
Validation Loss: 0.00034608
Epoch [128/300], Train Loss: 0.000346
Validation Loss: 0.00033345
Epoch [129/300], Train Loss: 0.000343
Validation Loss: 0.00032543
Epoch [130/300], Train Loss: 0.000342
Validation Loss: 0.00032502
Epoch [131/300], Train Loss: 0.000342
Validation Loss: 0.00033794
Epoch [132/300], Train Loss: 0.000338
Validation Loss: 0.00031944
Epoch [133/300], Train Loss: 0.000340
Validation Loss: 0.00033423
Epoch [134/300], Train Loss: 0.000340
Validation Loss: 0.00034067
Epoch [135/300], Train Loss: 0.000337
Validation Loss: 0.00031940
Epoch [136/300], Train Loss: 0.000338
Validation Loss: 0.00031747
Epoch [137/300], Train Loss: 0.000335
Validation Loss: 0.00032812
Epoch [138/300], Train Loss: 0.000335
Validation Loss: 0.00031602
Epoch [139/300], Train Loss: 0.000331
Validation Loss: 0.00031536
Epoch [140/300], Train Loss: 0.000329
Validation Loss: 0.00031607
Epoch [141/300], Train Loss: 0.000328
Validation Loss: 0.00031764
Epoch [142/300], Train Loss: 0.000330
Validation Loss: 0.00033814
Epoch [143/300], Train Loss: 0.000329
Validation Loss: 0.00031078
Epoch [144/300], Train Loss: 0.000330
Validation Loss: 0.00030821
Epoch [145/300], Train Loss: 0.000326
Validation Loss: 0.00031174
Epoch [146/300], Train Loss: 0.000326
Validation Loss: 0.00031052
Epoch [147/300], Train Loss: 0.000323
Validation Loss: 0.00030771
Epoch [148/300], Train Loss: 0.000324
Validation Loss: 0.00031367
Epoch [149/300], Train Loss: 0.000322
Validation Loss: 0.00030526
Epoch [150/300], Train Loss: 0.000326
Validation Loss: 0.00031093
Epoch [151/300], Train Loss: 0.000321
Validation Loss: 0.00030678
Epoch [152/300], Train Loss: 0.000320
Validation Loss: 0.00030610
Epoch [153/300], Train Loss: 0.000319
Validation Loss: 0.00030381
Epoch [154/300], Train Loss: 0.000318
Validation Loss: 0.00030468
Epoch [155/300], Train Loss: 0.000316
Validation Loss: 0.00031145
Epoch [156/300], Train Loss: 0.000317
Validation Loss: 0.00030703
Epoch [157/300], Train Loss: 0.000318
Validation Loss: 0.00032016
Epoch [158/300], Train Loss: 0.000315
Validation Loss: 0.00029957
Epoch [159/300], Train Loss: 0.000314
Validation Loss: 0.00029971
Epoch [160/300], Train Loss: 0.000314
Validation Loss: 0.00029694
Epoch [161/300], Train Loss: 0.000313
Validation Loss: 0.00029869
Epoch [162/300], Train Loss: 0.000311
Validation Loss: 0.00029863
Epoch [163/300], Train Loss: 0.000310
Validation Loss: 0.00029962
Epoch [164/300], Train Loss: 0.000315
Validation Loss: 0.00030433
Epoch [165/300], Train Loss: 0.000311
Validation Loss: 0.00029899
Epoch [166/300], Train Loss: 0.000309
Validation Loss: 0.00029654
Epoch [167/300], Train Loss: 0.000307
Validation Loss: 0.00029783
Epoch [168/300], Train Loss: 0.000308
Validation Loss: 0.00029212
Epoch [169/300], Train Loss: 0.000307
Validation Loss: 0.00029493
Epoch [170/300], Train Loss: 0.000307
Validation Loss: 0.00029513
Epoch [171/300], Train Loss: 0.000307
Validation Loss: 0.00029039
Epoch [172/300], Train Loss: 0.000305
Validation Loss: 0.00029246
Epoch [173/300], Train Loss: 0.000305
Validation Loss: 0.00029857
Epoch [174/300], Train Loss: 0.000307
Validation Loss: 0.00029294
Epoch [175/300], Train Loss: 0.000305
Validation Loss: 0.00028994
Epoch [176/300], Train Loss: 0.000303
Validation Loss: 0.00029781
Epoch [177/300], Train Loss: 0.000301
Validation Loss: 0.00029456
Epoch [178/300], Train Loss: 0.000305
Validation Loss: 0.00028930
Epoch [179/300], Train Loss: 0.000302
Validation Loss: 0.00029791
Epoch [180/300], Train Loss: 0.000302
Validation Loss: 0.00028719
Epoch [181/300], Train Loss: 0.000300
Validation Loss: 0.00028622
Epoch [182/300], Train Loss: 0.000299
Validation Loss: 0.00028791
Epoch [183/300], Train Loss: 0.000304
Validation Loss: 0.00028818
Epoch [184/300], Train Loss: 0.000299
Validation Loss: 0.00028850
Epoch [185/300], Train Loss: 0.000297
Validation Loss: 0.00028681
Epoch [186/300], Train Loss: 0.000297
Validation Loss: 0.00028418
Epoch [187/300], Train Loss: 0.000298
Validation Loss: 0.00029342
Epoch [188/300], Train Loss: 0.000298
Validation Loss: 0.00028434
Epoch [189/300], Train Loss: 0.000299
Validation Loss: 0.00028387
Epoch [190/300], Train Loss: 0.000297
Validation Loss: 0.00028362
Epoch [191/300], Train Loss: 0.000296
Validation Loss: 0.00028747
Epoch [192/300], Train Loss: 0.000295
Validation Loss: 0.00028473
Epoch [193/300], Train Loss: 0.000295
Validation Loss: 0.00028736
Epoch [194/300], Train Loss: 0.000294
Validation Loss: 0.00028443
Epoch [195/300], Train Loss: 0.000296
Validation Loss: 0.00028085
Epoch [196/300], Train Loss: 0.000292
Validation Loss: 0.00028119
Epoch [197/300], Train Loss: 0.000292
Validation Loss: 0.00027935
Epoch [198/300], Train Loss: 0.000291
Validation Loss: 0.00027978
Epoch [199/300], Train Loss: 0.000291
Validation Loss: 0.00028344
Epoch [200/300], Train Loss: 0.000293
Validation Loss: 0.00027706
Epoch [201/300], Train Loss: 0.000292
Validation Loss: 0.00027810
Epoch [202/300], Train Loss: 0.000292
Validation Loss: 0.00027789
Epoch [203/300], Train Loss: 0.000292
Validation Loss: 0.00028570
Epoch [204/300], Train Loss: 0.000290
Validation Loss: 0.00028761
Epoch [205/300], Train Loss: 0.000289
Validation Loss: 0.00027623
Epoch [206/300], Train Loss: 0.000289
Validation Loss: 0.00028373
Epoch [207/300], Train Loss: 0.000289
Validation Loss: 0.00027644
Epoch [208/300], Train Loss: 0.000287
Validation Loss: 0.00028073
Epoch [209/300], Train Loss: 0.000288
Validation Loss: 0.00027712
Epoch [210/300], Train Loss: 0.000287
Validation Loss: 0.00027378
Epoch [211/300], Train Loss: 0.000287
Validation Loss: 0.00027275
Epoch [212/300], Train Loss: 0.000286
Validation Loss: 0.00027501
Epoch [213/300], Train Loss: 0.000286
Validation Loss: 0.00027376
Epoch [214/300], Train Loss: 0.000288
Validation Loss: 0.00027557
Epoch [215/300], Train Loss: 0.000286
Validation Loss: 0.00027226
Epoch [216/300], Train Loss: 0.000284
Validation Loss: 0.00027407
Epoch [217/300], Train Loss: 0.000285
Validation Loss: 0.00027125
Epoch [218/300], Train Loss: 0.000284
Validation Loss: 0.00027380
Epoch [219/300], Train Loss: 0.000287
Validation Loss: 0.00027557
Epoch [220/300], Train Loss: 0.000283
Validation Loss: 0.00027585
Epoch [221/300], Train Loss: 0.000283
Validation Loss: 0.00027272
Epoch [222/300], Train Loss: 0.000281
Validation Loss: 0.00027586
Epoch [223/300], Train Loss: 0.000282
Validation Loss: 0.00027203
Epoch [224/300], Train Loss: 0.000282
Validation Loss: 0.00026892
Epoch [225/300], Train Loss: 0.000284
Validation Loss: 0.00026900
Epoch [226/300], Train Loss: 0.000281
Validation Loss: 0.00026924
Epoch [227/300], Train Loss: 0.000280
Validation Loss: 0.00029451
Epoch [228/300], Train Loss: 0.000279
Validation Loss: 0.00026872
Epoch [229/300], Train Loss: 0.000280
Validation Loss: 0.00027161
Epoch [230/300], Train Loss: 0.000279
Validation Loss: 0.00026700
Epoch [231/300], Train Loss: 0.000280
Validation Loss: 0.00026801
Epoch [232/300], Train Loss: 0.000280
Validation Loss: 0.00027319
Epoch [233/300], Train Loss: 0.000280
Validation Loss: 0.00026496
Epoch [234/300], Train Loss: 0.000278
Validation Loss: 0.00027586
Epoch [235/300], Train Loss: 0.000279
Validation Loss: 0.00026716
Epoch [236/300], Train Loss: 0.000278
Validation Loss: 0.00026856
Epoch [237/300], Train Loss: 0.000277
Validation Loss: 0.00026713
Epoch [238/300], Train Loss: 0.000276
Validation Loss: 0.00026762
Epoch [239/300], Train Loss: 0.000275
Validation Loss: 0.00026916
Epoch [240/300], Train Loss: 0.000276
Validation Loss: 0.00026574
Epoch [241/300], Train Loss: 0.000279
Validation Loss: 0.00027923
Epoch [242/300], Train Loss: 0.000276
Validation Loss: 0.00026455
Epoch [243/300], Train Loss: 0.000274
Validation Loss: 0.00026980
Epoch [244/300], Train Loss: 0.000275
Validation Loss: 0.00026540
Epoch [245/300], Train Loss: 0.000276
Validation Loss: 0.00026317
Epoch [246/300], Train Loss: 0.000277
Validation Loss: 0.00026615
Epoch [247/300], Train Loss: 0.000274
Validation Loss: 0.00026438
Epoch [248/300], Train Loss: 0.000275
Validation Loss: 0.00026576
Epoch [249/300], Train Loss: 0.000273
Validation Loss: 0.00026576
Epoch [250/300], Train Loss: 0.000273
Validation Loss: 0.00026647
Epoch [251/300], Train Loss: 0.000273
Validation Loss: 0.00026325
Epoch [252/300], Train Loss: 0.000272
Validation Loss: 0.00026029
Epoch [253/300], Train Loss: 0.000272
Validation Loss: 0.00026422
Epoch [254/300], Train Loss: 0.000272
Validation Loss: 0.00026086
Epoch [255/300], Train Loss: 0.000271
Validation Loss: 0.00026118
Epoch [256/300], Train Loss: 0.000270
Validation Loss: 0.00026005
Epoch [257/300], Train Loss: 0.000271
Validation Loss: 0.00026082
Epoch [258/300], Train Loss: 0.000272
Validation Loss: 0.00026147
Epoch [259/300], Train Loss: 0.000272
Validation Loss: 0.00026064
Epoch [260/300], Train Loss: 0.000273
Validation Loss: 0.00026086
Epoch [261/300], Train Loss: 0.000270
Validation Loss: 0.00026215
Epoch [262/300], Train Loss: 0.000269
Validation Loss: 0.00025794
Epoch [263/300], Train Loss: 0.000270
Validation Loss: 0.00025739
Epoch [264/300], Train Loss: 0.000269
Validation Loss: 0.00025814
Epoch [265/300], Train Loss: 0.000268
Validation Loss: 0.00026090
Epoch [266/300], Train Loss: 0.000269
Validation Loss: 0.00025749
Epoch [267/300], Train Loss: 0.000268
Validation Loss: 0.00025832
Epoch [268/300], Train Loss: 0.000267
Validation Loss: 0.00025654
Epoch [269/300], Train Loss: 0.000268
Validation Loss: 0.00026552
Epoch [270/300], Train Loss: 0.000270
Validation Loss: 0.00026244
Epoch [271/300], Train Loss: 0.000268
Validation Loss: 0.00025613
Epoch [272/300], Train Loss: 0.000269
Validation Loss: 0.00025615
Epoch [273/300], Train Loss: 0.000267
Validation Loss: 0.00025595
Epoch [274/300], Train Loss: 0.000268
Validation Loss: 0.00025610
Epoch [275/300], Train Loss: 0.000265
Validation Loss: 0.00026452
Epoch [276/300], Train Loss: 0.000268
Validation Loss: 0.00026058
Epoch [277/300], Train Loss: 0.000268
Validation Loss: 0.00025748
Epoch [278/300], Train Loss: 0.000265
Validation Loss: 0.00025688
Epoch [279/300], Train Loss: 0.000264
Validation Loss: 0.00025360
Epoch [280/300], Train Loss: 0.000268
Validation Loss: 0.00025978
Epoch [281/300], Train Loss: 0.000265
Validation Loss: 0.00025558
Epoch [282/300], Train Loss: 0.000264
Validation Loss: 0.00025613
Epoch [283/300], Train Loss: 0.000264
Validation Loss: 0.00025386
Epoch [284/300], Train Loss: 0.000266
Validation Loss: 0.00025514
Epoch [285/300], Train Loss: 0.000265
Validation Loss: 0.00025526
Epoch [286/300], Train Loss: 0.000263
Validation Loss: 0.00025516
Epoch [287/300], Train Loss: 0.000263
Validation Loss: 0.00025286
Epoch [288/300], Train Loss: 0.000266
Validation Loss: 0.00025689
Epoch [289/300], Train Loss: 0.000263
Validation Loss: 0.00026106
Epoch [290/300], Train Loss: 0.000264
Validation Loss: 0.00025275
Epoch [291/300], Train Loss: 0.000264
Validation Loss: 0.00025403
Epoch [292/300], Train Loss: 0.000262
Validation Loss: 0.00025773
Epoch [293/300], Train Loss: 0.000263
Validation Loss: 0.00025448
Epoch [294/300], Train Loss: 0.000263
Validation Loss: 0.00025296
Epoch [295/300], Train Loss: 0.000263
Validation Loss: 0.00025434
Epoch [296/300], Train Loss: 0.000262
Validation Loss: 0.00025272
Epoch [297/300], Train Loss: 0.000263
Validation Loss: 0.00025248
Epoch [298/300], Train Loss: 0.000262
Validation Loss: 0.00025261
Epoch [299/300], Train Loss: 0.000261
Validation Loss: 0.00025652
Epoch [300/300], Train Loss: 0.000261
Validation Loss: 0.00025255

Evaluating model for: Fridge
Run 30/72 completed in 9405.83 seconds with: {'MAE': np.float32(19.180939), 'MSE': np.float32(833.5605), 'RMSE': np.float32(28.871449), 'SAE': np.float32(0.018981181), 'NDE': np.float32(0.49667093)}

Run 31/72: hidden=256, seq_len=120, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000682
Validation Loss: 0.00068320
Epoch [2/300], Train Loss: 0.000658
Validation Loss: 0.00067537
Epoch [3/300], Train Loss: 0.000642
Validation Loss: 0.00063902
Epoch [4/300], Train Loss: 0.000620
Validation Loss: 0.00062734
Epoch [5/300], Train Loss: 0.000610
Validation Loss: 0.00061796
Epoch [6/300], Train Loss: 0.000605
Validation Loss: 0.00061251
Epoch [7/300], Train Loss: 0.000603
Validation Loss: 0.00061140
Epoch [8/300], Train Loss: 0.000603
Validation Loss: 0.00060788
Epoch [9/300], Train Loss: 0.000599
Validation Loss: 0.00060541
Epoch [10/300], Train Loss: 0.000597
Validation Loss: 0.00060295
Epoch [11/300], Train Loss: 0.000596
Validation Loss: 0.00060341
Epoch [12/300], Train Loss: 0.000594
Validation Loss: 0.00060055
Epoch [13/300], Train Loss: 0.000595
Validation Loss: 0.00060048
Epoch [14/300], Train Loss: 0.000592
Validation Loss: 0.00060422
Epoch [15/300], Train Loss: 0.000591
Validation Loss: 0.00060906
Epoch [16/300], Train Loss: 0.000589
Validation Loss: 0.00059600
Epoch [17/300], Train Loss: 0.000588
Validation Loss: 0.00059053
Epoch [18/300], Train Loss: 0.000587
Validation Loss: 0.00059322
Epoch [19/300], Train Loss: 0.000584
Validation Loss: 0.00058517
Epoch [20/300], Train Loss: 0.000582
Validation Loss: 0.00058697
Epoch [21/300], Train Loss: 0.000580
Validation Loss: 0.00058042
Epoch [22/300], Train Loss: 0.000579
Validation Loss: 0.00057363
Epoch [23/300], Train Loss: 0.000575
Validation Loss: 0.00057255
Epoch [24/300], Train Loss: 0.000570
Validation Loss: 0.00056717
Epoch [25/300], Train Loss: 0.000569
Validation Loss: 0.00057365
Epoch [26/300], Train Loss: 0.000565
Validation Loss: 0.00056782
Epoch [27/300], Train Loss: 0.000558
Validation Loss: 0.00055083
Epoch [28/300], Train Loss: 0.000554
Validation Loss: 0.00055149
Epoch [29/300], Train Loss: 0.000548
Validation Loss: 0.00054717
Epoch [30/300], Train Loss: 0.000542
Validation Loss: 0.00053103
Epoch [31/300], Train Loss: 0.000536
Validation Loss: 0.00052527
Epoch [32/300], Train Loss: 0.000534
Validation Loss: 0.00052629
Epoch [33/300], Train Loss: 0.000527
Validation Loss: 0.00052388
Epoch [34/300], Train Loss: 0.000525
Validation Loss: 0.00051609
Epoch [35/300], Train Loss: 0.000522
Validation Loss: 0.00051238
Epoch [36/300], Train Loss: 0.000520
Validation Loss: 0.00050360
Epoch [37/300], Train Loss: 0.000513
Validation Loss: 0.00049725
Epoch [38/300], Train Loss: 0.000511
Validation Loss: 0.00049427
Epoch [39/300], Train Loss: 0.000507
Validation Loss: 0.00049339
Epoch [40/300], Train Loss: 0.000499
Validation Loss: 0.00048628
Epoch [41/300], Train Loss: 0.000490
Validation Loss: 0.00048632
Epoch [42/300], Train Loss: 0.000487
Validation Loss: 0.00048831
Epoch [43/300], Train Loss: 0.000479
Validation Loss: 0.00046340
Epoch [44/300], Train Loss: 0.000473
Validation Loss: 0.00046639
Epoch [45/300], Train Loss: 0.000471
Validation Loss: 0.00046276
Epoch [46/300], Train Loss: 0.000468
Validation Loss: 0.00045696
Epoch [47/300], Train Loss: 0.000465
Validation Loss: 0.00049468
Epoch [48/300], Train Loss: 0.000463
Validation Loss: 0.00044487
Epoch [49/300], Train Loss: 0.000458
Validation Loss: 0.00044138
Epoch [50/300], Train Loss: 0.000455
Validation Loss: 0.00044339
Epoch [51/300], Train Loss: 0.000453
Validation Loss: 0.00044183
Epoch [52/300], Train Loss: 0.000447
Validation Loss: 0.00043614
Epoch [53/300], Train Loss: 0.000445
Validation Loss: 0.00043709
Epoch [54/300], Train Loss: 0.000444
Validation Loss: 0.00042498
Epoch [55/300], Train Loss: 0.000442
Validation Loss: 0.00044804
Epoch [56/300], Train Loss: 0.000442
Validation Loss: 0.00042592
Epoch [57/300], Train Loss: 0.000434
Validation Loss: 0.00041944
Epoch [58/300], Train Loss: 0.000435
Validation Loss: 0.00043207
Epoch [59/300], Train Loss: 0.000430
Validation Loss: 0.00042305
Epoch [60/300], Train Loss: 0.000430
Validation Loss: 0.00042315
Epoch [61/300], Train Loss: 0.000428
Validation Loss: 0.00042453
Epoch [62/300], Train Loss: 0.000429
Validation Loss: 0.00041516
Epoch [63/300], Train Loss: 0.000426
Validation Loss: 0.00041469
Epoch [64/300], Train Loss: 0.000428
Validation Loss: 0.00043088
Epoch [65/300], Train Loss: 0.000424
Validation Loss: 0.00041843
Epoch [66/300], Train Loss: 0.000427
Validation Loss: 0.00040985
Epoch [67/300], Train Loss: 0.000421
Validation Loss: 0.00040897
Epoch [68/300], Train Loss: 0.000421
Validation Loss: 0.00041030
Epoch [69/300], Train Loss: 0.000419
Validation Loss: 0.00041973
Epoch [70/300], Train Loss: 0.000421
Validation Loss: 0.00041832
Epoch [71/300], Train Loss: 0.000418
Validation Loss: 0.00041096
Epoch [72/300], Train Loss: 0.000418
Validation Loss: 0.00040639
Epoch [73/300], Train Loss: 0.000415
Validation Loss: 0.00041823
Epoch [74/300], Train Loss: 0.000416
Validation Loss: 0.00039953
Epoch [75/300], Train Loss: 0.000414
Validation Loss: 0.00041415
Epoch [76/300], Train Loss: 0.000412
Validation Loss: 0.00040994
Epoch [77/300], Train Loss: 0.000411
Validation Loss: 0.00040557
Epoch [78/300], Train Loss: 0.000410
Validation Loss: 0.00041019
Epoch [79/300], Train Loss: 0.000408
Validation Loss: 0.00039681
Epoch [80/300], Train Loss: 0.000409
Validation Loss: 0.00039874
Epoch [81/300], Train Loss: 0.000406
Validation Loss: 0.00041627
Epoch [82/300], Train Loss: 0.000408
Validation Loss: 0.00039354
Epoch [83/300], Train Loss: 0.000410
Validation Loss: 0.00039343
Epoch [84/300], Train Loss: 0.000405
Validation Loss: 0.00040561
Epoch [85/300], Train Loss: 0.000405
Validation Loss: 0.00039233
Epoch [86/300], Train Loss: 0.000403
Validation Loss: 0.00039428
Epoch [87/300], Train Loss: 0.000401
Validation Loss: 0.00040164
Epoch [88/300], Train Loss: 0.000404
Validation Loss: 0.00040418
Epoch [89/300], Train Loss: 0.000399
Validation Loss: 0.00039411
Epoch [90/300], Train Loss: 0.000400
Validation Loss: 0.00040468
Epoch [91/300], Train Loss: 0.000398
Validation Loss: 0.00038880
Epoch [92/300], Train Loss: 0.000398
Validation Loss: 0.00038774
Epoch [93/300], Train Loss: 0.000398
Validation Loss: 0.00038839
Epoch [94/300], Train Loss: 0.000397
Validation Loss: 0.00038410
Epoch [95/300], Train Loss: 0.000397
Validation Loss: 0.00039528
Epoch [96/300], Train Loss: 0.000396
Validation Loss: 0.00040016
Epoch [97/300], Train Loss: 0.000397
Validation Loss: 0.00039263
Epoch [98/300], Train Loss: 0.000394
Validation Loss: 0.00039181
Epoch [99/300], Train Loss: 0.000395
Validation Loss: 0.00038343
Epoch [100/300], Train Loss: 0.000391
Validation Loss: 0.00038415
Epoch [101/300], Train Loss: 0.000395
Validation Loss: 0.00039065
Epoch [102/300], Train Loss: 0.000396
Validation Loss: 0.00039074
Epoch [103/300], Train Loss: 0.000392
Validation Loss: 0.00037823
Epoch [104/300], Train Loss: 0.000390
Validation Loss: 0.00037844
Epoch [105/300], Train Loss: 0.000390
Validation Loss: 0.00038500
Epoch [106/300], Train Loss: 0.000389
Validation Loss: 0.00038825
Epoch [107/300], Train Loss: 0.000386
Validation Loss: 0.00038006
Epoch [108/300], Train Loss: 0.000386
Validation Loss: 0.00038011
Epoch [109/300], Train Loss: 0.000390
Validation Loss: 0.00037552
Epoch [110/300], Train Loss: 0.000387
Validation Loss: 0.00037368
Epoch [111/300], Train Loss: 0.000381
Validation Loss: 0.00039281
Epoch [112/300], Train Loss: 0.000385
Validation Loss: 0.00037257
Epoch [113/300], Train Loss: 0.000383
Validation Loss: 0.00037637
Epoch [114/300], Train Loss: 0.000384
Validation Loss: 0.00037190
Epoch [115/300], Train Loss: 0.000385
Validation Loss: 0.00037787
Epoch [116/300], Train Loss: 0.000383
Validation Loss: 0.00037015
Epoch [117/300], Train Loss: 0.000379
Validation Loss: 0.00038042
Epoch [118/300], Train Loss: 0.000380
Validation Loss: 0.00037775
Epoch [119/300], Train Loss: 0.000381
Validation Loss: 0.00036914
Epoch [120/300], Train Loss: 0.000377
Validation Loss: 0.00036991
Epoch [121/300], Train Loss: 0.000376
Validation Loss: 0.00036927
Epoch [122/300], Train Loss: 0.000374
Validation Loss: 0.00036560
Epoch [123/300], Train Loss: 0.000376
Validation Loss: 0.00036774
Epoch [124/300], Train Loss: 0.000376
Validation Loss: 0.00036839
Epoch [125/300], Train Loss: 0.000376
Validation Loss: 0.00037353
Epoch [126/300], Train Loss: 0.000373
Validation Loss: 0.00036357
Epoch [127/300], Train Loss: 0.000373
Validation Loss: 0.00036522
Epoch [128/300], Train Loss: 0.000374
Validation Loss: 0.00036205
Epoch [129/300], Train Loss: 0.000371
Validation Loss: 0.00036027
Epoch [130/300], Train Loss: 0.000369
Validation Loss: 0.00036238
Epoch [131/300], Train Loss: 0.000369
Validation Loss: 0.00036644
Epoch [132/300], Train Loss: 0.000368
Validation Loss: 0.00036786
Epoch [133/300], Train Loss: 0.000367
Validation Loss: 0.00036522
Epoch [134/300], Train Loss: 0.000369
Validation Loss: 0.00036411
Epoch [135/300], Train Loss: 0.000368
Validation Loss: 0.00036049
Epoch [136/300], Train Loss: 0.000365
Validation Loss: 0.00035524
Epoch [137/300], Train Loss: 0.000365
Validation Loss: 0.00035583
Epoch [138/300], Train Loss: 0.000364
Validation Loss: 0.00035490
Epoch [139/300], Train Loss: 0.000362
Validation Loss: 0.00035557
Epoch [140/300], Train Loss: 0.000362
Validation Loss: 0.00035494
Epoch [141/300], Train Loss: 0.000362
Validation Loss: 0.00035620
Epoch [142/300], Train Loss: 0.000361
Validation Loss: 0.00036728
Epoch [143/300], Train Loss: 0.000363
Validation Loss: 0.00035066
Epoch [144/300], Train Loss: 0.000359
Validation Loss: 0.00035280
Epoch [145/300], Train Loss: 0.000359
Validation Loss: 0.00036475
Epoch [146/300], Train Loss: 0.000359
Validation Loss: 0.00036238
Epoch [147/300], Train Loss: 0.000356
Validation Loss: 0.00035077
Epoch [148/300], Train Loss: 0.000356
Validation Loss: 0.00036957
Epoch [149/300], Train Loss: 0.000355
Validation Loss: 0.00034180
Epoch [150/300], Train Loss: 0.000351
Validation Loss: 0.00034577
Epoch [151/300], Train Loss: 0.000351
Validation Loss: 0.00034370
Epoch [152/300], Train Loss: 0.000348
Validation Loss: 0.00033864
Epoch [153/300], Train Loss: 0.000345
Validation Loss: 0.00033887
Epoch [154/300], Train Loss: 0.000343
Validation Loss: 0.00033133
Epoch [155/300], Train Loss: 0.000338
Validation Loss: 0.00034537
Epoch [156/300], Train Loss: 0.000338
Validation Loss: 0.00032172
Epoch [157/300], Train Loss: 0.000330
Validation Loss: 0.00032300
Epoch [158/300], Train Loss: 0.000327
Validation Loss: 0.00032122
Epoch [159/300], Train Loss: 0.000322
Validation Loss: 0.00032821
Epoch [160/300], Train Loss: 0.000319
Validation Loss: 0.00030505
Epoch [161/300], Train Loss: 0.000320
Validation Loss: 0.00030776
Epoch [162/300], Train Loss: 0.000315
Validation Loss: 0.00030598
Epoch [163/300], Train Loss: 0.000313
Validation Loss: 0.00030638
Epoch [164/300], Train Loss: 0.000314
Validation Loss: 0.00030784
Epoch [165/300], Train Loss: 0.000312
Validation Loss: 0.00030424
Epoch [166/300], Train Loss: 0.000312
Validation Loss: 0.00030126
Epoch [167/300], Train Loss: 0.000310
Validation Loss: 0.00030132
Epoch [168/300], Train Loss: 0.000307
Validation Loss: 0.00029899
Epoch [169/300], Train Loss: 0.000307
Validation Loss: 0.00030591
Epoch [170/300], Train Loss: 0.000311
Validation Loss: 0.00031068
Epoch [171/300], Train Loss: 0.000309
Validation Loss: 0.00030630
Epoch [172/300], Train Loss: 0.000306
Validation Loss: 0.00029773
Epoch [173/300], Train Loss: 0.000305
Validation Loss: 0.00030096
Epoch [174/300], Train Loss: 0.000305
Validation Loss: 0.00029922
Epoch [175/300], Train Loss: 0.000302
Validation Loss: 0.00029591
Epoch [176/300], Train Loss: 0.000301
Validation Loss: 0.00030465
Epoch [177/300], Train Loss: 0.000299
Validation Loss: 0.00029456
Epoch [178/300], Train Loss: 0.000301
Validation Loss: 0.00029663
Epoch [179/300], Train Loss: 0.000299
Validation Loss: 0.00029415
Epoch [180/300], Train Loss: 0.000298
Validation Loss: 0.00029059
Epoch [181/300], Train Loss: 0.000298
Validation Loss: 0.00029307
Epoch [182/300], Train Loss: 0.000295
Validation Loss: 0.00029446
Epoch [183/300], Train Loss: 0.000297
Validation Loss: 0.00029184
Epoch [184/300], Train Loss: 0.000300
Validation Loss: 0.00029299
Epoch [185/300], Train Loss: 0.000312
Validation Loss: 0.00029253
Epoch [186/300], Train Loss: 0.000294
Validation Loss: 0.00029091
Epoch [187/300], Train Loss: 0.000309
Validation Loss: 0.00030414
Epoch [188/300], Train Loss: 0.000297
Validation Loss: 0.00030229
Epoch [189/300], Train Loss: 0.000293
Validation Loss: 0.00028671
Epoch [190/300], Train Loss: 0.000291
Validation Loss: 0.00029014
Epoch [191/300], Train Loss: 0.000290
Validation Loss: 0.00028860
Epoch [192/300], Train Loss: 0.000291
Validation Loss: 0.00028587
Epoch [193/300], Train Loss: 0.000300
Validation Loss: 0.00029538
Epoch [194/300], Train Loss: 0.000296
Validation Loss: 0.00028754
Epoch [195/300], Train Loss: 0.000296
Validation Loss: 0.00028744
Epoch [196/300], Train Loss: 0.000295
Validation Loss: 0.00028907
Epoch [197/300], Train Loss: 0.000289
Validation Loss: 0.00029117
Epoch [198/300], Train Loss: 0.000288
Validation Loss: 0.00028354
Epoch [199/300], Train Loss: 0.000288
Validation Loss: 0.00028667
Epoch [200/300], Train Loss: 0.000288
Validation Loss: 0.00028436
Epoch [201/300], Train Loss: 0.000287
Validation Loss: 0.00028189
Epoch [202/300], Train Loss: 0.000286
Validation Loss: 0.00028252
Epoch [203/300], Train Loss: 0.000294
Validation Loss: 0.00028268
Epoch [204/300], Train Loss: 0.000286
Validation Loss: 0.00028157
Epoch [205/300], Train Loss: 0.000285
Validation Loss: 0.00028157
Epoch [206/300], Train Loss: 0.000286
Validation Loss: 0.00028256
Epoch [207/300], Train Loss: 0.000285
Validation Loss: 0.00028012
Epoch [208/300], Train Loss: 0.000284
Validation Loss: 0.00027942
Epoch [209/300], Train Loss: 0.000287
Validation Loss: 0.00028111
Epoch [210/300], Train Loss: 0.000285
Validation Loss: 0.00028190
Epoch [211/300], Train Loss: 0.000283
Validation Loss: 0.00027735
Epoch [212/300], Train Loss: 0.000281
Validation Loss: 0.00027778
Epoch [213/300], Train Loss: 0.000282
Validation Loss: 0.00028344
Epoch [214/300], Train Loss: 0.000284
Validation Loss: 0.00028179
Epoch [215/300], Train Loss: 0.000281
Validation Loss: 0.00028298
Epoch [216/300], Train Loss: 0.000279
Validation Loss: 0.00027576
Epoch [217/300], Train Loss: 0.000284
Validation Loss: 0.00027788
Epoch [218/300], Train Loss: 0.000281
Validation Loss: 0.00027383
Epoch [219/300], Train Loss: 0.000278
Validation Loss: 0.00028620
Epoch [220/300], Train Loss: 0.000279
Validation Loss: 0.00029690
Epoch [221/300], Train Loss: 0.000296
Validation Loss: 0.00029396
Epoch [222/300], Train Loss: 0.000278
Validation Loss: 0.00027587
Epoch [223/300], Train Loss: 0.000277
Validation Loss: 0.00027630
Epoch [224/300], Train Loss: 0.000276
Validation Loss: 0.00027504
Epoch [225/300], Train Loss: 0.000277
Validation Loss: 0.00027929
Epoch [226/300], Train Loss: 0.000276
Validation Loss: 0.00027399
Epoch [227/300], Train Loss: 0.000276
Validation Loss: 0.00030001
Epoch [228/300], Train Loss: 0.000277
Validation Loss: 0.00027979
Early stopping triggered

Evaluating model for: Fridge
Run 31/72 completed in 7479.48 seconds with: {'MAE': np.float32(21.07217), 'MSE': np.float32(922.58514), 'RMSE': np.float32(30.374086), 'SAE': np.float32(0.033335403), 'NDE': np.float32(0.52252066)}

Run 32/72: hidden=256, seq_len=120, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000880
Validation Loss: 0.00071073
Epoch [2/300], Train Loss: 0.000689
Validation Loss: 0.00070273
Epoch [3/300], Train Loss: 0.000680
Validation Loss: 0.00069801
Epoch [4/300], Train Loss: 0.000674
Validation Loss: 0.00069145
Epoch [5/300], Train Loss: 0.000649
Validation Loss: 0.00063987
Epoch [6/300], Train Loss: 0.000626
Validation Loss: 0.00062742
Epoch [7/300], Train Loss: 0.000619
Validation Loss: 0.00062434
Epoch [8/300], Train Loss: 0.000618
Validation Loss: 0.00062004
Epoch [9/300], Train Loss: 0.000613
Validation Loss: 0.00061654
Epoch [10/300], Train Loss: 0.000609
Validation Loss: 0.00061152
Epoch [11/300], Train Loss: 0.000606
Validation Loss: 0.00061529
Epoch [12/300], Train Loss: 0.000605
Validation Loss: 0.00060860
Epoch [13/300], Train Loss: 0.000605
Validation Loss: 0.00060676
Epoch [14/300], Train Loss: 0.000602
Validation Loss: 0.00061162
Epoch [15/300], Train Loss: 0.000602
Validation Loss: 0.00061515
Epoch [16/300], Train Loss: 0.000600
Validation Loss: 0.00060770
Epoch [17/300], Train Loss: 0.000599
Validation Loss: 0.00060122
Epoch [18/300], Train Loss: 0.000597
Validation Loss: 0.00060584
Epoch [19/300], Train Loss: 0.000596
Validation Loss: 0.00060275
Epoch [20/300], Train Loss: 0.000594
Validation Loss: 0.00060214
Epoch [21/300], Train Loss: 0.000593
Validation Loss: 0.00060068
Epoch [22/300], Train Loss: 0.000591
Validation Loss: 0.00058847
Epoch [23/300], Train Loss: 0.000588
Validation Loss: 0.00058418
Epoch [24/300], Train Loss: 0.000581
Validation Loss: 0.00057410
Epoch [25/300], Train Loss: 0.000580
Validation Loss: 0.00057787
Epoch [26/300], Train Loss: 0.000574
Validation Loss: 0.00056824
Epoch [27/300], Train Loss: 0.000569
Validation Loss: 0.00055755
Epoch [28/300], Train Loss: 0.000566
Validation Loss: 0.00056280
Epoch [29/300], Train Loss: 0.000561
Validation Loss: 0.00054940
Epoch [30/300], Train Loss: 0.000558
Validation Loss: 0.00054542
Epoch [31/300], Train Loss: 0.000550
Validation Loss: 0.00053574
Epoch [32/300], Train Loss: 0.000542
Validation Loss: 0.00053084
Epoch [33/300], Train Loss: 0.000538
Validation Loss: 0.00052843
Epoch [34/300], Train Loss: 0.000532
Validation Loss: 0.00051577
Epoch [35/300], Train Loss: 0.000528
Validation Loss: 0.00051179
Epoch [36/300], Train Loss: 0.000524
Validation Loss: 0.00051255
Epoch [37/300], Train Loss: 0.000521
Validation Loss: 0.00051694
Epoch [38/300], Train Loss: 0.000519
Validation Loss: 0.00051048
Epoch [39/300], Train Loss: 0.000517
Validation Loss: 0.00050498
Epoch [40/300], Train Loss: 0.000518
Validation Loss: 0.00050353
Epoch [41/300], Train Loss: 0.000515
Validation Loss: 0.00050186
Epoch [42/300], Train Loss: 0.000512
Validation Loss: 0.00049761
Epoch [43/300], Train Loss: 0.000509
Validation Loss: 0.00049784
Epoch [44/300], Train Loss: 0.000514
Validation Loss: 0.00050258
Epoch [45/300], Train Loss: 0.000506
Validation Loss: 0.00050032
Epoch [46/300], Train Loss: 0.000503
Validation Loss: 0.00048969
Epoch [47/300], Train Loss: 0.000501
Validation Loss: 0.00049108
Epoch [48/300], Train Loss: 0.000498
Validation Loss: 0.00048572
Epoch [49/300], Train Loss: 0.000505
Validation Loss: 0.00048055
Epoch [50/300], Train Loss: 0.000497
Validation Loss: 0.00049548
Epoch [51/300], Train Loss: 0.000491
Validation Loss: 0.00047701
Epoch [52/300], Train Loss: 0.000489
Validation Loss: 0.00047581
Epoch [53/300], Train Loss: 0.000486
Validation Loss: 0.00047411
Epoch [54/300], Train Loss: 0.000484
Validation Loss: 0.00046549
Epoch [55/300], Train Loss: 0.000484
Validation Loss: 0.00046461
Epoch [56/300], Train Loss: 0.000488
Validation Loss: 0.00047928
Epoch [57/300], Train Loss: 0.000486
Validation Loss: 0.00046323
Epoch [58/300], Train Loss: 0.000474
Validation Loss: 0.00046588
Epoch [59/300], Train Loss: 0.000471
Validation Loss: 0.00047235
Epoch [60/300], Train Loss: 0.000470
Validation Loss: 0.00045018
Epoch [61/300], Train Loss: 0.000466
Validation Loss: 0.00044799
Epoch [62/300], Train Loss: 0.000462
Validation Loss: 0.00043861
Epoch [63/300], Train Loss: 0.000460
Validation Loss: 0.00044525
Epoch [64/300], Train Loss: 0.000459
Validation Loss: 0.00044558
Epoch [65/300], Train Loss: 0.000472
Validation Loss: 0.00045252
Epoch [66/300], Train Loss: 0.000458
Validation Loss: 0.00044955
Epoch [67/300], Train Loss: 0.000453
Validation Loss: 0.00043413
Epoch [68/300], Train Loss: 0.000451
Validation Loss: 0.00044414
Epoch [69/300], Train Loss: 0.000447
Validation Loss: 0.00042916
Epoch [70/300], Train Loss: 0.000445
Validation Loss: 0.00042505
Epoch [71/300], Train Loss: 0.000441
Validation Loss: 0.00042556
Epoch [72/300], Train Loss: 0.000441
Validation Loss: 0.00042588
Epoch [73/300], Train Loss: 0.000448
Validation Loss: 0.00042123
Epoch [74/300], Train Loss: 0.000443
Validation Loss: 0.00042449
Epoch [75/300], Train Loss: 0.000435
Validation Loss: 0.00041858
Epoch [76/300], Train Loss: 0.000436
Validation Loss: 0.00041871
Epoch [77/300], Train Loss: 0.000431
Validation Loss: 0.00041283
Epoch [78/300], Train Loss: 0.000427
Validation Loss: 0.00043014
Epoch [79/300], Train Loss: 0.000430
Validation Loss: 0.00041905
Epoch [80/300], Train Loss: 0.000427
Validation Loss: 0.00041365
Epoch [81/300], Train Loss: 0.000426
Validation Loss: 0.00040854
Epoch [82/300], Train Loss: 0.000422
Validation Loss: 0.00040799
Epoch [83/300], Train Loss: 0.000420
Validation Loss: 0.00040674
Epoch [84/300], Train Loss: 0.000419
Validation Loss: 0.00040993
Epoch [85/300], Train Loss: 0.000419
Validation Loss: 0.00040554
Epoch [86/300], Train Loss: 0.000419
Validation Loss: 0.00040027
Epoch [87/300], Train Loss: 0.000417
Validation Loss: 0.00047965
Epoch [88/300], Train Loss: 0.000438
Validation Loss: 0.00040581
Epoch [89/300], Train Loss: 0.000414
Validation Loss: 0.00039575
Epoch [90/300], Train Loss: 0.000413
Validation Loss: 0.00039834
Epoch [91/300], Train Loss: 0.000413
Validation Loss: 0.00039718
Epoch [92/300], Train Loss: 0.000414
Validation Loss: 0.00043856
Epoch [93/300], Train Loss: 0.000416
Validation Loss: 0.00039213
Epoch [94/300], Train Loss: 0.000407
Validation Loss: 0.00039042
Epoch [95/300], Train Loss: 0.000405
Validation Loss: 0.00039140
Epoch [96/300], Train Loss: 0.000438
Validation Loss: 0.00041940
Epoch [97/300], Train Loss: 0.000424
Validation Loss: 0.00039522
Epoch [98/300], Train Loss: 0.000409
Validation Loss: 0.00039265
Epoch [99/300], Train Loss: 0.000406
Validation Loss: 0.00038997
Epoch [100/300], Train Loss: 0.000403
Validation Loss: 0.00038618
Epoch [101/300], Train Loss: 0.000401
Validation Loss: 0.00039531
Epoch [102/300], Train Loss: 0.000403
Validation Loss: 0.00038623
Epoch [103/300], Train Loss: 0.000400
Validation Loss: 0.00038748
Epoch [104/300], Train Loss: 0.000400
Validation Loss: 0.00038527
Epoch [105/300], Train Loss: 0.000398
Validation Loss: 0.00038345
Epoch [106/300], Train Loss: 0.000396
Validation Loss: 0.00038091
Epoch [107/300], Train Loss: 0.000393
Validation Loss: 0.00037876
Epoch [108/300], Train Loss: 0.000393
Validation Loss: 0.00037694
Epoch [109/300], Train Loss: 0.000393
Validation Loss: 0.00037670
Epoch [110/300], Train Loss: 0.000391
Validation Loss: 0.00037492
Epoch [111/300], Train Loss: 0.000390
Validation Loss: 0.00039083
Epoch [112/300], Train Loss: 0.000389
Validation Loss: 0.00037876
Epoch [113/300], Train Loss: 0.000433
Validation Loss: 0.00040479
Epoch [114/300], Train Loss: 0.000398
Validation Loss: 0.00037861
Epoch [115/300], Train Loss: 0.000389
Validation Loss: 0.00037723
Epoch [116/300], Train Loss: 0.000389
Validation Loss: 0.00037343
Epoch [117/300], Train Loss: 0.000392
Validation Loss: 0.00037410
Epoch [118/300], Train Loss: 0.000386
Validation Loss: 0.00036988
Epoch [119/300], Train Loss: 0.000439
Validation Loss: 0.00057885
Epoch [120/300], Train Loss: 0.000517
Validation Loss: 0.00046778
Epoch [121/300], Train Loss: 0.000475
Validation Loss: 0.00045171
Epoch [122/300], Train Loss: 0.000462
Validation Loss: 0.00045478
Epoch [123/300], Train Loss: 0.000454
Validation Loss: 0.00043113
Epoch [124/300], Train Loss: 0.000448
Validation Loss: 0.00042665
Epoch [125/300], Train Loss: 0.000443
Validation Loss: 0.00042670
Epoch [126/300], Train Loss: 0.000439
Validation Loss: 0.00042375
Epoch [127/300], Train Loss: 0.000434
Validation Loss: 0.00042224
Epoch [128/300], Train Loss: 0.000429
Validation Loss: 0.00041736
Early stopping triggered

Evaluating model for: Fridge
Run 32/72 completed in 4567.45 seconds with: {'MAE': np.float32(28.417664), 'MSE': np.float32(1446.1799), 'RMSE': np.float32(38.02867), 'SAE': np.float32(0.059026208), 'NDE': np.float32(0.6542012)}

Run 33/72: hidden=256, seq_len=360, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000791
Validation Loss: 0.00066455
Epoch [2/300], Train Loss: 0.000665
Validation Loss: 0.00065574
Epoch [3/300], Train Loss: 0.000654
Validation Loss: 0.00064438
Epoch [4/300], Train Loss: 0.000643
Validation Loss: 0.00063343
Epoch [5/300], Train Loss: 0.000633
Validation Loss: 0.00062926
Epoch [6/300], Train Loss: 0.000625
Validation Loss: 0.00061674
Epoch [7/300], Train Loss: 0.000616
Validation Loss: 0.00061063
Epoch [8/300], Train Loss: 0.000611
Validation Loss: 0.00061068
Epoch [9/300], Train Loss: 0.000609
Validation Loss: 0.00060714
Epoch [10/300], Train Loss: 0.000605
Validation Loss: 0.00060355
Epoch [11/300], Train Loss: 0.000604
Validation Loss: 0.00060414
Epoch [12/300], Train Loss: 0.000601
Validation Loss: 0.00059902
Epoch [13/300], Train Loss: 0.000600
Validation Loss: 0.00059763
Epoch [14/300], Train Loss: 0.000599
Validation Loss: 0.00059678
Epoch [15/300], Train Loss: 0.000598
Validation Loss: 0.00059796
Epoch [16/300], Train Loss: 0.000597
Validation Loss: 0.00059450
Epoch [17/300], Train Loss: 0.000596
Validation Loss: 0.00059367
Epoch [18/300], Train Loss: 0.000594
Validation Loss: 0.00059324
Epoch [19/300], Train Loss: 0.000594
Validation Loss: 0.00059180
Epoch [20/300], Train Loss: 0.000593
Validation Loss: 0.00059027
Epoch [21/300], Train Loss: 0.000591
Validation Loss: 0.00059020
Epoch [22/300], Train Loss: 0.000590
Validation Loss: 0.00058828
Epoch [23/300], Train Loss: 0.000588
Validation Loss: 0.00058753
Epoch [24/300], Train Loss: 0.000587
Validation Loss: 0.00058246
Epoch [25/300], Train Loss: 0.000584
Validation Loss: 0.00058341
Epoch [26/300], Train Loss: 0.000582
Validation Loss: 0.00057725
Epoch [27/300], Train Loss: 0.000579
Validation Loss: 0.00057454
Epoch [28/300], Train Loss: 0.000578
Validation Loss: 0.00058361
Epoch [29/300], Train Loss: 0.000575
Validation Loss: 0.00057061
Epoch [30/300], Train Loss: 0.000570
Validation Loss: 0.00056639
Epoch [31/300], Train Loss: 0.000567
Validation Loss: 0.00056307
Epoch [32/300], Train Loss: 0.000564
Validation Loss: 0.00055841
Epoch [33/300], Train Loss: 0.000561
Validation Loss: 0.00055811
Epoch [34/300], Train Loss: 0.000559
Validation Loss: 0.00055312
Epoch [35/300], Train Loss: 0.000555
Validation Loss: 0.00055724
Epoch [36/300], Train Loss: 0.000553
Validation Loss: 0.00054294
Epoch [37/300], Train Loss: 0.000547
Validation Loss: 0.00054415
Epoch [38/300], Train Loss: 0.000547
Validation Loss: 0.00053771
Epoch [39/300], Train Loss: 0.000539
Validation Loss: 0.00053406
Epoch [40/300], Train Loss: 0.000540
Validation Loss: 0.00053150
Epoch [41/300], Train Loss: 0.000536
Validation Loss: 0.00052360
Epoch [42/300], Train Loss: 0.000531
Validation Loss: 0.00053849
Epoch [43/300], Train Loss: 0.000528
Validation Loss: 0.00051768
Epoch [44/300], Train Loss: 0.000525
Validation Loss: 0.00051286
Epoch [45/300], Train Loss: 0.000523
Validation Loss: 0.00055953
Epoch [46/300], Train Loss: 0.000525
Validation Loss: 0.00054178
Epoch [47/300], Train Loss: 0.000526
Validation Loss: 0.00052770
Epoch [48/300], Train Loss: 0.000563
Validation Loss: 0.00054377
Epoch [49/300], Train Loss: 0.000546
Validation Loss: 0.00053659
Epoch [50/300], Train Loss: 0.000540
Validation Loss: 0.00052939
Epoch [51/300], Train Loss: 0.000524
Validation Loss: 0.00050739
Epoch [52/300], Train Loss: 0.000511
Validation Loss: 0.00051355
Epoch [53/300], Train Loss: 0.000529
Validation Loss: 0.00053421
Epoch [54/300], Train Loss: 0.000580
Validation Loss: 0.00060882
Epoch [55/300], Train Loss: 0.000612
Validation Loss: 0.00058117
Epoch [56/300], Train Loss: 0.000591
Validation Loss: 0.00057281
Epoch [57/300], Train Loss: 0.000582
Validation Loss: 0.00056655
Epoch [58/300], Train Loss: 0.000575
Validation Loss: 0.00056027
Epoch [59/300], Train Loss: 0.000569
Validation Loss: 0.00055629
Epoch [60/300], Train Loss: 0.000564
Validation Loss: 0.00055063
Epoch [61/300], Train Loss: 0.000559
Validation Loss: 0.00054796
Early stopping triggered

Evaluating model for: Fridge
Run 33/72 completed in 1464.35 seconds with: {'MAE': np.float32(34.637238), 'MSE': np.float32(1853.9298), 'RMSE': np.float32(43.057285), 'SAE': np.float32(0.08215955), 'NDE': np.float32(0.7440157)}

Run 34/72: hidden=256, seq_len=360, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000719
Validation Loss: 0.00066500
Epoch [2/300], Train Loss: 0.000669
Validation Loss: 0.00066393
Epoch [3/300], Train Loss: 0.000665
Validation Loss: 0.00066062
Epoch [4/300], Train Loss: 0.000657
Validation Loss: 0.00065068
Epoch [5/300], Train Loss: 0.000638
Validation Loss: 0.00062291
Epoch [6/300], Train Loss: 0.000619
Validation Loss: 0.00061494
Epoch [7/300], Train Loss: 0.000615
Validation Loss: 0.00061218
Epoch [8/300], Train Loss: 0.000611
Validation Loss: 0.00061044
Epoch [9/300], Train Loss: 0.000609
Validation Loss: 0.00060904
Epoch [10/300], Train Loss: 0.000608
Validation Loss: 0.00060863
Epoch [11/300], Train Loss: 0.000606
Validation Loss: 0.00060616
Epoch [12/300], Train Loss: 0.000604
Validation Loss: 0.00060432
Epoch [13/300], Train Loss: 0.000602
Validation Loss: 0.00060041
Epoch [14/300], Train Loss: 0.000601
Validation Loss: 0.00060101
Epoch [15/300], Train Loss: 0.000600
Validation Loss: 0.00060300
Epoch [16/300], Train Loss: 0.000597
Validation Loss: 0.00060048
Epoch [17/300], Train Loss: 0.000597
Validation Loss: 0.00059902
Epoch [18/300], Train Loss: 0.000596
Validation Loss: 0.00059622
Epoch [19/300], Train Loss: 0.000595
Validation Loss: 0.00059318
Epoch [20/300], Train Loss: 0.000595
Validation Loss: 0.00059582
Epoch [21/300], Train Loss: 0.000594
Validation Loss: 0.00059343
Epoch [22/300], Train Loss: 0.000593
Validation Loss: 0.00059150
Epoch [23/300], Train Loss: 0.000591
Validation Loss: 0.00059028
Epoch [24/300], Train Loss: 0.000591
Validation Loss: 0.00059046
Epoch [25/300], Train Loss: 0.000590
Validation Loss: 0.00059496
Epoch [26/300], Train Loss: 0.000589
Validation Loss: 0.00058867
Epoch [27/300], Train Loss: 0.000588
Validation Loss: 0.00058827
Epoch [28/300], Train Loss: 0.000588
Validation Loss: 0.00060233
Epoch [29/300], Train Loss: 0.000587
Validation Loss: 0.00058632
Epoch [30/300], Train Loss: 0.000585
Validation Loss: 0.00058423
Epoch [31/300], Train Loss: 0.000584
Validation Loss: 0.00058685
Epoch [32/300], Train Loss: 0.000583
Validation Loss: 0.00058379
Epoch [33/300], Train Loss: 0.000582
Validation Loss: 0.00058169
Epoch [34/300], Train Loss: 0.000581
Validation Loss: 0.00057975
Epoch [35/300], Train Loss: 0.000581
Validation Loss: 0.00058274
Epoch [36/300], Train Loss: 0.000579
Validation Loss: 0.00057774
Epoch [37/300], Train Loss: 0.000577
Validation Loss: 0.00057820
Epoch [38/300], Train Loss: 0.000575
Validation Loss: 0.00057138
Epoch [39/300], Train Loss: 0.000570
Validation Loss: 0.00056320
Epoch [40/300], Train Loss: 0.000561
Validation Loss: 0.00056648
Epoch [41/300], Train Loss: 0.000562
Validation Loss: 0.00058538
Epoch [42/300], Train Loss: 0.000554
Validation Loss: 0.00054318
Epoch [43/300], Train Loss: 0.000540
Validation Loss: 0.00053185
Epoch [44/300], Train Loss: 0.000540
Validation Loss: 0.00054250
Epoch [45/300], Train Loss: 0.000533
Validation Loss: 0.00052796
Epoch [46/300], Train Loss: 0.000525
Validation Loss: 0.00052032
Epoch [47/300], Train Loss: 0.000525
Validation Loss: 0.00053150
Epoch [48/300], Train Loss: 0.000532
Validation Loss: 0.00052425
Epoch [49/300], Train Loss: 0.000515
Validation Loss: 0.00050466
Epoch [50/300], Train Loss: 0.000515
Validation Loss: 0.00051588
Epoch [51/300], Train Loss: 0.000510
Validation Loss: 0.00051130
Epoch [52/300], Train Loss: 0.000508
Validation Loss: 0.00050732
Epoch [53/300], Train Loss: 0.000502
Validation Loss: 0.00049812
Epoch [54/300], Train Loss: 0.000512
Validation Loss: 0.00050111
Epoch [55/300], Train Loss: 0.000503
Validation Loss: 0.00050340
Epoch [56/300], Train Loss: 0.000500
Validation Loss: 0.00048675
Epoch [57/300], Train Loss: 0.000495
Validation Loss: 0.00050118
Epoch [58/300], Train Loss: 0.000493
Validation Loss: 0.00048881
Epoch [59/300], Train Loss: 0.000493
Validation Loss: 0.00048593
Epoch [60/300], Train Loss: 0.000492
Validation Loss: 0.00049638
Epoch [61/300], Train Loss: 0.000494
Validation Loss: 0.00052075
Epoch [62/300], Train Loss: 0.000488
Validation Loss: 0.00047722
Epoch [63/300], Train Loss: 0.000493
Validation Loss: 0.00048077
Epoch [64/300], Train Loss: 0.000483
Validation Loss: 0.00048824
Epoch [65/300], Train Loss: 0.000480
Validation Loss: 0.00049501
Epoch [66/300], Train Loss: 0.000477
Validation Loss: 0.00047251
Epoch [67/300], Train Loss: 0.000474
Validation Loss: 0.00047157
Epoch [68/300], Train Loss: 0.000478
Validation Loss: 0.00048222
Epoch [69/300], Train Loss: 0.000473
Validation Loss: 0.00048175
Epoch [70/300], Train Loss: 0.000467
Validation Loss: 0.00046503
Epoch [71/300], Train Loss: 0.000465
Validation Loss: 0.00048976
Epoch [72/300], Train Loss: 0.000469
Validation Loss: 0.00045846
Epoch [73/300], Train Loss: 0.000467
Validation Loss: 0.00045609
Epoch [74/300], Train Loss: 0.000458
Validation Loss: 0.00045945
Epoch [75/300], Train Loss: 0.000460
Validation Loss: 0.00045732
Epoch [76/300], Train Loss: 0.000452
Validation Loss: 0.00044942
Epoch [77/300], Train Loss: 0.000457
Validation Loss: 0.00045724
Epoch [78/300], Train Loss: 0.000451
Validation Loss: 0.00045580
Epoch [79/300], Train Loss: 0.000448
Validation Loss: 0.00044558
Epoch [80/300], Train Loss: 0.000445
Validation Loss: 0.00044199
Epoch [81/300], Train Loss: 0.000447
Validation Loss: 0.00044238
Epoch [82/300], Train Loss: 0.000443
Validation Loss: 0.00044726
Epoch [83/300], Train Loss: 0.000440
Validation Loss: 0.00044169
Epoch [84/300], Train Loss: 0.000443
Validation Loss: 0.00044278
Epoch [85/300], Train Loss: 0.000438
Validation Loss: 0.00044209
Epoch [86/300], Train Loss: 0.000438
Validation Loss: 0.00044444
Epoch [87/300], Train Loss: 0.000439
Validation Loss: 0.00043199
Epoch [88/300], Train Loss: 0.000437
Validation Loss: 0.00043077
Epoch [89/300], Train Loss: 0.000437
Validation Loss: 0.00043239
Epoch [90/300], Train Loss: 0.000434
Validation Loss: 0.00043093
Epoch [91/300], Train Loss: 0.000433
Validation Loss: 0.00042711
Epoch [92/300], Train Loss: 0.000430
Validation Loss: 0.00046440
Epoch [93/300], Train Loss: 0.000429
Validation Loss: 0.00043779
Epoch [94/300], Train Loss: 0.000429
Validation Loss: 0.00042534
Epoch [95/300], Train Loss: 0.000427
Validation Loss: 0.00042550
Epoch [96/300], Train Loss: 0.000426
Validation Loss: 0.00042574
Epoch [97/300], Train Loss: 0.000422
Validation Loss: 0.00042253
Epoch [98/300], Train Loss: 0.000421
Validation Loss: 0.00042482
Epoch [99/300], Train Loss: 0.000421
Validation Loss: 0.00042167
Epoch [100/300], Train Loss: 0.000422
Validation Loss: 0.00043268
Epoch [101/300], Train Loss: 0.000422
Validation Loss: 0.00041813
Epoch [102/300], Train Loss: 0.000419
Validation Loss: 0.00042389
Epoch [103/300], Train Loss: 0.000419
Validation Loss: 0.00042432
Epoch [104/300], Train Loss: 0.000419
Validation Loss: 0.00041816
Epoch [105/300], Train Loss: 0.000419
Validation Loss: 0.00041781
Epoch [106/300], Train Loss: 0.000417
Validation Loss: 0.00042464
Epoch [107/300], Train Loss: 0.000417
Validation Loss: 0.00041921
Epoch [108/300], Train Loss: 0.000415
Validation Loss: 0.00041296
Epoch [109/300], Train Loss: 0.000417
Validation Loss: 0.00041012
Epoch [110/300], Train Loss: 0.000413
Validation Loss: 0.00041952
Epoch [111/300], Train Loss: 0.000411
Validation Loss: 0.00041243
Epoch [112/300], Train Loss: 0.000412
Validation Loss: 0.00040826
Epoch [113/300], Train Loss: 0.000412
Validation Loss: 0.00040913
Epoch [114/300], Train Loss: 0.000411
Validation Loss: 0.00041011
Epoch [115/300], Train Loss: 0.000410
Validation Loss: 0.00040534
Epoch [116/300], Train Loss: 0.000410
Validation Loss: 0.00041121
Epoch [117/300], Train Loss: 0.000409
Validation Loss: 0.00040312
Epoch [118/300], Train Loss: 0.000404
Validation Loss: 0.00041823
Epoch [119/300], Train Loss: 0.000409
Validation Loss: 0.00040387
Epoch [120/300], Train Loss: 0.000405
Validation Loss: 0.00040719
Epoch [121/300], Train Loss: 0.000402
Validation Loss: 0.00040387
Epoch [122/300], Train Loss: 0.000403
Validation Loss: 0.00040454
Epoch [123/300], Train Loss: 0.000401
Validation Loss: 0.00040106
Epoch [124/300], Train Loss: 0.000401
Validation Loss: 0.00040798
Epoch [125/300], Train Loss: 0.000403
Validation Loss: 0.00040441
Epoch [126/300], Train Loss: 0.000400
Validation Loss: 0.00041077
Epoch [127/300], Train Loss: 0.000401
Validation Loss: 0.00039792
Epoch [128/300], Train Loss: 0.000397
Validation Loss: 0.00039565
Epoch [129/300], Train Loss: 0.000399
Validation Loss: 0.00039302
Epoch [130/300], Train Loss: 0.000396
Validation Loss: 0.00039736
Epoch [131/300], Train Loss: 0.000394
Validation Loss: 0.00040025
Epoch [132/300], Train Loss: 0.000397
Validation Loss: 0.00039414
Epoch [133/300], Train Loss: 0.000394
Validation Loss: 0.00039092
Epoch [134/300], Train Loss: 0.000393
Validation Loss: 0.00039299
Epoch [135/300], Train Loss: 0.000394
Validation Loss: 0.00039867
Epoch [136/300], Train Loss: 0.000391
Validation Loss: 0.00039490
Epoch [137/300], Train Loss: 0.000396
Validation Loss: 0.00040555
Epoch [138/300], Train Loss: 0.000391
Validation Loss: 0.00038952
Epoch [139/300], Train Loss: 0.000391
Validation Loss: 0.00040422
Epoch [140/300], Train Loss: 0.000394
Validation Loss: 0.00041326
Epoch [141/300], Train Loss: 0.000401
Validation Loss: 0.00039324
Epoch [142/300], Train Loss: 0.000392
Validation Loss: 0.00038784
Epoch [143/300], Train Loss: 0.000389
Validation Loss: 0.00040513
Epoch [144/300], Train Loss: 0.000390
Validation Loss: 0.00038813
Epoch [145/300], Train Loss: 0.000386
Validation Loss: 0.00038758
Epoch [146/300], Train Loss: 0.000387
Validation Loss: 0.00040049
Epoch [147/300], Train Loss: 0.000388
Validation Loss: 0.00038320
Epoch [148/300], Train Loss: 0.000384
Validation Loss: 0.00038284
Epoch [149/300], Train Loss: 0.000385
Validation Loss: 0.00038898
Epoch [150/300], Train Loss: 0.000386
Validation Loss: 0.00038228
Epoch [151/300], Train Loss: 0.000385
Validation Loss: 0.00038359
Epoch [152/300], Train Loss: 0.000382
Validation Loss: 0.00038851
Epoch [153/300], Train Loss: 0.000383
Validation Loss: 0.00037764
Epoch [154/300], Train Loss: 0.000383
Validation Loss: 0.00039263
Epoch [155/300], Train Loss: 0.000383
Validation Loss: 0.00038027
Epoch [156/300], Train Loss: 0.000380
Validation Loss: 0.00037959
Epoch [157/300], Train Loss: 0.000379
Validation Loss: 0.00037640
Epoch [158/300], Train Loss: 0.000379
Validation Loss: 0.00037803
Epoch [159/300], Train Loss: 0.000379
Validation Loss: 0.00037606
Epoch [160/300], Train Loss: 0.000381
Validation Loss: 0.00038988
Epoch [161/300], Train Loss: 0.000383
Validation Loss: 0.00037895
Epoch [162/300], Train Loss: 0.000379
Validation Loss: 0.00037458
Epoch [163/300], Train Loss: 0.000377
Validation Loss: 0.00038220
Epoch [164/300], Train Loss: 0.000376
Validation Loss: 0.00037233
Epoch [165/300], Train Loss: 0.000380
Validation Loss: 0.00037458
Epoch [166/300], Train Loss: 0.000377
Validation Loss: 0.00037648
Epoch [167/300], Train Loss: 0.000377
Validation Loss: 0.00037596
Epoch [168/300], Train Loss: 0.000376
Validation Loss: 0.00037426
Epoch [169/300], Train Loss: 0.000375
Validation Loss: 0.00038302
Epoch [170/300], Train Loss: 0.000383
Validation Loss: 0.00037127
Epoch [171/300], Train Loss: 0.000374
Validation Loss: 0.00037264
Epoch [172/300], Train Loss: 0.000373
Validation Loss: 0.00037204
Epoch [173/300], Train Loss: 0.000374
Validation Loss: 0.00037637
Epoch [174/300], Train Loss: 0.000373
Validation Loss: 0.00037162
Epoch [175/300], Train Loss: 0.000372
Validation Loss: 0.00036799
Epoch [176/300], Train Loss: 0.000373
Validation Loss: 0.00037191
Epoch [177/300], Train Loss: 0.000374
Validation Loss: 0.00037517
Epoch [178/300], Train Loss: 0.000372
Validation Loss: 0.00037432
Epoch [179/300], Train Loss: 0.000375
Validation Loss: 0.00036854
Epoch [180/300], Train Loss: 0.000372
Validation Loss: 0.00036846
Epoch [181/300], Train Loss: 0.000371
Validation Loss: 0.00037385
Epoch [182/300], Train Loss: 0.000371
Validation Loss: 0.00036664
Epoch [183/300], Train Loss: 0.000370
Validation Loss: 0.00037374
Epoch [184/300], Train Loss: 0.000371
Validation Loss: 0.00037486
Epoch [185/300], Train Loss: 0.000381
Validation Loss: 0.00059743
Epoch [186/300], Train Loss: 0.000418
Validation Loss: 0.00037815
Epoch [187/300], Train Loss: 0.000376
Validation Loss: 0.00037234
Epoch [188/300], Train Loss: 0.000373
Validation Loss: 0.00036851
Epoch [189/300], Train Loss: 0.000369
Validation Loss: 0.00036858
Epoch [190/300], Train Loss: 0.000370
Validation Loss: 0.00036668
Epoch [191/300], Train Loss: 0.000370
Validation Loss: 0.00036622
Epoch [192/300], Train Loss: 0.000368
Validation Loss: 0.00036554
Epoch [193/300], Train Loss: 0.000367
Validation Loss: 0.00036608
Epoch [194/300], Train Loss: 0.000369
Validation Loss: 0.00037256
Epoch [195/300], Train Loss: 0.000366
Validation Loss: 0.00036788
Epoch [196/300], Train Loss: 0.000365
Validation Loss: 0.00036204
Epoch [197/300], Train Loss: 0.000365
Validation Loss: 0.00036252
Epoch [198/300], Train Loss: 0.000368
Validation Loss: 0.00036441
Epoch [199/300], Train Loss: 0.000365
Validation Loss: 0.00036319
Epoch [200/300], Train Loss: 0.000364
Validation Loss: 0.00036224
Epoch [201/300], Train Loss: 0.000363
Validation Loss: 0.00036534
Epoch [202/300], Train Loss: 0.000365
Validation Loss: 0.00036257
Epoch [203/300], Train Loss: 0.000364
Validation Loss: 0.00036095
Epoch [204/300], Train Loss: 0.000363
Validation Loss: 0.00036330
Epoch [205/300], Train Loss: 0.000381
Validation Loss: 0.00037526
Epoch [206/300], Train Loss: 0.000377
Validation Loss: 0.00038727
Epoch [207/300], Train Loss: 0.000375
Validation Loss: 0.00036895
Epoch [208/300], Train Loss: 0.000373
Validation Loss: 0.00036770
Epoch [209/300], Train Loss: 0.000371
Validation Loss: 0.00036823
Epoch [210/300], Train Loss: 0.000370
Validation Loss: 0.00036704
Epoch [211/300], Train Loss: 0.000372
Validation Loss: 0.00037384
Epoch [212/300], Train Loss: 0.000370
Validation Loss: 0.00036646
Epoch [213/300], Train Loss: 0.000370
Validation Loss: 0.00036691
Early stopping triggered

Evaluating model for: Fridge
Run 34/72 completed in 5700.68 seconds with: {'MAE': np.float32(24.642113), 'MSE': np.float32(1236.9036), 'RMSE': np.float32(35.16964), 'SAE': np.float32(0.05133427), 'NDE': np.float32(0.60771966)}

Run 35/72: hidden=256, seq_len=360, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000770
Validation Loss: 0.00066966
Epoch [2/300], Train Loss: 0.000674
Validation Loss: 0.00066882
Epoch [3/300], Train Loss: 0.000671
Validation Loss: 0.00066474
Epoch [4/300], Train Loss: 0.000660
Validation Loss: 0.00065091
Epoch [5/300], Train Loss: 0.000637
Validation Loss: 0.00062254
Epoch [6/300], Train Loss: 0.000622
Validation Loss: 0.00061534
Epoch [7/300], Train Loss: 0.000617
Validation Loss: 0.00060977
Epoch [8/300], Train Loss: 0.000612
Validation Loss: 0.00060857
Epoch [9/300], Train Loss: 0.000611
Validation Loss: 0.00060837
Epoch [10/300], Train Loss: 0.000610
Validation Loss: 0.00060463
Epoch [11/300], Train Loss: 0.000607
Validation Loss: 0.00060265
Epoch [12/300], Train Loss: 0.000605
Validation Loss: 0.00060188
Epoch [13/300], Train Loss: 0.000603
Validation Loss: 0.00059938
Epoch [14/300], Train Loss: 0.000603
Validation Loss: 0.00059925
Epoch [15/300], Train Loss: 0.000601
Validation Loss: 0.00060012
Epoch [16/300], Train Loss: 0.000598
Validation Loss: 0.00059693
Epoch [17/300], Train Loss: 0.000597
Validation Loss: 0.00059676
Epoch [18/300], Train Loss: 0.000596
Validation Loss: 0.00059399
Epoch [19/300], Train Loss: 0.000595
Validation Loss: 0.00059090
Epoch [20/300], Train Loss: 0.000595
Validation Loss: 0.00059267
Epoch [21/300], Train Loss: 0.000594
Validation Loss: 0.00059149
Epoch [22/300], Train Loss: 0.000592
Validation Loss: 0.00058748
Epoch [23/300], Train Loss: 0.000590
Validation Loss: 0.00058714
Epoch [24/300], Train Loss: 0.000589
Validation Loss: 0.00058581
Epoch [25/300], Train Loss: 0.000588
Validation Loss: 0.00059419
Epoch [26/300], Train Loss: 0.000586
Validation Loss: 0.00058096
Epoch [27/300], Train Loss: 0.000584
Validation Loss: 0.00057821
Epoch [28/300], Train Loss: 0.000583
Validation Loss: 0.00058965
Epoch [29/300], Train Loss: 0.000581
Validation Loss: 0.00057642
Epoch [30/300], Train Loss: 0.000577
Validation Loss: 0.00057007
Epoch [31/300], Train Loss: 0.000574
Validation Loss: 0.00057040
Epoch [32/300], Train Loss: 0.000572
Validation Loss: 0.00056814
Epoch [33/300], Train Loss: 0.000569
Validation Loss: 0.00056619
Epoch [34/300], Train Loss: 0.000567
Validation Loss: 0.00056474
Epoch [35/300], Train Loss: 0.000567
Validation Loss: 0.00056221
Epoch [36/300], Train Loss: 0.000563
Validation Loss: 0.00055680
Epoch [37/300], Train Loss: 0.000561
Validation Loss: 0.00055648
Epoch [38/300], Train Loss: 0.000558
Validation Loss: 0.00055071
Epoch [39/300], Train Loss: 0.000553
Validation Loss: 0.00054399
Epoch [40/300], Train Loss: 0.000548
Validation Loss: 0.00053984
Epoch [41/300], Train Loss: 0.000545
Validation Loss: 0.00053416
Epoch [42/300], Train Loss: 0.000542
Validation Loss: 0.00053726
Epoch [43/300], Train Loss: 0.000538
Validation Loss: 0.00052814
Epoch [44/300], Train Loss: 0.000533
Validation Loss: 0.00052364
Epoch [45/300], Train Loss: 0.000530
Validation Loss: 0.00052451
Epoch [46/300], Train Loss: 0.000526
Validation Loss: 0.00053165
Epoch [47/300], Train Loss: 0.000521
Validation Loss: 0.00052701
Epoch [48/300], Train Loss: 0.000517
Validation Loss: 0.00050629
Epoch [49/300], Train Loss: 0.000512
Validation Loss: 0.00050697
Epoch [50/300], Train Loss: 0.000510
Validation Loss: 0.00051887
Epoch [51/300], Train Loss: 0.000508
Validation Loss: 0.00050151
Epoch [52/300], Train Loss: 0.000509
Validation Loss: 0.00049861
Epoch [53/300], Train Loss: 0.000502
Validation Loss: 0.00050614
Epoch [54/300], Train Loss: 0.000515
Validation Loss: 0.00051319
Epoch [55/300], Train Loss: 0.000503
Validation Loss: 0.00049696
Epoch [56/300], Train Loss: 0.000498
Validation Loss: 0.00049348
Epoch [57/300], Train Loss: 0.000509
Validation Loss: 0.00049107
Epoch [58/300], Train Loss: 0.000496
Validation Loss: 0.00049365
Epoch [59/300], Train Loss: 0.000494
Validation Loss: 0.00048848
Epoch [60/300], Train Loss: 0.000490
Validation Loss: 0.00048797
Epoch [61/300], Train Loss: 0.000498
Validation Loss: 0.00049150
Epoch [62/300], Train Loss: 0.000492
Validation Loss: 0.00048541
Epoch [63/300], Train Loss: 0.000490
Validation Loss: 0.00048827
Epoch [64/300], Train Loss: 0.000488
Validation Loss: 0.00048216
Epoch [65/300], Train Loss: 0.000490
Validation Loss: 0.00048184
Epoch [66/300], Train Loss: 0.000485
Validation Loss: 0.00048978
Epoch [67/300], Train Loss: 0.000486
Validation Loss: 0.00048370
Epoch [68/300], Train Loss: 0.000486
Validation Loss: 0.00047894
Epoch [69/300], Train Loss: 0.000481
Validation Loss: 0.00048223
Epoch [70/300], Train Loss: 0.000485
Validation Loss: 0.00047768
Epoch [71/300], Train Loss: 0.000479
Validation Loss: 0.00047419
Epoch [72/300], Train Loss: 0.000480
Validation Loss: 0.00047947
Epoch [73/300], Train Loss: 0.000482
Validation Loss: 0.00047283
Epoch [74/300], Train Loss: 0.000477
Validation Loss: 0.00047509
Epoch [75/300], Train Loss: 0.000495
Validation Loss: 0.00050849
Epoch [76/300], Train Loss: 0.000490
Validation Loss: 0.00047824
Epoch [77/300], Train Loss: 0.000475
Validation Loss: 0.00052877
Epoch [78/300], Train Loss: 0.000554
Validation Loss: 0.00052011
Epoch [79/300], Train Loss: 0.000519
Validation Loss: 0.00050478
Epoch [80/300], Train Loss: 0.000510
Validation Loss: 0.00050002
Epoch [81/300], Train Loss: 0.000501
Validation Loss: 0.00049269
Epoch [82/300], Train Loss: 0.000494
Validation Loss: 0.00048738
Epoch [83/300], Train Loss: 0.000489
Validation Loss: 0.00048964
Early stopping triggered

Evaluating model for: Fridge
Run 35/72 completed in 2445.73 seconds with: {'MAE': np.float32(31.931322), 'MSE': np.float32(1647.2163), 'RMSE': np.float32(40.58591), 'SAE': np.float32(0.028849363), 'NDE': np.float32(0.70131123)}

Run 36/72: hidden=256, seq_len=360, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000692
Validation Loss: 0.00066319
Epoch [2/300], Train Loss: 0.000664
Validation Loss: 0.00066308
Epoch [3/300], Train Loss: 0.000662
Validation Loss: 0.00066077
Epoch [4/300], Train Loss: 0.000643
Validation Loss: 0.00063678
Epoch [5/300], Train Loss: 0.000620
Validation Loss: 0.00061218
Epoch [6/300], Train Loss: 0.000608
Validation Loss: 0.00060601
Epoch [7/300], Train Loss: 0.000604
Validation Loss: 0.00060117
Epoch [8/300], Train Loss: 0.000601
Validation Loss: 0.00060138
Epoch [9/300], Train Loss: 0.000599
Validation Loss: 0.00059699
Epoch [10/300], Train Loss: 0.000596
Validation Loss: 0.00059228
Epoch [11/300], Train Loss: 0.000593
Validation Loss: 0.00059043
Epoch [12/300], Train Loss: 0.000591
Validation Loss: 0.00058690
Epoch [13/300], Train Loss: 0.000589
Validation Loss: 0.00058710
Epoch [14/300], Train Loss: 0.000589
Validation Loss: 0.00058788
Epoch [15/300], Train Loss: 0.000587
Validation Loss: 0.00058846
Epoch [16/300], Train Loss: 0.000584
Validation Loss: 0.00058465
Epoch [17/300], Train Loss: 0.000583
Validation Loss: 0.00058189
Epoch [18/300], Train Loss: 0.000582
Validation Loss: 0.00057921
Epoch [19/300], Train Loss: 0.000581
Validation Loss: 0.00057737
Epoch [20/300], Train Loss: 0.000579
Validation Loss: 0.00057954
Epoch [21/300], Train Loss: 0.000578
Validation Loss: 0.00057726
Epoch [22/300], Train Loss: 0.000576
Validation Loss: 0.00057348
Epoch [23/300], Train Loss: 0.000573
Validation Loss: 0.00057316
Epoch [24/300], Train Loss: 0.000572
Validation Loss: 0.00056807
Epoch [25/300], Train Loss: 0.000569
Validation Loss: 0.00056486
Epoch [26/300], Train Loss: 0.000562
Validation Loss: 0.00055823
Epoch [27/300], Train Loss: 0.000556
Validation Loss: 0.00055539
Epoch [28/300], Train Loss: 0.000551
Validation Loss: 0.00056000
Epoch [29/300], Train Loss: 0.000542
Validation Loss: 0.00053625
Epoch [30/300], Train Loss: 0.000538
Validation Loss: 0.00053662
Epoch [31/300], Train Loss: 0.000535
Validation Loss: 0.00053195
Epoch [32/300], Train Loss: 0.000528
Validation Loss: 0.00051838
Epoch [33/300], Train Loss: 0.000587
Validation Loss: 0.00057201
Epoch [34/300], Train Loss: 0.000568
Validation Loss: 0.00055801
Epoch [35/300], Train Loss: 0.000558
Validation Loss: 0.00054954
Epoch [36/300], Train Loss: 0.000554
Validation Loss: 0.00054424
Epoch [37/300], Train Loss: 0.000546
Validation Loss: 0.00053866
Epoch [38/300], Train Loss: 0.000536
Validation Loss: 0.00053000
Epoch [39/300], Train Loss: 0.000531
Validation Loss: 0.00052158
Epoch [40/300], Train Loss: 0.000528
Validation Loss: 0.00051962
Epoch [41/300], Train Loss: 0.000527
Validation Loss: 0.00052548
Epoch [42/300], Train Loss: 0.000526
Validation Loss: 0.00051164
Epoch [43/300], Train Loss: 0.000515
Validation Loss: 0.00050931
Epoch [44/300], Train Loss: 0.000517
Validation Loss: 0.00051693
Epoch [45/300], Train Loss: 0.000511
Validation Loss: 0.00050637
Epoch [46/300], Train Loss: 0.000520
Validation Loss: 0.00051579
Epoch [47/300], Train Loss: 0.000516
Validation Loss: 0.00050623
Epoch [48/300], Train Loss: 0.000508
Validation Loss: 0.00049678
Epoch [49/300], Train Loss: 0.000510
Validation Loss: 0.00052893
Epoch [50/300], Train Loss: 0.000506
Validation Loss: 0.00049439
Epoch [51/300], Train Loss: 0.000495
Validation Loss: 0.00049046
Epoch [52/300], Train Loss: 0.000499
Validation Loss: 0.00049418
Epoch [53/300], Train Loss: 0.000493
Validation Loss: 0.00048951
Epoch [54/300], Train Loss: 0.000516
Validation Loss: 0.00049538
Epoch [55/300], Train Loss: 0.000516
Validation Loss: 0.00059531
Epoch [56/300], Train Loss: 0.000551
Validation Loss: 0.00052966
Epoch [57/300], Train Loss: 0.000525
Validation Loss: 0.00051347
Epoch [58/300], Train Loss: 0.000519
Validation Loss: 0.00050636
Epoch [59/300], Train Loss: 0.000532
Validation Loss: 0.00058468
Epoch [60/300], Train Loss: 0.000557
Validation Loss: 0.00054712
Epoch [61/300], Train Loss: 0.000534
Validation Loss: 0.00052523
Epoch [62/300], Train Loss: 0.000520
Validation Loss: 0.00057357
Epoch [63/300], Train Loss: 0.000552
Validation Loss: 0.00054672
Early stopping triggered

Evaluating model for: Fridge
Run 36/72 completed in 2288.79 seconds with: {'MAE': np.float32(34.152225), 'MSE': np.float32(1817.4209), 'RMSE': np.float32(42.631218), 'SAE': np.float32(0.0014093696), 'NDE': np.float32(0.73665357)}

Run 37/72: hidden=256, seq_len=360, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000765
Validation Loss: 0.00066103
Epoch [2/300], Train Loss: 0.000666
Validation Loss: 0.00065805
Epoch [3/300], Train Loss: 0.000663
Validation Loss: 0.00065380
Epoch [4/300], Train Loss: 0.000659
Validation Loss: 0.00064913
Epoch [5/300], Train Loss: 0.000654
Validation Loss: 0.00064115
Epoch [6/300], Train Loss: 0.000648
Validation Loss: 0.00063304
Epoch [7/300], Train Loss: 0.000640
Validation Loss: 0.00062454
Epoch [8/300], Train Loss: 0.000629
Validation Loss: 0.00061396
Epoch [9/300], Train Loss: 0.000619
Validation Loss: 0.00060438
Epoch [10/300], Train Loss: 0.000612
Validation Loss: 0.00060642
Epoch [11/300], Train Loss: 0.000611
Validation Loss: 0.00059890
Epoch [12/300], Train Loss: 0.000609
Validation Loss: 0.00059791
Epoch [13/300], Train Loss: 0.000607
Validation Loss: 0.00059635
Epoch [14/300], Train Loss: 0.000606
Validation Loss: 0.00059750
Epoch [15/300], Train Loss: 0.000604
Validation Loss: 0.00059285
Epoch [16/300], Train Loss: 0.000603
Validation Loss: 0.00059197
Epoch [17/300], Train Loss: 0.000602
Validation Loss: 0.00059318
Epoch [18/300], Train Loss: 0.000600
Validation Loss: 0.00059073
Epoch [19/300], Train Loss: 0.000600
Validation Loss: 0.00059207
Epoch [20/300], Train Loss: 0.000599
Validation Loss: 0.00059123
Epoch [21/300], Train Loss: 0.000598
Validation Loss: 0.00058855
Epoch [22/300], Train Loss: 0.000598
Validation Loss: 0.00059241
Epoch [23/300], Train Loss: 0.000595
Validation Loss: 0.00058771
Epoch [24/300], Train Loss: 0.000594
Validation Loss: 0.00058617
Epoch [25/300], Train Loss: 0.000594
Validation Loss: 0.00058858
Epoch [26/300], Train Loss: 0.000596
Validation Loss: 0.00058572
Epoch [27/300], Train Loss: 0.000593
Validation Loss: 0.00058782
Epoch [28/300], Train Loss: 0.000593
Validation Loss: 0.00058258
Epoch [29/300], Train Loss: 0.000591
Validation Loss: 0.00058199
Epoch [30/300], Train Loss: 0.000589
Validation Loss: 0.00058056
Epoch [31/300], Train Loss: 0.000588
Validation Loss: 0.00058143
Epoch [32/300], Train Loss: 0.000586
Validation Loss: 0.00057836
Epoch [33/300], Train Loss: 0.000587
Validation Loss: 0.00057603
Epoch [34/300], Train Loss: 0.000584
Validation Loss: 0.00057625
Epoch [35/300], Train Loss: 0.000585
Validation Loss: 0.00057622
Epoch [36/300], Train Loss: 0.000583
Validation Loss: 0.00057383
Epoch [37/300], Train Loss: 0.000581
Validation Loss: 0.00057432
Epoch [38/300], Train Loss: 0.000581
Validation Loss: 0.00057087
Epoch [39/300], Train Loss: 0.000579
Validation Loss: 0.00056782
Epoch [40/300], Train Loss: 0.000579
Validation Loss: 0.00057609
Epoch [41/300], Train Loss: 0.000579
Validation Loss: 0.00057097
Epoch [42/300], Train Loss: 0.000576
Validation Loss: 0.00056680
Epoch [43/300], Train Loss: 0.000574
Validation Loss: 0.00056186
Epoch [44/300], Train Loss: 0.000576
Validation Loss: 0.00056724
Epoch [45/300], Train Loss: 0.000572
Validation Loss: 0.00055958
Epoch [46/300], Train Loss: 0.000570
Validation Loss: 0.00055821
Epoch [47/300], Train Loss: 0.000569
Validation Loss: 0.00055682
Epoch [48/300], Train Loss: 0.000568
Validation Loss: 0.00055888
Epoch [49/300], Train Loss: 0.000567
Validation Loss: 0.00055534
Epoch [50/300], Train Loss: 0.000565
Validation Loss: 0.00055509
Epoch [51/300], Train Loss: 0.000564
Validation Loss: 0.00055684
Epoch [52/300], Train Loss: 0.000562
Validation Loss: 0.00055120
Epoch [53/300], Train Loss: 0.000561
Validation Loss: 0.00055068
Epoch [54/300], Train Loss: 0.000560
Validation Loss: 0.00054794
Epoch [55/300], Train Loss: 0.000562
Validation Loss: 0.00055179
Epoch [56/300], Train Loss: 0.000559
Validation Loss: 0.00054503
Epoch [57/300], Train Loss: 0.000559
Validation Loss: 0.00054686
Epoch [58/300], Train Loss: 0.000557
Validation Loss: 0.00054573
Epoch [59/300], Train Loss: 0.000556
Validation Loss: 0.00054441
Epoch [60/300], Train Loss: 0.000554
Validation Loss: 0.00054331
Epoch [61/300], Train Loss: 0.000555
Validation Loss: 0.00054012
Epoch [62/300], Train Loss: 0.000551
Validation Loss: 0.00053973
Epoch [63/300], Train Loss: 0.000550
Validation Loss: 0.00053890
Epoch [64/300], Train Loss: 0.000551
Validation Loss: 0.00053725
Epoch [65/300], Train Loss: 0.000548
Validation Loss: 0.00053923
Epoch [66/300], Train Loss: 0.000550
Validation Loss: 0.00053707
Epoch [67/300], Train Loss: 0.000546
Validation Loss: 0.00053509
Epoch [68/300], Train Loss: 0.000544
Validation Loss: 0.00053701
Epoch [69/300], Train Loss: 0.000546
Validation Loss: 0.00053645
Epoch [70/300], Train Loss: 0.000542
Validation Loss: 0.00053149
Epoch [71/300], Train Loss: 0.000541
Validation Loss: 0.00053818
Epoch [72/300], Train Loss: 0.000542
Validation Loss: 0.00052897
Epoch [73/300], Train Loss: 0.000539
Validation Loss: 0.00052580
Epoch [74/300], Train Loss: 0.000538
Validation Loss: 0.00052662
Epoch [75/300], Train Loss: 0.000539
Validation Loss: 0.00052630
Epoch [76/300], Train Loss: 0.000540
Validation Loss: 0.00052681
Epoch [77/300], Train Loss: 0.000533
Validation Loss: 0.00052543
Epoch [78/300], Train Loss: 0.000532
Validation Loss: 0.00052708
Epoch [79/300], Train Loss: 0.000532
Validation Loss: 0.00052139
Epoch [80/300], Train Loss: 0.000536
Validation Loss: 0.00053724
Epoch [81/300], Train Loss: 0.000530
Validation Loss: 0.00052357
Epoch [82/300], Train Loss: 0.000526
Validation Loss: 0.00051318
Epoch [83/300], Train Loss: 0.000525
Validation Loss: 0.00050973
Epoch [84/300], Train Loss: 0.000525
Validation Loss: 0.00054832
Epoch [85/300], Train Loss: 0.000525
Validation Loss: 0.00051260
Epoch [86/300], Train Loss: 0.000518
Validation Loss: 0.00050554
Epoch [87/300], Train Loss: 0.000516
Validation Loss: 0.00050143
Epoch [88/300], Train Loss: 0.000518
Validation Loss: 0.00050507
Epoch [89/300], Train Loss: 0.000513
Validation Loss: 0.00050151
Epoch [90/300], Train Loss: 0.000512
Validation Loss: 0.00049647
Epoch [91/300], Train Loss: 0.000511
Validation Loss: 0.00049507
Epoch [92/300], Train Loss: 0.000510
Validation Loss: 0.00050418
Epoch [93/300], Train Loss: 0.000510
Validation Loss: 0.00049658
Epoch [94/300], Train Loss: 0.000509
Validation Loss: 0.00050196
Epoch [95/300], Train Loss: 0.000508
Validation Loss: 0.00049362
Epoch [96/300], Train Loss: 0.000505
Validation Loss: 0.00048883
Epoch [97/300], Train Loss: 0.000506
Validation Loss: 0.00049511
Epoch [98/300], Train Loss: 0.000507
Validation Loss: 0.00049465
Epoch [99/300], Train Loss: 0.000503
Validation Loss: 0.00048889
Epoch [100/300], Train Loss: 0.000500
Validation Loss: 0.00048447
Epoch [101/300], Train Loss: 0.000500
Validation Loss: 0.00048968
Epoch [102/300], Train Loss: 0.000506
Validation Loss: 0.00048539
Epoch [103/300], Train Loss: 0.000497
Validation Loss: 0.00048838
Epoch [104/300], Train Loss: 0.000496
Validation Loss: 0.00048454
Epoch [105/300], Train Loss: 0.000496
Validation Loss: 0.00048496
Epoch [106/300], Train Loss: 0.000496
Validation Loss: 0.00048329
Epoch [107/300], Train Loss: 0.000496
Validation Loss: 0.00048055
Epoch [108/300], Train Loss: 0.000493
Validation Loss: 0.00048017
Epoch [109/300], Train Loss: 0.000494
Validation Loss: 0.00048703
Epoch [110/300], Train Loss: 0.000493
Validation Loss: 0.00047686
Epoch [111/300], Train Loss: 0.000493
Validation Loss: 0.00048107
Epoch [112/300], Train Loss: 0.000492
Validation Loss: 0.00048311
Epoch [113/300], Train Loss: 0.000490
Validation Loss: 0.00048241
Epoch [114/300], Train Loss: 0.000488
Validation Loss: 0.00047815
Epoch [115/300], Train Loss: 0.000488
Validation Loss: 0.00047609
Epoch [116/300], Train Loss: 0.000487
Validation Loss: 0.00047485
Epoch [117/300], Train Loss: 0.000486
Validation Loss: 0.00048200
Epoch [118/300], Train Loss: 0.000488
Validation Loss: 0.00047048
Epoch [119/300], Train Loss: 0.000489
Validation Loss: 0.00047789
Epoch [120/300], Train Loss: 0.000486
Validation Loss: 0.00046970
Epoch [121/300], Train Loss: 0.000485
Validation Loss: 0.00047099
Epoch [122/300], Train Loss: 0.000482
Validation Loss: 0.00046764
Epoch [123/300], Train Loss: 0.000483
Validation Loss: 0.00046994
Epoch [124/300], Train Loss: 0.000482
Validation Loss: 0.00046766
Epoch [125/300], Train Loss: 0.000480
Validation Loss: 0.00047100
Epoch [126/300], Train Loss: 0.000481
Validation Loss: 0.00047476
Epoch [127/300], Train Loss: 0.000482
Validation Loss: 0.00046540
Epoch [128/300], Train Loss: 0.000481
Validation Loss: 0.00046213
Epoch [129/300], Train Loss: 0.000477
Validation Loss: 0.00046357
Epoch [130/300], Train Loss: 0.000476
Validation Loss: 0.00046799
Epoch [131/300], Train Loss: 0.000477
Validation Loss: 0.00046158
Epoch [132/300], Train Loss: 0.000475
Validation Loss: 0.00047159
Epoch [133/300], Train Loss: 0.000477
Validation Loss: 0.00046052
Epoch [134/300], Train Loss: 0.000475
Validation Loss: 0.00046171
Epoch [135/300], Train Loss: 0.000472
Validation Loss: 0.00045777
Epoch [136/300], Train Loss: 0.000475
Validation Loss: 0.00046772
Epoch [137/300], Train Loss: 0.000471
Validation Loss: 0.00045561
Epoch [138/300], Train Loss: 0.000471
Validation Loss: 0.00045542
Epoch [139/300], Train Loss: 0.000470
Validation Loss: 0.00045907
Epoch [140/300], Train Loss: 0.000468
Validation Loss: 0.00045874
Epoch [141/300], Train Loss: 0.000473
Validation Loss: 0.00045721
Epoch [142/300], Train Loss: 0.000471
Validation Loss: 0.00045960
Epoch [143/300], Train Loss: 0.000467
Validation Loss: 0.00045479
Epoch [144/300], Train Loss: 0.000469
Validation Loss: 0.00045972
Epoch [145/300], Train Loss: 0.000468
Validation Loss: 0.00045216
Epoch [146/300], Train Loss: 0.000464
Validation Loss: 0.00045970
Epoch [147/300], Train Loss: 0.000467
Validation Loss: 0.00045576
Epoch [148/300], Train Loss: 0.000463
Validation Loss: 0.00044949
Epoch [149/300], Train Loss: 0.000466
Validation Loss: 0.00045502
Epoch [150/300], Train Loss: 0.000463
Validation Loss: 0.00046108
Epoch [151/300], Train Loss: 0.000463
Validation Loss: 0.00044883
Epoch [152/300], Train Loss: 0.000463
Validation Loss: 0.00044732
Epoch [153/300], Train Loss: 0.000458
Validation Loss: 0.00044909
Epoch [154/300], Train Loss: 0.000459
Validation Loss: 0.00044490
Epoch [155/300], Train Loss: 0.000461
Validation Loss: 0.00044521
Epoch [156/300], Train Loss: 0.000459
Validation Loss: 0.00045209
Epoch [157/300], Train Loss: 0.000460
Validation Loss: 0.00044541
Epoch [158/300], Train Loss: 0.000456
Validation Loss: 0.00044337
Epoch [159/300], Train Loss: 0.000458
Validation Loss: 0.00044165
Epoch [160/300], Train Loss: 0.000457
Validation Loss: 0.00044006
Epoch [161/300], Train Loss: 0.000456
Validation Loss: 0.00043840
Epoch [162/300], Train Loss: 0.000454
Validation Loss: 0.00044034
Epoch [163/300], Train Loss: 0.000453
Validation Loss: 0.00044197
Epoch [164/300], Train Loss: 0.000452
Validation Loss: 0.00043772
Epoch [165/300], Train Loss: 0.000451
Validation Loss: 0.00043699
Epoch [166/300], Train Loss: 0.000453
Validation Loss: 0.00043729
Epoch [167/300], Train Loss: 0.000450
Validation Loss: 0.00043932
Epoch [168/300], Train Loss: 0.000450
Validation Loss: 0.00043387
Epoch [169/300], Train Loss: 0.000447
Validation Loss: 0.00043219
Epoch [170/300], Train Loss: 0.000453
Validation Loss: 0.00044467
Epoch [171/300], Train Loss: 0.000449
Validation Loss: 0.00043101
Epoch [172/300], Train Loss: 0.000448
Validation Loss: 0.00043180
Epoch [173/300], Train Loss: 0.000448
Validation Loss: 0.00043929
Epoch [174/300], Train Loss: 0.000447
Validation Loss: 0.00042826
Epoch [175/300], Train Loss: 0.000447
Validation Loss: 0.00043123
Epoch [176/300], Train Loss: 0.000445
Validation Loss: 0.00042842
Epoch [177/300], Train Loss: 0.000443
Validation Loss: 0.00042837
Epoch [178/300], Train Loss: 0.000441
Validation Loss: 0.00043136
Epoch [179/300], Train Loss: 0.000443
Validation Loss: 0.00042832
Epoch [180/300], Train Loss: 0.000441
Validation Loss: 0.00043268
Epoch [181/300], Train Loss: 0.000444
Validation Loss: 0.00042381
Epoch [182/300], Train Loss: 0.000440
Validation Loss: 0.00043393
Epoch [183/300], Train Loss: 0.000439
Validation Loss: 0.00042641
Epoch [184/300], Train Loss: 0.000443
Validation Loss: 0.00042324
Epoch [185/300], Train Loss: 0.000438
Validation Loss: 0.00042322
Epoch [186/300], Train Loss: 0.000438
Validation Loss: 0.00041914
Epoch [187/300], Train Loss: 0.000436
Validation Loss: 0.00042574
Epoch [188/300], Train Loss: 0.000437
Validation Loss: 0.00041779
Epoch [189/300], Train Loss: 0.000435
Validation Loss: 0.00042051
Epoch [190/300], Train Loss: 0.000434
Validation Loss: 0.00041911
Epoch [191/300], Train Loss: 0.000434
Validation Loss: 0.00041818
Epoch [192/300], Train Loss: 0.000435
Validation Loss: 0.00041495
Epoch [193/300], Train Loss: 0.000431
Validation Loss: 0.00041811
Epoch [194/300], Train Loss: 0.000432
Validation Loss: 0.00041443
Epoch [195/300], Train Loss: 0.000430
Validation Loss: 0.00041312
Epoch [196/300], Train Loss: 0.000431
Validation Loss: 0.00041403
Epoch [197/300], Train Loss: 0.000431
Validation Loss: 0.00041095
Epoch [198/300], Train Loss: 0.000433
Validation Loss: 0.00041200
Epoch [199/300], Train Loss: 0.000429
Validation Loss: 0.00041420
Epoch [200/300], Train Loss: 0.000428
Validation Loss: 0.00040989
Epoch [201/300], Train Loss: 0.000427
Validation Loss: 0.00040823
Epoch [202/300], Train Loss: 0.000426
Validation Loss: 0.00040822
Epoch [203/300], Train Loss: 0.000425
Validation Loss: 0.00041029
Epoch [204/300], Train Loss: 0.000424
Validation Loss: 0.00040815
Epoch [205/300], Train Loss: 0.000425
Validation Loss: 0.00040753
Epoch [206/300], Train Loss: 0.000423
Validation Loss: 0.00040559
Epoch [207/300], Train Loss: 0.000422
Validation Loss: 0.00040899
Epoch [208/300], Train Loss: 0.000422
Validation Loss: 0.00040438
Epoch [209/300], Train Loss: 0.000422
Validation Loss: 0.00040380
Epoch [210/300], Train Loss: 0.000421
Validation Loss: 0.00040569
Epoch [211/300], Train Loss: 0.000421
Validation Loss: 0.00040172
Epoch [212/300], Train Loss: 0.000418
Validation Loss: 0.00040618
Epoch [213/300], Train Loss: 0.000419
Validation Loss: 0.00040203
Epoch [214/300], Train Loss: 0.000417
Validation Loss: 0.00040021
Epoch [215/300], Train Loss: 0.000419
Validation Loss: 0.00040429
Epoch [216/300], Train Loss: 0.000417
Validation Loss: 0.00039861
Epoch [217/300], Train Loss: 0.000416
Validation Loss: 0.00039725
Epoch [218/300], Train Loss: 0.000414
Validation Loss: 0.00039779
Epoch [219/300], Train Loss: 0.000413
Validation Loss: 0.00039593
Epoch [220/300], Train Loss: 0.000414
Validation Loss: 0.00039638
Epoch [221/300], Train Loss: 0.000414
Validation Loss: 0.00040126
Epoch [222/300], Train Loss: 0.000415
Validation Loss: 0.00039701
Epoch [223/300], Train Loss: 0.000413
Validation Loss: 0.00039378
Epoch [224/300], Train Loss: 0.000413
Validation Loss: 0.00039664
Epoch [225/300], Train Loss: 0.000411
Validation Loss: 0.00039595
Epoch [226/300], Train Loss: 0.000410
Validation Loss: 0.00039249
Epoch [227/300], Train Loss: 0.000411
Validation Loss: 0.00039268
Epoch [228/300], Train Loss: 0.000410
Validation Loss: 0.00039470
Epoch [229/300], Train Loss: 0.000409
Validation Loss: 0.00039084
Epoch [230/300], Train Loss: 0.000408
Validation Loss: 0.00039003
Epoch [231/300], Train Loss: 0.000406
Validation Loss: 0.00039088
Epoch [232/300], Train Loss: 0.000406
Validation Loss: 0.00038943
Epoch [233/300], Train Loss: 0.000406
Validation Loss: 0.00038902
Epoch [234/300], Train Loss: 0.000407
Validation Loss: 0.00038942
Epoch [235/300], Train Loss: 0.000408
Validation Loss: 0.00039141
Epoch [236/300], Train Loss: 0.000409
Validation Loss: 0.00038690
Epoch [237/300], Train Loss: 0.000406
Validation Loss: 0.00039607
Epoch [238/300], Train Loss: 0.000407
Validation Loss: 0.00038952
Epoch [239/300], Train Loss: 0.000405
Validation Loss: 0.00038717
Epoch [240/300], Train Loss: 0.000402
Validation Loss: 0.00038605
Epoch [241/300], Train Loss: 0.000402
Validation Loss: 0.00038854
Epoch [242/300], Train Loss: 0.000402
Validation Loss: 0.00038660
Epoch [243/300], Train Loss: 0.000403
Validation Loss: 0.00040167
Epoch [244/300], Train Loss: 0.000403
Validation Loss: 0.00040139
Epoch [245/300], Train Loss: 0.000404
Validation Loss: 0.00038353
Epoch [246/300], Train Loss: 0.000402
Validation Loss: 0.00038360
Epoch [247/300], Train Loss: 0.000401
Validation Loss: 0.00038357
Epoch [248/300], Train Loss: 0.000400
Validation Loss: 0.00038342
Epoch [249/300], Train Loss: 0.000398
Validation Loss: 0.00038488
Epoch [250/300], Train Loss: 0.000399
Validation Loss: 0.00038122
Epoch [251/300], Train Loss: 0.000399
Validation Loss: 0.00038277
Epoch [252/300], Train Loss: 0.000399
Validation Loss: 0.00038219
Epoch [253/300], Train Loss: 0.000398
Validation Loss: 0.00038294
Epoch [254/300], Train Loss: 0.000399
Validation Loss: 0.00038538
Epoch [255/300], Train Loss: 0.000396
Validation Loss: 0.00037831
Epoch [256/300], Train Loss: 0.000395
Validation Loss: 0.00037864
Epoch [257/300], Train Loss: 0.000398
Validation Loss: 0.00037875
Epoch [258/300], Train Loss: 0.000396
Validation Loss: 0.00038800
Epoch [259/300], Train Loss: 0.000396
Validation Loss: 0.00037982
Epoch [260/300], Train Loss: 0.000398
Validation Loss: 0.00038094
Epoch [261/300], Train Loss: 0.000395
Validation Loss: 0.00038051
Epoch [262/300], Train Loss: 0.000394
Validation Loss: 0.00037735
Epoch [263/300], Train Loss: 0.000397
Validation Loss: 0.00038131
Epoch [264/300], Train Loss: 0.000393
Validation Loss: 0.00037902
Epoch [265/300], Train Loss: 0.000394
Validation Loss: 0.00037795
Epoch [266/300], Train Loss: 0.000392
Validation Loss: 0.00037582
Epoch [267/300], Train Loss: 0.000391
Validation Loss: 0.00037526
Epoch [268/300], Train Loss: 0.000393
Validation Loss: 0.00037885
Epoch [269/300], Train Loss: 0.000393
Validation Loss: 0.00037852
Epoch [270/300], Train Loss: 0.000392
Validation Loss: 0.00037914
Epoch [271/300], Train Loss: 0.000391
Validation Loss: 0.00037849
Epoch [272/300], Train Loss: 0.000391
Validation Loss: 0.00037508
Epoch [273/300], Train Loss: 0.000391
Validation Loss: 0.00037337
Epoch [274/300], Train Loss: 0.000390
Validation Loss: 0.00037573
Epoch [275/300], Train Loss: 0.000389
Validation Loss: 0.00037403
Epoch [276/300], Train Loss: 0.000388
Validation Loss: 0.00037301
Epoch [277/300], Train Loss: 0.000388
Validation Loss: 0.00037899
Epoch [278/300], Train Loss: 0.000388
Validation Loss: 0.00037261
Epoch [279/300], Train Loss: 0.000388
Validation Loss: 0.00037411
Epoch [280/300], Train Loss: 0.000387
Validation Loss: 0.00037513
Epoch [281/300], Train Loss: 0.000387
Validation Loss: 0.00037200
Epoch [282/300], Train Loss: 0.000388
Validation Loss: 0.00037284
Epoch [283/300], Train Loss: 0.000387
Validation Loss: 0.00037315
Epoch [284/300], Train Loss: 0.000388
Validation Loss: 0.00037175
Epoch [285/300], Train Loss: 0.000386
Validation Loss: 0.00037006
Epoch [286/300], Train Loss: 0.000388
Validation Loss: 0.00037066
Epoch [287/300], Train Loss: 0.000387
Validation Loss: 0.00036954
Epoch [288/300], Train Loss: 0.000385
Validation Loss: 0.00036991
Epoch [289/300], Train Loss: 0.000389
Validation Loss: 0.00036932
Epoch [290/300], Train Loss: 0.000384
Validation Loss: 0.00037325
Epoch [291/300], Train Loss: 0.000386
Validation Loss: 0.00037270
Epoch [292/300], Train Loss: 0.000385
Validation Loss: 0.00037074
Epoch [293/300], Train Loss: 0.000384
Validation Loss: 0.00037074
Epoch [294/300], Train Loss: 0.000384
Validation Loss: 0.00037167
Epoch [295/300], Train Loss: 0.000384
Validation Loss: 0.00037087
Epoch [296/300], Train Loss: 0.000384
Validation Loss: 0.00036787
Epoch [297/300], Train Loss: 0.000383
Validation Loss: 0.00036838
Epoch [298/300], Train Loss: 0.000383
Validation Loss: 0.00036785
Epoch [299/300], Train Loss: 0.000383
Validation Loss: 0.00036756
Epoch [300/300], Train Loss: 0.000381
Validation Loss: 0.00036667

Evaluating model for: Fridge
Run 37/72 completed in 3680.25 seconds with: {'MAE': np.float32(26.603382), 'MSE': np.float32(1251.8358), 'RMSE': np.float32(35.38129), 'SAE': np.float32(0.017027093), 'NDE': np.float32(0.6167248)}

Run 38/72: hidden=256, seq_len=360, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.001075
Validation Loss: 0.00068184
Epoch [2/300], Train Loss: 0.000679
Validation Loss: 0.00066840
Epoch [3/300], Train Loss: 0.000676
Validation Loss: 0.00066629
Epoch [4/300], Train Loss: 0.000674
Validation Loss: 0.00066403
Epoch [5/300], Train Loss: 0.000671
Validation Loss: 0.00065881
Epoch [6/300], Train Loss: 0.000666
Validation Loss: 0.00065153
Epoch [7/300], Train Loss: 0.000659
Validation Loss: 0.00064265
Epoch [8/300], Train Loss: 0.000652
Validation Loss: 0.00063270
Epoch [9/300], Train Loss: 0.000641
Validation Loss: 0.00061748
Epoch [10/300], Train Loss: 0.000629
Validation Loss: 0.00062047
Epoch [11/300], Train Loss: 0.000627
Validation Loss: 0.00061147
Epoch [12/300], Train Loss: 0.000625
Validation Loss: 0.00061161
Epoch [13/300], Train Loss: 0.000621
Validation Loss: 0.00060935
Epoch [14/300], Train Loss: 0.000621
Validation Loss: 0.00061012
Epoch [15/300], Train Loss: 0.000620
Validation Loss: 0.00060440
Epoch [16/300], Train Loss: 0.000618
Validation Loss: 0.00060495
Epoch [17/300], Train Loss: 0.000616
Validation Loss: 0.00060537
Epoch [18/300], Train Loss: 0.000616
Validation Loss: 0.00060232
Epoch [19/300], Train Loss: 0.000615
Validation Loss: 0.00060462
Epoch [20/300], Train Loss: 0.000614
Validation Loss: 0.00060350
Epoch [21/300], Train Loss: 0.000613
Validation Loss: 0.00060058
Epoch [22/300], Train Loss: 0.000613
Validation Loss: 0.00060421
Epoch [23/300], Train Loss: 0.000611
Validation Loss: 0.00060079
Epoch [24/300], Train Loss: 0.000610
Validation Loss: 0.00059905
Epoch [25/300], Train Loss: 0.000610
Validation Loss: 0.00060154
Epoch [26/300], Train Loss: 0.000612
Validation Loss: 0.00060059
Epoch [27/300], Train Loss: 0.000610
Validation Loss: 0.00060295
Epoch [28/300], Train Loss: 0.000609
Validation Loss: 0.00059692
Epoch [29/300], Train Loss: 0.000608
Validation Loss: 0.00059875
Epoch [30/300], Train Loss: 0.000607
Validation Loss: 0.00059628
Epoch [31/300], Train Loss: 0.000605
Validation Loss: 0.00059664
Epoch [32/300], Train Loss: 0.000604
Validation Loss: 0.00059337
Epoch [33/300], Train Loss: 0.000605
Validation Loss: 0.00059212
Epoch [34/300], Train Loss: 0.000603
Validation Loss: 0.00059282
Epoch [35/300], Train Loss: 0.000604
Validation Loss: 0.00059392
Epoch [36/300], Train Loss: 0.000602
Validation Loss: 0.00059238
Epoch [37/300], Train Loss: 0.000601
Validation Loss: 0.00059363
Epoch [38/300], Train Loss: 0.000601
Validation Loss: 0.00059028
Epoch [39/300], Train Loss: 0.000600
Validation Loss: 0.00058919
Epoch [40/300], Train Loss: 0.000600
Validation Loss: 0.00059327
Epoch [41/300], Train Loss: 0.000600
Validation Loss: 0.00059524
Epoch [42/300], Train Loss: 0.000599
Validation Loss: 0.00058962
Epoch [43/300], Train Loss: 0.000598
Validation Loss: 0.00058691
Epoch [44/300], Train Loss: 0.000599
Validation Loss: 0.00059317
Epoch [45/300], Train Loss: 0.000597
Validation Loss: 0.00058652
Epoch [46/300], Train Loss: 0.000596
Validation Loss: 0.00058709
Epoch [47/300], Train Loss: 0.000596
Validation Loss: 0.00058484
Epoch [48/300], Train Loss: 0.000596
Validation Loss: 0.00058830
Epoch [49/300], Train Loss: 0.000595
Validation Loss: 0.00058398
Epoch [50/300], Train Loss: 0.000594
Validation Loss: 0.00058527
Epoch [51/300], Train Loss: 0.000594
Validation Loss: 0.00058704
Epoch [52/300], Train Loss: 0.000593
Validation Loss: 0.00058269
Epoch [53/300], Train Loss: 0.000592
Validation Loss: 0.00058431
Epoch [54/300], Train Loss: 0.000592
Validation Loss: 0.00057995
Epoch [55/300], Train Loss: 0.000593
Validation Loss: 0.00058314
Epoch [56/300], Train Loss: 0.000593
Validation Loss: 0.00057961
Epoch [57/300], Train Loss: 0.000591
Validation Loss: 0.00058345
Epoch [58/300], Train Loss: 0.000592
Validation Loss: 0.00058138
Epoch [59/300], Train Loss: 0.000590
Validation Loss: 0.00057894
Epoch [60/300], Train Loss: 0.000589
Validation Loss: 0.00057537
Epoch [61/300], Train Loss: 0.000588
Validation Loss: 0.00057777
Epoch [62/300], Train Loss: 0.000586
Validation Loss: 0.00057551
Epoch [63/300], Train Loss: 0.000586
Validation Loss: 0.00057429
Epoch [64/300], Train Loss: 0.000587
Validation Loss: 0.00057295
Epoch [65/300], Train Loss: 0.000584
Validation Loss: 0.00057544
Epoch [66/300], Train Loss: 0.000583
Validation Loss: 0.00056968
Epoch [67/300], Train Loss: 0.000582
Validation Loss: 0.00057044
Epoch [68/300], Train Loss: 0.000580
Validation Loss: 0.00056946
Epoch [69/300], Train Loss: 0.000579
Validation Loss: 0.00056828
Epoch [70/300], Train Loss: 0.000576
Validation Loss: 0.00056363
Epoch [71/300], Train Loss: 0.000576
Validation Loss: 0.00056891
Epoch [72/300], Train Loss: 0.000577
Validation Loss: 0.00056355
Epoch [73/300], Train Loss: 0.000575
Validation Loss: 0.00056177
Epoch [74/300], Train Loss: 0.000574
Validation Loss: 0.00056585
Epoch [75/300], Train Loss: 0.000573
Validation Loss: 0.00056262
Epoch [76/300], Train Loss: 0.000573
Validation Loss: 0.00055702
Epoch [77/300], Train Loss: 0.000570
Validation Loss: 0.00056221
Epoch [78/300], Train Loss: 0.000570
Validation Loss: 0.00055526
Epoch [79/300], Train Loss: 0.000570
Validation Loss: 0.00055600
Epoch [80/300], Train Loss: 0.000568
Validation Loss: 0.00056043
Epoch [81/300], Train Loss: 0.000569
Validation Loss: 0.00056293
Epoch [82/300], Train Loss: 0.000567
Validation Loss: 0.00055371
Epoch [83/300], Train Loss: 0.000567
Validation Loss: 0.00055604
Epoch [84/300], Train Loss: 0.000568
Validation Loss: 0.00055335
Epoch [85/300], Train Loss: 0.000566
Validation Loss: 0.00055179
Epoch [86/300], Train Loss: 0.000564
Validation Loss: 0.00055294
Epoch [87/300], Train Loss: 0.000563
Validation Loss: 0.00055059
Epoch [88/300], Train Loss: 0.000564
Validation Loss: 0.00055554
Epoch [89/300], Train Loss: 0.000563
Validation Loss: 0.00055770
Epoch [90/300], Train Loss: 0.000562
Validation Loss: 0.00055499
Epoch [91/300], Train Loss: 0.000562
Validation Loss: 0.00054919
Epoch [92/300], Train Loss: 0.000561
Validation Loss: 0.00055026
Epoch [93/300], Train Loss: 0.000560
Validation Loss: 0.00055163
Epoch [94/300], Train Loss: 0.000560
Validation Loss: 0.00054557
Epoch [95/300], Train Loss: 0.000560
Validation Loss: 0.00055120
Epoch [96/300], Train Loss: 0.000559
Validation Loss: 0.00054502
Epoch [97/300], Train Loss: 0.000558
Validation Loss: 0.00055131
Epoch [98/300], Train Loss: 0.000557
Validation Loss: 0.00054543
Epoch [99/300], Train Loss: 0.000558
Validation Loss: 0.00054462
Epoch [100/300], Train Loss: 0.000555
Validation Loss: 0.00054319
Epoch [101/300], Train Loss: 0.000556
Validation Loss: 0.00054895
Epoch [102/300], Train Loss: 0.000555
Validation Loss: 0.00054194
Epoch [103/300], Train Loss: 0.000554
Validation Loss: 0.00054360
Epoch [104/300], Train Loss: 0.000552
Validation Loss: 0.00054159
Epoch [105/300], Train Loss: 0.000554
Validation Loss: 0.00054410
Epoch [106/300], Train Loss: 0.000550
Validation Loss: 0.00054090
Epoch [107/300], Train Loss: 0.000549
Validation Loss: 0.00053679
Epoch [108/300], Train Loss: 0.000548
Validation Loss: 0.00053698
Epoch [109/300], Train Loss: 0.000545
Validation Loss: 0.00054091
Epoch [110/300], Train Loss: 0.000544
Validation Loss: 0.00053577
Epoch [111/300], Train Loss: 0.000540
Validation Loss: 0.00052596
Epoch [112/300], Train Loss: 0.000538
Validation Loss: 0.00052623
Epoch [113/300], Train Loss: 0.000537
Validation Loss: 0.00052708
Epoch [114/300], Train Loss: 0.000527
Validation Loss: 0.00051259
Epoch [115/300], Train Loss: 0.000524
Validation Loss: 0.00051205
Epoch [116/300], Train Loss: 0.000521
Validation Loss: 0.00050575
Epoch [117/300], Train Loss: 0.000526
Validation Loss: 0.00050193
Epoch [118/300], Train Loss: 0.000518
Validation Loss: 0.00050155
Epoch [119/300], Train Loss: 0.000512
Validation Loss: 0.00050101
Epoch [120/300], Train Loss: 0.000510
Validation Loss: 0.00049779
Epoch [121/300], Train Loss: 0.000508
Validation Loss: 0.00049833
Epoch [122/300], Train Loss: 0.000514
Validation Loss: 0.00049989
Epoch [123/300], Train Loss: 0.000513
Validation Loss: 0.00049814
Epoch [124/300], Train Loss: 0.000508
Validation Loss: 0.00050348
Epoch [125/300], Train Loss: 0.000503
Validation Loss: 0.00049469
Epoch [126/300], Train Loss: 0.000513
Validation Loss: 0.00049991
Epoch [127/300], Train Loss: 0.000511
Validation Loss: 0.00050448
Epoch [128/300], Train Loss: 0.000512
Validation Loss: 0.00050637
Epoch [129/300], Train Loss: 0.000505
Validation Loss: 0.00049010
Epoch [130/300], Train Loss: 0.000504
Validation Loss: 0.00049175
Epoch [131/300], Train Loss: 0.000508
Validation Loss: 0.00050513
Epoch [132/300], Train Loss: 0.000511
Validation Loss: 0.00048991
Epoch [133/300], Train Loss: 0.000500
Validation Loss: 0.00049243
Epoch [134/300], Train Loss: 0.000519
Validation Loss: 0.00051767
Epoch [135/300], Train Loss: 0.000515
Validation Loss: 0.00050428
Epoch [136/300], Train Loss: 0.000517
Validation Loss: 0.00050062
Epoch [137/300], Train Loss: 0.000509
Validation Loss: 0.00050913
Epoch [138/300], Train Loss: 0.000531
Validation Loss: 0.00051229
Epoch [139/300], Train Loss: 0.000510
Validation Loss: 0.00049053
Epoch [140/300], Train Loss: 0.000500
Validation Loss: 0.00050128
Epoch [141/300], Train Loss: 0.000506
Validation Loss: 0.00049346
Epoch [142/300], Train Loss: 0.000500
Validation Loss: 0.00048726
Epoch [143/300], Train Loss: 0.000499
Validation Loss: 0.00049089
Epoch [144/300], Train Loss: 0.000496
Validation Loss: 0.00048990
Epoch [145/300], Train Loss: 0.000502
Validation Loss: 0.00048637
Epoch [146/300], Train Loss: 0.000495
Validation Loss: 0.00048742
Epoch [147/300], Train Loss: 0.000493
Validation Loss: 0.00048514
Epoch [148/300], Train Loss: 0.000495
Validation Loss: 0.00048747
Epoch [149/300], Train Loss: 0.000493
Validation Loss: 0.00048415
Epoch [150/300], Train Loss: 0.000502
Validation Loss: 0.00048865
Epoch [151/300], Train Loss: 0.000493
Validation Loss: 0.00048503
Epoch [152/300], Train Loss: 0.000494
Validation Loss: 0.00048407
Epoch [153/300], Train Loss: 0.000492
Validation Loss: 0.00048187
Epoch [154/300], Train Loss: 0.000493
Validation Loss: 0.00048464
Epoch [155/300], Train Loss: 0.000491
Validation Loss: 0.00048588
Epoch [156/300], Train Loss: 0.000492
Validation Loss: 0.00048288
Epoch [157/300], Train Loss: 0.000493
Validation Loss: 0.00048466
Epoch [158/300], Train Loss: 0.000489
Validation Loss: 0.00048135
Epoch [159/300], Train Loss: 0.000491
Validation Loss: 0.00048361
Epoch [160/300], Train Loss: 0.000487
Validation Loss: 0.00048156
Epoch [161/300], Train Loss: 0.000491
Validation Loss: 0.00048115
Epoch [162/300], Train Loss: 0.000488
Validation Loss: 0.00048324
Epoch [163/300], Train Loss: 0.000504
Validation Loss: 0.00052649
Epoch [164/300], Train Loss: 0.000502
Validation Loss: 0.00048062
Epoch [165/300], Train Loss: 0.000487
Validation Loss: 0.00047889
Epoch [166/300], Train Loss: 0.000487
Validation Loss: 0.00048128
Epoch [167/300], Train Loss: 0.000490
Validation Loss: 0.00048079
Epoch [168/300], Train Loss: 0.000493
Validation Loss: 0.00048519
Epoch [169/300], Train Loss: 0.000487
Validation Loss: 0.00049662
Epoch [170/300], Train Loss: 0.000488
Validation Loss: 0.00048640
Epoch [171/300], Train Loss: 0.000486
Validation Loss: 0.00047796
Epoch [172/300], Train Loss: 0.000488
Validation Loss: 0.00047993
Epoch [173/300], Train Loss: 0.000486
Validation Loss: 0.00050227
Epoch [174/300], Train Loss: 0.000488
Validation Loss: 0.00047717
Epoch [175/300], Train Loss: 0.000485
Validation Loss: 0.00047721
Epoch [176/300], Train Loss: 0.000484
Validation Loss: 0.00048045
Epoch [177/300], Train Loss: 0.000485
Validation Loss: 0.00047594
Epoch [178/300], Train Loss: 0.000485
Validation Loss: 0.00047497
Epoch [179/300], Train Loss: 0.000484
Validation Loss: 0.00047731
Epoch [180/300], Train Loss: 0.000484
Validation Loss: 0.00048059
Epoch [181/300], Train Loss: 0.000484
Validation Loss: 0.00047638
Epoch [182/300], Train Loss: 0.000483
Validation Loss: 0.00047584
Epoch [183/300], Train Loss: 0.000484
Validation Loss: 0.00047679
Epoch [184/300], Train Loss: 0.000482
Validation Loss: 0.00047522
Epoch [185/300], Train Loss: 0.000482
Validation Loss: 0.00047872
Epoch [186/300], Train Loss: 0.000485
Validation Loss: 0.00047960
Epoch [187/300], Train Loss: 0.000485
Validation Loss: 0.00047547
Epoch [188/300], Train Loss: 0.000483
Validation Loss: 0.00047210
Epoch [189/300], Train Loss: 0.000496
Validation Loss: 0.00047849
Epoch [190/300], Train Loss: 0.000483
Validation Loss: 0.00047387
Epoch [191/300], Train Loss: 0.000483
Validation Loss: 0.00047457
Epoch [192/300], Train Loss: 0.000483
Validation Loss: 0.00048600
Epoch [193/300], Train Loss: 0.000480
Validation Loss: 0.00047280
Epoch [194/300], Train Loss: 0.000483
Validation Loss: 0.00047986
Epoch [195/300], Train Loss: 0.000485
Validation Loss: 0.00049303
Epoch [196/300], Train Loss: 0.000486
Validation Loss: 0.00047529
Epoch [197/300], Train Loss: 0.000481
Validation Loss: 0.00047435
Epoch [198/300], Train Loss: 0.000489
Validation Loss: 0.00048902
Early stopping triggered

Evaluating model for: Fridge
Run 38/72 completed in 2675.03 seconds with: {'MAE': np.float32(31.668169), 'MSE': np.float32(1646.6566), 'RMSE': np.float32(40.579018), 'SAE': np.float32(0.035494223), 'NDE': np.float32(0.7073255)}

Run 39/72: hidden=256, seq_len=360, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000693
Validation Loss: 0.00066176
Epoch [2/300], Train Loss: 0.000664
Validation Loss: 0.00066018
Epoch [3/300], Train Loss: 0.000662
Validation Loss: 0.00065923
Epoch [4/300], Train Loss: 0.000661
Validation Loss: 0.00065821
Epoch [5/300], Train Loss: 0.000658
Validation Loss: 0.00065221
Epoch [6/300], Train Loss: 0.000653
Validation Loss: 0.00064489
Epoch [7/300], Train Loss: 0.000643
Validation Loss: 0.00062589
Epoch [8/300], Train Loss: 0.000624
Validation Loss: 0.00061087
Epoch [9/300], Train Loss: 0.000616
Validation Loss: 0.00060821
Epoch [10/300], Train Loss: 0.000611
Validation Loss: 0.00060476
Epoch [11/300], Train Loss: 0.000611
Validation Loss: 0.00060580
Epoch [12/300], Train Loss: 0.000608
Validation Loss: 0.00059836
Epoch [13/300], Train Loss: 0.000606
Validation Loss: 0.00060043
Epoch [14/300], Train Loss: 0.000605
Validation Loss: 0.00059847
Epoch [15/300], Train Loss: 0.000603
Validation Loss: 0.00059346
Epoch [16/300], Train Loss: 0.000600
Validation Loss: 0.00059420
Epoch [17/300], Train Loss: 0.000601
Validation Loss: 0.00059243
Epoch [18/300], Train Loss: 0.000599
Validation Loss: 0.00059312
Epoch [19/300], Train Loss: 0.000598
Validation Loss: 0.00059282
Epoch [20/300], Train Loss: 0.000597
Validation Loss: 0.00059270
Epoch [21/300], Train Loss: 0.000595
Validation Loss: 0.00058741
Epoch [22/300], Train Loss: 0.000594
Validation Loss: 0.00059094
Epoch [23/300], Train Loss: 0.000594
Validation Loss: 0.00058625
Epoch [24/300], Train Loss: 0.000592
Validation Loss: 0.00058286
Epoch [25/300], Train Loss: 0.000591
Validation Loss: 0.00058958
Epoch [26/300], Train Loss: 0.000590
Validation Loss: 0.00058567
Epoch [27/300], Train Loss: 0.000588
Validation Loss: 0.00058277
Epoch [28/300], Train Loss: 0.000588
Validation Loss: 0.00057853
Epoch [29/300], Train Loss: 0.000586
Validation Loss: 0.00057706
Epoch [30/300], Train Loss: 0.000583
Validation Loss: 0.00057892
Epoch [31/300], Train Loss: 0.000586
Validation Loss: 0.00057635
Epoch [32/300], Train Loss: 0.000581
Validation Loss: 0.00057341
Epoch [33/300], Train Loss: 0.000580
Validation Loss: 0.00057161
Epoch [34/300], Train Loss: 0.000578
Validation Loss: 0.00057335
Epoch [35/300], Train Loss: 0.000579
Validation Loss: 0.00057801
Epoch [36/300], Train Loss: 0.000575
Validation Loss: 0.00056853
Epoch [37/300], Train Loss: 0.000573
Validation Loss: 0.00057338
Epoch [38/300], Train Loss: 0.000572
Validation Loss: 0.00056349
Epoch [39/300], Train Loss: 0.000568
Validation Loss: 0.00056126
Epoch [40/300], Train Loss: 0.000568
Validation Loss: 0.00056206
Epoch [41/300], Train Loss: 0.000568
Validation Loss: 0.00056458
Epoch [42/300], Train Loss: 0.000564
Validation Loss: 0.00055736
Epoch [43/300], Train Loss: 0.000562
Validation Loss: 0.00055306
Epoch [44/300], Train Loss: 0.000560
Validation Loss: 0.00055639
Epoch [45/300], Train Loss: 0.000561
Validation Loss: 0.00055158
Epoch [46/300], Train Loss: 0.000554
Validation Loss: 0.00055214
Epoch [47/300], Train Loss: 0.000553
Validation Loss: 0.00055337
Epoch [48/300], Train Loss: 0.000553
Validation Loss: 0.00054770
Epoch [49/300], Train Loss: 0.000552
Validation Loss: 0.00054866
Epoch [50/300], Train Loss: 0.000549
Validation Loss: 0.00054836
Epoch [51/300], Train Loss: 0.000548
Validation Loss: 0.00054742
Epoch [52/300], Train Loss: 0.000544
Validation Loss: 0.00053891
Epoch [53/300], Train Loss: 0.000545
Validation Loss: 0.00053399
Epoch [54/300], Train Loss: 0.000538
Validation Loss: 0.00053053
Epoch [55/300], Train Loss: 0.000539
Validation Loss: 0.00053173
Epoch [56/300], Train Loss: 0.000538
Validation Loss: 0.00053067
Epoch [57/300], Train Loss: 0.000532
Validation Loss: 0.00052724
Epoch [58/300], Train Loss: 0.000529
Validation Loss: 0.00052537
Epoch [59/300], Train Loss: 0.000525
Validation Loss: 0.00052205
Epoch [60/300], Train Loss: 0.000523
Validation Loss: 0.00051569
Epoch [61/300], Train Loss: 0.000527
Validation Loss: 0.00051661
Epoch [62/300], Train Loss: 0.000519
Validation Loss: 0.00052667
Epoch [63/300], Train Loss: 0.000523
Validation Loss: 0.00051203
Epoch [64/300], Train Loss: 0.000517
Validation Loss: 0.00050818
Epoch [65/300], Train Loss: 0.000516
Validation Loss: 0.00051046
Epoch [66/300], Train Loss: 0.000515
Validation Loss: 0.00051519
Epoch [67/300], Train Loss: 0.000512
Validation Loss: 0.00050810
Epoch [68/300], Train Loss: 0.000506
Validation Loss: 0.00050435
Epoch [69/300], Train Loss: 0.000505
Validation Loss: 0.00050295
Epoch [70/300], Train Loss: 0.000500
Validation Loss: 0.00049358
Epoch [71/300], Train Loss: 0.000503
Validation Loss: 0.00049451
Epoch [72/300], Train Loss: 0.000502
Validation Loss: 0.00049408
Epoch [73/300], Train Loss: 0.000498
Validation Loss: 0.00048846
Epoch [74/300], Train Loss: 0.000494
Validation Loss: 0.00049930
Epoch [75/300], Train Loss: 0.000494
Validation Loss: 0.00049002
Epoch [76/300], Train Loss: 0.000495
Validation Loss: 0.00048276
Epoch [77/300], Train Loss: 0.000491
Validation Loss: 0.00049325
Epoch [78/300], Train Loss: 0.000490
Validation Loss: 0.00048212
Epoch [79/300], Train Loss: 0.000496
Validation Loss: 0.00048580
Epoch [80/300], Train Loss: 0.000486
Validation Loss: 0.00048308
Epoch [81/300], Train Loss: 0.000490
Validation Loss: 0.00048657
Epoch [82/300], Train Loss: 0.000486
Validation Loss: 0.00047864
Epoch [83/300], Train Loss: 0.000487
Validation Loss: 0.00047843
Epoch [84/300], Train Loss: 0.000483
Validation Loss: 0.00048297
Epoch [85/300], Train Loss: 0.000485
Validation Loss: 0.00048666
Epoch [86/300], Train Loss: 0.000485
Validation Loss: 0.00047965
Epoch [87/300], Train Loss: 0.000484
Validation Loss: 0.00047930
Epoch [88/300], Train Loss: 0.000490
Validation Loss: 0.00047773
Epoch [89/300], Train Loss: 0.000485
Validation Loss: 0.00047907
Epoch [90/300], Train Loss: 0.000482
Validation Loss: 0.00047327
Epoch [91/300], Train Loss: 0.000479
Validation Loss: 0.00046987
Epoch [92/300], Train Loss: 0.000483
Validation Loss: 0.00047218
Epoch [93/300], Train Loss: 0.000475
Validation Loss: 0.00046894
Epoch [94/300], Train Loss: 0.000475
Validation Loss: 0.00046864
Epoch [95/300], Train Loss: 0.000473
Validation Loss: 0.00047935
Epoch [96/300], Train Loss: 0.000474
Validation Loss: 0.00046666
Epoch [97/300], Train Loss: 0.000470
Validation Loss: 0.00046678
Epoch [98/300], Train Loss: 0.000472
Validation Loss: 0.00047567
Epoch [99/300], Train Loss: 0.000476
Validation Loss: 0.00047158
Epoch [100/300], Train Loss: 0.000468
Validation Loss: 0.00046614
Epoch [101/300], Train Loss: 0.000473
Validation Loss: 0.00046194
Epoch [102/300], Train Loss: 0.000467
Validation Loss: 0.00046129
Epoch [103/300], Train Loss: 0.000468
Validation Loss: 0.00045748
Epoch [104/300], Train Loss: 0.000463
Validation Loss: 0.00046024
Epoch [105/300], Train Loss: 0.000461
Validation Loss: 0.00045173
Epoch [106/300], Train Loss: 0.000465
Validation Loss: 0.00046142
Epoch [107/300], Train Loss: 0.000462
Validation Loss: 0.00045851
Epoch [108/300], Train Loss: 0.000458
Validation Loss: 0.00044758
Epoch [109/300], Train Loss: 0.000457
Validation Loss: 0.00044918
Epoch [110/300], Train Loss: 0.000457
Validation Loss: 0.00044423
Epoch [111/300], Train Loss: 0.000454
Validation Loss: 0.00044459
Epoch [112/300], Train Loss: 0.000453
Validation Loss: 0.00044439
Epoch [113/300], Train Loss: 0.000453
Validation Loss: 0.00044822
Epoch [114/300], Train Loss: 0.000458
Validation Loss: 0.00045207
Epoch [115/300], Train Loss: 0.000449
Validation Loss: 0.00044215
Epoch [116/300], Train Loss: 0.000452
Validation Loss: 0.00044496
Epoch [117/300], Train Loss: 0.000451
Validation Loss: 0.00044529
Epoch [118/300], Train Loss: 0.000453
Validation Loss: 0.00043725
Epoch [119/300], Train Loss: 0.000448
Validation Loss: 0.00043450
Epoch [120/300], Train Loss: 0.000445
Validation Loss: 0.00044201
Epoch [121/300], Train Loss: 0.000445
Validation Loss: 0.00043660
Epoch [122/300], Train Loss: 0.000442
Validation Loss: 0.00043379
Epoch [123/300], Train Loss: 0.000450
Validation Loss: 0.00043370
Epoch [124/300], Train Loss: 0.000445
Validation Loss: 0.00042959
Epoch [125/300], Train Loss: 0.000441
Validation Loss: 0.00043233
Epoch [126/300], Train Loss: 0.000445
Validation Loss: 0.00042884
Epoch [127/300], Train Loss: 0.000440
Validation Loss: 0.00042672
Epoch [128/300], Train Loss: 0.000441
Validation Loss: 0.00042812
Epoch [129/300], Train Loss: 0.000441
Validation Loss: 0.00042671
Epoch [130/300], Train Loss: 0.000437
Validation Loss: 0.00042481
Epoch [131/300], Train Loss: 0.000438
Validation Loss: 0.00042684
Epoch [132/300], Train Loss: 0.000436
Validation Loss: 0.00042351
Epoch [133/300], Train Loss: 0.000433
Validation Loss: 0.00042252
Epoch [134/300], Train Loss: 0.000446
Validation Loss: 0.00042697
Epoch [135/300], Train Loss: 0.000433
Validation Loss: 0.00042131
Epoch [136/300], Train Loss: 0.000433
Validation Loss: 0.00041867
Epoch [137/300], Train Loss: 0.000431
Validation Loss: 0.00043485
Epoch [138/300], Train Loss: 0.000432
Validation Loss: 0.00041714
Epoch [139/300], Train Loss: 0.000429
Validation Loss: 0.00041607
Epoch [140/300], Train Loss: 0.000436
Validation Loss: 0.00044735
Epoch [141/300], Train Loss: 0.000434
Validation Loss: 0.00043308
Epoch [142/300], Train Loss: 0.000428
Validation Loss: 0.00041373
Epoch [143/300], Train Loss: 0.000433
Validation Loss: 0.00042493
Epoch [144/300], Train Loss: 0.000430
Validation Loss: 0.00041445
Epoch [145/300], Train Loss: 0.000440
Validation Loss: 0.00041429
Epoch [146/300], Train Loss: 0.000425
Validation Loss: 0.00041295
Epoch [147/300], Train Loss: 0.000431
Validation Loss: 0.00043932
Epoch [148/300], Train Loss: 0.000427
Validation Loss: 0.00041788
Epoch [149/300], Train Loss: 0.000426
Validation Loss: 0.00042415
Epoch [150/300], Train Loss: 0.000426
Validation Loss: 0.00044048
Epoch [151/300], Train Loss: 0.000431
Validation Loss: 0.00041403
Epoch [152/300], Train Loss: 0.000422
Validation Loss: 0.00041658
Epoch [153/300], Train Loss: 0.000422
Validation Loss: 0.00040906
Epoch [154/300], Train Loss: 0.000419
Validation Loss: 0.00040581
Epoch [155/300], Train Loss: 0.000419
Validation Loss: 0.00042864
Epoch [156/300], Train Loss: 0.000424
Validation Loss: 0.00040892
Epoch [157/300], Train Loss: 0.000422
Validation Loss: 0.00042387
Epoch [158/300], Train Loss: 0.000420
Validation Loss: 0.00040585
Epoch [159/300], Train Loss: 0.000417
Validation Loss: 0.00040797
Epoch [160/300], Train Loss: 0.000417
Validation Loss: 0.00041355
Epoch [161/300], Train Loss: 0.000418
Validation Loss: 0.00040284
Epoch [162/300], Train Loss: 0.000417
Validation Loss: 0.00040961
Epoch [163/300], Train Loss: 0.000414
Validation Loss: 0.00040263
Epoch [164/300], Train Loss: 0.000419
Validation Loss: 0.00040593
Epoch [165/300], Train Loss: 0.000415
Validation Loss: 0.00041931
Epoch [166/300], Train Loss: 0.000417
Validation Loss: 0.00040447
Epoch [167/300], Train Loss: 0.000415
Validation Loss: 0.00040182
Epoch [168/300], Train Loss: 0.000419
Validation Loss: 0.00040912
Epoch [169/300], Train Loss: 0.000419
Validation Loss: 0.00040372
Epoch [170/300], Train Loss: 0.000417
Validation Loss: 0.00040682
Epoch [171/300], Train Loss: 0.000415
Validation Loss: 0.00039779
Epoch [172/300], Train Loss: 0.000415
Validation Loss: 0.00040003
Epoch [173/300], Train Loss: 0.000410
Validation Loss: 0.00040155
Epoch [174/300], Train Loss: 0.000412
Validation Loss: 0.00040646
Epoch [175/300], Train Loss: 0.000412
Validation Loss: 0.00040112
Epoch [176/300], Train Loss: 0.000410
Validation Loss: 0.00039806
Epoch [177/300], Train Loss: 0.000412
Validation Loss: 0.00040101
Epoch [178/300], Train Loss: 0.000409
Validation Loss: 0.00040786
Epoch [179/300], Train Loss: 0.000412
Validation Loss: 0.00039870
Epoch [180/300], Train Loss: 0.000412
Validation Loss: 0.00040388
Epoch [181/300], Train Loss: 0.000416
Validation Loss: 0.00039945
Early stopping triggered

Evaluating model for: Fridge
Run 39/72 completed in 2700.20 seconds with: {'MAE': np.float32(27.492306), 'MSE': np.float32(1384.8701), 'RMSE': np.float32(37.213844), 'SAE': np.float32(0.0074122325), 'NDE': np.float32(0.64866775)}

Run 40/72: hidden=256, seq_len=360, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000778
Validation Loss: 0.00066232
Epoch [2/300], Train Loss: 0.000665
Validation Loss: 0.00066224
Epoch [3/300], Train Loss: 0.000664
Validation Loss: 0.00066164
Epoch [4/300], Train Loss: 0.000664
Validation Loss: 0.00066171
Epoch [5/300], Train Loss: 0.000663
Validation Loss: 0.00066031
Epoch [6/300], Train Loss: 0.000662
Validation Loss: 0.00065785
Epoch [7/300], Train Loss: 0.000656
Validation Loss: 0.00064522
Epoch [8/300], Train Loss: 0.000643
Validation Loss: 0.00063117
Epoch [9/300], Train Loss: 0.000629
Validation Loss: 0.00061457
Epoch [10/300], Train Loss: 0.000619
Validation Loss: 0.00061394
Epoch [11/300], Train Loss: 0.000617
Validation Loss: 0.00060917
Epoch [12/300], Train Loss: 0.000613
Validation Loss: 0.00060454
Epoch [13/300], Train Loss: 0.000610
Validation Loss: 0.00060235
Epoch [14/300], Train Loss: 0.000609
Validation Loss: 0.00060315
Epoch [15/300], Train Loss: 0.000608
Validation Loss: 0.00059851
Epoch [16/300], Train Loss: 0.000607
Validation Loss: 0.00060052
Epoch [17/300], Train Loss: 0.000605
Validation Loss: 0.00059903
Epoch [18/300], Train Loss: 0.000605
Validation Loss: 0.00059648
Epoch [19/300], Train Loss: 0.000605
Validation Loss: 0.00059936
Epoch [20/300], Train Loss: 0.000604
Validation Loss: 0.00059866
Epoch [21/300], Train Loss: 0.000603
Validation Loss: 0.00059540
Epoch [22/300], Train Loss: 0.000603
Validation Loss: 0.00059957
Epoch [23/300], Train Loss: 0.000601
Validation Loss: 0.00059472
Epoch [24/300], Train Loss: 0.000600
Validation Loss: 0.00059442
Epoch [25/300], Train Loss: 0.000600
Validation Loss: 0.00059465
Epoch [26/300], Train Loss: 0.000601
Validation Loss: 0.00059433
Epoch [27/300], Train Loss: 0.000600
Validation Loss: 0.00059553
Epoch [28/300], Train Loss: 0.000600
Validation Loss: 0.00059044
Epoch [29/300], Train Loss: 0.000599
Validation Loss: 0.00059325
Epoch [30/300], Train Loss: 0.000600
Validation Loss: 0.00059151
Epoch [31/300], Train Loss: 0.000597
Validation Loss: 0.00059118
Epoch [32/300], Train Loss: 0.000596
Validation Loss: 0.00058842
Epoch [33/300], Train Loss: 0.000596
Validation Loss: 0.00058642
Epoch [34/300], Train Loss: 0.000595
Validation Loss: 0.00058633
Epoch [35/300], Train Loss: 0.000595
Validation Loss: 0.00058807
Epoch [36/300], Train Loss: 0.000594
Validation Loss: 0.00058655
Epoch [37/300], Train Loss: 0.000592
Validation Loss: 0.00058697
Epoch [38/300], Train Loss: 0.000592
Validation Loss: 0.00058260
Epoch [39/300], Train Loss: 0.000591
Validation Loss: 0.00058191
Epoch [40/300], Train Loss: 0.000591
Validation Loss: 0.00059041
Epoch [41/300], Train Loss: 0.000592
Validation Loss: 0.00058659
Epoch [42/300], Train Loss: 0.000590
Validation Loss: 0.00057977
Epoch [43/300], Train Loss: 0.000588
Validation Loss: 0.00057637
Epoch [44/300], Train Loss: 0.000589
Validation Loss: 0.00058700
Epoch [45/300], Train Loss: 0.000587
Validation Loss: 0.00057566
Epoch [46/300], Train Loss: 0.000586
Validation Loss: 0.00057369
Epoch [47/300], Train Loss: 0.000584
Validation Loss: 0.00057243
Epoch [48/300], Train Loss: 0.000584
Validation Loss: 0.00057667
Epoch [49/300], Train Loss: 0.000582
Validation Loss: 0.00057045
Epoch [50/300], Train Loss: 0.000580
Validation Loss: 0.00056995
Epoch [51/300], Train Loss: 0.000579
Validation Loss: 0.00057272
Epoch [52/300], Train Loss: 0.000578
Validation Loss: 0.00057001
Epoch [53/300], Train Loss: 0.000577
Validation Loss: 0.00056970
Epoch [54/300], Train Loss: 0.000577
Validation Loss: 0.00056155
Epoch [55/300], Train Loss: 0.000577
Validation Loss: 0.00056854
Epoch [56/300], Train Loss: 0.000574
Validation Loss: 0.00056044
Epoch [57/300], Train Loss: 0.000571
Validation Loss: 0.00056024
Epoch [58/300], Train Loss: 0.000565
Validation Loss: 0.00054415
Epoch [59/300], Train Loss: 0.000609
Validation Loss: 0.00060272
Epoch [60/300], Train Loss: 0.000600
Validation Loss: 0.00058802
Epoch [61/300], Train Loss: 0.000598
Validation Loss: 0.00059004
Epoch [62/300], Train Loss: 0.000593
Validation Loss: 0.00058238
Epoch [63/300], Train Loss: 0.000590
Validation Loss: 0.00057668
Epoch [64/300], Train Loss: 0.000588
Validation Loss: 0.00057350
Epoch [65/300], Train Loss: 0.000586
Validation Loss: 0.00057559
Epoch [66/300], Train Loss: 0.000585
Validation Loss: 0.00057010
Epoch [67/300], Train Loss: 0.000584
Validation Loss: 0.00057002
Epoch [68/300], Train Loss: 0.000580
Validation Loss: 0.00057343
Early stopping triggered

Evaluating model for: Fridge
Run 40/72 completed in 1230.13 seconds with: {'MAE': np.float32(35.504524), 'MSE': np.float32(1934.7858), 'RMSE': np.float32(43.9862), 'SAE': np.float32(0.05179278), 'NDE': np.float32(0.76671547)}

Run 41/72: hidden=256, seq_len=720, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000681
Validation Loss: 0.00065977
Epoch [2/300], Train Loss: 0.000666
Validation Loss: 0.00065475
Epoch [3/300], Train Loss: 0.000653
Validation Loss: 0.00064920
Epoch [4/300], Train Loss: 0.000646
Validation Loss: 0.00064243
Epoch [5/300], Train Loss: 0.000640
Validation Loss: 0.00063451
Epoch [6/300], Train Loss: 0.000633
Validation Loss: 0.00062305
Epoch [7/300], Train Loss: 0.000612
Validation Loss: 0.00061765
Epoch [8/300], Train Loss: 0.000618
Validation Loss: 0.00061146
Epoch [9/300], Train Loss: 0.000609
Validation Loss: 0.00060942
Epoch [10/300], Train Loss: 0.000609
Validation Loss: 0.00060377
Epoch [11/300], Train Loss: 0.000604
Validation Loss: 0.00060405
Epoch [12/300], Train Loss: 0.000607
Validation Loss: 0.00060143
Epoch [13/300], Train Loss: 0.000606
Validation Loss: 0.00059909
Epoch [14/300], Train Loss: 0.000605
Validation Loss: 0.00060198
Epoch [15/300], Train Loss: 0.000606
Validation Loss: 0.00060687
Epoch [16/300], Train Loss: 0.000603
Validation Loss: 0.00060299
Epoch [17/300], Train Loss: 0.000602
Validation Loss: 0.00059447
Epoch [18/300], Train Loss: 0.000591
Validation Loss: 0.00059383
Epoch [19/300], Train Loss: 0.000594
Validation Loss: 0.00059931
Epoch [20/300], Train Loss: 0.000593
Validation Loss: 0.00059365
Epoch [21/300], Train Loss: 0.000592
Validation Loss: 0.00059052
Epoch [22/300], Train Loss: 0.000593
Validation Loss: 0.00059038
Epoch [23/300], Train Loss: 0.000600
Validation Loss: 0.00058891
Epoch [24/300], Train Loss: 0.000593
Validation Loss: 0.00058993
Epoch [25/300], Train Loss: 0.000590
Validation Loss: 0.00058649
Epoch [26/300], Train Loss: 0.000589
Validation Loss: 0.00058524
Epoch [27/300], Train Loss: 0.000593
Validation Loss: 0.00059527
Epoch [28/300], Train Loss: 0.000592
Validation Loss: 0.00059405
Epoch [29/300], Train Loss: 0.000591
Validation Loss: 0.00058231
Epoch [30/300], Train Loss: 0.000590
Validation Loss: 0.00057959
Epoch [31/300], Train Loss: 0.000589
Validation Loss: 0.00057893
Epoch [32/300], Train Loss: 0.000583
Validation Loss: 0.00058911
Epoch [33/300], Train Loss: 0.000583
Validation Loss: 0.00058974
Epoch [34/300], Train Loss: 0.000585
Validation Loss: 0.00057684
Epoch [35/300], Train Loss: 0.000581
Validation Loss: 0.00057448
Epoch [36/300], Train Loss: 0.000584
Validation Loss: 0.00057113
Epoch [37/300], Train Loss: 0.000578
Validation Loss: 0.00057076
Epoch [38/300], Train Loss: 0.000578
Validation Loss: 0.00056885
Epoch [39/300], Train Loss: 0.000576
Validation Loss: 0.00056790
Epoch [40/300], Train Loss: 0.000573
Validation Loss: 0.00057209
Epoch [41/300], Train Loss: 0.000575
Validation Loss: 0.00056390
Epoch [42/300], Train Loss: 0.000574
Validation Loss: 0.00056948
Epoch [43/300], Train Loss: 0.000568
Validation Loss: 0.00056193
Epoch [44/300], Train Loss: 0.000567
Validation Loss: 0.00056010
Epoch [45/300], Train Loss: 0.000569
Validation Loss: 0.00056387
Epoch [46/300], Train Loss: 0.000566
Validation Loss: 0.00055789
Epoch [47/300], Train Loss: 0.000562
Validation Loss: 0.00055455
Epoch [48/300], Train Loss: 0.000564
Validation Loss: 0.00055175
Epoch [49/300], Train Loss: 0.000559
Validation Loss: 0.00055233
Epoch [50/300], Train Loss: 0.000561
Validation Loss: 0.00055389
Epoch [51/300], Train Loss: 0.000557
Validation Loss: 0.00056083
Epoch [52/300], Train Loss: 0.000559
Validation Loss: 0.00055371
Epoch [53/300], Train Loss: 0.000564
Validation Loss: 0.00055699
Epoch [54/300], Train Loss: 0.000554
Validation Loss: 0.00054706
Epoch [55/300], Train Loss: 0.000553
Validation Loss: 0.00054410
Epoch [56/300], Train Loss: 0.000550
Validation Loss: 0.00054069
Epoch [57/300], Train Loss: 0.000546
Validation Loss: 0.00054119
Epoch [58/300], Train Loss: 0.000554
Validation Loss: 0.00053988
Epoch [59/300], Train Loss: 0.000549
Validation Loss: 0.00056069
Epoch [60/300], Train Loss: 0.000555
Validation Loss: 0.00054673
Epoch [61/300], Train Loss: 0.000552
Validation Loss: 0.00055429
Epoch [62/300], Train Loss: 0.000548
Validation Loss: 0.00053788
Epoch [63/300], Train Loss: 0.000548
Validation Loss: 0.00052788
Epoch [64/300], Train Loss: 0.000540
Validation Loss: 0.00052883
Epoch [65/300], Train Loss: 0.000541
Validation Loss: 0.00052706
Epoch [66/300], Train Loss: 0.000543
Validation Loss: 0.00052613
Epoch [67/300], Train Loss: 0.000606
Validation Loss: 0.00059894
Epoch [68/300], Train Loss: 0.000600
Validation Loss: 0.00058342
Epoch [69/300], Train Loss: 0.000590
Validation Loss: 0.00057657
Epoch [70/300], Train Loss: 0.000578
Validation Loss: 0.00057496
Epoch [71/300], Train Loss: 0.000575
Validation Loss: 0.00056920
Epoch [72/300], Train Loss: 0.000577
Validation Loss: 0.00056557
Epoch [73/300], Train Loss: 0.000575
Validation Loss: 0.00056249
Epoch [74/300], Train Loss: 0.000573
Validation Loss: 0.00056584
Epoch [75/300], Train Loss: 0.000575
Validation Loss: 0.00056064
Epoch [76/300], Train Loss: 0.000564
Validation Loss: 0.00055614
Early stopping triggered

Evaluating model for: Fridge
Run 41/72 completed in 1104.66 seconds with: {'MAE': np.float32(36.27519), 'MSE': np.float32(1887.2056), 'RMSE': np.float32(43.44198), 'SAE': np.float32(0.0012616833), 'NDE': np.float32(0.7507301)}

Run 42/72: hidden=256, seq_len=720, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.001074
Validation Loss: 0.00067764
Epoch [2/300], Train Loss: 0.000678
Validation Loss: 0.00066558
Epoch [3/300], Train Loss: 0.000668
Validation Loss: 0.00066511
Epoch [4/300], Train Loss: 0.000667
Validation Loss: 0.00066460
Epoch [5/300], Train Loss: 0.000668
Validation Loss: 0.00066378
Epoch [6/300], Train Loss: 0.000669
Validation Loss: 0.00066247
Epoch [7/300], Train Loss: 0.000655
Validation Loss: 0.00066072
Epoch [8/300], Train Loss: 0.000660
Validation Loss: 0.00065624
Epoch [9/300], Train Loss: 0.000652
Validation Loss: 0.00065151
Epoch [10/300], Train Loss: 0.000646
Validation Loss: 0.00064512
Epoch [11/300], Train Loss: 0.000637
Validation Loss: 0.00063176
Epoch [12/300], Train Loss: 0.000633
Validation Loss: 0.00062429
Epoch [13/300], Train Loss: 0.000630
Validation Loss: 0.00061944
Epoch [14/300], Train Loss: 0.000624
Validation Loss: 0.00061973
Epoch [15/300], Train Loss: 0.000626
Validation Loss: 0.00062102
Epoch [16/300], Train Loss: 0.000620
Validation Loss: 0.00061333
Epoch [17/300], Train Loss: 0.000618
Validation Loss: 0.00060769
Epoch [18/300], Train Loss: 0.000607
Validation Loss: 0.00060546
Epoch [19/300], Train Loss: 0.000609
Validation Loss: 0.00060449
Epoch [20/300], Train Loss: 0.000605
Validation Loss: 0.00060416
Epoch [21/300], Train Loss: 0.000607
Validation Loss: 0.00060153
Epoch [22/300], Train Loss: 0.000607
Validation Loss: 0.00060192
Epoch [23/300], Train Loss: 0.000614
Validation Loss: 0.00060141
Epoch [24/300], Train Loss: 0.000607
Validation Loss: 0.00060233
Epoch [25/300], Train Loss: 0.000604
Validation Loss: 0.00059722
Epoch [26/300], Train Loss: 0.000602
Validation Loss: 0.00060360
Epoch [27/300], Train Loss: 0.000607
Validation Loss: 0.00061219
Epoch [28/300], Train Loss: 0.000610
Validation Loss: 0.00059603
Epoch [29/300], Train Loss: 0.000604
Validation Loss: 0.00059476
Epoch [30/300], Train Loss: 0.000605
Validation Loss: 0.00059577
Epoch [31/300], Train Loss: 0.000605
Validation Loss: 0.00059337
Epoch [32/300], Train Loss: 0.000599
Validation Loss: 0.00060993
Epoch [33/300], Train Loss: 0.000601
Validation Loss: 0.00059375
Epoch [34/300], Train Loss: 0.000600
Validation Loss: 0.00059192
Epoch [35/300], Train Loss: 0.000599
Validation Loss: 0.00059300
Epoch [36/300], Train Loss: 0.000603
Validation Loss: 0.00059120
Epoch [37/300], Train Loss: 0.000595
Validation Loss: 0.00059064
Epoch [38/300], Train Loss: 0.000599
Validation Loss: 0.00059248
Epoch [39/300], Train Loss: 0.000599
Validation Loss: 0.00059258
Epoch [40/300], Train Loss: 0.000592
Validation Loss: 0.00058889
Epoch [41/300], Train Loss: 0.000594
Validation Loss: 0.00058830
Epoch [42/300], Train Loss: 0.000594
Validation Loss: 0.00058824
Epoch [43/300], Train Loss: 0.000591
Validation Loss: 0.00058981
Epoch [44/300], Train Loss: 0.000591
Validation Loss: 0.00058878
Epoch [45/300], Train Loss: 0.000593
Validation Loss: 0.00059144
Epoch [46/300], Train Loss: 0.000592
Validation Loss: 0.00058496
Epoch [47/300], Train Loss: 0.000587
Validation Loss: 0.00058616
Epoch [48/300], Train Loss: 0.000593
Validation Loss: 0.00058528
Epoch [49/300], Train Loss: 0.000586
Validation Loss: 0.00058278
Epoch [50/300], Train Loss: 0.000587
Validation Loss: 0.00059002
Epoch [51/300], Train Loss: 0.000584
Validation Loss: 0.00058208
Epoch [52/300], Train Loss: 0.000587
Validation Loss: 0.00058142
Epoch [53/300], Train Loss: 0.000588
Validation Loss: 0.00058061
Epoch [54/300], Train Loss: 0.000580
Validation Loss: 0.00058147
Epoch [55/300], Train Loss: 0.000584
Validation Loss: 0.00058140
Epoch [56/300], Train Loss: 0.000581
Validation Loss: 0.00057822
Epoch [57/300], Train Loss: 0.000576
Validation Loss: 0.00057609
Epoch [58/300], Train Loss: 0.000585
Validation Loss: 0.00057551
Epoch [59/300], Train Loss: 0.000581
Validation Loss: 0.00057429
Epoch [60/300], Train Loss: 0.000584
Validation Loss: 0.00057606
Epoch [61/300], Train Loss: 0.000580
Validation Loss: 0.00057079
Epoch [62/300], Train Loss: 0.000578
Validation Loss: 0.00056925
Epoch [63/300], Train Loss: 0.000577
Validation Loss: 0.00057062
Epoch [64/300], Train Loss: 0.000574
Validation Loss: 0.00056789
Epoch [65/300], Train Loss: 0.000576
Validation Loss: 0.00056569
Epoch [66/300], Train Loss: 0.000572
Validation Loss: 0.00057012
Epoch [67/300], Train Loss: 0.000574
Validation Loss: 0.00056363
Epoch [68/300], Train Loss: 0.000569
Validation Loss: 0.00056198
Epoch [69/300], Train Loss: 0.000572
Validation Loss: 0.00056116
Epoch [70/300], Train Loss: 0.000564
Validation Loss: 0.00056015
Epoch [71/300], Train Loss: 0.000565
Validation Loss: 0.00056655
Epoch [72/300], Train Loss: 0.000569
Validation Loss: 0.00055533
Epoch [73/300], Train Loss: 0.000568
Validation Loss: 0.00056489
Epoch [74/300], Train Loss: 0.000569
Validation Loss: 0.00055757
Epoch [75/300], Train Loss: 0.000569
Validation Loss: 0.00055646
Epoch [76/300], Train Loss: 0.000560
Validation Loss: 0.00055148
Epoch [77/300], Train Loss: 0.000562
Validation Loss: 0.00055738
Epoch [78/300], Train Loss: 0.000560
Validation Loss: 0.00054995
Epoch [79/300], Train Loss: 0.000558
Validation Loss: 0.00054815
Epoch [80/300], Train Loss: 0.000560
Validation Loss: 0.00055741
Epoch [81/300], Train Loss: 0.000558
Validation Loss: 0.00054886
Epoch [82/300], Train Loss: 0.000560
Validation Loss: 0.00054626
Epoch [83/300], Train Loss: 0.000556
Validation Loss: 0.00054500
Epoch [84/300], Train Loss: 0.000551
Validation Loss: 0.00054713
Epoch [85/300], Train Loss: 0.000551
Validation Loss: 0.00054415
Epoch [86/300], Train Loss: 0.000553
Validation Loss: 0.00054131
Epoch [87/300], Train Loss: 0.000556
Validation Loss: 0.00054429
Epoch [88/300], Train Loss: 0.000557
Validation Loss: 0.00054013
Epoch [89/300], Train Loss: 0.000543
Validation Loss: 0.00053947
Epoch [90/300], Train Loss: 0.000551
Validation Loss: 0.00054038
Epoch [91/300], Train Loss: 0.000548
Validation Loss: 0.00053612
Epoch [92/300], Train Loss: 0.000549
Validation Loss: 0.00053751
Epoch [93/300], Train Loss: 0.000549
Validation Loss: 0.00054561
Epoch [94/300], Train Loss: 0.000552
Validation Loss: 0.00053660
Epoch [95/300], Train Loss: 0.000547
Validation Loss: 0.00053937
Epoch [96/300], Train Loss: 0.000542
Validation Loss: 0.00053050
Epoch [97/300], Train Loss: 0.000543
Validation Loss: 0.00053094
Epoch [98/300], Train Loss: 0.000541
Validation Loss: 0.00052930
Epoch [99/300], Train Loss: 0.000540
Validation Loss: 0.00052807
Epoch [100/300], Train Loss: 0.000541
Validation Loss: 0.00052982
Epoch [101/300], Train Loss: 0.000540
Validation Loss: 0.00053582
Epoch [102/300], Train Loss: 0.000560
Validation Loss: 0.00054456
Epoch [103/300], Train Loss: 0.000554
Validation Loss: 0.00054299
Epoch [104/300], Train Loss: 0.000548
Validation Loss: 0.00053663
Epoch [105/300], Train Loss: 0.000549
Validation Loss: 0.00053446
Epoch [106/300], Train Loss: 0.000551
Validation Loss: 0.00053920
Epoch [107/300], Train Loss: 0.000547
Validation Loss: 0.00053215
Epoch [108/300], Train Loss: 0.000545
Validation Loss: 0.00053507
Epoch [109/300], Train Loss: 0.000544
Validation Loss: 0.00052924
Early stopping triggered

Evaluating model for: Fridge
Run 42/72 completed in 1971.70 seconds with: {'MAE': np.float32(34.15539), 'MSE': np.float32(1794.7783), 'RMSE': np.float32(42.364822), 'SAE': np.float32(0.016318768), 'NDE': np.float32(0.7321154)}

Run 43/72: hidden=256, seq_len=720, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000706
Validation Loss: 0.00066352
Epoch [2/300], Train Loss: 0.000671
Validation Loss: 0.00066210
Epoch [3/300], Train Loss: 0.000661
Validation Loss: 0.00066169
Epoch [4/300], Train Loss: 0.000658
Validation Loss: 0.00065851
Epoch [5/300], Train Loss: 0.000653
Validation Loss: 0.00064621
Epoch [6/300], Train Loss: 0.000638
Validation Loss: 0.00062319
Epoch [7/300], Train Loss: 0.000611
Validation Loss: 0.00061895
Epoch [8/300], Train Loss: 0.000617
Validation Loss: 0.00061463
Epoch [9/300], Train Loss: 0.000612
Validation Loss: 0.00060861
Epoch [10/300], Train Loss: 0.000612
Validation Loss: 0.00061375
Epoch [11/300], Train Loss: 0.000608
Validation Loss: 0.00061299
Epoch [12/300], Train Loss: 0.000613
Validation Loss: 0.00060464
Epoch [13/300], Train Loss: 0.000611
Validation Loss: 0.00060479
Epoch [14/300], Train Loss: 0.000609
Validation Loss: 0.00060548
Epoch [15/300], Train Loss: 0.000612
Validation Loss: 0.00061372
Epoch [16/300], Train Loss: 0.000610
Validation Loss: 0.00060410
Epoch [17/300], Train Loss: 0.000608
Validation Loss: 0.00060139
Epoch [18/300], Train Loss: 0.000598
Validation Loss: 0.00060244
Epoch [19/300], Train Loss: 0.000601
Validation Loss: 0.00060081
Epoch [20/300], Train Loss: 0.000597
Validation Loss: 0.00059864
Epoch [21/300], Train Loss: 0.000600
Validation Loss: 0.00059812
Epoch [22/300], Train Loss: 0.000601
Validation Loss: 0.00059934
Epoch [23/300], Train Loss: 0.000609
Validation Loss: 0.00059791
Epoch [24/300], Train Loss: 0.000603
Validation Loss: 0.00060006
Epoch [25/300], Train Loss: 0.000601
Validation Loss: 0.00059598
Epoch [26/300], Train Loss: 0.000599
Validation Loss: 0.00059739
Epoch [27/300], Train Loss: 0.000602
Validation Loss: 0.00059881
Epoch [28/300], Train Loss: 0.000604
Validation Loss: 0.00059723
Epoch [29/300], Train Loss: 0.000600
Validation Loss: 0.00059405
Epoch [30/300], Train Loss: 0.000603
Validation Loss: 0.00059390
Epoch [31/300], Train Loss: 0.000602
Validation Loss: 0.00059259
Epoch [32/300], Train Loss: 0.000597
Validation Loss: 0.00060084
Epoch [33/300], Train Loss: 0.000599
Validation Loss: 0.00059246
Epoch [34/300], Train Loss: 0.000598
Validation Loss: 0.00059130
Epoch [35/300], Train Loss: 0.000598
Validation Loss: 0.00059225
Epoch [36/300], Train Loss: 0.000601
Validation Loss: 0.00059375
Epoch [37/300], Train Loss: 0.000593
Validation Loss: 0.00058992
Epoch [38/300], Train Loss: 0.000597
Validation Loss: 0.00058988
Epoch [39/300], Train Loss: 0.000595
Validation Loss: 0.00058851
Epoch [40/300], Train Loss: 0.000589
Validation Loss: 0.00058757
Epoch [41/300], Train Loss: 0.000592
Validation Loss: 0.00058570
Epoch [42/300], Train Loss: 0.000590
Validation Loss: 0.00058350
Epoch [43/300], Train Loss: 0.000584
Validation Loss: 0.00058173
Epoch [44/300], Train Loss: 0.000584
Validation Loss: 0.00057722
Epoch [45/300], Train Loss: 0.000585
Validation Loss: 0.00058010
Epoch [46/300], Train Loss: 0.000583
Validation Loss: 0.00057266
Epoch [47/300], Train Loss: 0.000577
Validation Loss: 0.00057085
Epoch [48/300], Train Loss: 0.000581
Validation Loss: 0.00056984
Epoch [49/300], Train Loss: 0.000575
Validation Loss: 0.00056728
Epoch [50/300], Train Loss: 0.000574
Validation Loss: 0.00057255
Epoch [51/300], Train Loss: 0.000572
Validation Loss: 0.00056593
Epoch [52/300], Train Loss: 0.000571
Validation Loss: 0.00056799
Epoch [53/300], Train Loss: 0.000572
Validation Loss: 0.00056182
Epoch [54/300], Train Loss: 0.000562
Validation Loss: 0.00055844
Epoch [55/300], Train Loss: 0.000565
Validation Loss: 0.00055677
Epoch [56/300], Train Loss: 0.000562
Validation Loss: 0.00055573
Epoch [57/300], Train Loss: 0.000557
Validation Loss: 0.00055358
Epoch [58/300], Train Loss: 0.000562
Validation Loss: 0.00055359
Epoch [59/300], Train Loss: 0.000559
Validation Loss: 0.00055459
Epoch [60/300], Train Loss: 0.000568
Validation Loss: 0.00054908
Epoch [61/300], Train Loss: 0.000558
Validation Loss: 0.00055057
Epoch [62/300], Train Loss: 0.000556
Validation Loss: 0.00054606
Epoch [63/300], Train Loss: 0.000556
Validation Loss: 0.00054714
Epoch [64/300], Train Loss: 0.000552
Validation Loss: 0.00054623
Epoch [65/300], Train Loss: 0.000553
Validation Loss: 0.00054291
Epoch [66/300], Train Loss: 0.000551
Validation Loss: 0.00054417
Epoch [67/300], Train Loss: 0.000552
Validation Loss: 0.00053803
Epoch [68/300], Train Loss: 0.000549
Validation Loss: 0.00053554
Epoch [69/300], Train Loss: 0.000550
Validation Loss: 0.00053427
Epoch [70/300], Train Loss: 0.000541
Validation Loss: 0.00053241
Epoch [71/300], Train Loss: 0.000542
Validation Loss: 0.00054585
Epoch [72/300], Train Loss: 0.000550
Validation Loss: 0.00052974
Epoch [73/300], Train Loss: 0.000543
Validation Loss: 0.00052926
Epoch [74/300], Train Loss: 0.000541
Validation Loss: 0.00053134
Epoch [75/300], Train Loss: 0.000543
Validation Loss: 0.00052984
Epoch [76/300], Train Loss: 0.000534
Validation Loss: 0.00052732
Epoch [77/300], Train Loss: 0.000534
Validation Loss: 0.00053760
Epoch [78/300], Train Loss: 0.000535
Validation Loss: 0.00052755
Epoch [79/300], Train Loss: 0.000531
Validation Loss: 0.00052557
Epoch [80/300], Train Loss: 0.000535
Validation Loss: 0.00052511
Epoch [81/300], Train Loss: 0.000531
Validation Loss: 0.00051892
Epoch [82/300], Train Loss: 0.000533
Validation Loss: 0.00052946
Epoch [83/300], Train Loss: 0.000534
Validation Loss: 0.00052112
Epoch [84/300], Train Loss: 0.000525
Validation Loss: 0.00051968
Epoch [85/300], Train Loss: 0.000525
Validation Loss: 0.00051491
Epoch [86/300], Train Loss: 0.000526
Validation Loss: 0.00051533
Epoch [87/300], Train Loss: 0.000529
Validation Loss: 0.00052087
Epoch [88/300], Train Loss: 0.000530
Validation Loss: 0.00051179
Epoch [89/300], Train Loss: 0.000520
Validation Loss: 0.00051214
Epoch [90/300], Train Loss: 0.000524
Validation Loss: 0.00051350
Epoch [91/300], Train Loss: 0.000521
Validation Loss: 0.00050851
Epoch [92/300], Train Loss: 0.000522
Validation Loss: 0.00050846
Epoch [93/300], Train Loss: 0.000521
Validation Loss: 0.00051397
Epoch [94/300], Train Loss: 0.000521
Validation Loss: 0.00050583
Epoch [95/300], Train Loss: 0.000518
Validation Loss: 0.00051218
Epoch [96/300], Train Loss: 0.000513
Validation Loss: 0.00050110
Epoch [97/300], Train Loss: 0.000513
Validation Loss: 0.00050644
Epoch [98/300], Train Loss: 0.000514
Validation Loss: 0.00049973
Epoch [99/300], Train Loss: 0.000509
Validation Loss: 0.00050362
Epoch [100/300], Train Loss: 0.000514
Validation Loss: 0.00049805
Epoch [101/300], Train Loss: 0.000506
Validation Loss: 0.00048584
Epoch [102/300], Train Loss: 0.000502
Validation Loss: 0.00048173
Epoch [103/300], Train Loss: 0.000497
Validation Loss: 0.00048367
Epoch [104/300], Train Loss: 0.000493
Validation Loss: 0.00047120
Epoch [105/300], Train Loss: 0.000489
Validation Loss: 0.00047810
Epoch [106/300], Train Loss: 0.000489
Validation Loss: 0.00047698
Epoch [107/300], Train Loss: 0.000488
Validation Loss: 0.00047022
Epoch [108/300], Train Loss: 0.000478
Validation Loss: 0.00047171
Epoch [109/300], Train Loss: 0.000482
Validation Loss: 0.00047243
Epoch [110/300], Train Loss: 0.000487
Validation Loss: 0.00046343
Epoch [111/300], Train Loss: 0.000480
Validation Loss: 0.00047406
Epoch [112/300], Train Loss: 0.000474
Validation Loss: 0.00046364
Epoch [113/300], Train Loss: 0.000478
Validation Loss: 0.00046616
Epoch [114/300], Train Loss: 0.000480
Validation Loss: 0.00047798
Epoch [115/300], Train Loss: 0.000487
Validation Loss: 0.00046801
Epoch [116/300], Train Loss: 0.000475
Validation Loss: 0.00045772
Epoch [117/300], Train Loss: 0.000472
Validation Loss: 0.00046412
Epoch [118/300], Train Loss: 0.000471
Validation Loss: 0.00046809
Epoch [119/300], Train Loss: 0.000476
Validation Loss: 0.00046532
Epoch [120/300], Train Loss: 0.000473
Validation Loss: 0.00045639
Epoch [121/300], Train Loss: 0.000470
Validation Loss: 0.00045663
Epoch [122/300], Train Loss: 0.000470
Validation Loss: 0.00046393
Epoch [123/300], Train Loss: 0.000476
Validation Loss: 0.00045364
Epoch [124/300], Train Loss: 0.000471
Validation Loss: 0.00045229
Epoch [125/300], Train Loss: 0.000466
Validation Loss: 0.00045846
Epoch [126/300], Train Loss: 0.000471
Validation Loss: 0.00046493
Epoch [127/300], Train Loss: 0.000469
Validation Loss: 0.00045553
Epoch [128/300], Train Loss: 0.000469
Validation Loss: 0.00045749
Epoch [129/300], Train Loss: 0.000466
Validation Loss: 0.00045135
Epoch [130/300], Train Loss: 0.000464
Validation Loss: 0.00044911
Epoch [131/300], Train Loss: 0.000471
Validation Loss: 0.00045516
Epoch [132/300], Train Loss: 0.000464
Validation Loss: 0.00045269
Epoch [133/300], Train Loss: 0.000461
Validation Loss: 0.00044774
Epoch [134/300], Train Loss: 0.000460
Validation Loss: 0.00044740
Epoch [135/300], Train Loss: 0.000457
Validation Loss: 0.00045260
Epoch [136/300], Train Loss: 0.000464
Validation Loss: 0.00045742
Epoch [137/300], Train Loss: 0.000462
Validation Loss: 0.00044691
Epoch [138/300], Train Loss: 0.000455
Validation Loss: 0.00046069
Epoch [139/300], Train Loss: 0.000465
Validation Loss: 0.00044781
Epoch [140/300], Train Loss: 0.000455
Validation Loss: 0.00044601
Epoch [141/300], Train Loss: 0.000463
Validation Loss: 0.00045346
Epoch [142/300], Train Loss: 0.000466
Validation Loss: 0.00045898
Epoch [143/300], Train Loss: 0.000462
Validation Loss: 0.00044530
Epoch [144/300], Train Loss: 0.000462
Validation Loss: 0.00044317
Epoch [145/300], Train Loss: 0.000462
Validation Loss: 0.00044393
Epoch [146/300], Train Loss: 0.000454
Validation Loss: 0.00044629
Epoch [147/300], Train Loss: 0.000459
Validation Loss: 0.00044874
Epoch [148/300], Train Loss: 0.000457
Validation Loss: 0.00044904
Epoch [149/300], Train Loss: 0.000463
Validation Loss: 0.00044283
Epoch [150/300], Train Loss: 0.000454
Validation Loss: 0.00044832
Epoch [151/300], Train Loss: 0.000460
Validation Loss: 0.00044840
Epoch [152/300], Train Loss: 0.000455
Validation Loss: 0.00044089
Epoch [153/300], Train Loss: 0.000454
Validation Loss: 0.00043979
Epoch [154/300], Train Loss: 0.000451
Validation Loss: 0.00044158
Epoch [155/300], Train Loss: 0.000456
Validation Loss: 0.00044016
Epoch [156/300], Train Loss: 0.000459
Validation Loss: 0.00044129
Epoch [157/300], Train Loss: 0.000461
Validation Loss: 0.00044484
Epoch [158/300], Train Loss: 0.000456
Validation Loss: 0.00044950
Epoch [159/300], Train Loss: 0.000459
Validation Loss: 0.00044373
Epoch [160/300], Train Loss: 0.000451
Validation Loss: 0.00044048
Epoch [161/300], Train Loss: 0.000449
Validation Loss: 0.00044213
Epoch [162/300], Train Loss: 0.000446
Validation Loss: 0.00043757
Epoch [163/300], Train Loss: 0.000449
Validation Loss: 0.00043648
Epoch [164/300], Train Loss: 0.000447
Validation Loss: 0.00044337
Epoch [165/300], Train Loss: 0.000454
Validation Loss: 0.00044574
Epoch [166/300], Train Loss: 0.000449
Validation Loss: 0.00043607
Epoch [167/300], Train Loss: 0.000447
Validation Loss: 0.00043499
Epoch [168/300], Train Loss: 0.000449
Validation Loss: 0.00043824
Epoch [169/300], Train Loss: 0.000451
Validation Loss: 0.00043768
Epoch [170/300], Train Loss: 0.000448
Validation Loss: 0.00043556
Epoch [171/300], Train Loss: 0.000442
Validation Loss: 0.00043417
Epoch [172/300], Train Loss: 0.000445
Validation Loss: 0.00043334
Epoch [173/300], Train Loss: 0.000447
Validation Loss: 0.00043427
Epoch [174/300], Train Loss: 0.000450
Validation Loss: 0.00044399
Epoch [175/300], Train Loss: 0.000448
Validation Loss: 0.00043903
Epoch [176/300], Train Loss: 0.000444
Validation Loss: 0.00043093
Epoch [177/300], Train Loss: 0.000444
Validation Loss: 0.00043576
Epoch [178/300], Train Loss: 0.000442
Validation Loss: 0.00043139
Epoch [179/300], Train Loss: 0.000447
Validation Loss: 0.00043480
Epoch [180/300], Train Loss: 0.000443
Validation Loss: 0.00042946
Epoch [181/300], Train Loss: 0.000439
Validation Loss: 0.00042914
Epoch [182/300], Train Loss: 0.000439
Validation Loss: 0.00043212
Epoch [183/300], Train Loss: 0.000438
Validation Loss: 0.00043155
Epoch [184/300], Train Loss: 0.000438
Validation Loss: 0.00042781
Epoch [185/300], Train Loss: 0.000440
Validation Loss: 0.00043202
Epoch [186/300], Train Loss: 0.000439
Validation Loss: 0.00042748
Epoch [187/300], Train Loss: 0.000441
Validation Loss: 0.00043297
Epoch [188/300], Train Loss: 0.000439
Validation Loss: 0.00043085
Epoch [189/300], Train Loss: 0.000436
Validation Loss: 0.00042661
Epoch [190/300], Train Loss: 0.000436
Validation Loss: 0.00042663
Epoch [191/300], Train Loss: 0.000437
Validation Loss: 0.00042893
Epoch [192/300], Train Loss: 0.000440
Validation Loss: 0.00042579
Epoch [193/300], Train Loss: 0.000435
Validation Loss: 0.00043112
Epoch [194/300], Train Loss: 0.000438
Validation Loss: 0.00042419
Epoch [195/300], Train Loss: 0.000438
Validation Loss: 0.00042600
Epoch [196/300], Train Loss: 0.000433
Validation Loss: 0.00042660
Epoch [197/300], Train Loss: 0.000439
Validation Loss: 0.00042531
Epoch [198/300], Train Loss: 0.000433
Validation Loss: 0.00043181
Epoch [199/300], Train Loss: 0.000437
Validation Loss: 0.00042952
Epoch [200/300], Train Loss: 0.000436
Validation Loss: 0.00042551
Epoch [201/300], Train Loss: 0.000433
Validation Loss: 0.00043636
Epoch [202/300], Train Loss: 0.000450
Validation Loss: 0.00042623
Epoch [203/300], Train Loss: 0.000433
Validation Loss: 0.00042405
Epoch [204/300], Train Loss: 0.000436
Validation Loss: 0.00042588
Epoch [205/300], Train Loss: 0.000434
Validation Loss: 0.00041865
Epoch [206/300], Train Loss: 0.000433
Validation Loss: 0.00041996
Epoch [207/300], Train Loss: 0.000425
Validation Loss: 0.00042074
Epoch [208/300], Train Loss: 0.000430
Validation Loss: 0.00041685
Epoch [209/300], Train Loss: 0.000426
Validation Loss: 0.00042135
Epoch [210/300], Train Loss: 0.000433
Validation Loss: 0.00041807
Epoch [211/300], Train Loss: 0.000428
Validation Loss: 0.00042617
Epoch [212/300], Train Loss: 0.000431
Validation Loss: 0.00041605
Epoch [213/300], Train Loss: 0.000425
Validation Loss: 0.00041534
Epoch [214/300], Train Loss: 0.000427
Validation Loss: 0.00041412
Epoch [215/300], Train Loss: 0.000422
Validation Loss: 0.00041364
Epoch [216/300], Train Loss: 0.000422
Validation Loss: 0.00041375
Epoch [217/300], Train Loss: 0.000424
Validation Loss: 0.00041753
Epoch [218/300], Train Loss: 0.000425
Validation Loss: 0.00041310
Epoch [219/300], Train Loss: 0.000422
Validation Loss: 0.00041097
Epoch [220/300], Train Loss: 0.000421
Validation Loss: 0.00041384
Epoch [221/300], Train Loss: 0.000422
Validation Loss: 0.00041286
Epoch [222/300], Train Loss: 0.000419
Validation Loss: 0.00041600
Epoch [223/300], Train Loss: 0.000428
Validation Loss: 0.00040900
Epoch [224/300], Train Loss: 0.000416
Validation Loss: 0.00040795
Epoch [225/300], Train Loss: 0.000422
Validation Loss: 0.00041773
Epoch [226/300], Train Loss: 0.000419
Validation Loss: 0.00041104
Epoch [227/300], Train Loss: 0.000419
Validation Loss: 0.00042964
Epoch [228/300], Train Loss: 0.000430
Validation Loss: 0.00041197
Epoch [229/300], Train Loss: 0.000420
Validation Loss: 0.00042061
Epoch [230/300], Train Loss: 0.000415
Validation Loss: 0.00040535
Epoch [231/300], Train Loss: 0.000414
Validation Loss: 0.00040592
Epoch [232/300], Train Loss: 0.000412
Validation Loss: 0.00040056
Epoch [233/300], Train Loss: 0.000407
Validation Loss: 0.00040065
Epoch [234/300], Train Loss: 0.000408
Validation Loss: 0.00039956
Epoch [235/300], Train Loss: 0.000410
Validation Loss: 0.00041402
Epoch [236/300], Train Loss: 0.000416
Validation Loss: 0.00042232
Epoch [237/300], Train Loss: 0.000414
Validation Loss: 0.00039867
Epoch [238/300], Train Loss: 0.000409
Validation Loss: 0.00040435
Epoch [239/300], Train Loss: 0.000414
Validation Loss: 0.00039814
Epoch [240/300], Train Loss: 0.000405
Validation Loss: 0.00039579
Epoch [241/300], Train Loss: 0.000404
Validation Loss: 0.00039637
Epoch [242/300], Train Loss: 0.000402
Validation Loss: 0.00039663
Epoch [243/300], Train Loss: 0.000405
Validation Loss: 0.00039799
Epoch [244/300], Train Loss: 0.000404
Validation Loss: 0.00039632
Epoch [245/300], Train Loss: 0.000404
Validation Loss: 0.00039789
Epoch [246/300], Train Loss: 0.000406
Validation Loss: 0.00040125
Epoch [247/300], Train Loss: 0.000411
Validation Loss: 0.00039548
Epoch [248/300], Train Loss: 0.000405
Validation Loss: 0.00039518
Epoch [249/300], Train Loss: 0.000404
Validation Loss: 0.00040737
Epoch [250/300], Train Loss: 0.000407
Validation Loss: 0.00039519
Epoch [251/300], Train Loss: 0.000402
Validation Loss: 0.00039193
Epoch [252/300], Train Loss: 0.000404
Validation Loss: 0.00039061
Epoch [253/300], Train Loss: 0.000396
Validation Loss: 0.00039673
Epoch [254/300], Train Loss: 0.000400
Validation Loss: 0.00038956
Epoch [255/300], Train Loss: 0.000400
Validation Loss: 0.00039719
Epoch [256/300], Train Loss: 0.000408
Validation Loss: 0.00039714
Epoch [257/300], Train Loss: 0.000400
Validation Loss: 0.00039218
Epoch [258/300], Train Loss: 0.000403
Validation Loss: 0.00039388
Epoch [259/300], Train Loss: 0.000399
Validation Loss: 0.00040070
Epoch [260/300], Train Loss: 0.000402
Validation Loss: 0.00039544
Epoch [261/300], Train Loss: 0.000397
Validation Loss: 0.00039106
Epoch [262/300], Train Loss: 0.000397
Validation Loss: 0.00038722
Epoch [263/300], Train Loss: 0.000402
Validation Loss: 0.00038762
Epoch [264/300], Train Loss: 0.000394
Validation Loss: 0.00038559
Epoch [265/300], Train Loss: 0.000397
Validation Loss: 0.00038901
Epoch [266/300], Train Loss: 0.000401
Validation Loss: 0.00038896
Epoch [267/300], Train Loss: 0.000395
Validation Loss: 0.00038692
Epoch [268/300], Train Loss: 0.000393
Validation Loss: 0.00038629
Epoch [269/300], Train Loss: 0.000397
Validation Loss: 0.00038703
Epoch [270/300], Train Loss: 0.000395
Validation Loss: 0.00038624
Epoch [271/300], Train Loss: 0.000395
Validation Loss: 0.00038766
Epoch [272/300], Train Loss: 0.000398
Validation Loss: 0.00038526
Epoch [273/300], Train Loss: 0.000396
Validation Loss: 0.00038291
Epoch [274/300], Train Loss: 0.000395
Validation Loss: 0.00038342
Epoch [275/300], Train Loss: 0.000396
Validation Loss: 0.00038337
Epoch [276/300], Train Loss: 0.000393
Validation Loss: 0.00038111
Epoch [277/300], Train Loss: 0.000394
Validation Loss: 0.00038377
Epoch [278/300], Train Loss: 0.000395
Validation Loss: 0.00038280
Epoch [279/300], Train Loss: 0.000390
Validation Loss: 0.00038732
Epoch [280/300], Train Loss: 0.000392
Validation Loss: 0.00038640
Epoch [281/300], Train Loss: 0.000393
Validation Loss: 0.00038032
Epoch [282/300], Train Loss: 0.000391
Validation Loss: 0.00038345
Epoch [283/300], Train Loss: 0.000391
Validation Loss: 0.00038357
Epoch [284/300], Train Loss: 0.000389
Validation Loss: 0.00038543
Epoch [285/300], Train Loss: 0.000395
Validation Loss: 0.00038460
Epoch [286/300], Train Loss: 0.000401
Validation Loss: 0.00037934
Epoch [287/300], Train Loss: 0.000396
Validation Loss: 0.00038571
Epoch [288/300], Train Loss: 0.000396
Validation Loss: 0.00037989
Epoch [289/300], Train Loss: 0.000391
Validation Loss: 0.00037990
Epoch [290/300], Train Loss: 0.000390
Validation Loss: 0.00037820
Epoch [291/300], Train Loss: 0.000388
Validation Loss: 0.00038197
Epoch [292/300], Train Loss: 0.000388
Validation Loss: 0.00039736
Epoch [293/300], Train Loss: 0.000399
Validation Loss: 0.00038436
Epoch [294/300], Train Loss: 0.000392
Validation Loss: 0.00037777
Epoch [295/300], Train Loss: 0.000391
Validation Loss: 0.00037698
Epoch [296/300], Train Loss: 0.000389
Validation Loss: 0.00037743
Epoch [297/300], Train Loss: 0.000383
Validation Loss: 0.00037793
Epoch [298/300], Train Loss: 0.000388
Validation Loss: 0.00037811
Epoch [299/300], Train Loss: 0.000389
Validation Loss: 0.00038561
Epoch [300/300], Train Loss: 0.000390
Validation Loss: 0.00037585

Evaluating model for: Fridge
Run 43/72 completed in 6322.06 seconds with: {'MAE': np.float32(25.857117), 'MSE': np.float32(1278.3091), 'RMSE': np.float32(35.75345), 'SAE': np.float32(0.0048792), 'NDE': np.float32(0.61786264)}

Run 44/72: hidden=256, seq_len=720, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000790
Validation Loss: 0.00066228
Epoch [2/300], Train Loss: 0.000671
Validation Loss: 0.00066221
Epoch [3/300], Train Loss: 0.000662
Validation Loss: 0.00066217
Epoch [4/300], Train Loss: 0.000661
Validation Loss: 0.00066204
Epoch [5/300], Train Loss: 0.000662
Validation Loss: 0.00066195
Epoch [6/300], Train Loss: 0.000665
Validation Loss: 0.00066179
Epoch [7/300], Train Loss: 0.000652
Validation Loss: 0.00066199
Epoch [8/300], Train Loss: 0.000660
Validation Loss: 0.00066119
Epoch [9/300], Train Loss: 0.000656
Validation Loss: 0.00066073
Epoch [10/300], Train Loss: 0.000655
Validation Loss: 0.00065243
Epoch [11/300], Train Loss: 0.000628
Validation Loss: 0.00062976
Epoch [12/300], Train Loss: 0.000618
Validation Loss: 0.00061014
Epoch [13/300], Train Loss: 0.000612
Validation Loss: 0.00060506
Epoch [14/300], Train Loss: 0.000609
Validation Loss: 0.00060569
Epoch [15/300], Train Loss: 0.000611
Validation Loss: 0.00060690
Epoch [16/300], Train Loss: 0.000607
Validation Loss: 0.00060521
Epoch [17/300], Train Loss: 0.000606
Validation Loss: 0.00059919
Epoch [18/300], Train Loss: 0.000596
Validation Loss: 0.00059979
Epoch [19/300], Train Loss: 0.000598
Validation Loss: 0.00059729
Epoch [20/300], Train Loss: 0.000595
Validation Loss: 0.00059636
Epoch [21/300], Train Loss: 0.000597
Validation Loss: 0.00059477
Epoch [22/300], Train Loss: 0.000597
Validation Loss: 0.00059650
Epoch [23/300], Train Loss: 0.000606
Validation Loss: 0.00059297
Epoch [24/300], Train Loss: 0.000598
Validation Loss: 0.00060008
Epoch [25/300], Train Loss: 0.000596
Validation Loss: 0.00059071
Epoch [26/300], Train Loss: 0.000595
Validation Loss: 0.00059821
Epoch [27/300], Train Loss: 0.000599
Validation Loss: 0.00061240
Epoch [28/300], Train Loss: 0.000604
Validation Loss: 0.00058923
Epoch [29/300], Train Loss: 0.000596
Validation Loss: 0.00058812
Epoch [30/300], Train Loss: 0.000598
Validation Loss: 0.00058812
Epoch [31/300], Train Loss: 0.000598
Validation Loss: 0.00058900
Epoch [32/300], Train Loss: 0.000591
Validation Loss: 0.00060150
Epoch [33/300], Train Loss: 0.000594
Validation Loss: 0.00058402
Epoch [34/300], Train Loss: 0.000591
Validation Loss: 0.00058365
Epoch [35/300], Train Loss: 0.000591
Validation Loss: 0.00058422
Epoch [36/300], Train Loss: 0.000594
Validation Loss: 0.00058432
Epoch [37/300], Train Loss: 0.000586
Validation Loss: 0.00058293
Epoch [38/300], Train Loss: 0.000588
Validation Loss: 0.00057992
Epoch [39/300], Train Loss: 0.000587
Validation Loss: 0.00058225
Epoch [40/300], Train Loss: 0.000582
Validation Loss: 0.00057830
Epoch [41/300], Train Loss: 0.000586
Validation Loss: 0.00057513
Epoch [42/300], Train Loss: 0.000585
Validation Loss: 0.00057814
Epoch [43/300], Train Loss: 0.000580
Validation Loss: 0.00057550
Epoch [44/300], Train Loss: 0.000581
Validation Loss: 0.00058029
Epoch [45/300], Train Loss: 0.000584
Validation Loss: 0.00057889
Epoch [46/300], Train Loss: 0.000582
Validation Loss: 0.00057016
Epoch [47/300], Train Loss: 0.000575
Validation Loss: 0.00056992
Epoch [48/300], Train Loss: 0.000578
Validation Loss: 0.00056849
Epoch [49/300], Train Loss: 0.000575
Validation Loss: 0.00056880
Epoch [50/300], Train Loss: 0.000574
Validation Loss: 0.00058220
Epoch [51/300], Train Loss: 0.000572
Validation Loss: 0.00056701
Epoch [52/300], Train Loss: 0.000573
Validation Loss: 0.00057514
Epoch [53/300], Train Loss: 0.000579
Validation Loss: 0.00056520
Epoch [54/300], Train Loss: 0.000564
Validation Loss: 0.00056220
Epoch [55/300], Train Loss: 0.000565
Validation Loss: 0.00056209
Epoch [56/300], Train Loss: 0.000560
Validation Loss: 0.00055349
Epoch [57/300], Train Loss: 0.000553
Validation Loss: 0.00055280
Epoch [58/300], Train Loss: 0.000560
Validation Loss: 0.00055145
Epoch [59/300], Train Loss: 0.000555
Validation Loss: 0.00054240
Epoch [60/300], Train Loss: 0.000553
Validation Loss: 0.00053922
Epoch [61/300], Train Loss: 0.000549
Validation Loss: 0.00054437
Epoch [62/300], Train Loss: 0.000543
Validation Loss: 0.00053480
Epoch [63/300], Train Loss: 0.000541
Validation Loss: 0.00052902
Epoch [64/300], Train Loss: 0.000538
Validation Loss: 0.00052554
Epoch [65/300], Train Loss: 0.000537
Validation Loss: 0.00052792
Epoch [66/300], Train Loss: 0.000537
Validation Loss: 0.00051751
Epoch [67/300], Train Loss: 0.000532
Validation Loss: 0.00051815
Epoch [68/300], Train Loss: 0.000532
Validation Loss: 0.00051821
Epoch [69/300], Train Loss: 0.000534
Validation Loss: 0.00051430
Epoch [70/300], Train Loss: 0.000525
Validation Loss: 0.00051071
Epoch [71/300], Train Loss: 0.000524
Validation Loss: 0.00056606
Epoch [72/300], Train Loss: 0.000534
Validation Loss: 0.00050877
Epoch [73/300], Train Loss: 0.000523
Validation Loss: 0.00051709
Epoch [74/300], Train Loss: 0.000522
Validation Loss: 0.00050790
Epoch [75/300], Train Loss: 0.000524
Validation Loss: 0.00050227
Epoch [76/300], Train Loss: 0.000512
Validation Loss: 0.00049905
Epoch [77/300], Train Loss: 0.000514
Validation Loss: 0.00054256
Epoch [78/300], Train Loss: 0.000522
Validation Loss: 0.00049812
Epoch [79/300], Train Loss: 0.000506
Validation Loss: 0.00048981
Epoch [80/300], Train Loss: 0.000511
Validation Loss: 0.00051895
Epoch [81/300], Train Loss: 0.000519
Validation Loss: 0.00051854
Epoch [82/300], Train Loss: 0.000514
Validation Loss: 0.00050449
Epoch [83/300], Train Loss: 0.000519
Validation Loss: 0.00048913
Epoch [84/300], Train Loss: 0.000500
Validation Loss: 0.00049531
Epoch [85/300], Train Loss: 0.000522
Validation Loss: 0.00060277
Epoch [86/300], Train Loss: 0.000553
Validation Loss: 0.00050845
Epoch [87/300], Train Loss: 0.000516
Validation Loss: 0.00049665
Epoch [88/300], Train Loss: 0.000505
Validation Loss: 0.00048486
Epoch [89/300], Train Loss: 0.000494
Validation Loss: 0.00048282
Epoch [90/300], Train Loss: 0.000503
Validation Loss: 0.00048715
Epoch [91/300], Train Loss: 0.000498
Validation Loss: 0.00048344
Epoch [92/300], Train Loss: 0.000498
Validation Loss: 0.00047995
Epoch [93/300], Train Loss: 0.000500
Validation Loss: 0.00048037
Epoch [94/300], Train Loss: 0.000497
Validation Loss: 0.00051330
Epoch [95/300], Train Loss: 0.000529
Validation Loss: 0.00058270
Epoch [96/300], Train Loss: 0.000513
Validation Loss: 0.00047984
Epoch [97/300], Train Loss: 0.000493
Validation Loss: 0.00047406
Epoch [98/300], Train Loss: 0.000513
Validation Loss: 0.00048640
Epoch [99/300], Train Loss: 0.000499
Validation Loss: 0.00047317
Epoch [100/300], Train Loss: 0.000499
Validation Loss: 0.00047658
Epoch [101/300], Train Loss: 0.000489
Validation Loss: 0.00047126
Epoch [102/300], Train Loss: 0.000488
Validation Loss: 0.00047059
Epoch [103/300], Train Loss: 0.000489
Validation Loss: 0.00047110
Epoch [104/300], Train Loss: 0.000489
Validation Loss: 0.00048111
Epoch [105/300], Train Loss: 0.000487
Validation Loss: 0.00046841
Epoch [106/300], Train Loss: 0.000486
Validation Loss: 0.00047797
Epoch [107/300], Train Loss: 0.000483
Validation Loss: 0.00046802
Epoch [108/300], Train Loss: 0.000482
Validation Loss: 0.00046804
Epoch [109/300], Train Loss: 0.000489
Validation Loss: 0.00046491
Epoch [110/300], Train Loss: 0.000491
Validation Loss: 0.00052149
Epoch [111/300], Train Loss: 0.000510
Validation Loss: 0.00046668
Epoch [112/300], Train Loss: 0.000475
Validation Loss: 0.00046538
Epoch [113/300], Train Loss: 0.000478
Validation Loss: 0.00046451
Epoch [114/300], Train Loss: 0.000482
Validation Loss: 0.00045887
Epoch [115/300], Train Loss: 0.000479
Validation Loss: 0.00046587
Epoch [116/300], Train Loss: 0.000477
Validation Loss: 0.00046994
Epoch [117/300], Train Loss: 0.000471
Validation Loss: 0.00045341
Epoch [118/300], Train Loss: 0.000469
Validation Loss: 0.00046730
Epoch [119/300], Train Loss: 0.000468
Validation Loss: 0.00046705
Epoch [120/300], Train Loss: 0.000467
Validation Loss: 0.00046458
Epoch [121/300], Train Loss: 0.000472
Validation Loss: 0.00045199
Epoch [122/300], Train Loss: 0.000462
Validation Loss: 0.00044460
Epoch [123/300], Train Loss: 0.000465
Validation Loss: 0.00044669
Epoch [124/300], Train Loss: 0.000465
Validation Loss: 0.00044575
Epoch [125/300], Train Loss: 0.000457
Validation Loss: 0.00045389
Epoch [126/300], Train Loss: 0.000470
Validation Loss: 0.00044866
Epoch [127/300], Train Loss: 0.000464
Validation Loss: 0.00046377
Epoch [128/300], Train Loss: 0.000473
Validation Loss: 0.00044899
Epoch [129/300], Train Loss: 0.000458
Validation Loss: 0.00044399
Epoch [130/300], Train Loss: 0.000475
Validation Loss: 0.00044891
Epoch [131/300], Train Loss: 0.000458
Validation Loss: 0.00045018
Epoch [132/300], Train Loss: 0.000455
Validation Loss: 0.00043863
Epoch [133/300], Train Loss: 0.000460
Validation Loss: 0.00044642
Epoch [134/300], Train Loss: 0.000454
Validation Loss: 0.00044001
Epoch [135/300], Train Loss: 0.000449
Validation Loss: 0.00044199
Epoch [136/300], Train Loss: 0.000452
Validation Loss: 0.00044013
Epoch [137/300], Train Loss: 0.000456
Validation Loss: 0.00043931
Epoch [138/300], Train Loss: 0.000442
Validation Loss: 0.00044943
Epoch [139/300], Train Loss: 0.000455
Validation Loss: 0.00045424
Epoch [140/300], Train Loss: 0.000453
Validation Loss: 0.00043453
Epoch [141/300], Train Loss: 0.000454
Validation Loss: 0.00044816
Epoch [142/300], Train Loss: 0.000457
Validation Loss: 0.00047744
Epoch [143/300], Train Loss: 0.000492
Validation Loss: 0.00045904
Epoch [144/300], Train Loss: 0.000461
Validation Loss: 0.00043973
Epoch [145/300], Train Loss: 0.000453
Validation Loss: 0.00043066
Epoch [146/300], Train Loss: 0.000466
Validation Loss: 0.00046157
Epoch [147/300], Train Loss: 0.000458
Validation Loss: 0.00043113
Epoch [148/300], Train Loss: 0.000441
Validation Loss: 0.00047575
Epoch [149/300], Train Loss: 0.000468
Validation Loss: 0.00043735
Epoch [150/300], Train Loss: 0.000512
Validation Loss: 0.00046466
Epoch [151/300], Train Loss: 0.000465
Validation Loss: 0.00043772
Epoch [152/300], Train Loss: 0.000451
Validation Loss: 0.00043218
Epoch [153/300], Train Loss: 0.000449
Validation Loss: 0.00043461
Epoch [154/300], Train Loss: 0.000440
Validation Loss: 0.00043003
Epoch [155/300], Train Loss: 0.000441
Validation Loss: 0.00042972
Epoch [156/300], Train Loss: 0.000455
Validation Loss: 0.00043484
Epoch [157/300], Train Loss: 0.000446
Validation Loss: 0.00042946
Epoch [158/300], Train Loss: 0.000446
Validation Loss: 0.00046975
Epoch [159/300], Train Loss: 0.000447
Validation Loss: 0.00042323
Epoch [160/300], Train Loss: 0.000432
Validation Loss: 0.00044020
Epoch [161/300], Train Loss: 0.000439
Validation Loss: 0.00049239
Epoch [162/300], Train Loss: 0.000446
Validation Loss: 0.00042197
Epoch [163/300], Train Loss: 0.000450
Validation Loss: 0.00042758
Epoch [164/300], Train Loss: 0.000437
Validation Loss: 0.00042789
Epoch [165/300], Train Loss: 0.000433
Validation Loss: 0.00042792
Epoch [166/300], Train Loss: 0.000428
Validation Loss: 0.00042556
Epoch [167/300], Train Loss: 0.000437
Validation Loss: 0.00041898
Epoch [168/300], Train Loss: 0.000435
Validation Loss: 0.00043358
Epoch [169/300], Train Loss: 0.000436
Validation Loss: 0.00042760
Epoch [170/300], Train Loss: 0.000429
Validation Loss: 0.00041691
Epoch [171/300], Train Loss: 0.000428
Validation Loss: 0.00041940
Epoch [172/300], Train Loss: 0.000428
Validation Loss: 0.00042293
Epoch [173/300], Train Loss: 0.000440
Validation Loss: 0.00043582
Epoch [174/300], Train Loss: 0.000436
Validation Loss: 0.00043428
Epoch [175/300], Train Loss: 0.000431
Validation Loss: 0.00041791
Epoch [176/300], Train Loss: 0.000425
Validation Loss: 0.00041231
Epoch [177/300], Train Loss: 0.000424
Validation Loss: 0.00041855
Epoch [178/300], Train Loss: 0.000423
Validation Loss: 0.00043072
Epoch [179/300], Train Loss: 0.000429
Validation Loss: 0.00043734
Epoch [180/300], Train Loss: 0.000434
Validation Loss: 0.00041194
Epoch [181/300], Train Loss: 0.000419
Validation Loss: 0.00041180
Epoch [182/300], Train Loss: 0.000418
Validation Loss: 0.00041035
Epoch [183/300], Train Loss: 0.000418
Validation Loss: 0.00040905
Epoch [184/300], Train Loss: 0.000416
Validation Loss: 0.00040757
Epoch [185/300], Train Loss: 0.000419
Validation Loss: 0.00041423
Epoch [186/300], Train Loss: 0.000419
Validation Loss: 0.00040971
Epoch [187/300], Train Loss: 0.000428
Validation Loss: 0.00041355
Epoch [188/300], Train Loss: 0.000430
Validation Loss: 0.00042115
Epoch [189/300], Train Loss: 0.000417
Validation Loss: 0.00046552
Epoch [190/300], Train Loss: 0.000434
Validation Loss: 0.00041486
Epoch [191/300], Train Loss: 0.000419
Validation Loss: 0.00041821
Epoch [192/300], Train Loss: 0.000421
Validation Loss: 0.00040794
Epoch [193/300], Train Loss: 0.000417
Validation Loss: 0.00041191
Epoch [194/300], Train Loss: 0.000418
Validation Loss: 0.00040360
Epoch [195/300], Train Loss: 0.000416
Validation Loss: 0.00040818
Epoch [196/300], Train Loss: 0.000413
Validation Loss: 0.00040392
Epoch [197/300], Train Loss: 0.000413
Validation Loss: 0.00040122
Epoch [198/300], Train Loss: 0.000412
Validation Loss: 0.00040440
Epoch [199/300], Train Loss: 0.000410
Validation Loss: 0.00040067
Epoch [200/300], Train Loss: 0.000410
Validation Loss: 0.00040715
Epoch [201/300], Train Loss: 0.000416
Validation Loss: 0.00039918
Epoch [202/300], Train Loss: 0.000413
Validation Loss: 0.00041190
Epoch [203/300], Train Loss: 0.000414
Validation Loss: 0.00040247
Epoch [204/300], Train Loss: 0.000414
Validation Loss: 0.00039893
Epoch [205/300], Train Loss: 0.000408
Validation Loss: 0.00039626
Epoch [206/300], Train Loss: 0.000414
Validation Loss: 0.00041609
Epoch [207/300], Train Loss: 0.000407
Validation Loss: 0.00039494
Epoch [208/300], Train Loss: 0.000409
Validation Loss: 0.00039562
Epoch [209/300], Train Loss: 0.000403
Validation Loss: 0.00041117
Epoch [210/300], Train Loss: 0.000414
Validation Loss: 0.00040142
Epoch [211/300], Train Loss: 0.000411
Validation Loss: 0.00041234
Epoch [212/300], Train Loss: 0.000408
Validation Loss: 0.00039518
Epoch [213/300], Train Loss: 0.000406
Validation Loss: 0.00040280
Epoch [214/300], Train Loss: 0.000407
Validation Loss: 0.00039765
Epoch [215/300], Train Loss: 0.000403
Validation Loss: 0.00039472
Epoch [216/300], Train Loss: 0.000404
Validation Loss: 0.00039796
Epoch [217/300], Train Loss: 0.000404
Validation Loss: 0.00039582
Epoch [218/300], Train Loss: 0.000405
Validation Loss: 0.00039359
Epoch [219/300], Train Loss: 0.000405
Validation Loss: 0.00039308
Epoch [220/300], Train Loss: 0.000407
Validation Loss: 0.00041332
Epoch [221/300], Train Loss: 0.000413
Validation Loss: 0.00039675
Epoch [222/300], Train Loss: 0.000404
Validation Loss: 0.00039597
Epoch [223/300], Train Loss: 0.000408
Validation Loss: 0.00038962
Epoch [224/300], Train Loss: 0.000397
Validation Loss: 0.00039648
Epoch [225/300], Train Loss: 0.000399
Validation Loss: 0.00040770
Epoch [226/300], Train Loss: 0.000399
Validation Loss: 0.00039211
Epoch [227/300], Train Loss: 0.000405
Validation Loss: 0.00040068
Epoch [228/300], Train Loss: 0.000409
Validation Loss: 0.00038950
Epoch [229/300], Train Loss: 0.000403
Validation Loss: 0.00040430
Epoch [230/300], Train Loss: 0.000399
Validation Loss: 0.00039411
Epoch [231/300], Train Loss: 0.000399
Validation Loss: 0.00039277
Epoch [232/300], Train Loss: 0.000398
Validation Loss: 0.00038636
Epoch [233/300], Train Loss: 0.000395
Validation Loss: 0.00038792
Epoch [234/300], Train Loss: 0.000395
Validation Loss: 0.00038964
Epoch [235/300], Train Loss: 0.000393
Validation Loss: 0.00040661
Epoch [236/300], Train Loss: 0.000399
Validation Loss: 0.00039071
Epoch [237/300], Train Loss: 0.000396
Validation Loss: 0.00038872
Epoch [238/300], Train Loss: 0.000395
Validation Loss: 0.00039451
Epoch [239/300], Train Loss: 0.000401
Validation Loss: 0.00039346
Epoch [240/300], Train Loss: 0.000395
Validation Loss: 0.00038445
Epoch [241/300], Train Loss: 0.000390
Validation Loss: 0.00038558
Epoch [242/300], Train Loss: 0.000389
Validation Loss: 0.00039076
Epoch [243/300], Train Loss: 0.000392
Validation Loss: 0.00038495
Epoch [244/300], Train Loss: 0.000395
Validation Loss: 0.00041164
Epoch [245/300], Train Loss: 0.000405
Validation Loss: 0.00039044
Epoch [246/300], Train Loss: 0.000400
Validation Loss: 0.00039122
Epoch [247/300], Train Loss: 0.000398
Validation Loss: 0.00038937
Epoch [248/300], Train Loss: 0.000392
Validation Loss: 0.00038573
Epoch [249/300], Train Loss: 0.000396
Validation Loss: 0.00040048
Epoch [250/300], Train Loss: 0.000399
Validation Loss: 0.00038483
Early stopping triggered

Evaluating model for: Fridge
Run 44/72 completed in 7022.66 seconds with: {'MAE': np.float32(25.4644), 'MSE': np.float32(1305.5717), 'RMSE': np.float32(36.132694), 'SAE': np.float32(0.040215336), 'NDE': np.float32(0.6244165)}

Run 45/72: hidden=256, seq_len=720, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.001112
Validation Loss: 0.00075619
Epoch [2/300], Train Loss: 0.000689
Validation Loss: 0.00067745
Epoch [3/300], Train Loss: 0.000671
Validation Loss: 0.00067034
Epoch [4/300], Train Loss: 0.000671
Validation Loss: 0.00066886
Epoch [5/300], Train Loss: 0.000668
Validation Loss: 0.00066805
Epoch [6/300], Train Loss: 0.000668
Validation Loss: 0.00066804
Epoch [7/300], Train Loss: 0.000666
Validation Loss: 0.00066651
Epoch [8/300], Train Loss: 0.000665
Validation Loss: 0.00066550
Epoch [9/300], Train Loss: 0.000665
Validation Loss: 0.00066451
Epoch [10/300], Train Loss: 0.000658
Validation Loss: 0.00066324
Epoch [11/300], Train Loss: 0.000660
Validation Loss: 0.00066235
Epoch [12/300], Train Loss: 0.000661
Validation Loss: 0.00066077
Epoch [13/300], Train Loss: 0.000656
Validation Loss: 0.00065906
Epoch [14/300], Train Loss: 0.000653
Validation Loss: 0.00065673
Epoch [15/300], Train Loss: 0.000651
Validation Loss: 0.00065537
Epoch [16/300], Train Loss: 0.000644
Validation Loss: 0.00065325
Epoch [17/300], Train Loss: 0.000643
Validation Loss: 0.00065063
Epoch [18/300], Train Loss: 0.000631
Validation Loss: 0.00064918
Epoch [19/300], Train Loss: 0.000634
Validation Loss: 0.00064688
Epoch [20/300], Train Loss: 0.000626
Validation Loss: 0.00064351
Epoch [21/300], Train Loss: 0.000626
Validation Loss: 0.00063841
Epoch [22/300], Train Loss: 0.000622
Validation Loss: 0.00063109
Epoch [23/300], Train Loss: 0.000618
Validation Loss: 0.00062766
Epoch [24/300], Train Loss: 0.000620
Validation Loss: 0.00062394
Epoch [25/300], Train Loss: 0.000612
Validation Loss: 0.00062410
Epoch [26/300], Train Loss: 0.000612
Validation Loss: 0.00062095
Epoch [27/300], Train Loss: 0.000611
Validation Loss: 0.00062068
Epoch [28/300], Train Loss: 0.000608
Validation Loss: 0.00061824
Epoch [29/300], Train Loss: 0.000610
Validation Loss: 0.00061843
Epoch [30/300], Train Loss: 0.000610
Validation Loss: 0.00061981
Epoch [31/300], Train Loss: 0.000606
Validation Loss: 0.00061658
Epoch [32/300], Train Loss: 0.000608
Validation Loss: 0.00061531
Epoch [33/300], Train Loss: 0.000604
Validation Loss: 0.00061720
Epoch [34/300], Train Loss: 0.000606
Validation Loss: 0.00061510
Epoch [35/300], Train Loss: 0.000604
Validation Loss: 0.00061384
Epoch [36/300], Train Loss: 0.000604
Validation Loss: 0.00061240
Epoch [37/300], Train Loss: 0.000603
Validation Loss: 0.00061165
Epoch [38/300], Train Loss: 0.000607
Validation Loss: 0.00061139
Epoch [39/300], Train Loss: 0.000599
Validation Loss: 0.00061245
Epoch [40/300], Train Loss: 0.000601
Validation Loss: 0.00061056
Epoch [41/300], Train Loss: 0.000603
Validation Loss: 0.00061236
Epoch [42/300], Train Loss: 0.000601
Validation Loss: 0.00060957
Epoch [43/300], Train Loss: 0.000601
Validation Loss: 0.00060821
Epoch [44/300], Train Loss: 0.000604
Validation Loss: 0.00060968
Epoch [45/300], Train Loss: 0.000600
Validation Loss: 0.00060847
Epoch [46/300], Train Loss: 0.000596
Validation Loss: 0.00060883
Epoch [47/300], Train Loss: 0.000600
Validation Loss: 0.00060702
Epoch [48/300], Train Loss: 0.000597
Validation Loss: 0.00061119
Epoch [49/300], Train Loss: 0.000598
Validation Loss: 0.00060671
Epoch [50/300], Train Loss: 0.000601
Validation Loss: 0.00060545
Epoch [51/300], Train Loss: 0.000599
Validation Loss: 0.00060547
Epoch [52/300], Train Loss: 0.000600
Validation Loss: 0.00060493
Epoch [53/300], Train Loss: 0.000595
Validation Loss: 0.00060545
Epoch [54/300], Train Loss: 0.000594
Validation Loss: 0.00060467
Epoch [55/300], Train Loss: 0.000597
Validation Loss: 0.00060823
Epoch [56/300], Train Loss: 0.000595
Validation Loss: 0.00060417
Epoch [57/300], Train Loss: 0.000598
Validation Loss: 0.00060272
Epoch [58/300], Train Loss: 0.000596
Validation Loss: 0.00060200
Epoch [59/300], Train Loss: 0.000593
Validation Loss: 0.00060140
Epoch [60/300], Train Loss: 0.000598
Validation Loss: 0.00060156
Epoch [61/300], Train Loss: 0.000592
Validation Loss: 0.00060039
Epoch [62/300], Train Loss: 0.000594
Validation Loss: 0.00060138
Epoch [63/300], Train Loss: 0.000596
Validation Loss: 0.00060132
Epoch [64/300], Train Loss: 0.000592
Validation Loss: 0.00060027
Epoch [65/300], Train Loss: 0.000591
Validation Loss: 0.00059987
Epoch [66/300], Train Loss: 0.000593
Validation Loss: 0.00059945
Epoch [67/300], Train Loss: 0.000594
Validation Loss: 0.00059880
Epoch [68/300], Train Loss: 0.000591
Validation Loss: 0.00059823
Epoch [69/300], Train Loss: 0.000593
Validation Loss: 0.00059772
Epoch [70/300], Train Loss: 0.000590
Validation Loss: 0.00059879
Epoch [71/300], Train Loss: 0.000592
Validation Loss: 0.00059720
Epoch [72/300], Train Loss: 0.000592
Validation Loss: 0.00059631
Epoch [73/300], Train Loss: 0.000593
Validation Loss: 0.00059752
Epoch [74/300], Train Loss: 0.000593
Validation Loss: 0.00059691
Epoch [75/300], Train Loss: 0.000594
Validation Loss: 0.00059624
Epoch [76/300], Train Loss: 0.000590
Validation Loss: 0.00059470
Epoch [77/300], Train Loss: 0.000591
Validation Loss: 0.00059547
Epoch [78/300], Train Loss: 0.000592
Validation Loss: 0.00059467
Epoch [79/300], Train Loss: 0.000591
Validation Loss: 0.00059547
Epoch [80/300], Train Loss: 0.000592
Validation Loss: 0.00059431
Epoch [81/300], Train Loss: 0.000591
Validation Loss: 0.00059455
Epoch [82/300], Train Loss: 0.000588
Validation Loss: 0.00059400
Epoch [83/300], Train Loss: 0.000592
Validation Loss: 0.00059588
Epoch [84/300], Train Loss: 0.000587
Validation Loss: 0.00059403
Epoch [85/300], Train Loss: 0.000589
Validation Loss: 0.00059252
Epoch [86/300], Train Loss: 0.000585
Validation Loss: 0.00059253
Epoch [87/300], Train Loss: 0.000583
Validation Loss: 0.00059246
Epoch [88/300], Train Loss: 0.000589
Validation Loss: 0.00059189
Epoch [89/300], Train Loss: 0.000589
Validation Loss: 0.00059104
Epoch [90/300], Train Loss: 0.000590
Validation Loss: 0.00059242
Epoch [91/300], Train Loss: 0.000590
Validation Loss: 0.00059099
Epoch [92/300], Train Loss: 0.000587
Validation Loss: 0.00059236
Epoch [93/300], Train Loss: 0.000587
Validation Loss: 0.00059187
Epoch [94/300], Train Loss: 0.000584
Validation Loss: 0.00059020
Epoch [95/300], Train Loss: 0.000589
Validation Loss: 0.00059009
Epoch [96/300], Train Loss: 0.000589
Validation Loss: 0.00058903
Epoch [97/300], Train Loss: 0.000589
Validation Loss: 0.00058936
Epoch [98/300], Train Loss: 0.000583
Validation Loss: 0.00059057
Epoch [99/300], Train Loss: 0.000585
Validation Loss: 0.00058786
Epoch [100/300], Train Loss: 0.000586
Validation Loss: 0.00058866
Epoch [101/300], Train Loss: 0.000587
Validation Loss: 0.00058912
Epoch [102/300], Train Loss: 0.000585
Validation Loss: 0.00058653
Epoch [103/300], Train Loss: 0.000585
Validation Loss: 0.00058701
Epoch [104/300], Train Loss: 0.000584
Validation Loss: 0.00058739
Epoch [105/300], Train Loss: 0.000588
Validation Loss: 0.00058650
Epoch [106/300], Train Loss: 0.000582
Validation Loss: 0.00058614
Epoch [107/300], Train Loss: 0.000587
Validation Loss: 0.00058626
Epoch [108/300], Train Loss: 0.000583
Validation Loss: 0.00058566
Epoch [109/300], Train Loss: 0.000578
Validation Loss: 0.00058505
Epoch [110/300], Train Loss: 0.000584
Validation Loss: 0.00058491
Epoch [111/300], Train Loss: 0.000582
Validation Loss: 0.00058514
Epoch [112/300], Train Loss: 0.000584
Validation Loss: 0.00058453
Epoch [113/300], Train Loss: 0.000584
Validation Loss: 0.00058395
Epoch [114/300], Train Loss: 0.000582
Validation Loss: 0.00058439
Epoch [115/300], Train Loss: 0.000581
Validation Loss: 0.00058444
Epoch [116/300], Train Loss: 0.000581
Validation Loss: 0.00058342
Epoch [117/300], Train Loss: 0.000579
Validation Loss: 0.00058297
Epoch [118/300], Train Loss: 0.000583
Validation Loss: 0.00058379
Epoch [119/300], Train Loss: 0.000583
Validation Loss: 0.00058260
Epoch [120/300], Train Loss: 0.000579
Validation Loss: 0.00058225
Epoch [121/300], Train Loss: 0.000579
Validation Loss: 0.00058236
Epoch [122/300], Train Loss: 0.000582
Validation Loss: 0.00058186
Epoch [123/300], Train Loss: 0.000583
Validation Loss: 0.00058180
Epoch [124/300], Train Loss: 0.000581
Validation Loss: 0.00058122
Epoch [125/300], Train Loss: 0.000581
Validation Loss: 0.00058205
Epoch [126/300], Train Loss: 0.000581
Validation Loss: 0.00058198
Epoch [127/300], Train Loss: 0.000581
Validation Loss: 0.00058045
Epoch [128/300], Train Loss: 0.000579
Validation Loss: 0.00058133
Epoch [129/300], Train Loss: 0.000577
Validation Loss: 0.00058073
Epoch [130/300], Train Loss: 0.000579
Validation Loss: 0.00058098
Epoch [131/300], Train Loss: 0.000577
Validation Loss: 0.00058061
Epoch [132/300], Train Loss: 0.000579
Validation Loss: 0.00057991
Epoch [133/300], Train Loss: 0.000577
Validation Loss: 0.00058043
Epoch [134/300], Train Loss: 0.000579
Validation Loss: 0.00057920
Epoch [135/300], Train Loss: 0.000578
Validation Loss: 0.00057821
Epoch [136/300], Train Loss: 0.000581
Validation Loss: 0.00057930
Epoch [137/300], Train Loss: 0.000575
Validation Loss: 0.00058000
Epoch [138/300], Train Loss: 0.000582
Validation Loss: 0.00057723
Epoch [139/300], Train Loss: 0.000576
Validation Loss: 0.00057778
Epoch [140/300], Train Loss: 0.000580
Validation Loss: 0.00057794
Epoch [141/300], Train Loss: 0.000577
Validation Loss: 0.00058063
Epoch [142/300], Train Loss: 0.000578
Validation Loss: 0.00057655
Epoch [143/300], Train Loss: 0.000577
Validation Loss: 0.00057821
Epoch [144/300], Train Loss: 0.000574
Validation Loss: 0.00057660
Epoch [145/300], Train Loss: 0.000578
Validation Loss: 0.00057724
Epoch [146/300], Train Loss: 0.000576
Validation Loss: 0.00057536
Epoch [147/300], Train Loss: 0.000575
Validation Loss: 0.00057783
Epoch [148/300], Train Loss: 0.000579
Validation Loss: 0.00057499
Epoch [149/300], Train Loss: 0.000575
Validation Loss: 0.00057575
Epoch [150/300], Train Loss: 0.000577
Validation Loss: 0.00057625
Epoch [151/300], Train Loss: 0.000575
Validation Loss: 0.00057512
Epoch [152/300], Train Loss: 0.000579
Validation Loss: 0.00057526
Epoch [153/300], Train Loss: 0.000575
Validation Loss: 0.00057426
Epoch [154/300], Train Loss: 0.000574
Validation Loss: 0.00057531
Epoch [155/300], Train Loss: 0.000575
Validation Loss: 0.00057367
Epoch [156/300], Train Loss: 0.000573
Validation Loss: 0.00057380
Epoch [157/300], Train Loss: 0.000576
Validation Loss: 0.00057402
Epoch [158/300], Train Loss: 0.000572
Validation Loss: 0.00057406
Epoch [159/300], Train Loss: 0.000573
Validation Loss: 0.00057423
Epoch [160/300], Train Loss: 0.000571
Validation Loss: 0.00057401
Epoch [161/300], Train Loss: 0.000572
Validation Loss: 0.00057309
Epoch [162/300], Train Loss: 0.000568
Validation Loss: 0.00057170
Epoch [163/300], Train Loss: 0.000576
Validation Loss: 0.00057149
Epoch [164/300], Train Loss: 0.000572
Validation Loss: 0.00057385
Epoch [165/300], Train Loss: 0.000572
Validation Loss: 0.00057230
Epoch [166/300], Train Loss: 0.000572
Validation Loss: 0.00057088
Epoch [167/300], Train Loss: 0.000571
Validation Loss: 0.00057103
Epoch [168/300], Train Loss: 0.000571
Validation Loss: 0.00057278
Epoch [169/300], Train Loss: 0.000574
Validation Loss: 0.00057077
Epoch [170/300], Train Loss: 0.000573
Validation Loss: 0.00056987
Epoch [171/300], Train Loss: 0.000574
Validation Loss: 0.00057062
Epoch [172/300], Train Loss: 0.000569
Validation Loss: 0.00057012
Epoch [173/300], Train Loss: 0.000575
Validation Loss: 0.00056959
Epoch [174/300], Train Loss: 0.000571
Validation Loss: 0.00057063
Epoch [175/300], Train Loss: 0.000571
Validation Loss: 0.00056829
Epoch [176/300], Train Loss: 0.000570
Validation Loss: 0.00056847
Epoch [177/300], Train Loss: 0.000569
Validation Loss: 0.00056801
Epoch [178/300], Train Loss: 0.000573
Validation Loss: 0.00056837
Epoch [179/300], Train Loss: 0.000570
Validation Loss: 0.00056692
Epoch [180/300], Train Loss: 0.000570
Validation Loss: 0.00056753
Epoch [181/300], Train Loss: 0.000570
Validation Loss: 0.00056724
Epoch [182/300], Train Loss: 0.000567
Validation Loss: 0.00056663
Epoch [183/300], Train Loss: 0.000570
Validation Loss: 0.00056622
Epoch [184/300], Train Loss: 0.000567
Validation Loss: 0.00056619
Epoch [185/300], Train Loss: 0.000570
Validation Loss: 0.00056509
Epoch [186/300], Train Loss: 0.000566
Validation Loss: 0.00056468
Epoch [187/300], Train Loss: 0.000566
Validation Loss: 0.00056537
Epoch [188/300], Train Loss: 0.000568
Validation Loss: 0.00056449
Epoch [189/300], Train Loss: 0.000566
Validation Loss: 0.00056517
Epoch [190/300], Train Loss: 0.000568
Validation Loss: 0.00056462
Epoch [191/300], Train Loss: 0.000570
Validation Loss: 0.00056425
Epoch [192/300], Train Loss: 0.000569
Validation Loss: 0.00056413
Epoch [193/300], Train Loss: 0.000567
Validation Loss: 0.00056384
Epoch [194/300], Train Loss: 0.000565
Validation Loss: 0.00056251
Epoch [195/300], Train Loss: 0.000564
Validation Loss: 0.00056501
Epoch [196/300], Train Loss: 0.000567
Validation Loss: 0.00056210
Epoch [197/300], Train Loss: 0.000561
Validation Loss: 0.00056237
Epoch [198/300], Train Loss: 0.000562
Validation Loss: 0.00056129
Epoch [199/300], Train Loss: 0.000561
Validation Loss: 0.00056082
Epoch [200/300], Train Loss: 0.000565
Validation Loss: 0.00056231
Epoch [201/300], Train Loss: 0.000565
Validation Loss: 0.00056084
Epoch [202/300], Train Loss: 0.000566
Validation Loss: 0.00056077
Epoch [203/300], Train Loss: 0.000562
Validation Loss: 0.00055968
Epoch [204/300], Train Loss: 0.000563
Validation Loss: 0.00056003
Epoch [205/300], Train Loss: 0.000566
Validation Loss: 0.00055929
Epoch [206/300], Train Loss: 0.000560
Validation Loss: 0.00055821
Epoch [207/300], Train Loss: 0.000563
Validation Loss: 0.00056016
Epoch [208/300], Train Loss: 0.000560
Validation Loss: 0.00055700
Epoch [209/300], Train Loss: 0.000564
Validation Loss: 0.00055739
Epoch [210/300], Train Loss: 0.000562
Validation Loss: 0.00055744
Epoch [211/300], Train Loss: 0.000563
Validation Loss: 0.00055583
Epoch [212/300], Train Loss: 0.000563
Validation Loss: 0.00055675
Epoch [213/300], Train Loss: 0.000558
Validation Loss: 0.00055492
Epoch [214/300], Train Loss: 0.000562
Validation Loss: 0.00055573
Epoch [215/300], Train Loss: 0.000558
Validation Loss: 0.00055400
Epoch [216/300], Train Loss: 0.000561
Validation Loss: 0.00055441
Epoch [217/300], Train Loss: 0.000558
Validation Loss: 0.00055367
Epoch [218/300], Train Loss: 0.000558
Validation Loss: 0.00055338
Epoch [219/300], Train Loss: 0.000561
Validation Loss: 0.00055204
Epoch [220/300], Train Loss: 0.000556
Validation Loss: 0.00055238
Epoch [221/300], Train Loss: 0.000557
Validation Loss: 0.00055106
Epoch [222/300], Train Loss: 0.000560
Validation Loss: 0.00055005
Epoch [223/300], Train Loss: 0.000556
Validation Loss: 0.00054961
Epoch [224/300], Train Loss: 0.000558
Validation Loss: 0.00054902
Epoch [225/300], Train Loss: 0.000557
Validation Loss: 0.00054852
Epoch [226/300], Train Loss: 0.000559
Validation Loss: 0.00054796
Epoch [227/300], Train Loss: 0.000557
Validation Loss: 0.00054749
Epoch [228/300], Train Loss: 0.000559
Validation Loss: 0.00054718
Epoch [229/300], Train Loss: 0.000553
Validation Loss: 0.00054570
Epoch [230/300], Train Loss: 0.000554
Validation Loss: 0.00054649
Epoch [231/300], Train Loss: 0.000551
Validation Loss: 0.00054495
Epoch [232/300], Train Loss: 0.000553
Validation Loss: 0.00054459
Epoch [233/300], Train Loss: 0.000554
Validation Loss: 0.00054408
Epoch [234/300], Train Loss: 0.000555
Validation Loss: 0.00054343
Epoch [235/300], Train Loss: 0.000549
Validation Loss: 0.00054334
Epoch [236/300], Train Loss: 0.000552
Validation Loss: 0.00054213
Epoch [237/300], Train Loss: 0.000551
Validation Loss: 0.00054328
Epoch [238/300], Train Loss: 0.000551
Validation Loss: 0.00054223
Epoch [239/300], Train Loss: 0.000550
Validation Loss: 0.00054194
Epoch [240/300], Train Loss: 0.000550
Validation Loss: 0.00054131
Epoch [241/300], Train Loss: 0.000552
Validation Loss: 0.00054210
Epoch [242/300], Train Loss: 0.000550
Validation Loss: 0.00054004
Epoch [243/300], Train Loss: 0.000547
Validation Loss: 0.00053963
Epoch [244/300], Train Loss: 0.000550
Validation Loss: 0.00053973
Epoch [245/300], Train Loss: 0.000550
Validation Loss: 0.00054069
Epoch [246/300], Train Loss: 0.000552
Validation Loss: 0.00053963
Epoch [247/300], Train Loss: 0.000547
Validation Loss: 0.00053846
Epoch [248/300], Train Loss: 0.000552
Validation Loss: 0.00054211
Epoch [249/300], Train Loss: 0.000549
Validation Loss: 0.00053826
Epoch [250/300], Train Loss: 0.000544
Validation Loss: 0.00053797
Epoch [251/300], Train Loss: 0.000545
Validation Loss: 0.00053687
Epoch [252/300], Train Loss: 0.000545
Validation Loss: 0.00053801
Epoch [253/300], Train Loss: 0.000547
Validation Loss: 0.00053671
Epoch [254/300], Train Loss: 0.000545
Validation Loss: 0.00053681
Epoch [255/300], Train Loss: 0.000547
Validation Loss: 0.00053552
Epoch [256/300], Train Loss: 0.000551
Validation Loss: 0.00053636
Epoch [257/300], Train Loss: 0.000544
Validation Loss: 0.00053657
Epoch [258/300], Train Loss: 0.000544
Validation Loss: 0.00053474
Epoch [259/300], Train Loss: 0.000546
Validation Loss: 0.00053530
Epoch [260/300], Train Loss: 0.000546
Validation Loss: 0.00053521
Epoch [261/300], Train Loss: 0.000547
Validation Loss: 0.00053394
Epoch [262/300], Train Loss: 0.000542
Validation Loss: 0.00053402
Epoch [263/300], Train Loss: 0.000546
Validation Loss: 0.00053483
Epoch [264/300], Train Loss: 0.000546
Validation Loss: 0.00053578
Epoch [265/300], Train Loss: 0.000543
Validation Loss: 0.00053436
Epoch [266/300], Train Loss: 0.000545
Validation Loss: 0.00053419
Epoch [267/300], Train Loss: 0.000542
Validation Loss: 0.00053531
Epoch [268/300], Train Loss: 0.000545
Validation Loss: 0.00053305
Epoch [269/300], Train Loss: 0.000544
Validation Loss: 0.00053327
Epoch [270/300], Train Loss: 0.000540
Validation Loss: 0.00053219
Epoch [271/300], Train Loss: 0.000540
Validation Loss: 0.00053306
Epoch [272/300], Train Loss: 0.000545
Validation Loss: 0.00053289
Epoch [273/300], Train Loss: 0.000543
Validation Loss: 0.00053173
Epoch [274/300], Train Loss: 0.000541
Validation Loss: 0.00053275
Epoch [275/300], Train Loss: 0.000544
Validation Loss: 0.00053189
Epoch [276/300], Train Loss: 0.000540
Validation Loss: 0.00053190
Epoch [277/300], Train Loss: 0.000539
Validation Loss: 0.00053140
Epoch [278/300], Train Loss: 0.000541
Validation Loss: 0.00053041
Epoch [279/300], Train Loss: 0.000540
Validation Loss: 0.00053256
Epoch [280/300], Train Loss: 0.000543
Validation Loss: 0.00053077
Epoch [281/300], Train Loss: 0.000538
Validation Loss: 0.00053020
Epoch [282/300], Train Loss: 0.000542
Validation Loss: 0.00053025
Epoch [283/300], Train Loss: 0.000540
Validation Loss: 0.00052977
Epoch [284/300], Train Loss: 0.000540
Validation Loss: 0.00053273
Epoch [285/300], Train Loss: 0.000544
Validation Loss: 0.00052910
Epoch [286/300], Train Loss: 0.000541
Validation Loss: 0.00052958
Epoch [287/300], Train Loss: 0.000542
Validation Loss: 0.00053137
Epoch [288/300], Train Loss: 0.000541
Validation Loss: 0.00052902
Epoch [289/300], Train Loss: 0.000542
Validation Loss: 0.00052821
Epoch [290/300], Train Loss: 0.000536
Validation Loss: 0.00052934
Epoch [291/300], Train Loss: 0.000543
Validation Loss: 0.00052835
Epoch [292/300], Train Loss: 0.000542
Validation Loss: 0.00052808
Epoch [293/300], Train Loss: 0.000543
Validation Loss: 0.00052804
Epoch [294/300], Train Loss: 0.000541
Validation Loss: 0.00053030
Epoch [295/300], Train Loss: 0.000537
Validation Loss: 0.00052885
Epoch [296/300], Train Loss: 0.000538
Validation Loss: 0.00052773
Epoch [297/300], Train Loss: 0.000538
Validation Loss: 0.00052772
Epoch [298/300], Train Loss: 0.000537
Validation Loss: 0.00052783
Epoch [299/300], Train Loss: 0.000536
Validation Loss: 0.00052782
Epoch [300/300], Train Loss: 0.000536
Validation Loss: 0.00052724

Evaluating model for: Fridge
Run 45/72 completed in 2249.88 seconds with: {'MAE': np.float32(34.293537), 'MSE': np.float32(1804.8766), 'RMSE': np.float32(42.48384), 'SAE': np.float32(0.052929074), 'NDE': np.float32(0.73621535)}

Run 46/72: hidden=256, seq_len=720, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.000999
Validation Loss: 0.00072209
Epoch [2/300], Train Loss: 0.000681
Validation Loss: 0.00067027
Epoch [3/300], Train Loss: 0.000670
Validation Loss: 0.00066928
Epoch [4/300], Train Loss: 0.000673
Validation Loss: 0.00066901
Epoch [5/300], Train Loss: 0.000670
Validation Loss: 0.00066851
Epoch [6/300], Train Loss: 0.000670
Validation Loss: 0.00066888
Epoch [7/300], Train Loss: 0.000668
Validation Loss: 0.00066764
Epoch [8/300], Train Loss: 0.000667
Validation Loss: 0.00066666
Epoch [9/300], Train Loss: 0.000667
Validation Loss: 0.00066568
Epoch [10/300], Train Loss: 0.000660
Validation Loss: 0.00066433
Epoch [11/300], Train Loss: 0.000660
Validation Loss: 0.00066339
Epoch [12/300], Train Loss: 0.000661
Validation Loss: 0.00066171
Epoch [13/300], Train Loss: 0.000654
Validation Loss: 0.00065947
Epoch [14/300], Train Loss: 0.000648
Validation Loss: 0.00065565
Epoch [15/300], Train Loss: 0.000642
Validation Loss: 0.00064507
Epoch [16/300], Train Loss: 0.000628
Validation Loss: 0.00063140
Epoch [17/300], Train Loss: 0.000621
Validation Loss: 0.00062661
Epoch [18/300], Train Loss: 0.000611
Validation Loss: 0.00062482
Epoch [19/300], Train Loss: 0.000616
Validation Loss: 0.00062339
Epoch [20/300], Train Loss: 0.000610
Validation Loss: 0.00062271
Epoch [21/300], Train Loss: 0.000612
Validation Loss: 0.00061880
Epoch [22/300], Train Loss: 0.000611
Validation Loss: 0.00061809
Epoch [23/300], Train Loss: 0.000611
Validation Loss: 0.00061812
Epoch [24/300], Train Loss: 0.000613
Validation Loss: 0.00061980
Epoch [25/300], Train Loss: 0.000609
Validation Loss: 0.00062330
Epoch [26/300], Train Loss: 0.000610
Validation Loss: 0.00061495
Epoch [27/300], Train Loss: 0.000609
Validation Loss: 0.00061486
Epoch [28/300], Train Loss: 0.000606
Validation Loss: 0.00061722
Epoch [29/300], Train Loss: 0.000609
Validation Loss: 0.00061300
Epoch [30/300], Train Loss: 0.000609
Validation Loss: 0.00061907
Epoch [31/300], Train Loss: 0.000605
Validation Loss: 0.00061595
Epoch [32/300], Train Loss: 0.000606
Validation Loss: 0.00061408
Epoch [33/300], Train Loss: 0.000603
Validation Loss: 0.00061244
Epoch [34/300], Train Loss: 0.000605
Validation Loss: 0.00061576
Epoch [35/300], Train Loss: 0.000604
Validation Loss: 0.00061153
Epoch [36/300], Train Loss: 0.000602
Validation Loss: 0.00061022
Epoch [37/300], Train Loss: 0.000603
Validation Loss: 0.00061021
Epoch [38/300], Train Loss: 0.000607
Validation Loss: 0.00060877
Epoch [39/300], Train Loss: 0.000599
Validation Loss: 0.00060802
Epoch [40/300], Train Loss: 0.000600
Validation Loss: 0.00060793
Epoch [41/300], Train Loss: 0.000601
Validation Loss: 0.00060844
Epoch [42/300], Train Loss: 0.000599
Validation Loss: 0.00060733
Epoch [43/300], Train Loss: 0.000600
Validation Loss: 0.00060736
Epoch [44/300], Train Loss: 0.000603
Validation Loss: 0.00060905
Epoch [45/300], Train Loss: 0.000599
Validation Loss: 0.00060684
Epoch [46/300], Train Loss: 0.000596
Validation Loss: 0.00060980
Epoch [47/300], Train Loss: 0.000598
Validation Loss: 0.00060613
Epoch [48/300], Train Loss: 0.000595
Validation Loss: 0.00060800
Epoch [49/300], Train Loss: 0.000596
Validation Loss: 0.00060447
Epoch [50/300], Train Loss: 0.000599
Validation Loss: 0.00060525
Epoch [51/300], Train Loss: 0.000598
Validation Loss: 0.00060835
Epoch [52/300], Train Loss: 0.000598
Validation Loss: 0.00060357
Epoch [53/300], Train Loss: 0.000592
Validation Loss: 0.00060458
Epoch [54/300], Train Loss: 0.000592
Validation Loss: 0.00060406
Epoch [55/300], Train Loss: 0.000594
Validation Loss: 0.00060099
Epoch [56/300], Train Loss: 0.000591
Validation Loss: 0.00060152
Epoch [57/300], Train Loss: 0.000595
Validation Loss: 0.00060298
Epoch [58/300], Train Loss: 0.000594
Validation Loss: 0.00060099
Epoch [59/300], Train Loss: 0.000589
Validation Loss: 0.00060280
Epoch [60/300], Train Loss: 0.000595
Validation Loss: 0.00060141
Epoch [61/300], Train Loss: 0.000589
Validation Loss: 0.00060006
Epoch [62/300], Train Loss: 0.000592
Validation Loss: 0.00059921
Epoch [63/300], Train Loss: 0.000592
Validation Loss: 0.00060055
Epoch [64/300], Train Loss: 0.000588
Validation Loss: 0.00059854
Epoch [65/300], Train Loss: 0.000587
Validation Loss: 0.00059882
Epoch [66/300], Train Loss: 0.000589
Validation Loss: 0.00059885
Epoch [67/300], Train Loss: 0.000589
Validation Loss: 0.00059843
Epoch [68/300], Train Loss: 0.000586
Validation Loss: 0.00059710
Epoch [69/300], Train Loss: 0.000590
Validation Loss: 0.00059623
Epoch [70/300], Train Loss: 0.000585
Validation Loss: 0.00059862
Epoch [71/300], Train Loss: 0.000589
Validation Loss: 0.00059846
Epoch [72/300], Train Loss: 0.000589
Validation Loss: 0.00059594
Epoch [73/300], Train Loss: 0.000588
Validation Loss: 0.00059715
Epoch [74/300], Train Loss: 0.000589
Validation Loss: 0.00059604
Epoch [75/300], Train Loss: 0.000588
Validation Loss: 0.00059713
Epoch [76/300], Train Loss: 0.000585
Validation Loss: 0.00059443
Epoch [77/300], Train Loss: 0.000586
Validation Loss: 0.00059368
Epoch [78/300], Train Loss: 0.000586
Validation Loss: 0.00059242
Epoch [79/300], Train Loss: 0.000584
Validation Loss: 0.00059306
Epoch [80/300], Train Loss: 0.000587
Validation Loss: 0.00059284
Epoch [81/300], Train Loss: 0.000585
Validation Loss: 0.00059308
Epoch [82/300], Train Loss: 0.000583
Validation Loss: 0.00059177
Epoch [83/300], Train Loss: 0.000587
Validation Loss: 0.00059134
Epoch [84/300], Train Loss: 0.000581
Validation Loss: 0.00059128
Epoch [85/300], Train Loss: 0.000585
Validation Loss: 0.00059352
Epoch [86/300], Train Loss: 0.000580
Validation Loss: 0.00059091
Epoch [87/300], Train Loss: 0.000576
Validation Loss: 0.00059031
Epoch [88/300], Train Loss: 0.000582
Validation Loss: 0.00058980
Epoch [89/300], Train Loss: 0.000582
Validation Loss: 0.00058825
Epoch [90/300], Train Loss: 0.000583
Validation Loss: 0.00059103
Epoch [91/300], Train Loss: 0.000584
Validation Loss: 0.00059066
Epoch [92/300], Train Loss: 0.000581
Validation Loss: 0.00058790
Epoch [93/300], Train Loss: 0.000580
Validation Loss: 0.00058695
Epoch [94/300], Train Loss: 0.000575
Validation Loss: 0.00058780
Epoch [95/300], Train Loss: 0.000581
Validation Loss: 0.00059444
Epoch [96/300], Train Loss: 0.000581
Validation Loss: 0.00058686
Epoch [97/300], Train Loss: 0.000580
Validation Loss: 0.00058542
Epoch [98/300], Train Loss: 0.000576
Validation Loss: 0.00058766
Epoch [99/300], Train Loss: 0.000576
Validation Loss: 0.00058477
Epoch [100/300], Train Loss: 0.000578
Validation Loss: 0.00058743
Epoch [101/300], Train Loss: 0.000581
Validation Loss: 0.00059260
Epoch [102/300], Train Loss: 0.000577
Validation Loss: 0.00058624
Epoch [103/300], Train Loss: 0.000578
Validation Loss: 0.00058382
Epoch [104/300], Train Loss: 0.000575
Validation Loss: 0.00058419
Epoch [105/300], Train Loss: 0.000578
Validation Loss: 0.00058689
Epoch [106/300], Train Loss: 0.000575
Validation Loss: 0.00058252
Epoch [107/300], Train Loss: 0.000576
Validation Loss: 0.00058311
Epoch [108/300], Train Loss: 0.000574
Validation Loss: 0.00058127
Epoch [109/300], Train Loss: 0.000569
Validation Loss: 0.00058055
Epoch [110/300], Train Loss: 0.000574
Validation Loss: 0.00058158
Epoch [111/300], Train Loss: 0.000574
Validation Loss: 0.00058039
Epoch [112/300], Train Loss: 0.000575
Validation Loss: 0.00057946
Epoch [113/300], Train Loss: 0.000572
Validation Loss: 0.00057961
Epoch [114/300], Train Loss: 0.000573
Validation Loss: 0.00058021
Epoch [115/300], Train Loss: 0.000573
Validation Loss: 0.00058095
Epoch [116/300], Train Loss: 0.000572
Validation Loss: 0.00057853
Epoch [117/300], Train Loss: 0.000570
Validation Loss: 0.00057931
Epoch [118/300], Train Loss: 0.000573
Validation Loss: 0.00058162
Epoch [119/300], Train Loss: 0.000572
Validation Loss: 0.00057696
Epoch [120/300], Train Loss: 0.000570
Validation Loss: 0.00057588
Epoch [121/300], Train Loss: 0.000568
Validation Loss: 0.00057638
Epoch [122/300], Train Loss: 0.000570
Validation Loss: 0.00057574
Epoch [123/300], Train Loss: 0.000572
Validation Loss: 0.00057649
Epoch [124/300], Train Loss: 0.000571
Validation Loss: 0.00057414
Epoch [125/300], Train Loss: 0.000568
Validation Loss: 0.00057573
Epoch [126/300], Train Loss: 0.000568
Validation Loss: 0.00057678
Epoch [127/300], Train Loss: 0.000566
Validation Loss: 0.00057416
Epoch [128/300], Train Loss: 0.000567
Validation Loss: 0.00057268
Epoch [129/300], Train Loss: 0.000563
Validation Loss: 0.00057722
Epoch [130/300], Train Loss: 0.000567
Validation Loss: 0.00057197
Epoch [131/300], Train Loss: 0.000564
Validation Loss: 0.00057142
Epoch [132/300], Train Loss: 0.000565
Validation Loss: 0.00057005
Epoch [133/300], Train Loss: 0.000563
Validation Loss: 0.00057271
Epoch [134/300], Train Loss: 0.000564
Validation Loss: 0.00056983
Epoch [135/300], Train Loss: 0.000564
Validation Loss: 0.00056844
Epoch [136/300], Train Loss: 0.000567
Validation Loss: 0.00056981
Epoch [137/300], Train Loss: 0.000560
Validation Loss: 0.00057150
Epoch [138/300], Train Loss: 0.000565
Validation Loss: 0.00056664
Epoch [139/300], Train Loss: 0.000562
Validation Loss: 0.00056601
Epoch [140/300], Train Loss: 0.000565
Validation Loss: 0.00056667
Epoch [141/300], Train Loss: 0.000560
Validation Loss: 0.00056445
Epoch [142/300], Train Loss: 0.000558
Validation Loss: 0.00056551
Epoch [143/300], Train Loss: 0.000560
Validation Loss: 0.00056436
Epoch [144/300], Train Loss: 0.000558
Validation Loss: 0.00056588
Epoch [145/300], Train Loss: 0.000560
Validation Loss: 0.00056920
Epoch [146/300], Train Loss: 0.000559
Validation Loss: 0.00056127
Epoch [147/300], Train Loss: 0.000556
Validation Loss: 0.00056208
Epoch [148/300], Train Loss: 0.000558
Validation Loss: 0.00056423
Epoch [149/300], Train Loss: 0.000554
Validation Loss: 0.00055999
Epoch [150/300], Train Loss: 0.000556
Validation Loss: 0.00056598
Epoch [151/300], Train Loss: 0.000556
Validation Loss: 0.00056392
Epoch [152/300], Train Loss: 0.000560
Validation Loss: 0.00056016
Epoch [153/300], Train Loss: 0.000552
Validation Loss: 0.00055677
Epoch [154/300], Train Loss: 0.000553
Validation Loss: 0.00056247
Epoch [155/300], Train Loss: 0.000555
Validation Loss: 0.00055487
Epoch [156/300], Train Loss: 0.000549
Validation Loss: 0.00055296
Epoch [157/300], Train Loss: 0.000552
Validation Loss: 0.00056017
Epoch [158/300], Train Loss: 0.000552
Validation Loss: 0.00055832
Epoch [159/300], Train Loss: 0.000549
Validation Loss: 0.00055128
Epoch [160/300], Train Loss: 0.000546
Validation Loss: 0.00056285
Epoch [161/300], Train Loss: 0.000546
Validation Loss: 0.00055199
Epoch [162/300], Train Loss: 0.000542
Validation Loss: 0.00055021
Epoch [163/300], Train Loss: 0.000548
Validation Loss: 0.00054806
Epoch [164/300], Train Loss: 0.000542
Validation Loss: 0.00054836
Epoch [165/300], Train Loss: 0.000543
Validation Loss: 0.00055733
Epoch [166/300], Train Loss: 0.000542
Validation Loss: 0.00054679
Epoch [167/300], Train Loss: 0.000542
Validation Loss: 0.00054500
Epoch [168/300], Train Loss: 0.000538
Validation Loss: 0.00054654
Epoch [169/300], Train Loss: 0.000543
Validation Loss: 0.00054030
Epoch [170/300], Train Loss: 0.000540
Validation Loss: 0.00055234
Epoch [171/300], Train Loss: 0.000541
Validation Loss: 0.00054019
Epoch [172/300], Train Loss: 0.000535
Validation Loss: 0.00054023
Epoch [173/300], Train Loss: 0.000536
Validation Loss: 0.00053949
Epoch [174/300], Train Loss: 0.000538
Validation Loss: 0.00054468
Epoch [175/300], Train Loss: 0.000537
Validation Loss: 0.00053983
Epoch [176/300], Train Loss: 0.000534
Validation Loss: 0.00053648
Epoch [177/300], Train Loss: 0.000532
Validation Loss: 0.00053416
Epoch [178/300], Train Loss: 0.000534
Validation Loss: 0.00053395
Epoch [179/300], Train Loss: 0.000533
Validation Loss: 0.00053375
Epoch [180/300], Train Loss: 0.000532
Validation Loss: 0.00053292
Epoch [181/300], Train Loss: 0.000530
Validation Loss: 0.00052966
Epoch [182/300], Train Loss: 0.000529
Validation Loss: 0.00053449
Epoch [183/300], Train Loss: 0.000534
Validation Loss: 0.00053403
Epoch [184/300], Train Loss: 0.000530
Validation Loss: 0.00052934
Epoch [185/300], Train Loss: 0.000533
Validation Loss: 0.00052909
Epoch [186/300], Train Loss: 0.000528
Validation Loss: 0.00052698
Epoch [187/300], Train Loss: 0.000526
Validation Loss: 0.00052544
Epoch [188/300], Train Loss: 0.000527
Validation Loss: 0.00052269
Epoch [189/300], Train Loss: 0.000524
Validation Loss: 0.00052632
Epoch [190/300], Train Loss: 0.000526
Validation Loss: 0.00052979
Epoch [191/300], Train Loss: 0.000529
Validation Loss: 0.00052391
Epoch [192/300], Train Loss: 0.000525
Validation Loss: 0.00052164
Epoch [193/300], Train Loss: 0.000522
Validation Loss: 0.00052072
Epoch [194/300], Train Loss: 0.000520
Validation Loss: 0.00051810
Epoch [195/300], Train Loss: 0.000518
Validation Loss: 0.00052463
Epoch [196/300], Train Loss: 0.000522
Validation Loss: 0.00051573
Epoch [197/300], Train Loss: 0.000514
Validation Loss: 0.00053683
Epoch [198/300], Train Loss: 0.000517
Validation Loss: 0.00051606
Epoch [199/300], Train Loss: 0.000515
Validation Loss: 0.00051702
Epoch [200/300], Train Loss: 0.000518
Validation Loss: 0.00051212
Epoch [201/300], Train Loss: 0.000516
Validation Loss: 0.00051467
Epoch [202/300], Train Loss: 0.000518
Validation Loss: 0.00051119
Epoch [203/300], Train Loss: 0.000515
Validation Loss: 0.00050842
Epoch [204/300], Train Loss: 0.000511
Validation Loss: 0.00051118
Epoch [205/300], Train Loss: 0.000515
Validation Loss: 0.00050651
Epoch [206/300], Train Loss: 0.000514
Validation Loss: 0.00050928
Epoch [207/300], Train Loss: 0.000513
Validation Loss: 0.00050988
Epoch [208/300], Train Loss: 0.000512
Validation Loss: 0.00050375
Epoch [209/300], Train Loss: 0.000514
Validation Loss: 0.00050220
Epoch [210/300], Train Loss: 0.000510
Validation Loss: 0.00050776
Epoch [211/300], Train Loss: 0.000513
Validation Loss: 0.00049993
Epoch [212/300], Train Loss: 0.000514
Validation Loss: 0.00050257
Epoch [213/300], Train Loss: 0.000505
Validation Loss: 0.00050456
Epoch [214/300], Train Loss: 0.000509
Validation Loss: 0.00050030
Epoch [215/300], Train Loss: 0.000502
Validation Loss: 0.00050747
Epoch [216/300], Train Loss: 0.000519
Validation Loss: 0.00051490
Epoch [217/300], Train Loss: 0.000509
Validation Loss: 0.00050070
Epoch [218/300], Train Loss: 0.000505
Validation Loss: 0.00049478
Epoch [219/300], Train Loss: 0.000503
Validation Loss: 0.00049059
Epoch [220/300], Train Loss: 0.000498
Validation Loss: 0.00049456
Epoch [221/300], Train Loss: 0.000502
Validation Loss: 0.00049146
Epoch [222/300], Train Loss: 0.000501
Validation Loss: 0.00048791
Epoch [223/300], Train Loss: 0.000501
Validation Loss: 0.00048947
Epoch [224/300], Train Loss: 0.000502
Validation Loss: 0.00048719
Epoch [225/300], Train Loss: 0.000500
Validation Loss: 0.00048615
Epoch [226/300], Train Loss: 0.000501
Validation Loss: 0.00048440
Epoch [227/300], Train Loss: 0.000497
Validation Loss: 0.00049223
Epoch [228/300], Train Loss: 0.000502
Validation Loss: 0.00048942
Epoch [229/300], Train Loss: 0.000497
Validation Loss: 0.00048526
Epoch [230/300], Train Loss: 0.000496
Validation Loss: 0.00048389
Epoch [231/300], Train Loss: 0.000492
Validation Loss: 0.00048389
Epoch [232/300], Train Loss: 0.000496
Validation Loss: 0.00048568
Epoch [233/300], Train Loss: 0.000499
Validation Loss: 0.00049441
Epoch [234/300], Train Loss: 0.000500
Validation Loss: 0.00047963
Epoch [235/300], Train Loss: 0.000490
Validation Loss: 0.00047698
Epoch [236/300], Train Loss: 0.000493
Validation Loss: 0.00048202
Epoch [237/300], Train Loss: 0.000493
Validation Loss: 0.00048061
Epoch [238/300], Train Loss: 0.000492
Validation Loss: 0.00047659
Epoch [239/300], Train Loss: 0.000489
Validation Loss: 0.00047475
Epoch [240/300], Train Loss: 0.000486
Validation Loss: 0.00047413
Epoch [241/300], Train Loss: 0.000490
Validation Loss: 0.00047645
Epoch [242/300], Train Loss: 0.000488
Validation Loss: 0.00047388
Epoch [243/300], Train Loss: 0.000485
Validation Loss: 0.00047694
Epoch [244/300], Train Loss: 0.000489
Validation Loss: 0.00049794
Epoch [245/300], Train Loss: 0.000494
Validation Loss: 0.00046969
Epoch [246/300], Train Loss: 0.000488
Validation Loss: 0.00047174
Epoch [247/300], Train Loss: 0.000485
Validation Loss: 0.00047049
Epoch [248/300], Train Loss: 0.000489
Validation Loss: 0.00046835
Epoch [249/300], Train Loss: 0.000486
Validation Loss: 0.00047813
Epoch [250/300], Train Loss: 0.000483
Validation Loss: 0.00046839
Epoch [251/300], Train Loss: 0.000482
Validation Loss: 0.00046678
Epoch [252/300], Train Loss: 0.000482
Validation Loss: 0.00047929
Epoch [253/300], Train Loss: 0.000484
Validation Loss: 0.00046619
Epoch [254/300], Train Loss: 0.000482
Validation Loss: 0.00046527
Epoch [255/300], Train Loss: 0.000480
Validation Loss: 0.00046484
Epoch [256/300], Train Loss: 0.000484
Validation Loss: 0.00046403
Epoch [257/300], Train Loss: 0.000475
Validation Loss: 0.00046515
Epoch [258/300], Train Loss: 0.000478
Validation Loss: 0.00046509
Epoch [259/300], Train Loss: 0.000481
Validation Loss: 0.00046547
Epoch [260/300], Train Loss: 0.000481
Validation Loss: 0.00046268
Epoch [261/300], Train Loss: 0.000480
Validation Loss: 0.00046219
Epoch [262/300], Train Loss: 0.000476
Validation Loss: 0.00046326
Epoch [263/300], Train Loss: 0.000477
Validation Loss: 0.00046140
Epoch [264/300], Train Loss: 0.000480
Validation Loss: 0.00046378
Epoch [265/300], Train Loss: 0.000477
Validation Loss: 0.00046330
Epoch [266/300], Train Loss: 0.000481
Validation Loss: 0.00046177
Epoch [267/300], Train Loss: 0.000477
Validation Loss: 0.00046011
Epoch [268/300], Train Loss: 0.000478
Validation Loss: 0.00046005
Epoch [269/300], Train Loss: 0.000477
Validation Loss: 0.00045943
Epoch [270/300], Train Loss: 0.000474
Validation Loss: 0.00046151
Epoch [271/300], Train Loss: 0.000472
Validation Loss: 0.00046725
Epoch [272/300], Train Loss: 0.000476
Validation Loss: 0.00046292
Epoch [273/300], Train Loss: 0.000477
Validation Loss: 0.00046125
Epoch [274/300], Train Loss: 0.000478
Validation Loss: 0.00045880
Epoch [275/300], Train Loss: 0.000476
Validation Loss: 0.00046024
Epoch [276/300], Train Loss: 0.000471
Validation Loss: 0.00045720
Epoch [277/300], Train Loss: 0.000471
Validation Loss: 0.00045693
Epoch [278/300], Train Loss: 0.000473
Validation Loss: 0.00045968
Epoch [279/300], Train Loss: 0.000470
Validation Loss: 0.00045753
Epoch [280/300], Train Loss: 0.000473
Validation Loss: 0.00046304
Epoch [281/300], Train Loss: 0.000469
Validation Loss: 0.00045632
Epoch [282/300], Train Loss: 0.000470
Validation Loss: 0.00045663
Epoch [283/300], Train Loss: 0.000470
Validation Loss: 0.00046034
Epoch [284/300], Train Loss: 0.000470
Validation Loss: 0.00046680
Epoch [285/300], Train Loss: 0.000474
Validation Loss: 0.00045431
Epoch [286/300], Train Loss: 0.000473
Validation Loss: 0.00046209
Epoch [287/300], Train Loss: 0.000472
Validation Loss: 0.00045952
Epoch [288/300], Train Loss: 0.000470
Validation Loss: 0.00045362
Epoch [289/300], Train Loss: 0.000471
Validation Loss: 0.00045678
Epoch [290/300], Train Loss: 0.000469
Validation Loss: 0.00045543
Epoch [291/300], Train Loss: 0.000474
Validation Loss: 0.00045307
Epoch [292/300], Train Loss: 0.000471
Validation Loss: 0.00045531
Epoch [293/300], Train Loss: 0.000473
Validation Loss: 0.00045512
Epoch [294/300], Train Loss: 0.000469
Validation Loss: 0.00045426
Epoch [295/300], Train Loss: 0.000465
Validation Loss: 0.00045352
Epoch [296/300], Train Loss: 0.000466
Validation Loss: 0.00045784
Epoch [297/300], Train Loss: 0.000468
Validation Loss: 0.00045629
Epoch [298/300], Train Loss: 0.000464
Validation Loss: 0.00045627
Epoch [299/300], Train Loss: 0.000465
Validation Loss: 0.00045186
Epoch [300/300], Train Loss: 0.000468
Validation Loss: 0.00045578

Evaluating model for: Fridge
Run 46/72 completed in 2750.99 seconds with: {'MAE': np.float32(29.96224), 'MSE': np.float32(1581.7471), 'RMSE': np.float32(39.771183), 'SAE': np.float32(0.06320175), 'NDE': np.float32(0.68920696)}

Run 47/72: hidden=256, seq_len=720, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.001186
Validation Loss: 0.00075921
Epoch [2/300], Train Loss: 0.000685
Validation Loss: 0.00067374
Epoch [3/300], Train Loss: 0.000669
Validation Loss: 0.00066988
Epoch [4/300], Train Loss: 0.000671
Validation Loss: 0.00066993
Epoch [5/300], Train Loss: 0.000669
Validation Loss: 0.00066962
Epoch [6/300], Train Loss: 0.000670
Validation Loss: 0.00067021
Epoch [7/300], Train Loss: 0.000668
Validation Loss: 0.00066939
Epoch [8/300], Train Loss: 0.000668
Validation Loss: 0.00066906
Epoch [9/300], Train Loss: 0.000669
Validation Loss: 0.00066886
Epoch [10/300], Train Loss: 0.000663
Validation Loss: 0.00066824
Epoch [11/300], Train Loss: 0.000664
Validation Loss: 0.00066779
Epoch [12/300], Train Loss: 0.000668
Validation Loss: 0.00066678
Epoch [13/300], Train Loss: 0.000662
Validation Loss: 0.00066530
Epoch [14/300], Train Loss: 0.000657
Validation Loss: 0.00066096
Epoch [15/300], Train Loss: 0.000648
Validation Loss: 0.00065370
Epoch [16/300], Train Loss: 0.000633
Validation Loss: 0.00063955
Epoch [17/300], Train Loss: 0.000629
Validation Loss: 0.00064048
Epoch [18/300], Train Loss: 0.000617
Validation Loss: 0.00063266
Epoch [19/300], Train Loss: 0.000621
Validation Loss: 0.00063063
Epoch [20/300], Train Loss: 0.000613
Validation Loss: 0.00062975
Epoch [21/300], Train Loss: 0.000615
Validation Loss: 0.00062335
Epoch [22/300], Train Loss: 0.000613
Validation Loss: 0.00062209
Epoch [23/300], Train Loss: 0.000612
Validation Loss: 0.00062288
Epoch [24/300], Train Loss: 0.000615
Validation Loss: 0.00062288
Epoch [25/300], Train Loss: 0.000611
Validation Loss: 0.00062773
Epoch [26/300], Train Loss: 0.000612
Validation Loss: 0.00062099
Epoch [27/300], Train Loss: 0.000611
Validation Loss: 0.00061870
Epoch [28/300], Train Loss: 0.000607
Validation Loss: 0.00062005
Epoch [29/300], Train Loss: 0.000610
Validation Loss: 0.00061729
Epoch [30/300], Train Loss: 0.000611
Validation Loss: 0.00062140
Epoch [31/300], Train Loss: 0.000607
Validation Loss: 0.00061885
Epoch [32/300], Train Loss: 0.000608
Validation Loss: 0.00061901
Epoch [33/300], Train Loss: 0.000605
Validation Loss: 0.00061646
Epoch [34/300], Train Loss: 0.000607
Validation Loss: 0.00061869
Epoch [35/300], Train Loss: 0.000606
Validation Loss: 0.00061589
Epoch [36/300], Train Loss: 0.000605
Validation Loss: 0.00061546
Epoch [37/300], Train Loss: 0.000605
Validation Loss: 0.00061458
Epoch [38/300], Train Loss: 0.000609
Validation Loss: 0.00061367
Epoch [39/300], Train Loss: 0.000603
Validation Loss: 0.00061334
Epoch [40/300], Train Loss: 0.000603
Validation Loss: 0.00061350
Epoch [41/300], Train Loss: 0.000605
Validation Loss: 0.00061285
Epoch [42/300], Train Loss: 0.000603
Validation Loss: 0.00061277
Epoch [43/300], Train Loss: 0.000603
Validation Loss: 0.00061225
Epoch [44/300], Train Loss: 0.000606
Validation Loss: 0.00061391
Epoch [45/300], Train Loss: 0.000603
Validation Loss: 0.00061269
Epoch [46/300], Train Loss: 0.000599
Validation Loss: 0.00061173
Epoch [47/300], Train Loss: 0.000602
Validation Loss: 0.00061219
Epoch [48/300], Train Loss: 0.000599
Validation Loss: 0.00061441
Epoch [49/300], Train Loss: 0.000600
Validation Loss: 0.00061027
Epoch [50/300], Train Loss: 0.000604
Validation Loss: 0.00061041
Epoch [51/300], Train Loss: 0.000602
Validation Loss: 0.00061413
Epoch [52/300], Train Loss: 0.000603
Validation Loss: 0.00061116
Epoch [53/300], Train Loss: 0.000597
Validation Loss: 0.00061087
Epoch [54/300], Train Loss: 0.000597
Validation Loss: 0.00061114
Epoch [55/300], Train Loss: 0.000600
Validation Loss: 0.00060835
Epoch [56/300], Train Loss: 0.000596
Validation Loss: 0.00061087
Epoch [57/300], Train Loss: 0.000602
Validation Loss: 0.00060993
Epoch [58/300], Train Loss: 0.000600
Validation Loss: 0.00060792
Epoch [59/300], Train Loss: 0.000596
Validation Loss: 0.00061163
Epoch [60/300], Train Loss: 0.000601
Validation Loss: 0.00060767
Epoch [61/300], Train Loss: 0.000595
Validation Loss: 0.00060701
Epoch [62/300], Train Loss: 0.000598
Validation Loss: 0.00060814
Epoch [63/300], Train Loss: 0.000599
Validation Loss: 0.00060947
Epoch [64/300], Train Loss: 0.000595
Validation Loss: 0.00060871
Epoch [65/300], Train Loss: 0.000594
Validation Loss: 0.00060838
Epoch [66/300], Train Loss: 0.000596
Validation Loss: 0.00060706
Epoch [67/300], Train Loss: 0.000597
Validation Loss: 0.00060915
Epoch [68/300], Train Loss: 0.000593
Validation Loss: 0.00060539
Epoch [69/300], Train Loss: 0.000596
Validation Loss: 0.00060467
Epoch [70/300], Train Loss: 0.000593
Validation Loss: 0.00060559
Epoch [71/300], Train Loss: 0.000595
Validation Loss: 0.00060491
Epoch [72/300], Train Loss: 0.000596
Validation Loss: 0.00060460
Epoch [73/300], Train Loss: 0.000595
Validation Loss: 0.00060577
Epoch [74/300], Train Loss: 0.000596
Validation Loss: 0.00060474
Epoch [75/300], Train Loss: 0.000594
Validation Loss: 0.00060518
Epoch [76/300], Train Loss: 0.000592
Validation Loss: 0.00060187
Epoch [77/300], Train Loss: 0.000592
Validation Loss: 0.00060164
Epoch [78/300], Train Loss: 0.000594
Validation Loss: 0.00060069
Epoch [79/300], Train Loss: 0.000592
Validation Loss: 0.00060123
Epoch [80/300], Train Loss: 0.000594
Validation Loss: 0.00060298
Epoch [81/300], Train Loss: 0.000593
Validation Loss: 0.00060058
Epoch [82/300], Train Loss: 0.000589
Validation Loss: 0.00059938
Epoch [83/300], Train Loss: 0.000593
Validation Loss: 0.00059921
Epoch [84/300], Train Loss: 0.000588
Validation Loss: 0.00059922
Epoch [85/300], Train Loss: 0.000592
Validation Loss: 0.00060060
Epoch [86/300], Train Loss: 0.000588
Validation Loss: 0.00059896
Epoch [87/300], Train Loss: 0.000584
Validation Loss: 0.00059903
Epoch [88/300], Train Loss: 0.000590
Validation Loss: 0.00059818
Epoch [89/300], Train Loss: 0.000590
Validation Loss: 0.00059684
Epoch [90/300], Train Loss: 0.000591
Validation Loss: 0.00059980
Epoch [91/300], Train Loss: 0.000591
Validation Loss: 0.00059656
Epoch [92/300], Train Loss: 0.000589
Validation Loss: 0.00059695
Epoch [93/300], Train Loss: 0.000588
Validation Loss: 0.00059694
Epoch [94/300], Train Loss: 0.000583
Validation Loss: 0.00059574
Epoch [95/300], Train Loss: 0.000589
Validation Loss: 0.00059654
Epoch [96/300], Train Loss: 0.000588
Validation Loss: 0.00059746
Epoch [97/300], Train Loss: 0.000590
Validation Loss: 0.00059726
Epoch [98/300], Train Loss: 0.000585
Validation Loss: 0.00059702
Epoch [99/300], Train Loss: 0.000585
Validation Loss: 0.00059465
Epoch [100/300], Train Loss: 0.000586
Validation Loss: 0.00059583
Epoch [101/300], Train Loss: 0.000589
Validation Loss: 0.00060096
Epoch [102/300], Train Loss: 0.000587
Validation Loss: 0.00059437
Epoch [103/300], Train Loss: 0.000584
Validation Loss: 0.00059308
Epoch [104/300], Train Loss: 0.000584
Validation Loss: 0.00059360
Epoch [105/300], Train Loss: 0.000587
Validation Loss: 0.00059533
Epoch [106/300], Train Loss: 0.000582
Validation Loss: 0.00059342
Epoch [107/300], Train Loss: 0.000585
Validation Loss: 0.00059318
Epoch [108/300], Train Loss: 0.000583
Validation Loss: 0.00059169
Epoch [109/300], Train Loss: 0.000578
Validation Loss: 0.00059272
Epoch [110/300], Train Loss: 0.000583
Validation Loss: 0.00059349
Epoch [111/300], Train Loss: 0.000582
Validation Loss: 0.00059207
Epoch [112/300], Train Loss: 0.000583
Validation Loss: 0.00059011
Epoch [113/300], Train Loss: 0.000581
Validation Loss: 0.00059075
Epoch [114/300], Train Loss: 0.000581
Validation Loss: 0.00059165
Epoch [115/300], Train Loss: 0.000581
Validation Loss: 0.00058986
Epoch [116/300], Train Loss: 0.000580
Validation Loss: 0.00059040
Epoch [117/300], Train Loss: 0.000580
Validation Loss: 0.00058909
Epoch [118/300], Train Loss: 0.000582
Validation Loss: 0.00058932
Epoch [119/300], Train Loss: 0.000581
Validation Loss: 0.00058727
Epoch [120/300], Train Loss: 0.000579
Validation Loss: 0.00058926
Epoch [121/300], Train Loss: 0.000578
Validation Loss: 0.00058846
Epoch [122/300], Train Loss: 0.000582
Validation Loss: 0.00058665
Epoch [123/300], Train Loss: 0.000582
Validation Loss: 0.00058677
Epoch [124/300], Train Loss: 0.000580
Validation Loss: 0.00058601
Epoch [125/300], Train Loss: 0.000578
Validation Loss: 0.00058662
Epoch [126/300], Train Loss: 0.000579
Validation Loss: 0.00058858
Epoch [127/300], Train Loss: 0.000578
Validation Loss: 0.00058612
Epoch [128/300], Train Loss: 0.000576
Validation Loss: 0.00058636
Epoch [129/300], Train Loss: 0.000573
Validation Loss: 0.00058570
Epoch [130/300], Train Loss: 0.000577
Validation Loss: 0.00058443
Epoch [131/300], Train Loss: 0.000574
Validation Loss: 0.00058326
Epoch [132/300], Train Loss: 0.000575
Validation Loss: 0.00058246
Epoch [133/300], Train Loss: 0.000572
Validation Loss: 0.00058519
Epoch [134/300], Train Loss: 0.000575
Validation Loss: 0.00058166
Epoch [135/300], Train Loss: 0.000573
Validation Loss: 0.00058057
Epoch [136/300], Train Loss: 0.000577
Validation Loss: 0.00058024
Epoch [137/300], Train Loss: 0.000571
Validation Loss: 0.00058102
Epoch [138/300], Train Loss: 0.000575
Validation Loss: 0.00057779
Epoch [139/300], Train Loss: 0.000572
Validation Loss: 0.00057763
Epoch [140/300], Train Loss: 0.000574
Validation Loss: 0.00057752
Epoch [141/300], Train Loss: 0.000570
Validation Loss: 0.00057462
Epoch [142/300], Train Loss: 0.000569
Validation Loss: 0.00057512
Epoch [143/300], Train Loss: 0.000570
Validation Loss: 0.00057294
Epoch [144/300], Train Loss: 0.000568
Validation Loss: 0.00057245
Epoch [145/300], Train Loss: 0.000570
Validation Loss: 0.00057169
Epoch [146/300], Train Loss: 0.000568
Validation Loss: 0.00056976
Epoch [147/300], Train Loss: 0.000566
Validation Loss: 0.00056829
Epoch [148/300], Train Loss: 0.000568
Validation Loss: 0.00056876
Epoch [149/300], Train Loss: 0.000566
Validation Loss: 0.00056740
Epoch [150/300], Train Loss: 0.000567
Validation Loss: 0.00056953
Epoch [151/300], Train Loss: 0.000565
Validation Loss: 0.00056684
Epoch [152/300], Train Loss: 0.000569
Validation Loss: 0.00056705
Epoch [153/300], Train Loss: 0.000565
Validation Loss: 0.00056514
Epoch [154/300], Train Loss: 0.000564
Validation Loss: 0.00056725
Epoch [155/300], Train Loss: 0.000565
Validation Loss: 0.00056292
Epoch [156/300], Train Loss: 0.000562
Validation Loss: 0.00056096
Epoch [157/300], Train Loss: 0.000565
Validation Loss: 0.00056119
Epoch [158/300], Train Loss: 0.000561
Validation Loss: 0.00056491
Epoch [159/300], Train Loss: 0.000561
Validation Loss: 0.00055996
Epoch [160/300], Train Loss: 0.000560
Validation Loss: 0.00056349
Epoch [161/300], Train Loss: 0.000560
Validation Loss: 0.00055880
Epoch [162/300], Train Loss: 0.000557
Validation Loss: 0.00055879
Epoch [163/300], Train Loss: 0.000562
Validation Loss: 0.00055726
Epoch [164/300], Train Loss: 0.000560
Validation Loss: 0.00055713
Epoch [165/300], Train Loss: 0.000560
Validation Loss: 0.00055754
Epoch [166/300], Train Loss: 0.000559
Validation Loss: 0.00055730
Epoch [167/300], Train Loss: 0.000560
Validation Loss: 0.00055778
Epoch [168/300], Train Loss: 0.000556
Validation Loss: 0.00056276
Epoch [169/300], Train Loss: 0.000564
Validation Loss: 0.00055562
Epoch [170/300], Train Loss: 0.000561
Validation Loss: 0.00055660
Epoch [171/300], Train Loss: 0.000563
Validation Loss: 0.00055567
Epoch [172/300], Train Loss: 0.000555
Validation Loss: 0.00055633
Epoch [173/300], Train Loss: 0.000561
Validation Loss: 0.00055458
Epoch [174/300], Train Loss: 0.000559
Validation Loss: 0.00055512
Epoch [175/300], Train Loss: 0.000559
Validation Loss: 0.00055312
Epoch [176/300], Train Loss: 0.000556
Validation Loss: 0.00055290
Epoch [177/300], Train Loss: 0.000557
Validation Loss: 0.00055098
Epoch [178/300], Train Loss: 0.000559
Validation Loss: 0.00055170
Epoch [179/300], Train Loss: 0.000556
Validation Loss: 0.00055152
Epoch [180/300], Train Loss: 0.000555
Validation Loss: 0.00055368
Epoch [181/300], Train Loss: 0.000555
Validation Loss: 0.00055147
Epoch [182/300], Train Loss: 0.000554
Validation Loss: 0.00055058
Epoch [183/300], Train Loss: 0.000556
Validation Loss: 0.00055167
Epoch [184/300], Train Loss: 0.000553
Validation Loss: 0.00054860
Epoch [185/300], Train Loss: 0.000555
Validation Loss: 0.00054976
Epoch [186/300], Train Loss: 0.000551
Validation Loss: 0.00054789
Epoch [187/300], Train Loss: 0.000549
Validation Loss: 0.00054691
Epoch [188/300], Train Loss: 0.000553
Validation Loss: 0.00054735
Epoch [189/300], Train Loss: 0.000551
Validation Loss: 0.00054526
Epoch [190/300], Train Loss: 0.000551
Validation Loss: 0.00054363
Epoch [191/300], Train Loss: 0.000553
Validation Loss: 0.00054714
Epoch [192/300], Train Loss: 0.000553
Validation Loss: 0.00054439
Epoch [193/300], Train Loss: 0.000550
Validation Loss: 0.00054209
Epoch [194/300], Train Loss: 0.000547
Validation Loss: 0.00054116
Epoch [195/300], Train Loss: 0.000544
Validation Loss: 0.00053952
Epoch [196/300], Train Loss: 0.000549
Validation Loss: 0.00053882
Epoch [197/300], Train Loss: 0.000541
Validation Loss: 0.00053827
Epoch [198/300], Train Loss: 0.000542
Validation Loss: 0.00053590
Epoch [199/300], Train Loss: 0.000539
Validation Loss: 0.00053618
Epoch [200/300], Train Loss: 0.000544
Validation Loss: 0.00053325
Epoch [201/300], Train Loss: 0.000542
Validation Loss: 0.00053281
Epoch [202/300], Train Loss: 0.000543
Validation Loss: 0.00053177
Epoch [203/300], Train Loss: 0.000540
Validation Loss: 0.00053191
Epoch [204/300], Train Loss: 0.000539
Validation Loss: 0.00053204
Epoch [205/300], Train Loss: 0.000541
Validation Loss: 0.00052942
Epoch [206/300], Train Loss: 0.000535
Validation Loss: 0.00052634
Epoch [207/300], Train Loss: 0.000537
Validation Loss: 0.00052689
Epoch [208/300], Train Loss: 0.000533
Validation Loss: 0.00052639
Epoch [209/300], Train Loss: 0.000536
Validation Loss: 0.00052197
Epoch [210/300], Train Loss: 0.000535
Validation Loss: 0.00052282
Epoch [211/300], Train Loss: 0.000533
Validation Loss: 0.00052038
Epoch [212/300], Train Loss: 0.000534
Validation Loss: 0.00052056
Epoch [213/300], Train Loss: 0.000526
Validation Loss: 0.00051659
Epoch [214/300], Train Loss: 0.000529
Validation Loss: 0.00051444
Epoch [215/300], Train Loss: 0.000524
Validation Loss: 0.00051640
Epoch [216/300], Train Loss: 0.000525
Validation Loss: 0.00051073
Epoch [217/300], Train Loss: 0.000520
Validation Loss: 0.00050867
Epoch [218/300], Train Loss: 0.000520
Validation Loss: 0.00050877
Epoch [219/300], Train Loss: 0.000520
Validation Loss: 0.00050279
Epoch [220/300], Train Loss: 0.000519
Validation Loss: 0.00050950
Epoch [221/300], Train Loss: 0.000517
Validation Loss: 0.00050796
Epoch [222/300], Train Loss: 0.000516
Validation Loss: 0.00050100
Epoch [223/300], Train Loss: 0.000512
Validation Loss: 0.00050031
Epoch [224/300], Train Loss: 0.000516
Validation Loss: 0.00050618
Epoch [225/300], Train Loss: 0.000515
Validation Loss: 0.00050543
Epoch [226/300], Train Loss: 0.000516
Validation Loss: 0.00049411
Epoch [227/300], Train Loss: 0.000508
Validation Loss: 0.00050021
Epoch [228/300], Train Loss: 0.000514
Validation Loss: 0.00050200
Epoch [229/300], Train Loss: 0.000503
Validation Loss: 0.00049017
Epoch [230/300], Train Loss: 0.000506
Validation Loss: 0.00049553
Epoch [231/300], Train Loss: 0.000504
Validation Loss: 0.00049375
Epoch [232/300], Train Loss: 0.000509
Validation Loss: 0.00049326
Epoch [233/300], Train Loss: 0.000506
Validation Loss: 0.00049193
Epoch [234/300], Train Loss: 0.000505
Validation Loss: 0.00049010
Epoch [235/300], Train Loss: 0.000509
Validation Loss: 0.00049239
Epoch [236/300], Train Loss: 0.000503
Validation Loss: 0.00048950
Epoch [237/300], Train Loss: 0.000503
Validation Loss: 0.00049049
Epoch [238/300], Train Loss: 0.000503
Validation Loss: 0.00049381
Epoch [239/300], Train Loss: 0.000502
Validation Loss: 0.00049352
Epoch [240/300], Train Loss: 0.000497
Validation Loss: 0.00048674
Epoch [241/300], Train Loss: 0.000499
Validation Loss: 0.00048839
Epoch [242/300], Train Loss: 0.000513
Validation Loss: 0.00049843
Epoch [243/300], Train Loss: 0.000501
Validation Loss: 0.00049133
Epoch [244/300], Train Loss: 0.000500
Validation Loss: 0.00048924
Epoch [245/300], Train Loss: 0.000498
Validation Loss: 0.00048603
Epoch [246/300], Train Loss: 0.000500
Validation Loss: 0.00048686
Epoch [247/300], Train Loss: 0.000500
Validation Loss: 0.00048661
Epoch [248/300], Train Loss: 0.000501
Validation Loss: 0.00048733
Epoch [249/300], Train Loss: 0.000496
Validation Loss: 0.00048534
Epoch [250/300], Train Loss: 0.000494
Validation Loss: 0.00048473
Epoch [251/300], Train Loss: 0.000497
Validation Loss: 0.00048578
Epoch [252/300], Train Loss: 0.000493
Validation Loss: 0.00048593
Epoch [253/300], Train Loss: 0.000494
Validation Loss: 0.00048335
Epoch [254/300], Train Loss: 0.000494
Validation Loss: 0.00048391
Epoch [255/300], Train Loss: 0.000495
Validation Loss: 0.00048924
Epoch [256/300], Train Loss: 0.000503
Validation Loss: 0.00048840
Epoch [257/300], Train Loss: 0.000497
Validation Loss: 0.00048694
Epoch [258/300], Train Loss: 0.000497
Validation Loss: 0.00048389
Epoch [259/300], Train Loss: 0.000498
Validation Loss: 0.00048313
Epoch [260/300], Train Loss: 0.000496
Validation Loss: 0.00048327
Epoch [261/300], Train Loss: 0.000495
Validation Loss: 0.00048166
Epoch [262/300], Train Loss: 0.000491
Validation Loss: 0.00048119
Epoch [263/300], Train Loss: 0.000493
Validation Loss: 0.00048231
Epoch [264/300], Train Loss: 0.000498
Validation Loss: 0.00048210
Epoch [265/300], Train Loss: 0.000494
Validation Loss: 0.00048192
Epoch [266/300], Train Loss: 0.000499
Validation Loss: 0.00048294
Epoch [267/300], Train Loss: 0.000493
Validation Loss: 0.00048548
Epoch [268/300], Train Loss: 0.000496
Validation Loss: 0.00048298
Epoch [269/300], Train Loss: 0.000493
Validation Loss: 0.00048103
Epoch [270/300], Train Loss: 0.000489
Validation Loss: 0.00048021
Epoch [271/300], Train Loss: 0.000488
Validation Loss: 0.00047966
Epoch [272/300], Train Loss: 0.000493
Validation Loss: 0.00047966
Epoch [273/300], Train Loss: 0.000491
Validation Loss: 0.00047969
Epoch [274/300], Train Loss: 0.000491
Validation Loss: 0.00048375
Epoch [275/300], Train Loss: 0.000495
Validation Loss: 0.00047983
Epoch [276/300], Train Loss: 0.000487
Validation Loss: 0.00047882
Epoch [277/300], Train Loss: 0.000486
Validation Loss: 0.00047837
Epoch [278/300], Train Loss: 0.000491
Validation Loss: 0.00048050
Epoch [279/300], Train Loss: 0.000490
Validation Loss: 0.00047840
Epoch [280/300], Train Loss: 0.000495
Validation Loss: 0.00047765
Epoch [281/300], Train Loss: 0.000487
Validation Loss: 0.00047715
Epoch [282/300], Train Loss: 0.000488
Validation Loss: 0.00047707
Epoch [283/300], Train Loss: 0.000487
Validation Loss: 0.00048160
Epoch [284/300], Train Loss: 0.000487
Validation Loss: 0.00048089
Epoch [285/300], Train Loss: 0.000493
Validation Loss: 0.00047634
Epoch [286/300], Train Loss: 0.000489
Validation Loss: 0.00047716
Epoch [287/300], Train Loss: 0.000489
Validation Loss: 0.00048209
Epoch [288/300], Train Loss: 0.000488
Validation Loss: 0.00047827
Epoch [289/300], Train Loss: 0.000490
Validation Loss: 0.00047746
Epoch [290/300], Train Loss: 0.000486
Validation Loss: 0.00047559
Epoch [291/300], Train Loss: 0.000491
Validation Loss: 0.00047598
Epoch [292/300], Train Loss: 0.000488
Validation Loss: 0.00047420
Epoch [293/300], Train Loss: 0.000491
Validation Loss: 0.00047398
Epoch [294/300], Train Loss: 0.000488
Validation Loss: 0.00047432
Epoch [295/300], Train Loss: 0.000484
Validation Loss: 0.00047462
Epoch [296/300], Train Loss: 0.000486
Validation Loss: 0.00047445
Epoch [297/300], Train Loss: 0.000487
Validation Loss: 0.00047511
Epoch [298/300], Train Loss: 0.000484
Validation Loss: 0.00047433
Epoch [299/300], Train Loss: 0.000482
Validation Loss: 0.00047314
Epoch [300/300], Train Loss: 0.000486
Validation Loss: 0.00047544

Evaluating model for: Fridge
Run 47/72 completed in 3215.35 seconds with: {'MAE': np.float32(31.81191), 'MSE': np.float32(1645.9637), 'RMSE': np.float32(40.57048), 'SAE': np.float32(0.017678266), 'NDE': np.float32(0.70305824)}

Run 48/72: hidden=256, seq_len=720, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.000692
Validation Loss: 0.00067303
Epoch [2/300], Train Loss: 0.000663
Validation Loss: 0.00066802
Epoch [3/300], Train Loss: 0.000663
Validation Loss: 0.00066645
Epoch [4/300], Train Loss: 0.000664
Validation Loss: 0.00066634
Epoch [5/300], Train Loss: 0.000662
Validation Loss: 0.00066674
Epoch [6/300], Train Loss: 0.000662
Validation Loss: 0.00066596
Epoch [7/300], Train Loss: 0.000661
Validation Loss: 0.00066562
Epoch [8/300], Train Loss: 0.000659
Validation Loss: 0.00066465
Epoch [9/300], Train Loss: 0.000658
Validation Loss: 0.00066202
Epoch [10/300], Train Loss: 0.000645
Validation Loss: 0.00065806
Epoch [11/300], Train Loss: 0.000637
Validation Loss: 0.00064577
Epoch [12/300], Train Loss: 0.000629
Validation Loss: 0.00063240
Epoch [13/300], Train Loss: 0.000617
Validation Loss: 0.00063060
Epoch [14/300], Train Loss: 0.000614
Validation Loss: 0.00062546
Epoch [15/300], Train Loss: 0.000611
Validation Loss: 0.00062326
Epoch [16/300], Train Loss: 0.000608
Validation Loss: 0.00061817
Epoch [17/300], Train Loss: 0.000608
Validation Loss: 0.00061821
Epoch [18/300], Train Loss: 0.000599
Validation Loss: 0.00061714
Epoch [19/300], Train Loss: 0.000604
Validation Loss: 0.00061691
Epoch [20/300], Train Loss: 0.000599
Validation Loss: 0.00061617
Epoch [21/300], Train Loss: 0.000602
Validation Loss: 0.00061258
Epoch [22/300], Train Loss: 0.000602
Validation Loss: 0.00061709
Epoch [23/300], Train Loss: 0.000602
Validation Loss: 0.00061323
Epoch [24/300], Train Loss: 0.000604
Validation Loss: 0.00061557
Epoch [25/300], Train Loss: 0.000602
Validation Loss: 0.00061507
Epoch [26/300], Train Loss: 0.000601
Validation Loss: 0.00061234
Epoch [27/300], Train Loss: 0.000601
Validation Loss: 0.00061139
Epoch [28/300], Train Loss: 0.000597
Validation Loss: 0.00061331
Epoch [29/300], Train Loss: 0.000600
Validation Loss: 0.00060922
Epoch [30/300], Train Loss: 0.000601
Validation Loss: 0.00061595
Epoch [31/300], Train Loss: 0.000596
Validation Loss: 0.00061003
Epoch [32/300], Train Loss: 0.000598
Validation Loss: 0.00061034
Epoch [33/300], Train Loss: 0.000595
Validation Loss: 0.00060910
Epoch [34/300], Train Loss: 0.000597
Validation Loss: 0.00061350
Epoch [35/300], Train Loss: 0.000597
Validation Loss: 0.00060853
Epoch [36/300], Train Loss: 0.000595
Validation Loss: 0.00060609
Epoch [37/300], Train Loss: 0.000594
Validation Loss: 0.00060567
Epoch [38/300], Train Loss: 0.000598
Validation Loss: 0.00060455
Epoch [39/300], Train Loss: 0.000592
Validation Loss: 0.00060395
Epoch [40/300], Train Loss: 0.000593
Validation Loss: 0.00060331
Epoch [41/300], Train Loss: 0.000593
Validation Loss: 0.00060531
Epoch [42/300], Train Loss: 0.000592
Validation Loss: 0.00060471
Epoch [43/300], Train Loss: 0.000593
Validation Loss: 0.00060247
Epoch [44/300], Train Loss: 0.000595
Validation Loss: 0.00060664
Epoch [45/300], Train Loss: 0.000592
Validation Loss: 0.00060201
Epoch [46/300], Train Loss: 0.000588
Validation Loss: 0.00060292
Epoch [47/300], Train Loss: 0.000590
Validation Loss: 0.00060104
Epoch [48/300], Train Loss: 0.000588
Validation Loss: 0.00060249
Epoch [49/300], Train Loss: 0.000588
Validation Loss: 0.00060154
Epoch [50/300], Train Loss: 0.000592
Validation Loss: 0.00060125
Epoch [51/300], Train Loss: 0.000590
Validation Loss: 0.00060513
Epoch [52/300], Train Loss: 0.000591
Validation Loss: 0.00059666
Epoch [53/300], Train Loss: 0.000584
Validation Loss: 0.00059851
Epoch [54/300], Train Loss: 0.000585
Validation Loss: 0.00059550
Epoch [55/300], Train Loss: 0.000588
Validation Loss: 0.00059367
Epoch [56/300], Train Loss: 0.000583
Validation Loss: 0.00059387
Epoch [57/300], Train Loss: 0.000589
Validation Loss: 0.00059606
Epoch [58/300], Train Loss: 0.000586
Validation Loss: 0.00059396
Epoch [59/300], Train Loss: 0.000582
Validation Loss: 0.00059314
Epoch [60/300], Train Loss: 0.000587
Validation Loss: 0.00059181
Epoch [61/300], Train Loss: 0.000580
Validation Loss: 0.00059214
Epoch [62/300], Train Loss: 0.000584
Validation Loss: 0.00059081
Epoch [63/300], Train Loss: 0.000584
Validation Loss: 0.00059120
Epoch [64/300], Train Loss: 0.000579
Validation Loss: 0.00059043
Epoch [65/300], Train Loss: 0.000577
Validation Loss: 0.00058806
Epoch [66/300], Train Loss: 0.000579
Validation Loss: 0.00058819
Epoch [67/300], Train Loss: 0.000579
Validation Loss: 0.00058824
Epoch [68/300], Train Loss: 0.000577
Validation Loss: 0.00058769
Epoch [69/300], Train Loss: 0.000580
Validation Loss: 0.00058649
Epoch [70/300], Train Loss: 0.000574
Validation Loss: 0.00058538
Epoch [71/300], Train Loss: 0.000576
Validation Loss: 0.00058855
Epoch [72/300], Train Loss: 0.000578
Validation Loss: 0.00058651
Epoch [73/300], Train Loss: 0.000576
Validation Loss: 0.00058578
Epoch [74/300], Train Loss: 0.000576
Validation Loss: 0.00058267
Epoch [75/300], Train Loss: 0.000573
Validation Loss: 0.00058319
Epoch [76/300], Train Loss: 0.000571
Validation Loss: 0.00057812
Epoch [77/300], Train Loss: 0.000572
Validation Loss: 0.00057787
Epoch [78/300], Train Loss: 0.000573
Validation Loss: 0.00057709
Epoch [79/300], Train Loss: 0.000568
Validation Loss: 0.00057587
Epoch [80/300], Train Loss: 0.000570
Validation Loss: 0.00057444
Epoch [81/300], Train Loss: 0.000570
Validation Loss: 0.00057367
Epoch [82/300], Train Loss: 0.000566
Validation Loss: 0.00057202
Epoch [83/300], Train Loss: 0.000569
Validation Loss: 0.00056921
Epoch [84/300], Train Loss: 0.000563
Validation Loss: 0.00056890
Epoch [85/300], Train Loss: 0.000566
Validation Loss: 0.00057145
Epoch [86/300], Train Loss: 0.000560
Validation Loss: 0.00056452
Epoch [87/300], Train Loss: 0.000555
Validation Loss: 0.00056761
Epoch [88/300], Train Loss: 0.000560
Validation Loss: 0.00056038
Epoch [89/300], Train Loss: 0.000558
Validation Loss: 0.00055960
Epoch [90/300], Train Loss: 0.000559
Validation Loss: 0.00055677
Epoch [91/300], Train Loss: 0.000558
Validation Loss: 0.00055489
Epoch [92/300], Train Loss: 0.000554
Validation Loss: 0.00055317
Epoch [93/300], Train Loss: 0.000554
Validation Loss: 0.00055175
Epoch [94/300], Train Loss: 0.000546
Validation Loss: 0.00055255
Epoch [95/300], Train Loss: 0.000550
Validation Loss: 0.00055869
Epoch [96/300], Train Loss: 0.000550
Validation Loss: 0.00054714
Epoch [97/300], Train Loss: 0.000552
Validation Loss: 0.00054290
Epoch [98/300], Train Loss: 0.000544
Validation Loss: 0.00054171
Epoch [99/300], Train Loss: 0.000543
Validation Loss: 0.00053807
Epoch [100/300], Train Loss: 0.000543
Validation Loss: 0.00053819
Epoch [101/300], Train Loss: 0.000544
Validation Loss: 0.00054172
Epoch [102/300], Train Loss: 0.000539
Validation Loss: 0.00053326
Epoch [103/300], Train Loss: 0.000537
Validation Loss: 0.00052975
Epoch [104/300], Train Loss: 0.000534
Validation Loss: 0.00053607
Epoch [105/300], Train Loss: 0.000538
Validation Loss: 0.00052516
Epoch [106/300], Train Loss: 0.000534
Validation Loss: 0.00052517
Epoch [107/300], Train Loss: 0.000535
Validation Loss: 0.00052611
Epoch [108/300], Train Loss: 0.000534
Validation Loss: 0.00052559
Epoch [109/300], Train Loss: 0.000528
Validation Loss: 0.00052593
Epoch [110/300], Train Loss: 0.000529
Validation Loss: 0.00052391
Epoch [111/300], Train Loss: 0.000528
Validation Loss: 0.00052090
Epoch [112/300], Train Loss: 0.000530
Validation Loss: 0.00052015
Epoch [113/300], Train Loss: 0.000527
Validation Loss: 0.00052093
Epoch [114/300], Train Loss: 0.000527
Validation Loss: 0.00052373
Epoch [115/300], Train Loss: 0.000525
Validation Loss: 0.00052524
Epoch [116/300], Train Loss: 0.000525
Validation Loss: 0.00052008
Epoch [117/300], Train Loss: 0.000525
Validation Loss: 0.00051980
Epoch [118/300], Train Loss: 0.000533
Validation Loss: 0.00052147
Epoch [119/300], Train Loss: 0.000526
Validation Loss: 0.00051911
Epoch [120/300], Train Loss: 0.000524
Validation Loss: 0.00051582
Epoch [121/300], Train Loss: 0.000521
Validation Loss: 0.00051570
Epoch [122/300], Train Loss: 0.000521
Validation Loss: 0.00051854
Epoch [123/300], Train Loss: 0.000526
Validation Loss: 0.00051883
Epoch [124/300], Train Loss: 0.000522
Validation Loss: 0.00051410
Epoch [125/300], Train Loss: 0.000524
Validation Loss: 0.00051957
Epoch [126/300], Train Loss: 0.000522
Validation Loss: 0.00051284
Epoch [127/300], Train Loss: 0.000519
Validation Loss: 0.00051380
Epoch [128/300], Train Loss: 0.000518
Validation Loss: 0.00051528
Epoch [129/300], Train Loss: 0.000517
Validation Loss: 0.00051510
Epoch [130/300], Train Loss: 0.000519
Validation Loss: 0.00051018
Epoch [131/300], Train Loss: 0.000517
Validation Loss: 0.00051527
Epoch [132/300], Train Loss: 0.000519
Validation Loss: 0.00051581
Epoch [133/300], Train Loss: 0.000517
Validation Loss: 0.00051352
Epoch [134/300], Train Loss: 0.000514
Validation Loss: 0.00051106
Epoch [135/300], Train Loss: 0.000515
Validation Loss: 0.00051086
Epoch [136/300], Train Loss: 0.000518
Validation Loss: 0.00051077
Epoch [137/300], Train Loss: 0.000514
Validation Loss: 0.00051084
Epoch [138/300], Train Loss: 0.000517
Validation Loss: 0.00050792
Epoch [139/300], Train Loss: 0.000513
Validation Loss: 0.00050742
Epoch [140/300], Train Loss: 0.000517
Validation Loss: 0.00050815
Epoch [141/300], Train Loss: 0.000516
Validation Loss: 0.00051221
Epoch [142/300], Train Loss: 0.000511
Validation Loss: 0.00050897
Epoch [143/300], Train Loss: 0.000515
Validation Loss: 0.00050716
Epoch [144/300], Train Loss: 0.000513
Validation Loss: 0.00050720
Epoch [145/300], Train Loss: 0.000512
Validation Loss: 0.00050629
Epoch [146/300], Train Loss: 0.000510
Validation Loss: 0.00050455
Epoch [147/300], Train Loss: 0.000509
Validation Loss: 0.00050670
Epoch [148/300], Train Loss: 0.000509
Validation Loss: 0.00050299
Epoch [149/300], Train Loss: 0.000508
Validation Loss: 0.00050350
Epoch [150/300], Train Loss: 0.000512
Validation Loss: 0.00050722
Epoch [151/300], Train Loss: 0.000507
Validation Loss: 0.00050069
Epoch [152/300], Train Loss: 0.000513
Validation Loss: 0.00050432
Epoch [153/300], Train Loss: 0.000505
Validation Loss: 0.00050258
Epoch [154/300], Train Loss: 0.000508
Validation Loss: 0.00050104
Epoch [155/300], Train Loss: 0.000505
Validation Loss: 0.00049732
Epoch [156/300], Train Loss: 0.000504
Validation Loss: 0.00049639
Epoch [157/300], Train Loss: 0.000507
Validation Loss: 0.00049568
Epoch [158/300], Train Loss: 0.000501
Validation Loss: 0.00049985
Epoch [159/300], Train Loss: 0.000497
Validation Loss: 0.00048877
Epoch [160/300], Train Loss: 0.000507
Validation Loss: 0.00050703
Epoch [161/300], Train Loss: 0.000499
Validation Loss: 0.00049069
Epoch [162/300], Train Loss: 0.000494
Validation Loss: 0.00049061
Epoch [163/300], Train Loss: 0.000498
Validation Loss: 0.00048649
Epoch [164/300], Train Loss: 0.000496
Validation Loss: 0.00048271
Epoch [165/300], Train Loss: 0.000496
Validation Loss: 0.00048202
Epoch [166/300], Train Loss: 0.000494
Validation Loss: 0.00048750
Epoch [167/300], Train Loss: 0.000495
Validation Loss: 0.00048384
Epoch [168/300], Train Loss: 0.000489
Validation Loss: 0.00047961
Epoch [169/300], Train Loss: 0.000496
Validation Loss: 0.00048086
Epoch [170/300], Train Loss: 0.000494
Validation Loss: 0.00048388
Epoch [171/300], Train Loss: 0.000498
Validation Loss: 0.00048011
Epoch [172/300], Train Loss: 0.000491
Validation Loss: 0.00051304
Epoch [173/300], Train Loss: 0.000498
Validation Loss: 0.00048118
Epoch [174/300], Train Loss: 0.000492
Validation Loss: 0.00047986
Epoch [175/300], Train Loss: 0.000493
Validation Loss: 0.00047820
Epoch [176/300], Train Loss: 0.000488
Validation Loss: 0.00048220
Epoch [177/300], Train Loss: 0.000509
Validation Loss: 0.00051357
Epoch [178/300], Train Loss: 0.000522
Validation Loss: 0.00050814
Epoch [179/300], Train Loss: 0.000501
Validation Loss: 0.00048105
Epoch [180/300], Train Loss: 0.000498
Validation Loss: 0.00049710
Epoch [181/300], Train Loss: 0.000496
Validation Loss: 0.00048082
Epoch [182/300], Train Loss: 0.000490
Validation Loss: 0.00047454
Epoch [183/300], Train Loss: 0.000511
Validation Loss: 0.00050032
Epoch [184/300], Train Loss: 0.000497
Validation Loss: 0.00047838
Epoch [185/300], Train Loss: 0.000491
Validation Loss: 0.00047814
Epoch [186/300], Train Loss: 0.000485
Validation Loss: 0.00047437
Epoch [187/300], Train Loss: 0.000485
Validation Loss: 0.00047549
Epoch [188/300], Train Loss: 0.000488
Validation Loss: 0.00047596
Epoch [189/300], Train Loss: 0.000485
Validation Loss: 0.00047188
Epoch [190/300], Train Loss: 0.000486
Validation Loss: 0.00047077
Epoch [191/300], Train Loss: 0.000488
Validation Loss: 0.00047595
Epoch [192/300], Train Loss: 0.000489
Validation Loss: 0.00047445
Epoch [193/300], Train Loss: 0.000487
Validation Loss: 0.00047453
Epoch [194/300], Train Loss: 0.000485
Validation Loss: 0.00049009
Epoch [195/300], Train Loss: 0.000483
Validation Loss: 0.00047271
Epoch [196/300], Train Loss: 0.000486
Validation Loss: 0.00047165
Epoch [197/300], Train Loss: 0.000479
Validation Loss: 0.00047986
Epoch [198/300], Train Loss: 0.000483
Validation Loss: 0.00047185
Epoch [199/300], Train Loss: 0.000481
Validation Loss: 0.00047283
Epoch [200/300], Train Loss: 0.000484
Validation Loss: 0.00047488
Early stopping triggered

Evaluating model for: Fridge
Run 48/72 completed in 2877.56 seconds with: {'MAE': np.float32(31.208801), 'MSE': np.float32(1629.2882), 'RMSE': np.float32(40.36444), 'SAE': np.float32(0.0030311414), 'NDE': np.float32(0.6994877)}

Run 49/72: hidden=512, seq_len=120, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000637
Validation Loss: 0.00059497
Epoch [2/300], Train Loss: 0.000607
Validation Loss: 0.00058877
Epoch [3/300], Train Loss: 0.000598
Validation Loss: 0.00058256
Epoch [4/300], Train Loss: 0.000589
Validation Loss: 0.00057446
Epoch [5/300], Train Loss: 0.000580
Validation Loss: 0.00056315
Epoch [6/300], Train Loss: 0.000570
Validation Loss: 0.00055551
Epoch [7/300], Train Loss: 0.000564
Validation Loss: 0.00055366
Epoch [8/300], Train Loss: 0.000559
Validation Loss: 0.00054062
Epoch [9/300], Train Loss: 0.000548
Validation Loss: 0.00053125
Epoch [10/300], Train Loss: 0.000535
Validation Loss: 0.00052708
Epoch [11/300], Train Loss: 0.000526
Validation Loss: 0.00051098
Epoch [12/300], Train Loss: 0.000513
Validation Loss: 0.00049541
Epoch [13/300], Train Loss: 0.000504
Validation Loss: 0.00048523
Epoch [14/300], Train Loss: 0.000493
Validation Loss: 0.00047825
Epoch [15/300], Train Loss: 0.000481
Validation Loss: 0.00045534
Epoch [16/300], Train Loss: 0.000463
Validation Loss: 0.00044173
Epoch [17/300], Train Loss: 0.000451
Validation Loss: 0.00044181
Epoch [18/300], Train Loss: 0.000441
Validation Loss: 0.00042201
Epoch [19/300], Train Loss: 0.000431
Validation Loss: 0.00042264
Epoch [20/300], Train Loss: 0.000420
Validation Loss: 0.00040946
Epoch [21/300], Train Loss: 0.000412
Validation Loss: 0.00040156
Epoch [22/300], Train Loss: 0.000399
Validation Loss: 0.00040646
Epoch [23/300], Train Loss: 0.000391
Validation Loss: 0.00037290
Epoch [24/300], Train Loss: 0.000379
Validation Loss: 0.00037746
Epoch [25/300], Train Loss: 0.000370
Validation Loss: 0.00037427
Epoch [26/300], Train Loss: 0.000365
Validation Loss: 0.00035118
Epoch [27/300], Train Loss: 0.000356
Validation Loss: 0.00034339
Epoch [28/300], Train Loss: 0.000352
Validation Loss: 0.00034060
Epoch [29/300], Train Loss: 0.000347
Validation Loss: 0.00044473
Epoch [30/300], Train Loss: 0.000346
Validation Loss: 0.00033086
Epoch [31/300], Train Loss: 0.000333
Validation Loss: 0.00032525
Epoch [32/300], Train Loss: 0.000330
Validation Loss: 0.00032213
Epoch [33/300], Train Loss: 0.000323
Validation Loss: 0.00031898
Epoch [34/300], Train Loss: 0.000322
Validation Loss: 0.00031979
Epoch [35/300], Train Loss: 0.000323
Validation Loss: 0.00032263
Epoch [36/300], Train Loss: 0.000313
Validation Loss: 0.00030696
Epoch [37/300], Train Loss: 0.000310
Validation Loss: 0.00030977
Epoch [38/300], Train Loss: 0.000310
Validation Loss: 0.00030163
Epoch [39/300], Train Loss: 0.000303
Validation Loss: 0.00029487
Epoch [40/300], Train Loss: 0.000301
Validation Loss: 0.00029597
Epoch [41/300], Train Loss: 0.000300
Validation Loss: 0.00029516
Epoch [42/300], Train Loss: 0.000294
Validation Loss: 0.00028711
Epoch [43/300], Train Loss: 0.000289
Validation Loss: 0.00029414
Epoch [44/300], Train Loss: 0.000288
Validation Loss: 0.00027586
Epoch [45/300], Train Loss: 0.000284
Validation Loss: 0.00030959
Epoch [46/300], Train Loss: 0.000280
Validation Loss: 0.00027741
Epoch [47/300], Train Loss: 0.000280
Validation Loss: 0.00027609
Epoch [48/300], Train Loss: 0.000280
Validation Loss: 0.00027326
Epoch [49/300], Train Loss: 0.000275
Validation Loss: 0.00027204
Epoch [50/300], Train Loss: 0.000273
Validation Loss: 0.00026791
Epoch [51/300], Train Loss: 0.000273
Validation Loss: 0.00026527
Epoch [52/300], Train Loss: 0.000269
Validation Loss: 0.00026331
Epoch [53/300], Train Loss: 0.000265
Validation Loss: 0.00026376
Epoch [54/300], Train Loss: 0.000265
Validation Loss: 0.00028046
Epoch [55/300], Train Loss: 0.000261
Validation Loss: 0.00026630
Epoch [56/300], Train Loss: 0.000262
Validation Loss: 0.00025552
Epoch [57/300], Train Loss: 0.000259
Validation Loss: 0.00025116
Epoch [58/300], Train Loss: 0.000259
Validation Loss: 0.00025009
Epoch [59/300], Train Loss: 0.000259
Validation Loss: 0.00024585
Epoch [60/300], Train Loss: 0.000252
Validation Loss: 0.00024508
Epoch [61/300], Train Loss: 0.000253
Validation Loss: 0.00024232
Epoch [62/300], Train Loss: 0.000248
Validation Loss: 0.00024725
Epoch [63/300], Train Loss: 0.000249
Validation Loss: 0.00023753
Epoch [64/300], Train Loss: 0.000246
Validation Loss: 0.00023860
Epoch [65/300], Train Loss: 0.000246
Validation Loss: 0.00023233
Epoch [66/300], Train Loss: 0.000243
Validation Loss: 0.00023892
Epoch [67/300], Train Loss: 0.000244
Validation Loss: 0.00023105
Epoch [68/300], Train Loss: 0.000241
Validation Loss: 0.00025151
Epoch [69/300], Train Loss: 0.000239
Validation Loss: 0.00023330
Epoch [70/300], Train Loss: 0.000238
Validation Loss: 0.00022979
Epoch [71/300], Train Loss: 0.000240
Validation Loss: 0.00024503
Epoch [72/300], Train Loss: 0.000234
Validation Loss: 0.00023744
Epoch [73/300], Train Loss: 0.000232
Validation Loss: 0.00023487
Epoch [74/300], Train Loss: 0.000233
Validation Loss: 0.00022251
Epoch [75/300], Train Loss: 0.000233
Validation Loss: 0.00023352
Epoch [76/300], Train Loss: 0.000231
Validation Loss: 0.00022583
Epoch [77/300], Train Loss: 0.000230
Validation Loss: 0.00022569
Epoch [78/300], Train Loss: 0.000225
Validation Loss: 0.00021992
Epoch [79/300], Train Loss: 0.000231
Validation Loss: 0.00021541
Epoch [80/300], Train Loss: 0.000224
Validation Loss: 0.00022232
Epoch [81/300], Train Loss: 0.000222
Validation Loss: 0.00021598
Epoch [82/300], Train Loss: 0.000223
Validation Loss: 0.00022933
Epoch [83/300], Train Loss: 0.000225
Validation Loss: 0.00022286
Epoch [84/300], Train Loss: 0.000221
Validation Loss: 0.00022774
Epoch [85/300], Train Loss: 0.000224
Validation Loss: 0.00023903
Epoch [86/300], Train Loss: 0.000222
Validation Loss: 0.00022033
Epoch [87/300], Train Loss: 0.000217
Validation Loss: 0.00021724
Epoch [88/300], Train Loss: 0.000216
Validation Loss: 0.00021146
Epoch [89/300], Train Loss: 0.000220
Validation Loss: 0.00021437
Epoch [90/300], Train Loss: 0.000216
Validation Loss: 0.00020891
Epoch [91/300], Train Loss: 0.000213
Validation Loss: 0.00020263
Epoch [92/300], Train Loss: 0.000213
Validation Loss: 0.00021217
Epoch [93/300], Train Loss: 0.000208
Validation Loss: 0.00021011
Epoch [94/300], Train Loss: 0.000208
Validation Loss: 0.00021569
Epoch [95/300], Train Loss: 0.000209
Validation Loss: 0.00020846
Epoch [96/300], Train Loss: 0.000211
Validation Loss: 0.00020028
Epoch [97/300], Train Loss: 0.000209
Validation Loss: 0.00021029
Epoch [98/300], Train Loss: 0.000205
Validation Loss: 0.00020092
Epoch [99/300], Train Loss: 0.000205
Validation Loss: 0.00019793
Epoch [100/300], Train Loss: 0.000207
Validation Loss: 0.00019947
Epoch [101/300], Train Loss: 0.000207
Validation Loss: 0.00019931
Epoch [102/300], Train Loss: 0.000203
Validation Loss: 0.00019500
Epoch [103/300], Train Loss: 0.000203
Validation Loss: 0.00019534
Epoch [104/300], Train Loss: 0.000203
Validation Loss: 0.00019419
Epoch [105/300], Train Loss: 0.000201
Validation Loss: 0.00019729
Epoch [106/300], Train Loss: 0.000198
Validation Loss: 0.00019131
Epoch [107/300], Train Loss: 0.000199
Validation Loss: 0.00020766
Epoch [108/300], Train Loss: 0.000200
Validation Loss: 0.00019386
Epoch [109/300], Train Loss: 0.000197
Validation Loss: 0.00020116
Epoch [110/300], Train Loss: 0.000198
Validation Loss: 0.00019213
Epoch [111/300], Train Loss: 0.000197
Validation Loss: 0.00019235
Epoch [112/300], Train Loss: 0.000194
Validation Loss: 0.00019066
Epoch [113/300], Train Loss: 0.000196
Validation Loss: 0.00018992
Epoch [114/300], Train Loss: 0.000195
Validation Loss: 0.00018756
Epoch [115/300], Train Loss: 0.000193
Validation Loss: 0.00019284
Epoch [116/300], Train Loss: 0.000195
Validation Loss: 0.00019393
Epoch [117/300], Train Loss: 0.000192
Validation Loss: 0.00018741
Epoch [118/300], Train Loss: 0.000192
Validation Loss: 0.00019321
Epoch [119/300], Train Loss: 0.000192
Validation Loss: 0.00018275
Epoch [120/300], Train Loss: 0.000195
Validation Loss: 0.00018855
Epoch [121/300], Train Loss: 0.000190
Validation Loss: 0.00018390
Epoch [122/300], Train Loss: 0.000189
Validation Loss: 0.00019233
Epoch [123/300], Train Loss: 0.000189
Validation Loss: 0.00018476
Epoch [124/300], Train Loss: 0.000189
Validation Loss: 0.00018988
Epoch [125/300], Train Loss: 0.000188
Validation Loss: 0.00018135
Epoch [126/300], Train Loss: 0.000189
Validation Loss: 0.00018304
Epoch [127/300], Train Loss: 0.000187
Validation Loss: 0.00018743
Epoch [128/300], Train Loss: 0.000187
Validation Loss: 0.00018247
Epoch [129/300], Train Loss: 0.000185
Validation Loss: 0.00018366
Epoch [130/300], Train Loss: 0.000184
Validation Loss: 0.00017744
Epoch [131/300], Train Loss: 0.000186
Validation Loss: 0.00019270
Epoch [132/300], Train Loss: 0.000186
Validation Loss: 0.00018130
Epoch [133/300], Train Loss: 0.000188
Validation Loss: 0.00018276
Epoch [134/300], Train Loss: 0.000187
Validation Loss: 0.00017664
Epoch [135/300], Train Loss: 0.000184
Validation Loss: 0.00017686
Epoch [136/300], Train Loss: 0.000181
Validation Loss: 0.00017572
Epoch [137/300], Train Loss: 0.000182
Validation Loss: 0.00019056
Epoch [138/300], Train Loss: 0.000184
Validation Loss: 0.00017889
Epoch [139/300], Train Loss: 0.000181
Validation Loss: 0.00018089
Epoch [140/300], Train Loss: 0.000181
Validation Loss: 0.00017819
Epoch [141/300], Train Loss: 0.000180
Validation Loss: 0.00017891
Epoch [142/300], Train Loss: 0.000182
Validation Loss: 0.00017567
Epoch [143/300], Train Loss: 0.000183
Validation Loss: 0.00017437
Epoch [144/300], Train Loss: 0.000182
Validation Loss: 0.00018599
Epoch [145/300], Train Loss: 0.000179
Validation Loss: 0.00017285
Epoch [146/300], Train Loss: 0.000178
Validation Loss: 0.00017844
Epoch [147/300], Train Loss: 0.000179
Validation Loss: 0.00017380
Epoch [148/300], Train Loss: 0.000179
Validation Loss: 0.00018718
Epoch [149/300], Train Loss: 0.000179
Validation Loss: 0.00017260
Epoch [150/300], Train Loss: 0.000177
Validation Loss: 0.00018205
Epoch [151/300], Train Loss: 0.000177
Validation Loss: 0.00017281
Epoch [152/300], Train Loss: 0.000174
Validation Loss: 0.00017130
Epoch [153/300], Train Loss: 0.000177
Validation Loss: 0.00017506
Epoch [154/300], Train Loss: 0.000176
Validation Loss: 0.00018052
Epoch [155/300], Train Loss: 0.000176
Validation Loss: 0.00017727
Epoch [156/300], Train Loss: 0.000175
Validation Loss: 0.00017485
Epoch [157/300], Train Loss: 0.000179
Validation Loss: 0.00017973
Epoch [158/300], Train Loss: 0.000177
Validation Loss: 0.00017925
Epoch [159/300], Train Loss: 0.000176
Validation Loss: 0.00017275
Epoch [160/300], Train Loss: 0.000174
Validation Loss: 0.00016970
Epoch [161/300], Train Loss: 0.000173
Validation Loss: 0.00017522
Epoch [162/300], Train Loss: 0.000174
Validation Loss: 0.00016953
Epoch [163/300], Train Loss: 0.000176
Validation Loss: 0.00016964
Epoch [164/300], Train Loss: 0.000171
Validation Loss: 0.00017137
Epoch [165/300], Train Loss: 0.000182
Validation Loss: 0.00017024
Epoch [166/300], Train Loss: 0.000169
Validation Loss: 0.00016721
Epoch [167/300], Train Loss: 0.000170
Validation Loss: 0.00016448
Epoch [168/300], Train Loss: 0.000170
Validation Loss: 0.00016819
Epoch [169/300], Train Loss: 0.000170
Validation Loss: 0.00016437
Epoch [170/300], Train Loss: 0.000173
Validation Loss: 0.00016632
Epoch [171/300], Train Loss: 0.000169
Validation Loss: 0.00016993
Epoch [172/300], Train Loss: 0.000172
Validation Loss: 0.00016547
Epoch [173/300], Train Loss: 0.000169
Validation Loss: 0.00016726
Epoch [174/300], Train Loss: 0.000172
Validation Loss: 0.00016538
Epoch [175/300], Train Loss: 0.000167
Validation Loss: 0.00016669
Epoch [176/300], Train Loss: 0.000169
Validation Loss: 0.00017407
Epoch [177/300], Train Loss: 0.000169
Validation Loss: 0.00017029
Epoch [178/300], Train Loss: 0.000169
Validation Loss: 0.00016880
Epoch [179/300], Train Loss: 0.000168
Validation Loss: 0.00016357
Epoch [180/300], Train Loss: 0.000168
Validation Loss: 0.00016375
Epoch [181/300], Train Loss: 0.000168
Validation Loss: 0.00016138
Epoch [182/300], Train Loss: 0.000166
Validation Loss: 0.00016368
Epoch [183/300], Train Loss: 0.000170
Validation Loss: 0.00016570
Epoch [184/300], Train Loss: 0.000166
Validation Loss: 0.00016155
Epoch [185/300], Train Loss: 0.000172
Validation Loss: 0.00016477
Epoch [186/300], Train Loss: 0.000172
Validation Loss: 0.00017217
Epoch [187/300], Train Loss: 0.000169
Validation Loss: 0.00016131
Epoch [188/300], Train Loss: 0.000165
Validation Loss: 0.00016181
Epoch [189/300], Train Loss: 0.000164
Validation Loss: 0.00015988
Epoch [190/300], Train Loss: 0.000165
Validation Loss: 0.00016579
Epoch [191/300], Train Loss: 0.000179
Validation Loss: 0.00016243
Epoch [192/300], Train Loss: 0.000164
Validation Loss: 0.00016373
Epoch [193/300], Train Loss: 0.000164
Validation Loss: 0.00015946
Epoch [194/300], Train Loss: 0.000164
Validation Loss: 0.00016248
Epoch [195/300], Train Loss: 0.000164
Validation Loss: 0.00016042
Epoch [196/300], Train Loss: 0.000163
Validation Loss: 0.00016068
Epoch [197/300], Train Loss: 0.000164
Validation Loss: 0.00018203
Epoch [198/300], Train Loss: 0.000166
Validation Loss: 0.00016094
Epoch [199/300], Train Loss: 0.000162
Validation Loss: 0.00016256
Epoch [200/300], Train Loss: 0.000162
Validation Loss: 0.00016130
Epoch [201/300], Train Loss: 0.000164
Validation Loss: 0.00016013
Epoch [202/300], Train Loss: 0.000162
Validation Loss: 0.00016441
Epoch [203/300], Train Loss: 0.000160
Validation Loss: 0.00016104
Early stopping triggered

Evaluating model for: Fridge
Run 49/72 completed in 13286.34 seconds with: {'MAE': np.float32(13.657178), 'MSE': np.float32(524.0162), 'RMSE': np.float32(22.8914), 'SAE': np.float32(0.034612782), 'NDE': np.float32(0.39658117)}

Run 50/72: hidden=512, seq_len=120, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000647
Validation Loss: 0.00060555
Epoch [2/300], Train Loss: 0.000613
Validation Loss: 0.00059104
Epoch [3/300], Train Loss: 0.000601
Validation Loss: 0.00058752
Epoch [4/300], Train Loss: 0.000593
Validation Loss: 0.00057631
Epoch [5/300], Train Loss: 0.000589
Validation Loss: 0.00057264
Epoch [6/300], Train Loss: 0.000581
Validation Loss: 0.00056546
Epoch [7/300], Train Loss: 0.000575
Validation Loss: 0.00056148
Epoch [8/300], Train Loss: 0.000570
Validation Loss: 0.00055762
Epoch [9/300], Train Loss: 0.000561
Validation Loss: 0.00054684
Epoch [10/300], Train Loss: 0.000552
Validation Loss: 0.00055245
Epoch [11/300], Train Loss: 0.000545
Validation Loss: 0.00053333
Epoch [12/300], Train Loss: 0.000533
Validation Loss: 0.00051726
Epoch [13/300], Train Loss: 0.000520
Validation Loss: 0.00050459
Epoch [14/300], Train Loss: 0.000509
Validation Loss: 0.00049040
Epoch [15/300], Train Loss: 0.000498
Validation Loss: 0.00048537
Epoch [16/300], Train Loss: 0.000490
Validation Loss: 0.00048180
Epoch [17/300], Train Loss: 0.000478
Validation Loss: 0.00046802
Epoch [18/300], Train Loss: 0.000467
Validation Loss: 0.00046117
Epoch [19/300], Train Loss: 0.000460
Validation Loss: 0.00044749
Epoch [20/300], Train Loss: 0.000447
Validation Loss: 0.00043000
Epoch [21/300], Train Loss: 0.000437
Validation Loss: 0.00042607
Epoch [22/300], Train Loss: 0.000419
Validation Loss: 0.00040475
Epoch [23/300], Train Loss: 0.000403
Validation Loss: 0.00037967
Epoch [24/300], Train Loss: 0.000385
Validation Loss: 0.00037629
Epoch [25/300], Train Loss: 0.000374
Validation Loss: 0.00036129
Epoch [26/300], Train Loss: 0.000365
Validation Loss: 0.00035111
Epoch [27/300], Train Loss: 0.000359
Validation Loss: 0.00034125
Epoch [28/300], Train Loss: 0.000351
Validation Loss: 0.00033053
Epoch [29/300], Train Loss: 0.000342
Validation Loss: 0.00039731
Epoch [30/300], Train Loss: 0.000339
Validation Loss: 0.00033088
Epoch [31/300], Train Loss: 0.000327
Validation Loss: 0.00031601
Epoch [32/300], Train Loss: 0.000322
Validation Loss: 0.00030807
Epoch [33/300], Train Loss: 0.000314
Validation Loss: 0.00030330
Epoch [34/300], Train Loss: 0.000310
Validation Loss: 0.00031849
Epoch [35/300], Train Loss: 0.000306
Validation Loss: 0.00031618
Epoch [36/300], Train Loss: 0.000304
Validation Loss: 0.00032064
Epoch [37/300], Train Loss: 0.000300
Validation Loss: 0.00029298
Epoch [38/300], Train Loss: 0.000304
Validation Loss: 0.00029461
Epoch [39/300], Train Loss: 0.000295
Validation Loss: 0.00031388
Epoch [40/300], Train Loss: 0.000291
Validation Loss: 0.00027755
Epoch [41/300], Train Loss: 0.000286
Validation Loss: 0.00027639
Epoch [42/300], Train Loss: 0.000287
Validation Loss: 0.00027437
Epoch [43/300], Train Loss: 0.000286
Validation Loss: 0.00029966
Epoch [44/300], Train Loss: 0.000278
Validation Loss: 0.00026484
Epoch [45/300], Train Loss: 0.000277
Validation Loss: 0.00027341
Epoch [46/300], Train Loss: 0.000274
Validation Loss: 0.00027736
Epoch [47/300], Train Loss: 0.000274
Validation Loss: 0.00026332
Epoch [48/300], Train Loss: 0.000270
Validation Loss: 0.00025937
Epoch [49/300], Train Loss: 0.000271
Validation Loss: 0.00026607
Epoch [50/300], Train Loss: 0.000266
Validation Loss: 0.00028623
Epoch [51/300], Train Loss: 0.000272
Validation Loss: 0.00026098
Epoch [52/300], Train Loss: 0.000265
Validation Loss: 0.00026047
Epoch [53/300], Train Loss: 0.000262
Validation Loss: 0.00025949
Epoch [54/300], Train Loss: 0.000260
Validation Loss: 0.00034031
Epoch [55/300], Train Loss: 0.000261
Validation Loss: 0.00030994
Epoch [56/300], Train Loss: 0.000261
Validation Loss: 0.00025591
Epoch [57/300], Train Loss: 0.000256
Validation Loss: 0.00024705
Epoch [58/300], Train Loss: 0.000252
Validation Loss: 0.00024841
Epoch [59/300], Train Loss: 0.000250
Validation Loss: 0.00024406
Epoch [60/300], Train Loss: 0.000249
Validation Loss: 0.00024645
Epoch [61/300], Train Loss: 0.000254
Validation Loss: 0.00024576
Epoch [62/300], Train Loss: 0.000244
Validation Loss: 0.00024655
Epoch [63/300], Train Loss: 0.000244
Validation Loss: 0.00024145
Epoch [64/300], Train Loss: 0.000248
Validation Loss: 0.00031944
Epoch [65/300], Train Loss: 0.000244
Validation Loss: 0.00024173
Epoch [66/300], Train Loss: 0.000243
Validation Loss: 0.00024878
Epoch [67/300], Train Loss: 0.000242
Validation Loss: 0.00026451
Epoch [68/300], Train Loss: 0.000242
Validation Loss: 0.00023495
Epoch [69/300], Train Loss: 0.000236
Validation Loss: 0.00024405
Epoch [70/300], Train Loss: 0.000248
Validation Loss: 0.00022813
Epoch [71/300], Train Loss: 0.000232
Validation Loss: 0.00024070
Epoch [72/300], Train Loss: 0.000232
Validation Loss: 0.00022821
Epoch [73/300], Train Loss: 0.000232
Validation Loss: 0.00029316
Epoch [74/300], Train Loss: 0.000233
Validation Loss: 0.00022825
Epoch [75/300], Train Loss: 0.000230
Validation Loss: 0.00022236
Epoch [76/300], Train Loss: 0.000230
Validation Loss: 0.00021729
Epoch [77/300], Train Loss: 0.000228
Validation Loss: 0.00022039
Epoch [78/300], Train Loss: 0.000220
Validation Loss: 0.00022181
Epoch [79/300], Train Loss: 0.000223
Validation Loss: 0.00021372
Epoch [80/300], Train Loss: 0.000222
Validation Loss: 0.00022943
Epoch [81/300], Train Loss: 0.000229
Validation Loss: 0.00021461
Epoch [82/300], Train Loss: 0.000221
Validation Loss: 0.00021970
Epoch [83/300], Train Loss: 0.000216
Validation Loss: 0.00022602
Epoch [84/300], Train Loss: 0.000225
Validation Loss: 0.00021135
Epoch [85/300], Train Loss: 0.000282
Validation Loss: 0.00025504
Epoch [86/300], Train Loss: 0.000253
Validation Loss: 0.00024963
Epoch [87/300], Train Loss: 0.000240
Validation Loss: 0.00022963
Epoch [88/300], Train Loss: 0.000234
Validation Loss: 0.00023137
Epoch [89/300], Train Loss: 0.000233
Validation Loss: 0.00022408
Epoch [90/300], Train Loss: 0.000228
Validation Loss: 0.00021762
Epoch [91/300], Train Loss: 0.000221
Validation Loss: 0.00021206
Epoch [92/300], Train Loss: 0.000246
Validation Loss: 0.00022552
Epoch [93/300], Train Loss: 0.000217
Validation Loss: 0.00021410
Epoch [94/300], Train Loss: 0.000216
Validation Loss: 0.00021141
Early stopping triggered

Evaluating model for: Fridge
Run 50/72 completed in 6446.30 seconds with: {'MAE': np.float32(17.039402), 'MSE': np.float32(689.8733), 'RMSE': np.float32(26.26544), 'SAE': np.float32(0.03618252), 'NDE': np.float32(0.45503455)}

Run 51/72: hidden=512, seq_len=120, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000666
Validation Loss: 0.00062609
Epoch [2/300], Train Loss: 0.000622
Validation Loss: 0.00060062
Epoch [3/300], Train Loss: 0.000610
Validation Loss: 0.00059527
Epoch [4/300], Train Loss: 0.000603
Validation Loss: 0.00058703
Epoch [5/300], Train Loss: 0.000597
Validation Loss: 0.00057979
Epoch [6/300], Train Loss: 0.000591
Validation Loss: 0.00057695
Epoch [7/300], Train Loss: 0.000581
Validation Loss: 0.00056904
Epoch [8/300], Train Loss: 0.000573
Validation Loss: 0.00055495
Epoch [9/300], Train Loss: 0.000563
Validation Loss: 0.00054630
Epoch [10/300], Train Loss: 0.000548
Validation Loss: 0.00054578
Epoch [11/300], Train Loss: 0.000538
Validation Loss: 0.00054851
Epoch [12/300], Train Loss: 0.000522
Validation Loss: 0.00049649
Epoch [13/300], Train Loss: 0.000496
Validation Loss: 0.00046074
Epoch [14/300], Train Loss: 0.000460
Validation Loss: 0.00043116
Epoch [15/300], Train Loss: 0.000435
Validation Loss: 0.00040936
Epoch [16/300], Train Loss: 0.000413
Validation Loss: 0.00039576
Epoch [17/300], Train Loss: 0.000385
Validation Loss: 0.00036488
Epoch [18/300], Train Loss: 0.000379
Validation Loss: 0.00036219
Epoch [19/300], Train Loss: 0.000369
Validation Loss: 0.00041611
Epoch [20/300], Train Loss: 0.000362
Validation Loss: 0.00034322
Epoch [21/300], Train Loss: 0.000356
Validation Loss: 0.00034678
Epoch [22/300], Train Loss: 0.000346
Validation Loss: 0.00034586
Epoch [23/300], Train Loss: 0.000335
Validation Loss: 0.00032336
Epoch [24/300], Train Loss: 0.000330
Validation Loss: 0.00033035
Epoch [25/300], Train Loss: 0.000330
Validation Loss: 0.00032386
Epoch [26/300], Train Loss: 0.000321
Validation Loss: 0.00031225
Epoch [27/300], Train Loss: 0.000315
Validation Loss: 0.00030211
Epoch [28/300], Train Loss: 0.000314
Validation Loss: 0.00029442
Epoch [29/300], Train Loss: 0.000308
Validation Loss: 0.00035780
Epoch [30/300], Train Loss: 0.000304
Validation Loss: 0.00032884
Epoch [31/300], Train Loss: 0.000297
Validation Loss: 0.00028399
Epoch [32/300], Train Loss: 0.000294
Validation Loss: 0.00028080
Epoch [33/300], Train Loss: 0.000287
Validation Loss: 0.00027444
Epoch [34/300], Train Loss: 0.000284
Validation Loss: 0.00028106
Epoch [35/300], Train Loss: 0.000283
Validation Loss: 0.00028509
Epoch [36/300], Train Loss: 0.000280
Validation Loss: 0.00029041
Epoch [37/300], Train Loss: 0.000280
Validation Loss: 0.00027204
Epoch [38/300], Train Loss: 0.000280
Validation Loss: 0.00027991
Epoch [39/300], Train Loss: 0.000277
Validation Loss: 0.00030177
Epoch [40/300], Train Loss: 0.000271
Validation Loss: 0.00027395
Epoch [41/300], Train Loss: 0.000269
Validation Loss: 0.00026877
Epoch [42/300], Train Loss: 0.000263
Validation Loss: 0.00025213
Epoch [43/300], Train Loss: 0.000270
Validation Loss: 0.00029135
Epoch [44/300], Train Loss: 0.000260
Validation Loss: 0.00025430
Epoch [45/300], Train Loss: 0.000260
Validation Loss: 0.00026764
Epoch [46/300], Train Loss: 0.000250
Validation Loss: 0.00025776
Epoch [47/300], Train Loss: 0.000250
Validation Loss: 0.00027145
Epoch [48/300], Train Loss: 0.000249
Validation Loss: 0.00024330
Epoch [49/300], Train Loss: 0.000250
Validation Loss: 0.00024911
Epoch [50/300], Train Loss: 0.000249
Validation Loss: 0.00024118
Epoch [51/300], Train Loss: 0.000290
Validation Loss: 0.00027098
Epoch [52/300], Train Loss: 0.000264
Validation Loss: 0.00026184
Epoch [53/300], Train Loss: 0.000255
Validation Loss: 0.00025594
Epoch [54/300], Train Loss: 0.000251
Validation Loss: 0.00030896
Epoch [55/300], Train Loss: 0.000250
Validation Loss: 0.00025195
Epoch [56/300], Train Loss: 0.000250
Validation Loss: 0.00023531
Epoch [57/300], Train Loss: 0.000269
Validation Loss: 0.00026156
Epoch [58/300], Train Loss: 0.000249
Validation Loss: 0.00024332
Epoch [59/300], Train Loss: 0.000239
Validation Loss: 0.00022780
Epoch [60/300], Train Loss: 0.000238
Validation Loss: 0.00022411
Epoch [61/300], Train Loss: 0.000236
Validation Loss: 0.00025769
Epoch [62/300], Train Loss: 0.000232
Validation Loss: 0.00023235
Epoch [63/300], Train Loss: 0.000230
Validation Loss: 0.00022802
Epoch [64/300], Train Loss: 0.000238
Validation Loss: 0.00021883
Epoch [65/300], Train Loss: 0.000233
Validation Loss: 0.00033107
Epoch [66/300], Train Loss: 0.000237
Validation Loss: 0.00031298
Epoch [67/300], Train Loss: 0.000239
Validation Loss: 0.00021546
Epoch [68/300], Train Loss: 0.000228
Validation Loss: 0.00021656
Epoch [69/300], Train Loss: 0.000219
Validation Loss: 0.00021286
Epoch [70/300], Train Loss: 0.000218
Validation Loss: 0.00021602
Epoch [71/300], Train Loss: 0.000442
Validation Loss: 0.00050673
Epoch [72/300], Train Loss: 0.000469
Validation Loss: 0.00042665
Epoch [73/300], Train Loss: 0.000387
Validation Loss: 0.00034440
Epoch [74/300], Train Loss: 0.000351
Validation Loss: 0.00031202
Epoch [75/300], Train Loss: 0.000329
Validation Loss: 0.00030494
Epoch [76/300], Train Loss: 0.000308
Validation Loss: 0.00029033
Epoch [77/300], Train Loss: 0.000296
Validation Loss: 0.00028022
Epoch [78/300], Train Loss: 0.000290
Validation Loss: 0.00028012
Epoch [79/300], Train Loss: 0.000280
Validation Loss: 0.00026817
Early stopping triggered

Evaluating model for: Fridge
Run 51/72 completed in 6363.42 seconds with: {'MAE': np.float32(20.40022), 'MSE': np.float32(868.1383), 'RMSE': np.float32(29.464188), 'SAE': np.float32(0.048290297), 'NDE': np.float32(0.51045144)}

Run 52/72: hidden=512, seq_len=120, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 41046 windows

Epoch [1/300], Train Loss: 0.000654
Validation Loss: 0.00060664
Epoch [2/300], Train Loss: 0.000617
Validation Loss: 0.00060047
Epoch [3/300], Train Loss: 0.000609
Validation Loss: 0.00059403
Epoch [4/300], Train Loss: 0.000606
Validation Loss: 0.00058984
Epoch [5/300], Train Loss: 0.000601
Validation Loss: 0.00058862
Epoch [6/300], Train Loss: 0.000598
Validation Loss: 0.00058558
Epoch [7/300], Train Loss: 0.000595
Validation Loss: 0.00058289
Epoch [8/300], Train Loss: 0.000590
Validation Loss: 0.00057808
Epoch [9/300], Train Loss: 0.000585
Validation Loss: 0.00057453
Epoch [10/300], Train Loss: 0.000573
Validation Loss: 0.00056065
Epoch [11/300], Train Loss: 0.000562
Validation Loss: 0.00053955
Epoch [12/300], Train Loss: 0.000547
Validation Loss: 0.00053526
Epoch [13/300], Train Loss: 0.000538
Validation Loss: 0.00052424
Epoch [14/300], Train Loss: 0.000526
Validation Loss: 0.00050664
Epoch [15/300], Train Loss: 0.000516
Validation Loss: 0.00050171
Epoch [16/300], Train Loss: 0.000507
Validation Loss: 0.00049231
Epoch [17/300], Train Loss: 0.000495
Validation Loss: 0.00048503
Epoch [18/300], Train Loss: 0.000485
Validation Loss: 0.00046875
Epoch [19/300], Train Loss: 0.000474
Validation Loss: 0.00045904
Epoch [20/300], Train Loss: 0.000464
Validation Loss: 0.00046432
Epoch [21/300], Train Loss: 0.000456
Validation Loss: 0.00044817
Epoch [22/300], Train Loss: 0.000446
Validation Loss: 0.00043368
Epoch [23/300], Train Loss: 0.000435
Validation Loss: 0.00042268
Epoch [24/300], Train Loss: 0.000425
Validation Loss: 0.00040739
Epoch [25/300], Train Loss: 0.000411
Validation Loss: 0.00040572
Epoch [26/300], Train Loss: 0.000398
Validation Loss: 0.00039216
Epoch [27/300], Train Loss: 0.000391
Validation Loss: 0.00037745
Epoch [28/300], Train Loss: 0.000363
Validation Loss: 0.00033414
Epoch [29/300], Train Loss: 0.000333
Validation Loss: 0.00031993
Epoch [30/300], Train Loss: 0.000441
Validation Loss: 0.00037038
Epoch [31/300], Train Loss: 0.000359
Validation Loss: 0.00032637
Epoch [32/300], Train Loss: 0.000342
Validation Loss: 0.00033994
Epoch [33/300], Train Loss: 0.000323
Validation Loss: 0.00030274
Epoch [34/300], Train Loss: 0.000327
Validation Loss: 0.00034342
Epoch [35/300], Train Loss: 0.000309
Validation Loss: 0.00028690
Epoch [36/300], Train Loss: 0.000358
Validation Loss: 0.00030126
Epoch [37/300], Train Loss: 0.000303
Validation Loss: 0.00029262
Epoch [38/300], Train Loss: 0.000301
Validation Loss: 0.00038731
Epoch [39/300], Train Loss: 0.000299
Validation Loss: 0.00029888
Epoch [40/300], Train Loss: 0.000325
Validation Loss: 0.00028160
Epoch [41/300], Train Loss: 0.000327
Validation Loss: 0.00028916
Epoch [42/300], Train Loss: 0.000284
Validation Loss: 0.00026411
Epoch [43/300], Train Loss: 0.000272
Validation Loss: 0.00026150
Epoch [44/300], Train Loss: 0.000265
Validation Loss: 0.00030063
Epoch [45/300], Train Loss: 0.000264
Validation Loss: 0.00025236
Epoch [46/300], Train Loss: 0.000263
Validation Loss: 0.00025649
Epoch [47/300], Train Loss: 0.000255
Validation Loss: 0.00026720
Epoch [48/300], Train Loss: 0.000257
Validation Loss: 0.00025854
Epoch [49/300], Train Loss: 0.000267
Validation Loss: 0.00024563
Epoch [50/300], Train Loss: 0.000246
Validation Loss: 0.00023853
Epoch [51/300], Train Loss: 0.000249
Validation Loss: 0.00023739
Epoch [52/300], Train Loss: 0.000253
Validation Loss: 0.00024212
Epoch [53/300], Train Loss: 0.000244
Validation Loss: 0.00025028
Epoch [54/300], Train Loss: 0.000241
Validation Loss: 0.00024655
Epoch [55/300], Train Loss: 0.000236
Validation Loss: 0.00024183
Epoch [56/300], Train Loss: 0.000247
Validation Loss: 0.00022967
Epoch [57/300], Train Loss: 0.000233
Validation Loss: 0.00022855
Epoch [58/300], Train Loss: 0.000233
Validation Loss: 0.00022656
Epoch [59/300], Train Loss: 0.000230
Validation Loss: 0.00023998
Epoch [60/300], Train Loss: 0.000228
Validation Loss: 0.00022877
Epoch [61/300], Train Loss: 0.000221
Validation Loss: 0.00022989
Epoch [62/300], Train Loss: 0.000220
Validation Loss: 0.00027251
Epoch [63/300], Train Loss: 0.000238
Validation Loss: 0.00022046
Epoch [64/300], Train Loss: 0.000215
Validation Loss: 0.00021055
Epoch [65/300], Train Loss: 0.000213
Validation Loss: 0.00021446
Epoch [66/300], Train Loss: 0.000215
Validation Loss: 0.00023847
Epoch [67/300], Train Loss: 0.000209
Validation Loss: 0.00020773
Epoch [68/300], Train Loss: 0.000214
Validation Loss: 0.00020752
Epoch [69/300], Train Loss: 0.000208
Validation Loss: 0.00021032
Epoch [70/300], Train Loss: 0.000210
Validation Loss: 0.00024484
Epoch [71/300], Train Loss: 0.000227
Validation Loss: 0.00028658
Epoch [72/300], Train Loss: 0.000223
Validation Loss: 0.00030416
Epoch [73/300], Train Loss: 0.000216
Validation Loss: 0.00023160
Epoch [74/300], Train Loss: 0.000204
Validation Loss: 0.00021535
Epoch [75/300], Train Loss: 0.000213
Validation Loss: 0.00021144
Epoch [76/300], Train Loss: 0.000201
Validation Loss: 0.00021128
Epoch [77/300], Train Loss: 0.000199
Validation Loss: 0.00019964
Epoch [78/300], Train Loss: 0.000197
Validation Loss: 0.00020927
Epoch [79/300], Train Loss: 0.000219
Validation Loss: 0.00020038
Epoch [80/300], Train Loss: 0.000197
Validation Loss: 0.00030055
Epoch [81/300], Train Loss: 0.000207
Validation Loss: 0.00020804
Epoch [82/300], Train Loss: 0.000194
Validation Loss: 0.00021525
Epoch [83/300], Train Loss: 0.000198
Validation Loss: 0.00019473
Epoch [84/300], Train Loss: 0.000193
Validation Loss: 0.00018697
Epoch [85/300], Train Loss: 0.000188
Validation Loss: 0.00018831
Epoch [86/300], Train Loss: 0.000237
Validation Loss: 0.00024302
Epoch [87/300], Train Loss: 0.000217
Validation Loss: 0.00020945
Epoch [88/300], Train Loss: 0.000312
Validation Loss: 0.00026496
Epoch [89/300], Train Loss: 0.000249
Validation Loss: 0.00023419
Epoch [90/300], Train Loss: 0.000237
Validation Loss: 0.00022291
Epoch [91/300], Train Loss: 0.000221
Validation Loss: 0.00022168
Epoch [92/300], Train Loss: 0.000216
Validation Loss: 0.00020933
Epoch [93/300], Train Loss: 0.000209
Validation Loss: 0.00020653
Epoch [94/300], Train Loss: 0.000210
Validation Loss: 0.00021326
Early stopping triggered

Evaluating model for: Fridge
Run 52/72 completed in 8780.19 seconds with: {'MAE': np.float32(17.309513), 'MSE': np.float32(686.2413), 'RMSE': np.float32(26.196207), 'SAE': np.float32(0.00883714), 'NDE': np.float32(0.4538353)}

Run 53/72: hidden=512, seq_len=120, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000659
Validation Loss: 0.00066121
Epoch [2/300], Train Loss: 0.000634
Validation Loss: 0.00063120
Epoch [3/300], Train Loss: 0.000616
Validation Loss: 0.00061926
Epoch [4/300], Train Loss: 0.000608
Validation Loss: 0.00061235
Epoch [5/300], Train Loss: 0.000601
Validation Loss: 0.00060902
Epoch [6/300], Train Loss: 0.000596
Validation Loss: 0.00059745
Epoch [7/300], Train Loss: 0.000589
Validation Loss: 0.00058911
Epoch [8/300], Train Loss: 0.000591
Validation Loss: 0.00058552
Epoch [9/300], Train Loss: 0.000583
Validation Loss: 0.00057819
Epoch [10/300], Train Loss: 0.000579
Validation Loss: 0.00057270
Epoch [11/300], Train Loss: 0.000576
Validation Loss: 0.00058013
Epoch [12/300], Train Loss: 0.000572
Validation Loss: 0.00057274
Epoch [13/300], Train Loss: 0.000571
Validation Loss: 0.00057763
Epoch [14/300], Train Loss: 0.000567
Validation Loss: 0.00056591
Epoch [15/300], Train Loss: 0.000565
Validation Loss: 0.00056329
Epoch [16/300], Train Loss: 0.000558
Validation Loss: 0.00055535
Epoch [17/300], Train Loss: 0.000554
Validation Loss: 0.00055644
Epoch [18/300], Train Loss: 0.000552
Validation Loss: 0.00054622
Epoch [19/300], Train Loss: 0.000547
Validation Loss: 0.00054781
Epoch [20/300], Train Loss: 0.000543
Validation Loss: 0.00053859
Epoch [21/300], Train Loss: 0.000539
Validation Loss: 0.00054028
Epoch [22/300], Train Loss: 0.000535
Validation Loss: 0.00052331
Epoch [23/300], Train Loss: 0.000530
Validation Loss: 0.00052347
Epoch [24/300], Train Loss: 0.000523
Validation Loss: 0.00052327
Epoch [25/300], Train Loss: 0.000527
Validation Loss: 0.00052591
Epoch [26/300], Train Loss: 0.000521
Validation Loss: 0.00051800
Epoch [27/300], Train Loss: 0.000516
Validation Loss: 0.00051033
Epoch [28/300], Train Loss: 0.000516
Validation Loss: 0.00051563
Epoch [29/300], Train Loss: 0.000510
Validation Loss: 0.00050202
Epoch [30/300], Train Loss: 0.000508
Validation Loss: 0.00049975
Epoch [31/300], Train Loss: 0.000503
Validation Loss: 0.00051214
Epoch [32/300], Train Loss: 0.000499
Validation Loss: 0.00052197
Epoch [33/300], Train Loss: 0.000499
Validation Loss: 0.00050659
Epoch [34/300], Train Loss: 0.000491
Validation Loss: 0.00048269
Epoch [35/300], Train Loss: 0.000487
Validation Loss: 0.00048165
Epoch [36/300], Train Loss: 0.000482
Validation Loss: 0.00048183
Epoch [37/300], Train Loss: 0.000478
Validation Loss: 0.00047059
Epoch [38/300], Train Loss: 0.000475
Validation Loss: 0.00047362
Epoch [39/300], Train Loss: 0.000470
Validation Loss: 0.00046261
Epoch [40/300], Train Loss: 0.000466
Validation Loss: 0.00045631
Epoch [41/300], Train Loss: 0.000461
Validation Loss: 0.00046760
Epoch [42/300], Train Loss: 0.000458
Validation Loss: 0.00045766
Epoch [43/300], Train Loss: 0.000457
Validation Loss: 0.00044394
Epoch [44/300], Train Loss: 0.000452
Validation Loss: 0.00045730
Epoch [45/300], Train Loss: 0.000447
Validation Loss: 0.00043445
Epoch [46/300], Train Loss: 0.000441
Validation Loss: 0.00043519
Epoch [47/300], Train Loss: 0.000439
Validation Loss: 0.00045812
Epoch [48/300], Train Loss: 0.000436
Validation Loss: 0.00043282
Epoch [49/300], Train Loss: 0.000430
Validation Loss: 0.00041826
Epoch [50/300], Train Loss: 0.000431
Validation Loss: 0.00042047
Epoch [51/300], Train Loss: 0.000427
Validation Loss: 0.00041576
Epoch [52/300], Train Loss: 0.000420
Validation Loss: 0.00041744
Epoch [53/300], Train Loss: 0.000421
Validation Loss: 0.00040553
Epoch [54/300], Train Loss: 0.000416
Validation Loss: 0.00040129
Epoch [55/300], Train Loss: 0.000415
Validation Loss: 0.00039866
Epoch [56/300], Train Loss: 0.000411
Validation Loss: 0.00040073
Epoch [57/300], Train Loss: 0.000404
Validation Loss: 0.00039047
Epoch [58/300], Train Loss: 0.000402
Validation Loss: 0.00040840
Epoch [59/300], Train Loss: 0.000403
Validation Loss: 0.00039440
Epoch [60/300], Train Loss: 0.000402
Validation Loss: 0.00038976
Epoch [61/300], Train Loss: 0.000394
Validation Loss: 0.00038380
Epoch [62/300], Train Loss: 0.000400
Validation Loss: 0.00040836
Epoch [63/300], Train Loss: 0.000389
Validation Loss: 0.00037734
Epoch [64/300], Train Loss: 0.000389
Validation Loss: 0.00038502
Epoch [65/300], Train Loss: 0.000384
Validation Loss: 0.00037989
Epoch [66/300], Train Loss: 0.000388
Validation Loss: 0.00037395
Epoch [67/300], Train Loss: 0.000384
Validation Loss: 0.00037383
Epoch [68/300], Train Loss: 0.000377
Validation Loss: 0.00036395
Epoch [69/300], Train Loss: 0.000378
Validation Loss: 0.00039445
Epoch [70/300], Train Loss: 0.000375
Validation Loss: 0.00035958
Epoch [71/300], Train Loss: 0.000369
Validation Loss: 0.00036261
Epoch [72/300], Train Loss: 0.000367
Validation Loss: 0.00036290
Epoch [73/300], Train Loss: 0.000369
Validation Loss: 0.00035593
Epoch [74/300], Train Loss: 0.000366
Validation Loss: 0.00036754
Epoch [75/300], Train Loss: 0.000363
Validation Loss: 0.00034784
Epoch [76/300], Train Loss: 0.000354
Validation Loss: 0.00034933
Epoch [77/300], Train Loss: 0.000357
Validation Loss: 0.00038444
Epoch [78/300], Train Loss: 0.000354
Validation Loss: 0.00039297
Epoch [79/300], Train Loss: 0.000350
Validation Loss: 0.00039368
Epoch [80/300], Train Loss: 0.000347
Validation Loss: 0.00035957
Epoch [81/300], Train Loss: 0.000346
Validation Loss: 0.00034718
Epoch [82/300], Train Loss: 0.000344
Validation Loss: 0.00032978
Epoch [83/300], Train Loss: 0.000337
Validation Loss: 0.00033067
Epoch [84/300], Train Loss: 0.000337
Validation Loss: 0.00032275
Epoch [85/300], Train Loss: 0.000335
Validation Loss: 0.00038502
Epoch [86/300], Train Loss: 0.000334
Validation Loss: 0.00031986
Epoch [87/300], Train Loss: 0.000333
Validation Loss: 0.00032565
Epoch [88/300], Train Loss: 0.000327
Validation Loss: 0.00032428
Epoch [89/300], Train Loss: 0.000325
Validation Loss: 0.00032006
Epoch [90/300], Train Loss: 0.000327
Validation Loss: 0.00033715
Epoch [91/300], Train Loss: 0.000319
Validation Loss: 0.00030953
Epoch [92/300], Train Loss: 0.000317
Validation Loss: 0.00031298
Epoch [93/300], Train Loss: 0.000326
Validation Loss: 0.00031248
Epoch [94/300], Train Loss: 0.000315
Validation Loss: 0.00031424
Epoch [95/300], Train Loss: 0.000314
Validation Loss: 0.00031959
Epoch [96/300], Train Loss: 0.000315
Validation Loss: 0.00029807
Epoch [97/300], Train Loss: 0.000313
Validation Loss: 0.00029905
Epoch [98/300], Train Loss: 0.000308
Validation Loss: 0.00031249
Epoch [99/300], Train Loss: 0.000307
Validation Loss: 0.00029726
Epoch [100/300], Train Loss: 0.000308
Validation Loss: 0.00030191
Epoch [101/300], Train Loss: 0.000309
Validation Loss: 0.00030484
Epoch [102/300], Train Loss: 0.000305
Validation Loss: 0.00030143
Epoch [103/300], Train Loss: 0.000302
Validation Loss: 0.00028921
Epoch [104/300], Train Loss: 0.000300
Validation Loss: 0.00031213
Epoch [105/300], Train Loss: 0.000302
Validation Loss: 0.00028826
Epoch [106/300], Train Loss: 0.000296
Validation Loss: 0.00029978
Epoch [107/300], Train Loss: 0.000303
Validation Loss: 0.00029044
Epoch [108/300], Train Loss: 0.000300
Validation Loss: 0.00030111
Epoch [109/300], Train Loss: 0.000295
Validation Loss: 0.00030399
Epoch [110/300], Train Loss: 0.000297
Validation Loss: 0.00028698
Epoch [111/300], Train Loss: 0.000292
Validation Loss: 0.00030107
Epoch [112/300], Train Loss: 0.000293
Validation Loss: 0.00028207
Epoch [113/300], Train Loss: 0.000290
Validation Loss: 0.00029296
Epoch [114/300], Train Loss: 0.000290
Validation Loss: 0.00028150
Epoch [115/300], Train Loss: 0.000289
Validation Loss: 0.00029077
Epoch [116/300], Train Loss: 0.000290
Validation Loss: 0.00031367
Epoch [117/300], Train Loss: 0.000285
Validation Loss: 0.00028833
Epoch [118/300], Train Loss: 0.000287
Validation Loss: 0.00027816
Epoch [119/300], Train Loss: 0.000288
Validation Loss: 0.00028833
Epoch [120/300], Train Loss: 0.000284
Validation Loss: 0.00028059
Epoch [121/300], Train Loss: 0.000283
Validation Loss: 0.00027380
Epoch [122/300], Train Loss: 0.000282
Validation Loss: 0.00028195
Epoch [123/300], Train Loss: 0.000282
Validation Loss: 0.00028695
Epoch [124/300], Train Loss: 0.000284
Validation Loss: 0.00027300
Epoch [125/300], Train Loss: 0.000280
Validation Loss: 0.00027797
Epoch [126/300], Train Loss: 0.000279
Validation Loss: 0.00027553
Epoch [127/300], Train Loss: 0.000291
Validation Loss: 0.00030581
Epoch [128/300], Train Loss: 0.000281
Validation Loss: 0.00027933
Epoch [129/300], Train Loss: 0.000276
Validation Loss: 0.00027148
Epoch [130/300], Train Loss: 0.000277
Validation Loss: 0.00027461
Epoch [131/300], Train Loss: 0.000280
Validation Loss: 0.00027399
Epoch [132/300], Train Loss: 0.000275
Validation Loss: 0.00028313
Epoch [133/300], Train Loss: 0.000273
Validation Loss: 0.00027078
Epoch [134/300], Train Loss: 0.000274
Validation Loss: 0.00026783
Epoch [135/300], Train Loss: 0.000272
Validation Loss: 0.00028811
Epoch [136/300], Train Loss: 0.000273
Validation Loss: 0.00026749
Epoch [137/300], Train Loss: 0.000271
Validation Loss: 0.00029410
Epoch [138/300], Train Loss: 0.000277
Validation Loss: 0.00027596
Epoch [139/300], Train Loss: 0.000270
Validation Loss: 0.00027510
Epoch [140/300], Train Loss: 0.000270
Validation Loss: 0.00027025
Epoch [141/300], Train Loss: 0.000270
Validation Loss: 0.00030085
Epoch [142/300], Train Loss: 0.000269
Validation Loss: 0.00027466
Epoch [143/300], Train Loss: 0.000280
Validation Loss: 0.00026350
Epoch [144/300], Train Loss: 0.000271
Validation Loss: 0.00025967
Epoch [145/300], Train Loss: 0.000267
Validation Loss: 0.00026661
Epoch [146/300], Train Loss: 0.000266
Validation Loss: 0.00026830
Epoch [147/300], Train Loss: 0.000265
Validation Loss: 0.00026781
Epoch [148/300], Train Loss: 0.000267
Validation Loss: 0.00026299
Epoch [149/300], Train Loss: 0.000265
Validation Loss: 0.00025926
Epoch [150/300], Train Loss: 0.000263
Validation Loss: 0.00026139
Epoch [151/300], Train Loss: 0.000263
Validation Loss: 0.00025709
Epoch [152/300], Train Loss: 0.000266
Validation Loss: 0.00026076
Epoch [153/300], Train Loss: 0.000261
Validation Loss: 0.00025663
Epoch [154/300], Train Loss: 0.000264
Validation Loss: 0.00027502
Epoch [155/300], Train Loss: 0.000262
Validation Loss: 0.00025788
Epoch [156/300], Train Loss: 0.000262
Validation Loss: 0.00026252
Epoch [157/300], Train Loss: 0.000258
Validation Loss: 0.00025913
Epoch [158/300], Train Loss: 0.000262
Validation Loss: 0.00028489
Epoch [159/300], Train Loss: 0.000260
Validation Loss: 0.00025780
Epoch [160/300], Train Loss: 0.000269
Validation Loss: 0.00025609
Epoch [161/300], Train Loss: 0.000258
Validation Loss: 0.00025598
Epoch [162/300], Train Loss: 0.000258
Validation Loss: 0.00026876
Epoch [163/300], Train Loss: 0.000258
Validation Loss: 0.00025606
Epoch [164/300], Train Loss: 0.000258
Validation Loss: 0.00025370
Epoch [165/300], Train Loss: 0.000258
Validation Loss: 0.00025244
Epoch [166/300], Train Loss: 0.000258
Validation Loss: 0.00025374
Epoch [167/300], Train Loss: 0.000254
Validation Loss: 0.00026187
Epoch [168/300], Train Loss: 0.000255
Validation Loss: 0.00024994
Epoch [169/300], Train Loss: 0.000256
Validation Loss: 0.00024970
Epoch [170/300], Train Loss: 0.000254
Validation Loss: 0.00025009
Epoch [171/300], Train Loss: 0.000255
Validation Loss: 0.00026009
Epoch [172/300], Train Loss: 0.000255
Validation Loss: 0.00025553
Epoch [173/300], Train Loss: 0.000258
Validation Loss: 0.00025408
Epoch [174/300], Train Loss: 0.000254
Validation Loss: 0.00025134
Epoch [175/300], Train Loss: 0.000252
Validation Loss: 0.00024979
Epoch [176/300], Train Loss: 0.000249
Validation Loss: 0.00024573
Epoch [177/300], Train Loss: 0.000248
Validation Loss: 0.00026888
Epoch [178/300], Train Loss: 0.000252
Validation Loss: 0.00024559
Epoch [179/300], Train Loss: 0.000248
Validation Loss: 0.00025661
Epoch [180/300], Train Loss: 0.000251
Validation Loss: 0.00024639
Epoch [181/300], Train Loss: 0.000247
Validation Loss: 0.00024445
Epoch [182/300], Train Loss: 0.000247
Validation Loss: 0.00024935
Epoch [183/300], Train Loss: 0.000247
Validation Loss: 0.00024340
Epoch [184/300], Train Loss: 0.000251
Validation Loss: 0.00024599
Epoch [185/300], Train Loss: 0.000247
Validation Loss: 0.00024484
Epoch [186/300], Train Loss: 0.000246
Validation Loss: 0.00024191
Epoch [187/300], Train Loss: 0.000249
Validation Loss: 0.00024545
Epoch [188/300], Train Loss: 0.000248
Validation Loss: 0.00024499
Epoch [189/300], Train Loss: 0.000249
Validation Loss: 0.00025692
Epoch [190/300], Train Loss: 0.000244
Validation Loss: 0.00024780
Epoch [191/300], Train Loss: 0.000248
Validation Loss: 0.00026571
Epoch [192/300], Train Loss: 0.000247
Validation Loss: 0.00025587
Epoch [193/300], Train Loss: 0.000246
Validation Loss: 0.00023857
Epoch [194/300], Train Loss: 0.000245
Validation Loss: 0.00024504
Epoch [195/300], Train Loss: 0.000244
Validation Loss: 0.00023989
Epoch [196/300], Train Loss: 0.000244
Validation Loss: 0.00025352
Epoch [197/300], Train Loss: 0.000244
Validation Loss: 0.00023874
Epoch [198/300], Train Loss: 0.000253
Validation Loss: 0.00024553
Epoch [199/300], Train Loss: 0.000244
Validation Loss: 0.00024547
Epoch [200/300], Train Loss: 0.000241
Validation Loss: 0.00024220
Epoch [201/300], Train Loss: 0.000245
Validation Loss: 0.00023979
Epoch [202/300], Train Loss: 0.000242
Validation Loss: 0.00023803
Epoch [203/300], Train Loss: 0.000241
Validation Loss: 0.00026277
Epoch [204/300], Train Loss: 0.000245
Validation Loss: 0.00023827
Epoch [205/300], Train Loss: 0.000239
Validation Loss: 0.00025313
Epoch [206/300], Train Loss: 0.000241
Validation Loss: 0.00025479
Epoch [207/300], Train Loss: 0.000240
Validation Loss: 0.00025329
Epoch [208/300], Train Loss: 0.000247
Validation Loss: 0.00024403
Epoch [209/300], Train Loss: 0.000238
Validation Loss: 0.00023364
Epoch [210/300], Train Loss: 0.000242
Validation Loss: 0.00024263
Epoch [211/300], Train Loss: 0.000237
Validation Loss: 0.00023997
Epoch [212/300], Train Loss: 0.000237
Validation Loss: 0.00023649
Epoch [213/300], Train Loss: 0.000237
Validation Loss: 0.00023727
Epoch [214/300], Train Loss: 0.000237
Validation Loss: 0.00023401
Epoch [215/300], Train Loss: 0.000235
Validation Loss: 0.00023477
Epoch [216/300], Train Loss: 0.000236
Validation Loss: 0.00024755
Epoch [217/300], Train Loss: 0.000237
Validation Loss: 0.00023528
Epoch [218/300], Train Loss: 0.000235
Validation Loss: 0.00023625
Epoch [219/300], Train Loss: 0.000235
Validation Loss: 0.00023426
Early stopping triggered

Evaluating model for: Fridge
Run 53/72 completed in 8052.28 seconds with: {'MAE': np.float32(18.02833), 'MSE': np.float32(775.35175), 'RMSE': np.float32(27.845139), 'SAE': np.float32(0.049656786), 'NDE': np.float32(0.4790155)}

Run 54/72: hidden=512, seq_len=120, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000663
Validation Loss: 0.00067645
Epoch [2/300], Train Loss: 0.000643
Validation Loss: 0.00064402
Epoch [3/300], Train Loss: 0.000620
Validation Loss: 0.00062767
Epoch [4/300], Train Loss: 0.000612
Validation Loss: 0.00062040
Epoch [5/300], Train Loss: 0.000605
Validation Loss: 0.00061589
Epoch [6/300], Train Loss: 0.000602
Validation Loss: 0.00060889
Epoch [7/300], Train Loss: 0.000598
Validation Loss: 0.00060405
Epoch [8/300], Train Loss: 0.000599
Validation Loss: 0.00060557
Epoch [9/300], Train Loss: 0.000594
Validation Loss: 0.00059787
Epoch [10/300], Train Loss: 0.000592
Validation Loss: 0.00059559
Epoch [11/300], Train Loss: 0.000590
Validation Loss: 0.00059876
Epoch [12/300], Train Loss: 0.000588
Validation Loss: 0.00059528
Epoch [13/300], Train Loss: 0.000589
Validation Loss: 0.00059509
Epoch [14/300], Train Loss: 0.000585
Validation Loss: 0.00059946
Epoch [15/300], Train Loss: 0.000584
Validation Loss: 0.00060005
Epoch [16/300], Train Loss: 0.000578
Validation Loss: 0.00057798
Epoch [17/300], Train Loss: 0.000571
Validation Loss: 0.00057012
Epoch [18/300], Train Loss: 0.000568
Validation Loss: 0.00056882
Epoch [19/300], Train Loss: 0.000566
Validation Loss: 0.00056077
Epoch [20/300], Train Loss: 0.000557
Validation Loss: 0.00055452
Epoch [21/300], Train Loss: 0.000553
Validation Loss: 0.00055282
Epoch [22/300], Train Loss: 0.000550
Validation Loss: 0.00054246
Epoch [23/300], Train Loss: 0.000544
Validation Loss: 0.00053506
Epoch [24/300], Train Loss: 0.000537
Validation Loss: 0.00054253
Epoch [25/300], Train Loss: 0.000537
Validation Loss: 0.00054224
Epoch [26/300], Train Loss: 0.000531
Validation Loss: 0.00052951
Epoch [27/300], Train Loss: 0.000522
Validation Loss: 0.00051415
Epoch [28/300], Train Loss: 0.000525
Validation Loss: 0.00052008
Epoch [29/300], Train Loss: 0.000508
Validation Loss: 0.00049884
Epoch [30/300], Train Loss: 0.000507
Validation Loss: 0.00049998
Epoch [31/300], Train Loss: 0.000496
Validation Loss: 0.00048743
Epoch [32/300], Train Loss: 0.000490
Validation Loss: 0.00048106
Epoch [33/300], Train Loss: 0.000480
Validation Loss: 0.00047195
Epoch [34/300], Train Loss: 0.000467
Validation Loss: 0.00046166
Epoch [35/300], Train Loss: 0.000464
Validation Loss: 0.00046691
Epoch [36/300], Train Loss: 0.000454
Validation Loss: 0.00045193
Epoch [37/300], Train Loss: 0.000451
Validation Loss: 0.00044878
Epoch [38/300], Train Loss: 0.000444
Validation Loss: 0.00042571
Epoch [39/300], Train Loss: 0.000442
Validation Loss: 0.00042917
Epoch [40/300], Train Loss: 0.000427
Validation Loss: 0.00043743
Epoch [41/300], Train Loss: 0.000424
Validation Loss: 0.00042515
Epoch [42/300], Train Loss: 0.000417
Validation Loss: 0.00046432
Epoch [43/300], Train Loss: 0.000412
Validation Loss: 0.00041981
Epoch [44/300], Train Loss: 0.000405
Validation Loss: 0.00041176
Epoch [45/300], Train Loss: 0.000403
Validation Loss: 0.00040011
Epoch [46/300], Train Loss: 0.000394
Validation Loss: 0.00037614
Epoch [47/300], Train Loss: 0.000387
Validation Loss: 0.00039240
Epoch [48/300], Train Loss: 0.000392
Validation Loss: 0.00037418
Epoch [49/300], Train Loss: 0.000377
Validation Loss: 0.00035451
Epoch [50/300], Train Loss: 0.000373
Validation Loss: 0.00035102
Epoch [51/300], Train Loss: 0.000372
Validation Loss: 0.00035401
Epoch [52/300], Train Loss: 0.000365
Validation Loss: 0.00034748
Epoch [53/300], Train Loss: 0.000365
Validation Loss: 0.00037069
Epoch [54/300], Train Loss: 0.000355
Validation Loss: 0.00036157
Epoch [55/300], Train Loss: 0.000361
Validation Loss: 0.00034187
Epoch [56/300], Train Loss: 0.000356
Validation Loss: 0.00034527
Epoch [57/300], Train Loss: 0.000351
Validation Loss: 0.00033623
Epoch [58/300], Train Loss: 0.000345
Validation Loss: 0.00036390
Epoch [59/300], Train Loss: 0.000348
Validation Loss: 0.00034662
Epoch [60/300], Train Loss: 0.000347
Validation Loss: 0.00034468
Epoch [61/300], Train Loss: 0.000339
Validation Loss: 0.00032257
Epoch [62/300], Train Loss: 0.000337
Validation Loss: 0.00032490
Epoch [63/300], Train Loss: 0.000337
Validation Loss: 0.00033933
Epoch [64/300], Train Loss: 0.000332
Validation Loss: 0.00033400
Epoch [65/300], Train Loss: 0.000333
Validation Loss: 0.00032544
Epoch [66/300], Train Loss: 0.000329
Validation Loss: 0.00033319
Epoch [67/300], Train Loss: 0.000329
Validation Loss: 0.00031626
Epoch [68/300], Train Loss: 0.000323
Validation Loss: 0.00031559
Epoch [69/300], Train Loss: 0.000323
Validation Loss: 0.00032599
Epoch [70/300], Train Loss: 0.000323
Validation Loss: 0.00031049
Epoch [71/300], Train Loss: 0.000321
Validation Loss: 0.00032019
Epoch [72/300], Train Loss: 0.000319
Validation Loss: 0.00030720
Epoch [73/300], Train Loss: 0.000323
Validation Loss: 0.00030717
Epoch [74/300], Train Loss: 0.000322
Validation Loss: 0.00033125
Epoch [75/300], Train Loss: 0.000317
Validation Loss: 0.00029876
Epoch [76/300], Train Loss: 0.000308
Validation Loss: 0.00030594
Epoch [77/300], Train Loss: 0.000309
Validation Loss: 0.00033989
Epoch [78/300], Train Loss: 0.000311
Validation Loss: 0.00031309
Epoch [79/300], Train Loss: 0.000306
Validation Loss: 0.00030791
Epoch [80/300], Train Loss: 0.000303
Validation Loss: 0.00029919
Epoch [81/300], Train Loss: 0.000304
Validation Loss: 0.00029171
Epoch [82/300], Train Loss: 0.000301
Validation Loss: 0.00029142
Epoch [83/300], Train Loss: 0.000299
Validation Loss: 0.00028526
Epoch [84/300], Train Loss: 0.000303
Validation Loss: 0.00029547
Epoch [85/300], Train Loss: 0.000299
Validation Loss: 0.00029598
Epoch [86/300], Train Loss: 0.000301
Validation Loss: 0.00028720
Epoch [87/300], Train Loss: 0.000299
Validation Loss: 0.00029269
Epoch [88/300], Train Loss: 0.000296
Validation Loss: 0.00030165
Epoch [89/300], Train Loss: 0.000291
Validation Loss: 0.00028505
Epoch [90/300], Train Loss: 0.000293
Validation Loss: 0.00028211
Epoch [91/300], Train Loss: 0.000287
Validation Loss: 0.00028270
Epoch [92/300], Train Loss: 0.000287
Validation Loss: 0.00028141
Epoch [93/300], Train Loss: 0.000289
Validation Loss: 0.00028665
Epoch [94/300], Train Loss: 0.000283
Validation Loss: 0.00027574
Epoch [95/300], Train Loss: 0.000282
Validation Loss: 0.00027565
Epoch [96/300], Train Loss: 0.000284
Validation Loss: 0.00027356
Epoch [97/300], Train Loss: 0.000285
Validation Loss: 0.00027206
Epoch [98/300], Train Loss: 0.000280
Validation Loss: 0.00028336
Epoch [99/300], Train Loss: 0.000279
Validation Loss: 0.00026981
Epoch [100/300], Train Loss: 0.000277
Validation Loss: 0.00026806
Epoch [101/300], Train Loss: 0.000282
Validation Loss: 0.00029823
Epoch [102/300], Train Loss: 0.000277
Validation Loss: 0.00027390
Epoch [103/300], Train Loss: 0.000275
Validation Loss: 0.00027518
Epoch [104/300], Train Loss: 0.000272
Validation Loss: 0.00026738
Epoch [105/300], Train Loss: 0.000273
Validation Loss: 0.00026444
Epoch [106/300], Train Loss: 0.000272
Validation Loss: 0.00026663
Epoch [107/300], Train Loss: 0.000271
Validation Loss: 0.00026033
Epoch [108/300], Train Loss: 0.000274
Validation Loss: 0.00026893
Epoch [109/300], Train Loss: 0.000271
Validation Loss: 0.00026942
Epoch [110/300], Train Loss: 0.000268
Validation Loss: 0.00025918
Epoch [111/300], Train Loss: 0.000267
Validation Loss: 0.00028790
Epoch [112/300], Train Loss: 0.000266
Validation Loss: 0.00025971
Epoch [113/300], Train Loss: 0.000267
Validation Loss: 0.00025549
Epoch [114/300], Train Loss: 0.000265
Validation Loss: 0.00025704
Epoch [115/300], Train Loss: 0.000268
Validation Loss: 0.00025735
Epoch [116/300], Train Loss: 0.000267
Validation Loss: 0.00025971
Epoch [117/300], Train Loss: 0.000261
Validation Loss: 0.00025477
Epoch [118/300], Train Loss: 0.000261
Validation Loss: 0.00025100
Epoch [119/300], Train Loss: 0.000260
Validation Loss: 0.00025231
Epoch [120/300], Train Loss: 0.000259
Validation Loss: 0.00025740
Epoch [121/300], Train Loss: 0.000258
Validation Loss: 0.00024982
Epoch [122/300], Train Loss: 0.000258
Validation Loss: 0.00025170
Epoch [123/300], Train Loss: 0.000259
Validation Loss: 0.00024609
Epoch [124/300], Train Loss: 0.000261
Validation Loss: 0.00025133
Epoch [125/300], Train Loss: 0.000256
Validation Loss: 0.00024756
Epoch [126/300], Train Loss: 0.000255
Validation Loss: 0.00024633
Epoch [127/300], Train Loss: 0.000259
Validation Loss: 0.00025060
Epoch [128/300], Train Loss: 0.000255
Validation Loss: 0.00024936
Epoch [129/300], Train Loss: 0.000251
Validation Loss: 0.00024545
Epoch [130/300], Train Loss: 0.000251
Validation Loss: 0.00025457
Epoch [131/300], Train Loss: 0.000254
Validation Loss: 0.00024850
Epoch [132/300], Train Loss: 0.000249
Validation Loss: 0.00023945
Epoch [133/300], Train Loss: 0.000250
Validation Loss: 0.00025108
Epoch [134/300], Train Loss: 0.000250
Validation Loss: 0.00023855
Epoch [135/300], Train Loss: 0.000251
Validation Loss: 0.00024333
Epoch [136/300], Train Loss: 0.000248
Validation Loss: 0.00024454
Epoch [137/300], Train Loss: 0.000246
Validation Loss: 0.00025195
Epoch [138/300], Train Loss: 0.000248
Validation Loss: 0.00024954
Epoch [139/300], Train Loss: 0.000245
Validation Loss: 0.00024068
Epoch [140/300], Train Loss: 0.000243
Validation Loss: 0.00024929
Epoch [141/300], Train Loss: 0.000245
Validation Loss: 0.00023770
Epoch [142/300], Train Loss: 0.000242
Validation Loss: 0.00025918
Epoch [143/300], Train Loss: 0.000245
Validation Loss: 0.00023721
Epoch [144/300], Train Loss: 0.000245
Validation Loss: 0.00023479
Epoch [145/300], Train Loss: 0.000243
Validation Loss: 0.00024307
Epoch [146/300], Train Loss: 0.000248
Validation Loss: 0.00024155
Epoch [147/300], Train Loss: 0.000240
Validation Loss: 0.00023457
Epoch [148/300], Train Loss: 0.000241
Validation Loss: 0.00023243
Epoch [149/300], Train Loss: 0.000238
Validation Loss: 0.00023404
Epoch [150/300], Train Loss: 0.000238
Validation Loss: 0.00023997
Epoch [151/300], Train Loss: 0.000239
Validation Loss: 0.00023109
Epoch [152/300], Train Loss: 0.000244
Validation Loss: 0.00025333
Epoch [153/300], Train Loss: 0.000238
Validation Loss: 0.00023709
Epoch [154/300], Train Loss: 0.000236
Validation Loss: 0.00023182
Epoch [155/300], Train Loss: 0.000237
Validation Loss: 0.00023559
Epoch [156/300], Train Loss: 0.000236
Validation Loss: 0.00025878
Epoch [157/300], Train Loss: 0.000234
Validation Loss: 0.00023276
Epoch [158/300], Train Loss: 0.000236
Validation Loss: 0.00022623
Epoch [159/300], Train Loss: 0.000232
Validation Loss: 0.00023603
Epoch [160/300], Train Loss: 0.000238
Validation Loss: 0.00022774
Epoch [161/300], Train Loss: 0.000233
Validation Loss: 0.00023020
Epoch [162/300], Train Loss: 0.000230
Validation Loss: 0.00022558
Epoch [163/300], Train Loss: 0.000232
Validation Loss: 0.00023434
Epoch [164/300], Train Loss: 0.000230
Validation Loss: 0.00022380
Epoch [165/300], Train Loss: 0.000230
Validation Loss: 0.00022404
Epoch [166/300], Train Loss: 0.000233
Validation Loss: 0.00023242
Epoch [167/300], Train Loss: 0.000227
Validation Loss: 0.00022611
Epoch [168/300], Train Loss: 0.000227
Validation Loss: 0.00022492
Epoch [169/300], Train Loss: 0.000229
Validation Loss: 0.00022483
Epoch [170/300], Train Loss: 0.000229
Validation Loss: 0.00022647
Epoch [171/300], Train Loss: 0.000230
Validation Loss: 0.00022870
Epoch [172/300], Train Loss: 0.000225
Validation Loss: 0.00022872
Epoch [173/300], Train Loss: 0.000229
Validation Loss: 0.00022944
Epoch [174/300], Train Loss: 0.000227
Validation Loss: 0.00022265
Epoch [175/300], Train Loss: 0.000225
Validation Loss: 0.00022916
Epoch [176/300], Train Loss: 0.000226
Validation Loss: 0.00023880
Epoch [177/300], Train Loss: 0.000225
Validation Loss: 0.00022132
Epoch [178/300], Train Loss: 0.000225
Validation Loss: 0.00022901
Epoch [179/300], Train Loss: 0.000221
Validation Loss: 0.00022402
Epoch [180/300], Train Loss: 0.000222
Validation Loss: 0.00022056
Epoch [181/300], Train Loss: 0.000223
Validation Loss: 0.00021865
Epoch [182/300], Train Loss: 0.000221
Validation Loss: 0.00025897
Epoch [183/300], Train Loss: 0.000232
Validation Loss: 0.00021805
Epoch [184/300], Train Loss: 0.000220
Validation Loss: 0.00021713
Epoch [185/300], Train Loss: 0.000218
Validation Loss: 0.00022119
Epoch [186/300], Train Loss: 0.000221
Validation Loss: 0.00021841
Epoch [187/300], Train Loss: 0.000218
Validation Loss: 0.00021687
Epoch [188/300], Train Loss: 0.000228
Validation Loss: 0.00021924
Epoch [189/300], Train Loss: 0.000217
Validation Loss: 0.00021646
Epoch [190/300], Train Loss: 0.000216
Validation Loss: 0.00021526
Epoch [191/300], Train Loss: 0.000224
Validation Loss: 0.00021943
Epoch [192/300], Train Loss: 0.000216
Validation Loss: 0.00023087
Epoch [193/300], Train Loss: 0.000219
Validation Loss: 0.00021681
Epoch [194/300], Train Loss: 0.000216
Validation Loss: 0.00021920
Epoch [195/300], Train Loss: 0.000219
Validation Loss: 0.00022175
Epoch [196/300], Train Loss: 0.000218
Validation Loss: 0.00021290
Epoch [197/300], Train Loss: 0.000215
Validation Loss: 0.00021278
Epoch [198/300], Train Loss: 0.000214
Validation Loss: 0.00022565
Epoch [199/300], Train Loss: 0.000218
Validation Loss: 0.00021234
Epoch [200/300], Train Loss: 0.000216
Validation Loss: 0.00021578
Epoch [201/300], Train Loss: 0.000214
Validation Loss: 0.00021252
Epoch [202/300], Train Loss: 0.000213
Validation Loss: 0.00021418
Epoch [203/300], Train Loss: 0.000212
Validation Loss: 0.00022493
Epoch [204/300], Train Loss: 0.000213
Validation Loss: 0.00021166
Epoch [205/300], Train Loss: 0.000213
Validation Loss: 0.00022735
Epoch [206/300], Train Loss: 0.000214
Validation Loss: 0.00021331
Epoch [207/300], Train Loss: 0.000213
Validation Loss: 0.00021639
Epoch [208/300], Train Loss: 0.000214
Validation Loss: 0.00022319
Epoch [209/300], Train Loss: 0.000212
Validation Loss: 0.00021054
Epoch [210/300], Train Loss: 0.000212
Validation Loss: 0.00021182
Epoch [211/300], Train Loss: 0.000211
Validation Loss: 0.00021612
Epoch [212/300], Train Loss: 0.000212
Validation Loss: 0.00021443
Epoch [213/300], Train Loss: 0.000214
Validation Loss: 0.00020995
Epoch [214/300], Train Loss: 0.000208
Validation Loss: 0.00020768
Epoch [215/300], Train Loss: 0.000226
Validation Loss: 0.00020987
Epoch [216/300], Train Loss: 0.000211
Validation Loss: 0.00020854
Epoch [217/300], Train Loss: 0.000209
Validation Loss: 0.00021137
Epoch [218/300], Train Loss: 0.000208
Validation Loss: 0.00020963
Epoch [219/300], Train Loss: 0.000208
Validation Loss: 0.00020645
Epoch [220/300], Train Loss: 0.000209
Validation Loss: 0.00023367
Epoch [221/300], Train Loss: 0.000222
Validation Loss: 0.00021240
Epoch [222/300], Train Loss: 0.000208
Validation Loss: 0.00020766
Epoch [223/300], Train Loss: 0.000208
Validation Loss: 0.00021048
Epoch [224/300], Train Loss: 0.000206
Validation Loss: 0.00020625
Epoch [225/300], Train Loss: 0.000206
Validation Loss: 0.00020972
Epoch [226/300], Train Loss: 0.000206
Validation Loss: 0.00022394
Epoch [227/300], Train Loss: 0.000216
Validation Loss: 0.00020523
Epoch [228/300], Train Loss: 0.000204
Validation Loss: 0.00020806
Epoch [229/300], Train Loss: 0.000204
Validation Loss: 0.00020585
Epoch [230/300], Train Loss: 0.000204
Validation Loss: 0.00020660
Epoch [231/300], Train Loss: 0.000213
Validation Loss: 0.00020249
Epoch [232/300], Train Loss: 0.000204
Validation Loss: 0.00020710
Epoch [233/300], Train Loss: 0.000205
Validation Loss: 0.00020778
Epoch [234/300], Train Loss: 0.000203
Validation Loss: 0.00020257
Epoch [235/300], Train Loss: 0.000203
Validation Loss: 0.00020607
Epoch [236/300], Train Loss: 0.000204
Validation Loss: 0.00020179
Epoch [237/300], Train Loss: 0.000202
Validation Loss: 0.00020690
Epoch [238/300], Train Loss: 0.000207
Validation Loss: 0.00020742
Epoch [239/300], Train Loss: 0.000202
Validation Loss: 0.00020428
Epoch [240/300], Train Loss: 0.000201
Validation Loss: 0.00020313
Epoch [241/300], Train Loss: 0.000203
Validation Loss: 0.00021563
Epoch [242/300], Train Loss: 0.000204
Validation Loss: 0.00020247
Epoch [243/300], Train Loss: 0.000200
Validation Loss: 0.00020602
Epoch [244/300], Train Loss: 0.000200
Validation Loss: 0.00020104
Epoch [245/300], Train Loss: 0.000201
Validation Loss: 0.00020370
Epoch [246/300], Train Loss: 0.000202
Validation Loss: 0.00020527
Epoch [247/300], Train Loss: 0.000199
Validation Loss: 0.00020335
Epoch [248/300], Train Loss: 0.000201
Validation Loss: 0.00020206
Epoch [249/300], Train Loss: 0.000203
Validation Loss: 0.00020548
Epoch [250/300], Train Loss: 0.000202
Validation Loss: 0.00022462
Epoch [251/300], Train Loss: 0.000200
Validation Loss: 0.00020182
Epoch [252/300], Train Loss: 0.000201
Validation Loss: 0.00020456
Epoch [253/300], Train Loss: 0.000205
Validation Loss: 0.00021149
Epoch [254/300], Train Loss: 0.000199
Validation Loss: 0.00020055
Epoch [255/300], Train Loss: 0.000198
Validation Loss: 0.00020418
Epoch [256/300], Train Loss: 0.000198
Validation Loss: 0.00019999
Epoch [257/300], Train Loss: 0.000205
Validation Loss: 0.00020500
Epoch [258/300], Train Loss: 0.000198
Validation Loss: 0.00020094
Epoch [259/300], Train Loss: 0.000201
Validation Loss: 0.00019869
Epoch [260/300], Train Loss: 0.000197
Validation Loss: 0.00020065
Epoch [261/300], Train Loss: 0.000200
Validation Loss: 0.00020025
Epoch [262/300], Train Loss: 0.000196
Validation Loss: 0.00019809
Epoch [263/300], Train Loss: 0.000196
Validation Loss: 0.00020147
Epoch [264/300], Train Loss: 0.000198
Validation Loss: 0.00019825
Epoch [265/300], Train Loss: 0.000197
Validation Loss: 0.00020111
Epoch [266/300], Train Loss: 0.000196
Validation Loss: 0.00019824
Epoch [267/300], Train Loss: 0.000195
Validation Loss: 0.00019945
Epoch [268/300], Train Loss: 0.000195
Validation Loss: 0.00020172
Epoch [269/300], Train Loss: 0.000196
Validation Loss: 0.00019821
Epoch [270/300], Train Loss: 0.000199
Validation Loss: 0.00020427
Epoch [271/300], Train Loss: 0.000197
Validation Loss: 0.00020739
Epoch [272/300], Train Loss: 0.000200
Validation Loss: 0.00019871
Early stopping triggered

Evaluating model for: Fridge
Run 54/72 completed in 11233.28 seconds with: {'MAE': np.float32(16.108637), 'MSE': np.float32(634.92535), 'RMSE': np.float32(25.197725), 'SAE': np.float32(0.012426859), 'NDE': np.float32(0.43347248)}

Run 55/72: hidden=512, seq_len=120, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000660
Validation Loss: 0.00067534
Epoch [2/300], Train Loss: 0.000631
Validation Loss: 0.00063212
Epoch [3/300], Train Loss: 0.000614
Validation Loss: 0.00061635
Epoch [4/300], Train Loss: 0.000605
Validation Loss: 0.00060794
Epoch [5/300], Train Loss: 0.000599
Validation Loss: 0.00062098
Epoch [6/300], Train Loss: 0.000598
Validation Loss: 0.00060575
Epoch [7/300], Train Loss: 0.000594
Validation Loss: 0.00060184
Epoch [8/300], Train Loss: 0.000596
Validation Loss: 0.00059828
Epoch [9/300], Train Loss: 0.000591
Validation Loss: 0.00059589
Epoch [10/300], Train Loss: 0.000588
Validation Loss: 0.00058930
Epoch [11/300], Train Loss: 0.000585
Validation Loss: 0.00060547
Epoch [12/300], Train Loss: 0.000584
Validation Loss: 0.00059027
Epoch [13/300], Train Loss: 0.000578
Validation Loss: 0.00058991
Epoch [14/300], Train Loss: 0.000575
Validation Loss: 0.00058374
Epoch [15/300], Train Loss: 0.000572
Validation Loss: 0.00057037
Epoch [16/300], Train Loss: 0.000563
Validation Loss: 0.00056064
Epoch [17/300], Train Loss: 0.000560
Validation Loss: 0.00056258
Epoch [18/300], Train Loss: 0.000558
Validation Loss: 0.00055478
Epoch [19/300], Train Loss: 0.000555
Validation Loss: 0.00054968
Epoch [20/300], Train Loss: 0.000549
Validation Loss: 0.00054444
Epoch [21/300], Train Loss: 0.000545
Validation Loss: 0.00054467
Epoch [22/300], Train Loss: 0.000540
Validation Loss: 0.00052797
Epoch [23/300], Train Loss: 0.000531
Validation Loss: 0.00053377
Epoch [24/300], Train Loss: 0.000522
Validation Loss: 0.00051119
Epoch [25/300], Train Loss: 0.000526
Validation Loss: 0.00051332
Epoch [26/300], Train Loss: 0.000523
Validation Loss: 0.00052486
Epoch [27/300], Train Loss: 0.000518
Validation Loss: 0.00050297
Epoch [28/300], Train Loss: 0.000512
Validation Loss: 0.00050845
Epoch [29/300], Train Loss: 0.000506
Validation Loss: 0.00051862
Epoch [30/300], Train Loss: 0.000506
Validation Loss: 0.00049798
Epoch [31/300], Train Loss: 0.000502
Validation Loss: 0.00050040
Epoch [32/300], Train Loss: 0.000499
Validation Loss: 0.00049174
Epoch [33/300], Train Loss: 0.000497
Validation Loss: 0.00050331
Epoch [34/300], Train Loss: 0.000494
Validation Loss: 0.00048849
Epoch [35/300], Train Loss: 0.000491
Validation Loss: 0.00048402
Epoch [36/300], Train Loss: 0.000491
Validation Loss: 0.00048350
Epoch [37/300], Train Loss: 0.000481
Validation Loss: 0.00047944
Epoch [38/300], Train Loss: 0.000482
Validation Loss: 0.00047538
Epoch [39/300], Train Loss: 0.000475
Validation Loss: 0.00046966
Epoch [40/300], Train Loss: 0.000475
Validation Loss: 0.00046901
Epoch [41/300], Train Loss: 0.000471
Validation Loss: 0.00046256
Epoch [42/300], Train Loss: 0.000462
Validation Loss: 0.00046065
Epoch [43/300], Train Loss: 0.000455
Validation Loss: 0.00044289
Epoch [44/300], Train Loss: 0.000450
Validation Loss: 0.00044575
Epoch [45/300], Train Loss: 0.000441
Validation Loss: 0.00041929
Epoch [46/300], Train Loss: 0.000429
Validation Loss: 0.00044003
Epoch [47/300], Train Loss: 0.000419
Validation Loss: 0.00042885
Epoch [48/300], Train Loss: 0.000412
Validation Loss: 0.00039841
Epoch [49/300], Train Loss: 0.000409
Validation Loss: 0.00038990
Epoch [50/300], Train Loss: 0.000401
Validation Loss: 0.00039069
Epoch [51/300], Train Loss: 0.000395
Validation Loss: 0.00039107
Epoch [52/300], Train Loss: 0.000389
Validation Loss: 0.00037666
Epoch [53/300], Train Loss: 0.000387
Validation Loss: 0.00038289
Epoch [54/300], Train Loss: 0.000382
Validation Loss: 0.00035802
Epoch [55/300], Train Loss: 0.000382
Validation Loss: 0.00036844
Epoch [56/300], Train Loss: 0.000373
Validation Loss: 0.00036273
Epoch [57/300], Train Loss: 0.000368
Validation Loss: 0.00035463
Epoch [58/300], Train Loss: 0.000370
Validation Loss: 0.00035217
Epoch [59/300], Train Loss: 0.000365
Validation Loss: 0.00034772
Epoch [60/300], Train Loss: 0.000361
Validation Loss: 0.00035478
Epoch [61/300], Train Loss: 0.000357
Validation Loss: 0.00034837
Epoch [62/300], Train Loss: 0.000356
Validation Loss: 0.00033995
Epoch [63/300], Train Loss: 0.000353
Validation Loss: 0.00033707
Epoch [64/300], Train Loss: 0.000352
Validation Loss: 0.00033015
Epoch [65/300], Train Loss: 0.000350
Validation Loss: 0.00034378
Epoch [66/300], Train Loss: 0.000353
Validation Loss: 0.00036713
Epoch [67/300], Train Loss: 0.000345
Validation Loss: 0.00032993
Epoch [68/300], Train Loss: 0.000341
Validation Loss: 0.00032491
Epoch [69/300], Train Loss: 0.000345
Validation Loss: 0.00032645
Epoch [70/300], Train Loss: 0.000344
Validation Loss: 0.00032165
Epoch [71/300], Train Loss: 0.000359
Validation Loss: 0.00035163
Epoch [72/300], Train Loss: 0.000345
Validation Loss: 0.00034133
Epoch [73/300], Train Loss: 0.000339
Validation Loss: 0.00034399
Epoch [74/300], Train Loss: 0.000338
Validation Loss: 0.00034925
Epoch [75/300], Train Loss: 0.000337
Validation Loss: 0.00032745
Epoch [76/300], Train Loss: 0.000329
Validation Loss: 0.00031924
Epoch [77/300], Train Loss: 0.000328
Validation Loss: 0.00030928
Epoch [78/300], Train Loss: 0.000327
Validation Loss: 0.00034082
Epoch [79/300], Train Loss: 0.000366
Validation Loss: 0.00039647
Epoch [80/300], Train Loss: 0.000360
Validation Loss: 0.00035231
Epoch [81/300], Train Loss: 0.000337
Validation Loss: 0.00032273
Epoch [82/300], Train Loss: 0.000331
Validation Loss: 0.00032226
Epoch [83/300], Train Loss: 0.000330
Validation Loss: 0.00042619
Epoch [84/300], Train Loss: 0.000326
Validation Loss: 0.00030966
Epoch [85/300], Train Loss: 0.000322
Validation Loss: 0.00037186
Epoch [86/300], Train Loss: 0.000456
Validation Loss: 0.00041255
Epoch [87/300], Train Loss: 0.000409
Validation Loss: 0.00039593
Early stopping triggered

Evaluating model for: Fridge
Run 55/72 completed in 3752.21 seconds with: {'MAE': np.float32(28.45163), 'MSE': np.float32(1346.9318), 'RMSE': np.float32(36.70057), 'SAE': np.float32(0.1778711), 'NDE': np.float32(0.63135415)}

Run 56/72: hidden=512, seq_len=120, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 20546 windows

Epoch [1/300], Train Loss: 0.000707
Validation Loss: 0.00069175
Epoch [2/300], Train Loss: 0.000665
Validation Loss: 0.00068296
Epoch [3/300], Train Loss: 0.000642
Validation Loss: 0.00064705
Epoch [4/300], Train Loss: 0.000623
Validation Loss: 0.00062443
Epoch [5/300], Train Loss: 0.000612
Validation Loss: 0.00062193
Epoch [6/300], Train Loss: 0.000609
Validation Loss: 0.00061335
Epoch [7/300], Train Loss: 0.000604
Validation Loss: 0.00061034
Epoch [8/300], Train Loss: 0.000605
Validation Loss: 0.00061039
Epoch [9/300], Train Loss: 0.000600
Validation Loss: 0.00060441
Epoch [10/300], Train Loss: 0.000598
Validation Loss: 0.00060293
Epoch [11/300], Train Loss: 0.000597
Validation Loss: 0.00060742
Epoch [12/300], Train Loss: 0.000596
Validation Loss: 0.00060138
Epoch [13/300], Train Loss: 0.000596
Validation Loss: 0.00059949
Epoch [14/300], Train Loss: 0.000593
Validation Loss: 0.00060654
Epoch [15/300], Train Loss: 0.000593
Validation Loss: 0.00061073
Epoch [16/300], Train Loss: 0.000589
Validation Loss: 0.00059737
Epoch [17/300], Train Loss: 0.000587
Validation Loss: 0.00059268
Epoch [18/300], Train Loss: 0.000585
Validation Loss: 0.00058750
Epoch [19/300], Train Loss: 0.000580
Validation Loss: 0.00057615
Epoch [20/300], Train Loss: 0.000576
Validation Loss: 0.00057374
Epoch [21/300], Train Loss: 0.000570
Validation Loss: 0.00056467
Epoch [22/300], Train Loss: 0.000568
Validation Loss: 0.00056075
Epoch [23/300], Train Loss: 0.000563
Validation Loss: 0.00055027
Epoch [24/300], Train Loss: 0.000557
Validation Loss: 0.00054674
Epoch [25/300], Train Loss: 0.000552
Validation Loss: 0.00054426
Epoch [26/300], Train Loss: 0.000546
Validation Loss: 0.00053196
Epoch [27/300], Train Loss: 0.000536
Validation Loss: 0.00051715
Epoch [28/300], Train Loss: 0.000529
Validation Loss: 0.00052413
Epoch [29/300], Train Loss: 0.000523
Validation Loss: 0.00052215
Epoch [30/300], Train Loss: 0.000522
Validation Loss: 0.00051415
Epoch [31/300], Train Loss: 0.000512
Validation Loss: 0.00050032
Epoch [32/300], Train Loss: 0.000506
Validation Loss: 0.00049697
Epoch [33/300], Train Loss: 0.000502
Validation Loss: 0.00049684
Epoch [34/300], Train Loss: 0.000495
Validation Loss: 0.00048199
Epoch [35/300], Train Loss: 0.000492
Validation Loss: 0.00047198
Epoch [36/300], Train Loss: 0.000487
Validation Loss: 0.00047676
Epoch [37/300], Train Loss: 0.000481
Validation Loss: 0.00046554
Epoch [38/300], Train Loss: 0.000474
Validation Loss: 0.00046449
Epoch [39/300], Train Loss: 0.000466
Validation Loss: 0.00045064
Epoch [40/300], Train Loss: 0.000461
Validation Loss: 0.00043813
Epoch [41/300], Train Loss: 0.000445
Validation Loss: 0.00042593
Epoch [42/300], Train Loss: 0.000432
Validation Loss: 0.00042044
Epoch [43/300], Train Loss: 0.000421
Validation Loss: 0.00039387
Epoch [44/300], Train Loss: 0.000409
Validation Loss: 0.00039671
Epoch [45/300], Train Loss: 0.000399
Validation Loss: 0.00038836
Epoch [46/300], Train Loss: 0.000389
Validation Loss: 0.00037581
Epoch [47/300], Train Loss: 0.000383
Validation Loss: 0.00038171
Epoch [48/300], Train Loss: 0.000379
Validation Loss: 0.00038567
Epoch [49/300], Train Loss: 0.000371
Validation Loss: 0.00035324
Epoch [50/300], Train Loss: 0.000371
Validation Loss: 0.00036004
Epoch [51/300], Train Loss: 0.000361
Validation Loss: 0.00035361
Epoch [52/300], Train Loss: 0.000355
Validation Loss: 0.00034552
Epoch [53/300], Train Loss: 0.000353
Validation Loss: 0.00033383
Epoch [54/300], Train Loss: 0.000348
Validation Loss: 0.00033600
Epoch [55/300], Train Loss: 0.000346
Validation Loss: 0.00033895
Epoch [56/300], Train Loss: 0.000341
Validation Loss: 0.00032812
Epoch [57/300], Train Loss: 0.000340
Validation Loss: 0.00031651
Epoch [58/300], Train Loss: 0.000338
Validation Loss: 0.00031858
Epoch [59/300], Train Loss: 0.000331
Validation Loss: 0.00031649
Epoch [60/300], Train Loss: 0.000327
Validation Loss: 0.00031633
Epoch [61/300], Train Loss: 0.000326
Validation Loss: 0.00031573
Epoch [62/300], Train Loss: 0.000328
Validation Loss: 0.00033298
Epoch [63/300], Train Loss: 0.000320
Validation Loss: 0.00031621
Epoch [64/300], Train Loss: 0.000326
Validation Loss: 0.00030751
Epoch [65/300], Train Loss: 0.000319
Validation Loss: 0.00030723
Epoch [66/300], Train Loss: 0.000314
Validation Loss: 0.00031174
Epoch [67/300], Train Loss: 0.000311
Validation Loss: 0.00029725
Epoch [68/300], Train Loss: 0.000307
Validation Loss: 0.00029746
Epoch [69/300], Train Loss: 0.000312
Validation Loss: 0.00031047
Epoch [70/300], Train Loss: 0.000309
Validation Loss: 0.00029764
Epoch [71/300], Train Loss: 0.000310
Validation Loss: 0.00029616
Epoch [72/300], Train Loss: 0.000301
Validation Loss: 0.00029024
Epoch [73/300], Train Loss: 0.000300
Validation Loss: 0.00030781
Epoch [74/300], Train Loss: 0.000303
Validation Loss: 0.00031602
Epoch [75/300], Train Loss: 0.000304
Validation Loss: 0.00030914
Epoch [76/300], Train Loss: 0.000299
Validation Loss: 0.00028672
Epoch [77/300], Train Loss: 0.000298
Validation Loss: 0.00029733
Epoch [78/300], Train Loss: 0.000296
Validation Loss: 0.00028588
Epoch [79/300], Train Loss: 0.000290
Validation Loss: 0.00029150
Epoch [80/300], Train Loss: 0.000292
Validation Loss: 0.00027806
Epoch [81/300], Train Loss: 0.000287
Validation Loss: 0.00028119
Epoch [82/300], Train Loss: 0.000291
Validation Loss: 0.00028381
Epoch [83/300], Train Loss: 0.000287
Validation Loss: 0.00031586
Epoch [84/300], Train Loss: 0.000294
Validation Loss: 0.00028401
Epoch [85/300], Train Loss: 0.000280
Validation Loss: 0.00028188
Epoch [86/300], Train Loss: 0.000284
Validation Loss: 0.00029314
Epoch [87/300], Train Loss: 0.000284
Validation Loss: 0.00027908
Epoch [88/300], Train Loss: 0.000284
Validation Loss: 0.00029626
Epoch [89/300], Train Loss: 0.000284
Validation Loss: 0.00028240
Epoch [90/300], Train Loss: 0.000275
Validation Loss: 0.00027524
Epoch [91/300], Train Loss: 0.000281
Validation Loss: 0.00026901
Epoch [92/300], Train Loss: 0.000408
Validation Loss: 0.00046143
Epoch [93/300], Train Loss: 0.000421
Validation Loss: 0.00037169
Epoch [94/300], Train Loss: 0.000378
Validation Loss: 0.00034953
Epoch [95/300], Train Loss: 0.000358
Validation Loss: 0.00033459
Epoch [96/300], Train Loss: 0.000349
Validation Loss: 0.00032430
Epoch [97/300], Train Loss: 0.000337
Validation Loss: 0.00031839
Epoch [98/300], Train Loss: 0.000329
Validation Loss: 0.00030771
Epoch [99/300], Train Loss: 0.000320
Validation Loss: 0.00032943
Epoch [100/300], Train Loss: 0.000321
Validation Loss: 0.00031253
Epoch [101/300], Train Loss: 0.000316
Validation Loss: 0.00033099
Early stopping triggered

Evaluating model for: Fridge
Run 56/72 completed in 4567.96 seconds with: {'MAE': np.float32(23.130419), 'MSE': np.float32(1135.4922), 'RMSE': np.float32(33.697063), 'SAE': np.float32(0.16954264), 'NDE': np.float32(0.5796853)}

Run 57/72: hidden=512, seq_len=360, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000655
Validation Loss: 0.00064982
Epoch [2/300], Train Loss: 0.000636
Validation Loss: 0.00062736
Epoch [3/300], Train Loss: 0.000621
Validation Loss: 0.00061494
Epoch [4/300], Train Loss: 0.000610
Validation Loss: 0.00060804
Epoch [5/300], Train Loss: 0.000608
Validation Loss: 0.00060534
Epoch [6/300], Train Loss: 0.000602
Validation Loss: 0.00060455
Epoch [7/300], Train Loss: 0.000599
Validation Loss: 0.00059766
Epoch [8/300], Train Loss: 0.000596
Validation Loss: 0.00059720
Epoch [9/300], Train Loss: 0.000594
Validation Loss: 0.00059549
Epoch [10/300], Train Loss: 0.000594
Validation Loss: 0.00059317
Epoch [11/300], Train Loss: 0.000588
Validation Loss: 0.00058853
Epoch [12/300], Train Loss: 0.000585
Validation Loss: 0.00058512
Epoch [13/300], Train Loss: 0.000582
Validation Loss: 0.00058557
Epoch [14/300], Train Loss: 0.000581
Validation Loss: 0.00058667
Epoch [15/300], Train Loss: 0.000579
Validation Loss: 0.00058216
Epoch [16/300], Train Loss: 0.000575
Validation Loss: 0.00057701
Epoch [17/300], Train Loss: 0.000573
Validation Loss: 0.00057204
Epoch [18/300], Train Loss: 0.000570
Validation Loss: 0.00057624
Epoch [19/300], Train Loss: 0.000569
Validation Loss: 0.00056780
Epoch [20/300], Train Loss: 0.000564
Validation Loss: 0.00056549
Epoch [21/300], Train Loss: 0.000559
Validation Loss: 0.00055572
Epoch [22/300], Train Loss: 0.000555
Validation Loss: 0.00054969
Epoch [23/300], Train Loss: 0.000551
Validation Loss: 0.00056554
Epoch [24/300], Train Loss: 0.000545
Validation Loss: 0.00053746
Epoch [25/300], Train Loss: 0.000539
Validation Loss: 0.00054745
Epoch [26/300], Train Loss: 0.000538
Validation Loss: 0.00053028
Epoch [27/300], Train Loss: 0.000530
Validation Loss: 0.00054195
Epoch [28/300], Train Loss: 0.000529
Validation Loss: 0.00055714
Epoch [29/300], Train Loss: 0.000528
Validation Loss: 0.00052816
Epoch [30/300], Train Loss: 0.000519
Validation Loss: 0.00050830
Epoch [31/300], Train Loss: 0.000513
Validation Loss: 0.00050542
Epoch [32/300], Train Loss: 0.000509
Validation Loss: 0.00050963
Epoch [33/300], Train Loss: 0.000511
Validation Loss: 0.00050644
Epoch [34/300], Train Loss: 0.000506
Validation Loss: 0.00050206
Epoch [35/300], Train Loss: 0.000507
Validation Loss: 0.00049894
Epoch [36/300], Train Loss: 0.000500
Validation Loss: 0.00049462
Epoch [37/300], Train Loss: 0.000502
Validation Loss: 0.00048991
Epoch [38/300], Train Loss: 0.000494
Validation Loss: 0.00049057
Epoch [39/300], Train Loss: 0.000543
Validation Loss: 0.00059973
Epoch [40/300], Train Loss: 0.000586
Validation Loss: 0.00056240
Epoch [41/300], Train Loss: 0.000565
Validation Loss: 0.00055213
Epoch [42/300], Train Loss: 0.000556
Validation Loss: 0.00055094
Epoch [43/300], Train Loss: 0.000548
Validation Loss: 0.00053982
Epoch [44/300], Train Loss: 0.000542
Validation Loss: 0.00053495
Epoch [45/300], Train Loss: 0.000537
Validation Loss: 0.00053039
Epoch [46/300], Train Loss: 0.000531
Validation Loss: 0.00052431
Epoch [47/300], Train Loss: 0.000526
Validation Loss: 0.00052308
Early stopping triggered

Evaluating model for: Fridge
Run 57/72 completed in 1468.12 seconds with: {'MAE': np.float32(34.05057), 'MSE': np.float32(1751.4834), 'RMSE': np.float32(41.850727), 'SAE': np.float32(0.003145738), 'NDE': np.float32(0.72316694)}

Run 58/72: hidden=512, seq_len=360, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000682
Validation Loss: 0.00066117
Epoch [2/300], Train Loss: 0.000651
Validation Loss: 0.00064320
Epoch [3/300], Train Loss: 0.000625
Validation Loss: 0.00061177
Epoch [4/300], Train Loss: 0.000610
Validation Loss: 0.00060499
Epoch [5/300], Train Loss: 0.000608
Validation Loss: 0.00060995
Epoch [6/300], Train Loss: 0.000605
Validation Loss: 0.00060266
Epoch [7/300], Train Loss: 0.000601
Validation Loss: 0.00059788
Epoch [8/300], Train Loss: 0.000598
Validation Loss: 0.00060072
Epoch [9/300], Train Loss: 0.000596
Validation Loss: 0.00059407
Epoch [10/300], Train Loss: 0.000591
Validation Loss: 0.00058765
Epoch [11/300], Train Loss: 0.000587
Validation Loss: 0.00058672
Epoch [12/300], Train Loss: 0.000585
Validation Loss: 0.00057931
Epoch [13/300], Train Loss: 0.000584
Validation Loss: 0.00058489
Epoch [14/300], Train Loss: 0.000580
Validation Loss: 0.00058641
Epoch [15/300], Train Loss: 0.000577
Validation Loss: 0.00057057
Epoch [16/300], Train Loss: 0.000571
Validation Loss: 0.00057930
Epoch [17/300], Train Loss: 0.000572
Validation Loss: 0.00056390
Epoch [18/300], Train Loss: 0.000565
Validation Loss: 0.00056404
Epoch [19/300], Train Loss: 0.000560
Validation Loss: 0.00055719
Epoch [20/300], Train Loss: 0.000559
Validation Loss: 0.00054906
Epoch [21/300], Train Loss: 0.000546
Validation Loss: 0.00054574
Epoch [22/300], Train Loss: 0.000540
Validation Loss: 0.00052487
Epoch [23/300], Train Loss: 0.000531
Validation Loss: 0.00052562
Epoch [24/300], Train Loss: 0.000524
Validation Loss: 0.00055280
Epoch [25/300], Train Loss: 0.000523
Validation Loss: 0.00051287
Epoch [26/300], Train Loss: 0.000501
Validation Loss: 0.00051794
Epoch [27/300], Train Loss: 0.000492
Validation Loss: 0.00048110
Epoch [28/300], Train Loss: 0.000481
Validation Loss: 0.00046665
Epoch [29/300], Train Loss: 0.000478
Validation Loss: 0.00046327
Epoch [30/300], Train Loss: 0.000460
Validation Loss: 0.00045155
Epoch [31/300], Train Loss: 0.000447
Validation Loss: 0.00044370
Epoch [32/300], Train Loss: 0.000442
Validation Loss: 0.00042964
Epoch [33/300], Train Loss: 0.000446
Validation Loss: 0.00043673
Epoch [34/300], Train Loss: 0.000441
Validation Loss: 0.00042125
Epoch [35/300], Train Loss: 0.000439
Validation Loss: 0.00042313
Epoch [36/300], Train Loss: 0.000437
Validation Loss: 0.00043256
Epoch [37/300], Train Loss: 0.000426
Validation Loss: 0.00041741
Epoch [38/300], Train Loss: 0.000426
Validation Loss: 0.00042786
Epoch [39/300], Train Loss: 0.000428
Validation Loss: 0.00041881
Epoch [40/300], Train Loss: 0.000421
Validation Loss: 0.00045728
Epoch [41/300], Train Loss: 0.000421
Validation Loss: 0.00040475
Epoch [42/300], Train Loss: 0.000415
Validation Loss: 0.00041983
Epoch [43/300], Train Loss: 0.000426
Validation Loss: 0.00040155
Epoch [44/300], Train Loss: 0.000411
Validation Loss: 0.00043357
Epoch [45/300], Train Loss: 0.000410
Validation Loss: 0.00040690
Epoch [46/300], Train Loss: 0.000403
Validation Loss: 0.00040240
Epoch [47/300], Train Loss: 0.000408
Validation Loss: 0.00039827
Epoch [48/300], Train Loss: 0.000406
Validation Loss: 0.00039630
Epoch [49/300], Train Loss: 0.000398
Validation Loss: 0.00039432
Epoch [50/300], Train Loss: 0.000398
Validation Loss: 0.00039052
Epoch [51/300], Train Loss: 0.000400
Validation Loss: 0.00040070
Epoch [52/300], Train Loss: 0.000403
Validation Loss: 0.00041163
Epoch [53/300], Train Loss: 0.000397
Validation Loss: 0.00038937
Epoch [54/300], Train Loss: 0.000397
Validation Loss: 0.00039294
Epoch [55/300], Train Loss: 0.000399
Validation Loss: 0.00039244
Epoch [56/300], Train Loss: 0.000393
Validation Loss: 0.00038411
Epoch [57/300], Train Loss: 0.000390
Validation Loss: 0.00038784
Epoch [58/300], Train Loss: 0.000391
Validation Loss: 0.00039083
Epoch [59/300], Train Loss: 0.000393
Validation Loss: 0.00038291
Epoch [60/300], Train Loss: 0.000392
Validation Loss: 0.00039861
Epoch [61/300], Train Loss: 0.000395
Validation Loss: 0.00038257
Epoch [62/300], Train Loss: 0.000388
Validation Loss: 0.00038370
Epoch [63/300], Train Loss: 0.000384
Validation Loss: 0.00037783
Epoch [64/300], Train Loss: 0.000386
Validation Loss: 0.00038084
Epoch [65/300], Train Loss: 0.000384
Validation Loss: 0.00039881
Epoch [66/300], Train Loss: 0.000383
Validation Loss: 0.00037451
Epoch [67/300], Train Loss: 0.000381
Validation Loss: 0.00038364
Epoch [68/300], Train Loss: 0.000381
Validation Loss: 0.00038123
Epoch [69/300], Train Loss: 0.000380
Validation Loss: 0.00038191
Epoch [70/300], Train Loss: 0.000380
Validation Loss: 0.00038318
Epoch [71/300], Train Loss: 0.000379
Validation Loss: 0.00036803
Epoch [72/300], Train Loss: 0.000380
Validation Loss: 0.00037048
Epoch [73/300], Train Loss: 0.000378
Validation Loss: 0.00037363
Epoch [74/300], Train Loss: 0.000376
Validation Loss: 0.00038731
Epoch [75/300], Train Loss: 0.000377
Validation Loss: 0.00038807
Epoch [76/300], Train Loss: 0.000373
Validation Loss: 0.00036511
Epoch [77/300], Train Loss: 0.000375
Validation Loss: 0.00036536
Epoch [78/300], Train Loss: 0.000380
Validation Loss: 0.00036459
Epoch [79/300], Train Loss: 0.000374
Validation Loss: 0.00036235
Epoch [80/300], Train Loss: 0.000369
Validation Loss: 0.00037002
Epoch [81/300], Train Loss: 0.000374
Validation Loss: 0.00039394
Epoch [82/300], Train Loss: 0.000371
Validation Loss: 0.00036614
Epoch [83/300], Train Loss: 0.000367
Validation Loss: 0.00036857
Epoch [84/300], Train Loss: 0.000369
Validation Loss: 0.00036198
Epoch [85/300], Train Loss: 0.000374
Validation Loss: 0.00038384
Epoch [86/300], Train Loss: 0.000370
Validation Loss: 0.00036044
Epoch [87/300], Train Loss: 0.000371
Validation Loss: 0.00035963
Epoch [88/300], Train Loss: 0.000364
Validation Loss: 0.00035903
Epoch [89/300], Train Loss: 0.000365
Validation Loss: 0.00037335
Epoch [90/300], Train Loss: 0.000366
Validation Loss: 0.00035576
Epoch [91/300], Train Loss: 0.000364
Validation Loss: 0.00036269
Epoch [92/300], Train Loss: 0.000361
Validation Loss: 0.00035390
Epoch [93/300], Train Loss: 0.000365
Validation Loss: 0.00035711
Epoch [94/300], Train Loss: 0.000358
Validation Loss: 0.00035181
Epoch [95/300], Train Loss: 0.000358
Validation Loss: 0.00035463
Epoch [96/300], Train Loss: 0.000359
Validation Loss: 0.00035904
Epoch [97/300], Train Loss: 0.000359
Validation Loss: 0.00037561
Epoch [98/300], Train Loss: 0.000362
Validation Loss: 0.00035854
Epoch [99/300], Train Loss: 0.000362
Validation Loss: 0.00035275
Epoch [100/300], Train Loss: 0.000358
Validation Loss: 0.00034814
Epoch [101/300], Train Loss: 0.000360
Validation Loss: 0.00034540
Epoch [102/300], Train Loss: 0.000358
Validation Loss: 0.00035074
Epoch [103/300], Train Loss: 0.000357
Validation Loss: 0.00035514
Epoch [104/300], Train Loss: 0.000353
Validation Loss: 0.00034846
Epoch [105/300], Train Loss: 0.000355
Validation Loss: 0.00034772
Epoch [106/300], Train Loss: 0.000352
Validation Loss: 0.00034410
Epoch [107/300], Train Loss: 0.000351
Validation Loss: 0.00034319
Epoch [108/300], Train Loss: 0.000351
Validation Loss: 0.00034684
Epoch [109/300], Train Loss: 0.000348
Validation Loss: 0.00034133
Epoch [110/300], Train Loss: 0.000350
Validation Loss: 0.00034073
Epoch [111/300], Train Loss: 0.000358
Validation Loss: 0.00033913
Epoch [112/300], Train Loss: 0.000350
Validation Loss: 0.00033651
Epoch [113/300], Train Loss: 0.000343
Validation Loss: 0.00034052
Epoch [114/300], Train Loss: 0.000344
Validation Loss: 0.00033870
Epoch [115/300], Train Loss: 0.000344
Validation Loss: 0.00033467
Epoch [116/300], Train Loss: 0.000341
Validation Loss: 0.00033370
Epoch [117/300], Train Loss: 0.000342
Validation Loss: 0.00034025
Epoch [118/300], Train Loss: 0.000462
Validation Loss: 0.00040064
Epoch [119/300], Train Loss: 0.000391
Validation Loss: 0.00037977
Epoch [120/300], Train Loss: 0.000381
Validation Loss: 0.00037822
Epoch [121/300], Train Loss: 0.000370
Validation Loss: 0.00036031
Epoch [122/300], Train Loss: 0.000368
Validation Loss: 0.00035958
Epoch [123/300], Train Loss: 0.000360
Validation Loss: 0.00035247
Epoch [124/300], Train Loss: 0.000357
Validation Loss: 0.00036648
Epoch [125/300], Train Loss: 0.000357
Validation Loss: 0.00034742
Epoch [126/300], Train Loss: 0.000350
Validation Loss: 0.00036379
Early stopping triggered

Evaluating model for: Fridge
Run 58/72 completed in 4588.32 seconds with: {'MAE': np.float32(25.519495), 'MSE': np.float32(1221.427), 'RMSE': np.float32(34.94892), 'SAE': np.float32(0.073627286), 'NDE': np.float32(0.60390586)}

Run 59/72: hidden=512, seq_len=360, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000679
Validation Loss: 0.00066365
Epoch [2/300], Train Loss: 0.000658
Validation Loss: 0.00065210
Epoch [3/300], Train Loss: 0.000631
Validation Loss: 0.00061986
Epoch [4/300], Train Loss: 0.000614
Validation Loss: 0.00061485
Epoch [5/300], Train Loss: 0.000612
Validation Loss: 0.00061487
Epoch [6/300], Train Loss: 0.000608
Validation Loss: 0.00060956
Epoch [7/300], Train Loss: 0.000605
Validation Loss: 0.00060299
Epoch [8/300], Train Loss: 0.000602
Validation Loss: 0.00060540
Epoch [9/300], Train Loss: 0.000601
Validation Loss: 0.00059947
Epoch [10/300], Train Loss: 0.000598
Validation Loss: 0.00059854
Epoch [11/300], Train Loss: 0.000594
Validation Loss: 0.00059369
Epoch [12/300], Train Loss: 0.000592
Validation Loss: 0.00059234
Epoch [13/300], Train Loss: 0.000591
Validation Loss: 0.00058982
Epoch [14/300], Train Loss: 0.000588
Validation Loss: 0.00059143
Epoch [15/300], Train Loss: 0.000584
Validation Loss: 0.00057514
Epoch [16/300], Train Loss: 0.000578
Validation Loss: 0.00057409
Epoch [17/300], Train Loss: 0.000575
Validation Loss: 0.00056991
Epoch [18/300], Train Loss: 0.000572
Validation Loss: 0.00057096
Epoch [19/300], Train Loss: 0.000569
Validation Loss: 0.00056787
Epoch [20/300], Train Loss: 0.000565
Validation Loss: 0.00056070
Epoch [21/300], Train Loss: 0.000559
Validation Loss: 0.00055653
Epoch [22/300], Train Loss: 0.000557
Validation Loss: 0.00055113
Epoch [23/300], Train Loss: 0.000555
Validation Loss: 0.00056140
Epoch [24/300], Train Loss: 0.000551
Validation Loss: 0.00054437
Epoch [25/300], Train Loss: 0.000548
Validation Loss: 0.00054890
Epoch [26/300], Train Loss: 0.000542
Validation Loss: 0.00053441
Epoch [27/300], Train Loss: 0.000537
Validation Loss: 0.00052894
Epoch [28/300], Train Loss: 0.000530
Validation Loss: 0.00052086
Epoch [29/300], Train Loss: 0.000522
Validation Loss: 0.00051101
Epoch [30/300], Train Loss: 0.000521
Validation Loss: 0.00051376
Epoch [31/300], Train Loss: 0.000517
Validation Loss: 0.00051575
Epoch [32/300], Train Loss: 0.000559
Validation Loss: 0.00056485
Epoch [33/300], Train Loss: 0.000557
Validation Loss: 0.00054490
Epoch [34/300], Train Loss: 0.000546
Validation Loss: 0.00053939
Epoch [35/300], Train Loss: 0.000539
Validation Loss: 0.00053264
Epoch [36/300], Train Loss: 0.000533
Validation Loss: 0.00053485
Epoch [37/300], Train Loss: 0.000528
Validation Loss: 0.00052039
Epoch [38/300], Train Loss: 0.000522
Validation Loss: 0.00051607
Epoch [39/300], Train Loss: 0.000516
Validation Loss: 0.00051585
Early stopping triggered

Evaluating model for: Fridge
Run 59/72 completed in 1621.70 seconds with: {'MAE': np.float32(33.510517), 'MSE': np.float32(1725.3535), 'RMSE': np.float32(41.537376), 'SAE': np.float32(0.053080086), 'NDE': np.float32(0.71775216)}

Run 60/72: hidden=512, seq_len=360, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 13590 windows

Epoch [1/300], Train Loss: 0.000672
Validation Loss: 0.00066461
Epoch [2/300], Train Loss: 0.000661
Validation Loss: 0.00066265
Epoch [3/300], Train Loss: 0.000657
Validation Loss: 0.00064664
Epoch [4/300], Train Loss: 0.000624
Validation Loss: 0.00061358
Epoch [5/300], Train Loss: 0.000612
Validation Loss: 0.00061974
Epoch [6/300], Train Loss: 0.000608
Validation Loss: 0.00060635
Epoch [7/300], Train Loss: 0.000604
Validation Loss: 0.00060446
Epoch [8/300], Train Loss: 0.000602
Validation Loss: 0.00060825
Epoch [9/300], Train Loss: 0.000602
Validation Loss: 0.00060032
Epoch [10/300], Train Loss: 0.000600
Validation Loss: 0.00060033
Epoch [11/300], Train Loss: 0.000598
Validation Loss: 0.00059576
Epoch [12/300], Train Loss: 0.000596
Validation Loss: 0.00059431
Epoch [13/300], Train Loss: 0.000594
Validation Loss: 0.00059436
Epoch [14/300], Train Loss: 0.000593
Validation Loss: 0.00060187
Epoch [15/300], Train Loss: 0.000591
Validation Loss: 0.00058911
Epoch [16/300], Train Loss: 0.000589
Validation Loss: 0.00058770
Epoch [17/300], Train Loss: 0.000587
Validation Loss: 0.00058562
Epoch [18/300], Train Loss: 0.000584
Validation Loss: 0.00058221
Epoch [19/300], Train Loss: 0.000582
Validation Loss: 0.00058253
Epoch [20/300], Train Loss: 0.000579
Validation Loss: 0.00057301
Epoch [21/300], Train Loss: 0.000573
Validation Loss: 0.00057501
Epoch [22/300], Train Loss: 0.000567
Validation Loss: 0.00055615
Epoch [23/300], Train Loss: 0.000562
Validation Loss: 0.00055845
Epoch [24/300], Train Loss: 0.000551
Validation Loss: 0.00059072
Epoch [25/300], Train Loss: 0.000542
Validation Loss: 0.00055534
Epoch [26/300], Train Loss: 0.000529
Validation Loss: 0.00052943
Epoch [27/300], Train Loss: 0.000520
Validation Loss: 0.00056133
Epoch [28/300], Train Loss: 0.000510
Validation Loss: 0.00049381
Epoch [29/300], Train Loss: 0.000495
Validation Loss: 0.00047304
Epoch [30/300], Train Loss: 0.000483
Validation Loss: 0.00050039
Epoch [31/300], Train Loss: 0.000471
Validation Loss: 0.00046137
Epoch [32/300], Train Loss: 0.000473
Validation Loss: 0.00046306
Epoch [33/300], Train Loss: 0.000462
Validation Loss: 0.00045411
Epoch [34/300], Train Loss: 0.000465
Validation Loss: 0.00045237
Epoch [35/300], Train Loss: 0.000458
Validation Loss: 0.00045081
Epoch [36/300], Train Loss: 0.000449
Validation Loss: 0.00047471
Epoch [37/300], Train Loss: 0.000444
Validation Loss: 0.00044108
Epoch [38/300], Train Loss: 0.000443
Validation Loss: 0.00044610
Epoch [39/300], Train Loss: 0.000443
Validation Loss: 0.00043332
Epoch [40/300], Train Loss: 0.000441
Validation Loss: 0.00043153
Epoch [41/300], Train Loss: 0.000431
Validation Loss: 0.00043022
Epoch [42/300], Train Loss: 0.000425
Validation Loss: 0.00042257
Epoch [43/300], Train Loss: 0.000434
Validation Loss: 0.00041758
Epoch [44/300], Train Loss: 0.000424
Validation Loss: 0.00043316
Epoch [45/300], Train Loss: 0.000419
Validation Loss: 0.00043201
Epoch [46/300], Train Loss: 0.000414
Validation Loss: 0.00042664
Epoch [47/300], Train Loss: 0.000414
Validation Loss: 0.00040897
Epoch [48/300], Train Loss: 0.000411
Validation Loss: 0.00040315
Epoch [49/300], Train Loss: 0.000406
Validation Loss: 0.00040263
Epoch [50/300], Train Loss: 0.000409
Validation Loss: 0.00040367
Epoch [51/300], Train Loss: 0.000406
Validation Loss: 0.00040475
Epoch [52/300], Train Loss: 0.000405
Validation Loss: 0.00041423
Epoch [53/300], Train Loss: 0.000402
Validation Loss: 0.00039760
Epoch [54/300], Train Loss: 0.000401
Validation Loss: 0.00039030
Epoch [55/300], Train Loss: 0.000403
Validation Loss: 0.00039071
Epoch [56/300], Train Loss: 0.000398
Validation Loss: 0.00039428
Epoch [57/300], Train Loss: 0.000391
Validation Loss: 0.00039039
Epoch [58/300], Train Loss: 0.000393
Validation Loss: 0.00039356
Epoch [59/300], Train Loss: 0.000388
Validation Loss: 0.00037966
Epoch [60/300], Train Loss: 0.000386
Validation Loss: 0.00038101
Epoch [61/300], Train Loss: 0.000391
Validation Loss: 0.00040475
Epoch [62/300], Train Loss: 0.000381
Validation Loss: 0.00038230
Epoch [63/300], Train Loss: 0.000383
Validation Loss: 0.00037291
Epoch [64/300], Train Loss: 0.000379
Validation Loss: 0.00037692
Epoch [65/300], Train Loss: 0.000376
Validation Loss: 0.00038472
Epoch [66/300], Train Loss: 0.000373
Validation Loss: 0.00037159
Epoch [67/300], Train Loss: 0.000371
Validation Loss: 0.00037183
Epoch [68/300], Train Loss: 0.000372
Validation Loss: 0.00037464
Epoch [69/300], Train Loss: 0.000372
Validation Loss: 0.00036304
Epoch [70/300], Train Loss: 0.000369
Validation Loss: 0.00036751
Epoch [71/300], Train Loss: 0.000364
Validation Loss: 0.00035886
Epoch [72/300], Train Loss: 0.000365
Validation Loss: 0.00036052
Epoch [73/300], Train Loss: 0.000363
Validation Loss: 0.00036130
Epoch [74/300], Train Loss: 0.000362
Validation Loss: 0.00035676
Epoch [75/300], Train Loss: 0.000364
Validation Loss: 0.00034990
Epoch [76/300], Train Loss: 0.000354
Validation Loss: 0.00034353
Epoch [77/300], Train Loss: 0.000347
Validation Loss: 0.00033498
Epoch [78/300], Train Loss: 0.000343
Validation Loss: 0.00032983
Epoch [79/300], Train Loss: 0.000337
Validation Loss: 0.00032443
Epoch [80/300], Train Loss: 0.000331
Validation Loss: 0.00032111
Epoch [81/300], Train Loss: 0.000336
Validation Loss: 0.00032020
Epoch [82/300], Train Loss: 0.000321
Validation Loss: 0.00032407
Epoch [83/300], Train Loss: 0.000449
Validation Loss: 0.00040521
Epoch [84/300], Train Loss: 0.000391
Validation Loss: 0.00036194
Epoch [85/300], Train Loss: 0.000354
Validation Loss: 0.00035158
Epoch [86/300], Train Loss: 0.000342
Validation Loss: 0.00032636
Epoch [87/300], Train Loss: 0.000335
Validation Loss: 0.00032035
Epoch [88/300], Train Loss: 0.000333
Validation Loss: 0.00031436
Epoch [89/300], Train Loss: 0.000319
Validation Loss: 0.00030745
Epoch [90/300], Train Loss: 0.000319
Validation Loss: 0.00030259
Epoch [91/300], Train Loss: 0.000312
Validation Loss: 0.00029947
Epoch [92/300], Train Loss: 0.000309
Validation Loss: 0.00031765
Epoch [93/300], Train Loss: 0.000309
Validation Loss: 0.00031449
Epoch [94/300], Train Loss: 0.000303
Validation Loss: 0.00029268
Epoch [95/300], Train Loss: 0.000304
Validation Loss: 0.00031160
Epoch [96/300], Train Loss: 0.000301
Validation Loss: 0.00028602
Epoch [97/300], Train Loss: 0.000300
Validation Loss: 0.00028390
Epoch [98/300], Train Loss: 0.000293
Validation Loss: 0.00028139
Epoch [99/300], Train Loss: 0.000300
Validation Loss: 0.00028226
Epoch [100/300], Train Loss: 0.000297
Validation Loss: 0.00028762
Epoch [101/300], Train Loss: 0.000293
Validation Loss: 0.00027815
Epoch [102/300], Train Loss: 0.000292
Validation Loss: 0.00027882
Epoch [103/300], Train Loss: 0.000288
Validation Loss: 0.00027863
Epoch [104/300], Train Loss: 0.000290
Validation Loss: 0.00027879
Epoch [105/300], Train Loss: 0.000282
Validation Loss: 0.00027334
Epoch [106/300], Train Loss: 0.000279
Validation Loss: 0.00028269
Epoch [107/300], Train Loss: 0.000280
Validation Loss: 0.00026575
Epoch [108/300], Train Loss: 0.000281
Validation Loss: 0.00029604
Epoch [109/300], Train Loss: 0.000279
Validation Loss: 0.00026373
Epoch [110/300], Train Loss: 0.000275
Validation Loss: 0.00026135
Epoch [111/300], Train Loss: 0.000274
Validation Loss: 0.00027567
Epoch [112/300], Train Loss: 0.000272
Validation Loss: 0.00025736
Epoch [113/300], Train Loss: 0.000274
Validation Loss: 0.00028619
Epoch [114/300], Train Loss: 0.000273
Validation Loss: 0.00026345
Epoch [115/300], Train Loss: 0.000276
Validation Loss: 0.00026699
Epoch [116/300], Train Loss: 0.000267
Validation Loss: 0.00026525
Epoch [117/300], Train Loss: 0.000272
Validation Loss: 0.00029764
Epoch [118/300], Train Loss: 0.000270
Validation Loss: 0.00026567
Epoch [119/300], Train Loss: 0.000266
Validation Loss: 0.00025079
Epoch [120/300], Train Loss: 0.000271
Validation Loss: 0.00025993
Epoch [121/300], Train Loss: 0.000261
Validation Loss: 0.00025286
Epoch [122/300], Train Loss: 0.000261
Validation Loss: 0.00026456
Epoch [123/300], Train Loss: 0.000264
Validation Loss: 0.00025184
Epoch [124/300], Train Loss: 0.000260
Validation Loss: 0.00026215
Epoch [125/300], Train Loss: 0.000258
Validation Loss: 0.00024426
Epoch [126/300], Train Loss: 0.000256
Validation Loss: 0.00025176
Epoch [127/300], Train Loss: 0.000254
Validation Loss: 0.00024549
Epoch [128/300], Train Loss: 0.000254
Validation Loss: 0.00024319
Epoch [129/300], Train Loss: 0.000257
Validation Loss: 0.00025776
Epoch [130/300], Train Loss: 0.000254
Validation Loss: 0.00024349
Epoch [131/300], Train Loss: 0.000256
Validation Loss: 0.00024093
Epoch [132/300], Train Loss: 0.000252
Validation Loss: 0.00024001
Epoch [133/300], Train Loss: 0.000258
Validation Loss: 0.00025693
Epoch [134/300], Train Loss: 0.000254
Validation Loss: 0.00024353
Epoch [135/300], Train Loss: 0.000259
Validation Loss: 0.00024014
Epoch [136/300], Train Loss: 0.000262
Validation Loss: 0.00024100
Epoch [137/300], Train Loss: 0.000245
Validation Loss: 0.00027022
Epoch [138/300], Train Loss: 0.000274
Validation Loss: 0.00023732
Epoch [139/300], Train Loss: 0.000247
Validation Loss: 0.00023664
Epoch [140/300], Train Loss: 0.000245
Validation Loss: 0.00023570
Epoch [141/300], Train Loss: 0.000241
Validation Loss: 0.00023376
Epoch [142/300], Train Loss: 0.000245
Validation Loss: 0.00023427
Epoch [143/300], Train Loss: 0.000249
Validation Loss: 0.00023071
Epoch [144/300], Train Loss: 0.000240
Validation Loss: 0.00023108
Epoch [145/300], Train Loss: 0.000243
Validation Loss: 0.00023561
Epoch [146/300], Train Loss: 0.000243
Validation Loss: 0.00025884
Epoch [147/300], Train Loss: 0.000245
Validation Loss: 0.00022941
Epoch [148/300], Train Loss: 0.000242
Validation Loss: 0.00023214
Epoch [149/300], Train Loss: 0.000237
Validation Loss: 0.00023750
Epoch [150/300], Train Loss: 0.000238
Validation Loss: 0.00023484
Epoch [151/300], Train Loss: 0.000239
Validation Loss: 0.00023994
Epoch [152/300], Train Loss: 0.000241
Validation Loss: 0.00022594
Epoch [153/300], Train Loss: 0.000238
Validation Loss: 0.00022656
Epoch [154/300], Train Loss: 0.000236
Validation Loss: 0.00024632
Epoch [155/300], Train Loss: 0.000237
Validation Loss: 0.00023136
Epoch [156/300], Train Loss: 0.000234
Validation Loss: 0.00023018
Epoch [157/300], Train Loss: 0.000279
Validation Loss: 0.00022765
Epoch [158/300], Train Loss: 0.000270
Validation Loss: 0.00024793
Epoch [159/300], Train Loss: 0.000246
Validation Loss: 0.00022675
Epoch [160/300], Train Loss: 0.000246
Validation Loss: 0.00023527
Epoch [161/300], Train Loss: 0.000235
Validation Loss: 0.00022439
Epoch [162/300], Train Loss: 0.000233
Validation Loss: 0.00023113
Epoch [163/300], Train Loss: 0.000230
Validation Loss: 0.00022230
Epoch [164/300], Train Loss: 0.000235
Validation Loss: 0.00023088
Epoch [165/300], Train Loss: 0.000236
Validation Loss: 0.00022438
Epoch [166/300], Train Loss: 0.000231
Validation Loss: 0.00022747
Epoch [167/300], Train Loss: 0.000230
Validation Loss: 0.00022049
Epoch [168/300], Train Loss: 0.000227
Validation Loss: 0.00022873
Epoch [169/300], Train Loss: 0.000227
Validation Loss: 0.00022302
Epoch [170/300], Train Loss: 0.000230
Validation Loss: 0.00022311
Epoch [171/300], Train Loss: 0.000230
Validation Loss: 0.00022679
Epoch [172/300], Train Loss: 0.000224
Validation Loss: 0.00021694
Epoch [173/300], Train Loss: 0.000223
Validation Loss: 0.00022142
Epoch [174/300], Train Loss: 0.000246
Validation Loss: 0.00045017
Epoch [175/300], Train Loss: 0.000267
Validation Loss: 0.00022512
Epoch [176/300], Train Loss: 0.000226
Validation Loss: 0.00021813
Epoch [177/300], Train Loss: 0.000225
Validation Loss: 0.00024612
Epoch [178/300], Train Loss: 0.000259
Validation Loss: 0.00023835
Epoch [179/300], Train Loss: 0.000226
Validation Loss: 0.00021812
Epoch [180/300], Train Loss: 0.000222
Validation Loss: 0.00021941
Epoch [181/300], Train Loss: 0.000223
Validation Loss: 0.00021826
Epoch [182/300], Train Loss: 0.000227
Validation Loss: 0.00022992
Early stopping triggered

Evaluating model for: Fridge
Run 60/72 completed in 9253.67 seconds with: {'MAE': np.float32(18.952045), 'MSE': np.float32(805.2879), 'RMSE': np.float32(28.377596), 'SAE': np.float32(0.025955314), 'NDE': np.float32(0.49035534)}

Run 61/72: hidden=512, seq_len=360, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000665
Validation Loss: 0.00065072
Epoch [2/300], Train Loss: 0.000649
Validation Loss: 0.00063461
Epoch [3/300], Train Loss: 0.000636
Validation Loss: 0.00062438
Epoch [4/300], Train Loss: 0.000626
Validation Loss: 0.00061277
Epoch [5/300], Train Loss: 0.000615
Validation Loss: 0.00060421
Epoch [6/300], Train Loss: 0.000608
Validation Loss: 0.00060515
Epoch [7/300], Train Loss: 0.000604
Validation Loss: 0.00059747
Epoch [8/300], Train Loss: 0.000603
Validation Loss: 0.00059621
Epoch [9/300], Train Loss: 0.000601
Validation Loss: 0.00059865
Epoch [10/300], Train Loss: 0.000596
Validation Loss: 0.00059202
Epoch [11/300], Train Loss: 0.000597
Validation Loss: 0.00058952
Epoch [12/300], Train Loss: 0.000595
Validation Loss: 0.00059449
Epoch [13/300], Train Loss: 0.000594
Validation Loss: 0.00058751
Epoch [14/300], Train Loss: 0.000593
Validation Loss: 0.00058727
Epoch [15/300], Train Loss: 0.000591
Validation Loss: 0.00058734
Epoch [16/300], Train Loss: 0.000590
Validation Loss: 0.00058737
Epoch [17/300], Train Loss: 0.000588
Validation Loss: 0.00058691
Epoch [18/300], Train Loss: 0.000587
Validation Loss: 0.00058148
Epoch [19/300], Train Loss: 0.000586
Validation Loss: 0.00058371
Epoch [20/300], Train Loss: 0.000586
Validation Loss: 0.00058795
Epoch [21/300], Train Loss: 0.000585
Validation Loss: 0.00057911
Epoch [22/300], Train Loss: 0.000583
Validation Loss: 0.00057930
Epoch [23/300], Train Loss: 0.000581
Validation Loss: 0.00057865
Epoch [24/300], Train Loss: 0.000579
Validation Loss: 0.00057575
Epoch [25/300], Train Loss: 0.000579
Validation Loss: 0.00057426
Epoch [26/300], Train Loss: 0.000579
Validation Loss: 0.00057059
Epoch [27/300], Train Loss: 0.000578
Validation Loss: 0.00057424
Epoch [28/300], Train Loss: 0.000577
Validation Loss: 0.00056785
Epoch [29/300], Train Loss: 0.000576
Validation Loss: 0.00056738
Epoch [30/300], Train Loss: 0.000573
Validation Loss: 0.00056475
Epoch [31/300], Train Loss: 0.000573
Validation Loss: 0.00057005
Epoch [32/300], Train Loss: 0.000570
Validation Loss: 0.00056181
Epoch [33/300], Train Loss: 0.000572
Validation Loss: 0.00056425
Epoch [34/300], Train Loss: 0.000569
Validation Loss: 0.00056010
Epoch [35/300], Train Loss: 0.000569
Validation Loss: 0.00056532
Epoch [36/300], Train Loss: 0.000568
Validation Loss: 0.00055996
Epoch [37/300], Train Loss: 0.000566
Validation Loss: 0.00056098
Epoch [38/300], Train Loss: 0.000565
Validation Loss: 0.00055400
Epoch [39/300], Train Loss: 0.000565
Validation Loss: 0.00055233
Epoch [40/300], Train Loss: 0.000563
Validation Loss: 0.00055366
Epoch [41/300], Train Loss: 0.000562
Validation Loss: 0.00055464
Epoch [42/300], Train Loss: 0.000561
Validation Loss: 0.00055349
Epoch [43/300], Train Loss: 0.000559
Validation Loss: 0.00055368
Epoch [44/300], Train Loss: 0.000563
Validation Loss: 0.00055420
Epoch [45/300], Train Loss: 0.000556
Validation Loss: 0.00054233
Epoch [46/300], Train Loss: 0.000554
Validation Loss: 0.00053981
Epoch [47/300], Train Loss: 0.000554
Validation Loss: 0.00053773
Epoch [48/300], Train Loss: 0.000551
Validation Loss: 0.00054119
Epoch [49/300], Train Loss: 0.000549
Validation Loss: 0.00053622
Epoch [50/300], Train Loss: 0.000549
Validation Loss: 0.00053427
Epoch [51/300], Train Loss: 0.000546
Validation Loss: 0.00053559
Epoch [52/300], Train Loss: 0.000543
Validation Loss: 0.00053250
Epoch [53/300], Train Loss: 0.000544
Validation Loss: 0.00053431
Epoch [54/300], Train Loss: 0.000541
Validation Loss: 0.00052511
Epoch [55/300], Train Loss: 0.000544
Validation Loss: 0.00053005
Epoch [56/300], Train Loss: 0.000543
Validation Loss: 0.00052478
Epoch [57/300], Train Loss: 0.000543
Validation Loss: 0.00053749
Epoch [58/300], Train Loss: 0.000538
Validation Loss: 0.00052369
Epoch [59/300], Train Loss: 0.000536
Validation Loss: 0.00051889
Epoch [60/300], Train Loss: 0.000536
Validation Loss: 0.00051903
Epoch [61/300], Train Loss: 0.000533
Validation Loss: 0.00051615
Epoch [62/300], Train Loss: 0.000530
Validation Loss: 0.00051683
Epoch [63/300], Train Loss: 0.000530
Validation Loss: 0.00051728
Epoch [64/300], Train Loss: 0.000532
Validation Loss: 0.00051865
Epoch [65/300], Train Loss: 0.000528
Validation Loss: 0.00051391
Epoch [66/300], Train Loss: 0.000525
Validation Loss: 0.00051331
Epoch [67/300], Train Loss: 0.000524
Validation Loss: 0.00051027
Epoch [68/300], Train Loss: 0.000519
Validation Loss: 0.00050324
Epoch [69/300], Train Loss: 0.000516
Validation Loss: 0.00051388
Epoch [70/300], Train Loss: 0.000516
Validation Loss: 0.00049949
Epoch [71/300], Train Loss: 0.000512
Validation Loss: 0.00050207
Epoch [72/300], Train Loss: 0.000509
Validation Loss: 0.00049630
Epoch [73/300], Train Loss: 0.000510
Validation Loss: 0.00049304
Epoch [74/300], Train Loss: 0.000501
Validation Loss: 0.00049041
Epoch [75/300], Train Loss: 0.000504
Validation Loss: 0.00048641
Epoch [76/300], Train Loss: 0.000499
Validation Loss: 0.00048275
Epoch [77/300], Train Loss: 0.000495
Validation Loss: 0.00049362
Epoch [78/300], Train Loss: 0.000503
Validation Loss: 0.00047576
Epoch [79/300], Train Loss: 0.000492
Validation Loss: 0.00047422
Epoch [80/300], Train Loss: 0.000491
Validation Loss: 0.00047941
Epoch [81/300], Train Loss: 0.000494
Validation Loss: 0.00047935
Epoch [82/300], Train Loss: 0.000487
Validation Loss: 0.00047480
Epoch [83/300], Train Loss: 0.000492
Validation Loss: 0.00047393
Epoch [84/300], Train Loss: 0.000489
Validation Loss: 0.00048268
Epoch [85/300], Train Loss: 0.000487
Validation Loss: 0.00047257
Epoch [86/300], Train Loss: 0.000482
Validation Loss: 0.00046618
Epoch [87/300], Train Loss: 0.000478
Validation Loss: 0.00046272
Epoch [88/300], Train Loss: 0.000479
Validation Loss: 0.00046634
Epoch [89/300], Train Loss: 0.000477
Validation Loss: 0.00046304
Epoch [90/300], Train Loss: 0.000476
Validation Loss: 0.00046274
Epoch [91/300], Train Loss: 0.000477
Validation Loss: 0.00045784
Epoch [92/300], Train Loss: 0.000472
Validation Loss: 0.00046116
Epoch [93/300], Train Loss: 0.000484
Validation Loss: 0.00046340
Epoch [94/300], Train Loss: 0.000472
Validation Loss: 0.00045921
Epoch [95/300], Train Loss: 0.000470
Validation Loss: 0.00045832
Epoch [96/300], Train Loss: 0.000471
Validation Loss: 0.00045328
Epoch [97/300], Train Loss: 0.000475
Validation Loss: 0.00045902
Epoch [98/300], Train Loss: 0.000515
Validation Loss: 0.00056967
Epoch [99/300], Train Loss: 0.000588
Validation Loss: 0.00053545
Epoch [100/300], Train Loss: 0.000559
Validation Loss: 0.00052603
Epoch [101/300], Train Loss: 0.000552
Validation Loss: 0.00052480
Epoch [102/300], Train Loss: 0.000547
Validation Loss: 0.00051885
Epoch [103/300], Train Loss: 0.000543
Validation Loss: 0.00051932
Epoch [104/300], Train Loss: 0.000538
Validation Loss: 0.00051272
Epoch [105/300], Train Loss: 0.000536
Validation Loss: 0.00050951
Epoch [106/300], Train Loss: 0.000532
Validation Loss: 0.00051097
Early stopping triggered

Evaluating model for: Fridge
Run 61/72 completed in 1579.01 seconds with: {'MAE': np.float32(34.85657), 'MSE': np.float32(1731.0696), 'RMSE': np.float32(41.606125), 'SAE': np.float32(0.06920773), 'NDE': np.float32(0.72522885)}

Run 62/72: hidden=512, seq_len=360, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000672
Validation Loss: 0.00065975
Epoch [2/300], Train Loss: 0.000660
Validation Loss: 0.00065658
Epoch [3/300], Train Loss: 0.000655
Validation Loss: 0.00065366
Epoch [4/300], Train Loss: 0.000651
Validation Loss: 0.00063955
Epoch [5/300], Train Loss: 0.000641
Validation Loss: 0.00062430
Epoch [6/300], Train Loss: 0.000624
Validation Loss: 0.00061191
Epoch [7/300], Train Loss: 0.000616
Validation Loss: 0.00060617
Epoch [8/300], Train Loss: 0.000614
Validation Loss: 0.00060281
Epoch [9/300], Train Loss: 0.000609
Validation Loss: 0.00060287
Epoch [10/300], Train Loss: 0.000605
Validation Loss: 0.00059526
Epoch [11/300], Train Loss: 0.000605
Validation Loss: 0.00059606
Epoch [12/300], Train Loss: 0.000603
Validation Loss: 0.00059676
Epoch [13/300], Train Loss: 0.000601
Validation Loss: 0.00059463
Epoch [14/300], Train Loss: 0.000600
Validation Loss: 0.00058980
Epoch [15/300], Train Loss: 0.000598
Validation Loss: 0.00059500
Epoch [16/300], Train Loss: 0.000594
Validation Loss: 0.00058530
Epoch [17/300], Train Loss: 0.000595
Validation Loss: 0.00058528
Epoch [18/300], Train Loss: 0.000594
Validation Loss: 0.00058670
Epoch [19/300], Train Loss: 0.000596
Validation Loss: 0.00058531
Epoch [20/300], Train Loss: 0.000593
Validation Loss: 0.00058797
Epoch [21/300], Train Loss: 0.000590
Validation Loss: 0.00058169
Epoch [22/300], Train Loss: 0.000589
Validation Loss: 0.00058341
Epoch [23/300], Train Loss: 0.000588
Validation Loss: 0.00057951
Epoch [24/300], Train Loss: 0.000588
Validation Loss: 0.00057830
Epoch [25/300], Train Loss: 0.000587
Validation Loss: 0.00058338
Epoch [26/300], Train Loss: 0.000589
Validation Loss: 0.00057709
Epoch [27/300], Train Loss: 0.000585
Validation Loss: 0.00057914
Epoch [28/300], Train Loss: 0.000584
Validation Loss: 0.00057194
Epoch [29/300], Train Loss: 0.000583
Validation Loss: 0.00057209
Epoch [30/300], Train Loss: 0.000581
Validation Loss: 0.00057138
Epoch [31/300], Train Loss: 0.000582
Validation Loss: 0.00056959
Epoch [32/300], Train Loss: 0.000578
Validation Loss: 0.00056937
Epoch [33/300], Train Loss: 0.000579
Validation Loss: 0.00057284
Epoch [34/300], Train Loss: 0.000579
Validation Loss: 0.00056900
Epoch [35/300], Train Loss: 0.000583
Validation Loss: 0.00058058
Epoch [36/300], Train Loss: 0.000577
Validation Loss: 0.00056798
Epoch [37/300], Train Loss: 0.000574
Validation Loss: 0.00057048
Epoch [38/300], Train Loss: 0.000573
Validation Loss: 0.00056329
Epoch [39/300], Train Loss: 0.000571
Validation Loss: 0.00055834
Epoch [40/300], Train Loss: 0.000570
Validation Loss: 0.00056358
Epoch [41/300], Train Loss: 0.000570
Validation Loss: 0.00056079
Epoch [42/300], Train Loss: 0.000568
Validation Loss: 0.00055835
Epoch [43/300], Train Loss: 0.000565
Validation Loss: 0.00055465
Epoch [44/300], Train Loss: 0.000563
Validation Loss: 0.00055284
Epoch [45/300], Train Loss: 0.000562
Validation Loss: 0.00055738
Epoch [46/300], Train Loss: 0.000561
Validation Loss: 0.00054674
Epoch [47/300], Train Loss: 0.000557
Validation Loss: 0.00054475
Epoch [48/300], Train Loss: 0.000556
Validation Loss: 0.00054768
Epoch [49/300], Train Loss: 0.000556
Validation Loss: 0.00054984
Epoch [50/300], Train Loss: 0.000554
Validation Loss: 0.00054034
Epoch [51/300], Train Loss: 0.000550
Validation Loss: 0.00054132
Epoch [52/300], Train Loss: 0.000546
Validation Loss: 0.00054336
Epoch [53/300], Train Loss: 0.000545
Validation Loss: 0.00053544
Epoch [54/300], Train Loss: 0.000541
Validation Loss: 0.00052791
Epoch [55/300], Train Loss: 0.000540
Validation Loss: 0.00053577
Epoch [56/300], Train Loss: 0.000540
Validation Loss: 0.00052715
Epoch [57/300], Train Loss: 0.000536
Validation Loss: 0.00053219
Epoch [58/300], Train Loss: 0.000536
Validation Loss: 0.00053076
Epoch [59/300], Train Loss: 0.000532
Validation Loss: 0.00052808
Epoch [60/300], Train Loss: 0.000530
Validation Loss: 0.00051289
Epoch [61/300], Train Loss: 0.000529
Validation Loss: 0.00051454
Epoch [62/300], Train Loss: 0.000525
Validation Loss: 0.00051129
Epoch [63/300], Train Loss: 0.000522
Validation Loss: 0.00050311
Epoch [64/300], Train Loss: 0.000516
Validation Loss: 0.00050234
Epoch [65/300], Train Loss: 0.000521
Validation Loss: 0.00051180
Epoch [66/300], Train Loss: 0.000541
Validation Loss: 0.00051513
Epoch [67/300], Train Loss: 0.000513
Validation Loss: 0.00049648
Epoch [68/300], Train Loss: 0.000509
Validation Loss: 0.00049892
Epoch [69/300], Train Loss: 0.000505
Validation Loss: 0.00049600
Epoch [70/300], Train Loss: 0.000505
Validation Loss: 0.00049207
Epoch [71/300], Train Loss: 0.000502
Validation Loss: 0.00048956
Epoch [72/300], Train Loss: 0.000515
Validation Loss: 0.00048369
Epoch [73/300], Train Loss: 0.000503
Validation Loss: 0.00052383
Epoch [74/300], Train Loss: 0.000520
Validation Loss: 0.00050794
Epoch [75/300], Train Loss: 0.000505
Validation Loss: 0.00049532
Epoch [76/300], Train Loss: 0.000496
Validation Loss: 0.00047787
Epoch [77/300], Train Loss: 0.000496
Validation Loss: 0.00047281
Epoch [78/300], Train Loss: 0.000505
Validation Loss: 0.00048571
Epoch [79/300], Train Loss: 0.000491
Validation Loss: 0.00046823
Epoch [80/300], Train Loss: 0.000497
Validation Loss: 0.00052406
Epoch [81/300], Train Loss: 0.000497
Validation Loss: 0.00047091
Epoch [82/300], Train Loss: 0.000625
Validation Loss: 0.00054291
Epoch [83/300], Train Loss: 0.000552
Validation Loss: 0.00052562
Epoch [84/300], Train Loss: 0.000539
Validation Loss: 0.00051599
Epoch [85/300], Train Loss: 0.000534
Validation Loss: 0.00051273
Epoch [86/300], Train Loss: 0.000526
Validation Loss: 0.00051132
Epoch [87/300], Train Loss: 0.000522
Validation Loss: 0.00050775
Epoch [88/300], Train Loss: 0.000518
Validation Loss: 0.00050993
Epoch [89/300], Train Loss: 0.000515
Validation Loss: 0.00050624
Early stopping triggered

Evaluating model for: Fridge
Run 62/72 completed in 1589.38 seconds with: {'MAE': np.float32(34.1997), 'MSE': np.float32(1714.6448), 'RMSE': np.float32(41.408268), 'SAE': np.float32(0.04552142), 'NDE': np.float32(0.72178006)}

Run 63/72: hidden=512, seq_len=360, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000830
Validation Loss: 0.00066668
Epoch [2/300], Train Loss: 0.000670
Validation Loss: 0.00066578
Epoch [3/300], Train Loss: 0.000668
Validation Loss: 0.00066478
Epoch [4/300], Train Loss: 0.000666
Validation Loss: 0.00066397
Epoch [5/300], Train Loss: 0.000664
Validation Loss: 0.00065751
Epoch [6/300], Train Loss: 0.000653
Validation Loss: 0.00062943
Epoch [7/300], Train Loss: 0.000632
Validation Loss: 0.00061130
Epoch [8/300], Train Loss: 0.000623
Validation Loss: 0.00060739
Epoch [9/300], Train Loss: 0.000617
Validation Loss: 0.00060566
Epoch [10/300], Train Loss: 0.000613
Validation Loss: 0.00060550
Epoch [11/300], Train Loss: 0.000616
Validation Loss: 0.00060266
Epoch [12/300], Train Loss: 0.000612
Validation Loss: 0.00060797
Epoch [13/300], Train Loss: 0.000611
Validation Loss: 0.00060223
Epoch [14/300], Train Loss: 0.000610
Validation Loss: 0.00059876
Epoch [15/300], Train Loss: 0.000607
Validation Loss: 0.00060268
Epoch [16/300], Train Loss: 0.000607
Validation Loss: 0.00059536
Epoch [17/300], Train Loss: 0.000606
Validation Loss: 0.00059775
Epoch [18/300], Train Loss: 0.000604
Validation Loss: 0.00059518
Epoch [19/300], Train Loss: 0.000605
Validation Loss: 0.00059508
Epoch [20/300], Train Loss: 0.000605
Validation Loss: 0.00060031
Epoch [21/300], Train Loss: 0.000603
Validation Loss: 0.00059256
Epoch [22/300], Train Loss: 0.000602
Validation Loss: 0.00059436
Epoch [23/300], Train Loss: 0.000599
Validation Loss: 0.00059406
Epoch [24/300], Train Loss: 0.000598
Validation Loss: 0.00059365
Epoch [25/300], Train Loss: 0.000598
Validation Loss: 0.00059093
Epoch [26/300], Train Loss: 0.000600
Validation Loss: 0.00058996
Epoch [27/300], Train Loss: 0.000598
Validation Loss: 0.00059663
Epoch [28/300], Train Loss: 0.000598
Validation Loss: 0.00058680
Epoch [29/300], Train Loss: 0.000596
Validation Loss: 0.00058957
Epoch [30/300], Train Loss: 0.000594
Validation Loss: 0.00058651
Epoch [31/300], Train Loss: 0.000594
Validation Loss: 0.00058653
Epoch [32/300], Train Loss: 0.000591
Validation Loss: 0.00058607
Epoch [33/300], Train Loss: 0.000592
Validation Loss: 0.00058312
Epoch [34/300], Train Loss: 0.000590
Validation Loss: 0.00058383
Epoch [35/300], Train Loss: 0.000591
Validation Loss: 0.00058297
Epoch [36/300], Train Loss: 0.000590
Validation Loss: 0.00057951
Epoch [37/300], Train Loss: 0.000586
Validation Loss: 0.00058003
Epoch [38/300], Train Loss: 0.000584
Validation Loss: 0.00058014
Epoch [39/300], Train Loss: 0.000584
Validation Loss: 0.00057257
Epoch [40/300], Train Loss: 0.000581
Validation Loss: 0.00057431
Epoch [41/300], Train Loss: 0.000580
Validation Loss: 0.00056590
Epoch [42/300], Train Loss: 0.000581
Validation Loss: 0.00057296
Epoch [43/300], Train Loss: 0.000578
Validation Loss: 0.00057416
Epoch [44/300], Train Loss: 0.000578
Validation Loss: 0.00056826
Epoch [45/300], Train Loss: 0.000575
Validation Loss: 0.00056617
Epoch [46/300], Train Loss: 0.000573
Validation Loss: 0.00056150
Epoch [47/300], Train Loss: 0.000572
Validation Loss: 0.00056703
Epoch [48/300], Train Loss: 0.000572
Validation Loss: 0.00055956
Epoch [49/300], Train Loss: 0.000569
Validation Loss: 0.00055976
Epoch [50/300], Train Loss: 0.000570
Validation Loss: 0.00055447
Epoch [51/300], Train Loss: 0.000566
Validation Loss: 0.00055615
Epoch [52/300], Train Loss: 0.000564
Validation Loss: 0.00055856
Epoch [53/300], Train Loss: 0.000563
Validation Loss: 0.00055452
Epoch [54/300], Train Loss: 0.000561
Validation Loss: 0.00054940
Epoch [55/300], Train Loss: 0.000563
Validation Loss: 0.00055470
Epoch [56/300], Train Loss: 0.000563
Validation Loss: 0.00054741
Epoch [57/300], Train Loss: 0.000557
Validation Loss: 0.00054466
Epoch [58/300], Train Loss: 0.000556
Validation Loss: 0.00054574
Epoch [59/300], Train Loss: 0.000555
Validation Loss: 0.00054667
Epoch [60/300], Train Loss: 0.000556
Validation Loss: 0.00054016
Epoch [61/300], Train Loss: 0.000555
Validation Loss: 0.00053868
Epoch [62/300], Train Loss: 0.000550
Validation Loss: 0.00054085
Epoch [63/300], Train Loss: 0.000550
Validation Loss: 0.00054567
Epoch [64/300], Train Loss: 0.000547
Validation Loss: 0.00053720
Epoch [65/300], Train Loss: 0.000545
Validation Loss: 0.00053390
Epoch [66/300], Train Loss: 0.000542
Validation Loss: 0.00053939
Epoch [67/300], Train Loss: 0.000541
Validation Loss: 0.00053319
Epoch [68/300], Train Loss: 0.000537
Validation Loss: 0.00052657
Epoch [69/300], Train Loss: 0.000535
Validation Loss: 0.00052847
Epoch [70/300], Train Loss: 0.000535
Validation Loss: 0.00052346
Epoch [71/300], Train Loss: 0.000531
Validation Loss: 0.00052894
Epoch [72/300], Train Loss: 0.000528
Validation Loss: 0.00051488
Epoch [73/300], Train Loss: 0.000525
Validation Loss: 0.00051419
Epoch [74/300], Train Loss: 0.000520
Validation Loss: 0.00051613
Epoch [75/300], Train Loss: 0.000521
Validation Loss: 0.00051151
Epoch [76/300], Train Loss: 0.000519
Validation Loss: 0.00050856
Epoch [77/300], Train Loss: 0.000515
Validation Loss: 0.00051117
Epoch [78/300], Train Loss: 0.000514
Validation Loss: 0.00050707
Epoch [79/300], Train Loss: 0.000510
Validation Loss: 0.00049539
Epoch [80/300], Train Loss: 0.000503
Validation Loss: 0.00049230
Epoch [81/300], Train Loss: 0.000505
Validation Loss: 0.00049225
Epoch [82/300], Train Loss: 0.000503
Validation Loss: 0.00048432
Epoch [83/300], Train Loss: 0.000498
Validation Loss: 0.00048867
Epoch [84/300], Train Loss: 0.000493
Validation Loss: 0.00047671
Epoch [85/300], Train Loss: 0.000493
Validation Loss: 0.00047648
Epoch [86/300], Train Loss: 0.000489
Validation Loss: 0.00048029
Epoch [87/300], Train Loss: 0.000492
Validation Loss: 0.00047911
Epoch [88/300], Train Loss: 0.000492
Validation Loss: 0.00049075
Epoch [89/300], Train Loss: 0.000498
Validation Loss: 0.00050014
Epoch [90/300], Train Loss: 0.000487
Validation Loss: 0.00047105
Epoch [91/300], Train Loss: 0.000518
Validation Loss: 0.00056052
Epoch [92/300], Train Loss: 0.000538
Validation Loss: 0.00051619
Epoch [93/300], Train Loss: 0.000518
Validation Loss: 0.00050831
Epoch [94/300], Train Loss: 0.000513
Validation Loss: 0.00050050
Epoch [95/300], Train Loss: 0.000510
Validation Loss: 0.00050610
Epoch [96/300], Train Loss: 0.000506
Validation Loss: 0.00049836
Epoch [97/300], Train Loss: 0.000505
Validation Loss: 0.00050911
Epoch [98/300], Train Loss: 0.000500
Validation Loss: 0.00049556
Epoch [99/300], Train Loss: 0.000498
Validation Loss: 0.00048930
Epoch [100/300], Train Loss: 0.000494
Validation Loss: 0.00048270
Early stopping triggered

Evaluating model for: Fridge
Run 63/72 completed in 2093.36 seconds with: {'MAE': np.float32(32.18155), 'MSE': np.float32(1657.1095), 'RMSE': np.float32(40.70761), 'SAE': np.float32(0.01826373), 'NDE': np.float32(0.7095669)}

Run 64/72: hidden=512, seq_len=360, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 6818 windows

Epoch [1/300], Train Loss: 0.000687
Validation Loss: 0.00066048
Epoch [2/300], Train Loss: 0.000661
Validation Loss: 0.00066034
Epoch [3/300], Train Loss: 0.000660
Validation Loss: 0.00066019
Epoch [4/300], Train Loss: 0.000660
Validation Loss: 0.00066158
Epoch [5/300], Train Loss: 0.000660
Validation Loss: 0.00065901
Epoch [6/300], Train Loss: 0.000657
Validation Loss: 0.00064979
Epoch [7/300], Train Loss: 0.000643
Validation Loss: 0.00062779
Epoch [8/300], Train Loss: 0.000622
Validation Loss: 0.00061220
Epoch [9/300], Train Loss: 0.000612
Validation Loss: 0.00060454
Epoch [10/300], Train Loss: 0.000607
Validation Loss: 0.00059651
Epoch [11/300], Train Loss: 0.000607
Validation Loss: 0.00059617
Epoch [12/300], Train Loss: 0.000606
Validation Loss: 0.00059917
Epoch [13/300], Train Loss: 0.000604
Validation Loss: 0.00060181
Epoch [14/300], Train Loss: 0.000602
Validation Loss: 0.00059449
Epoch [15/300], Train Loss: 0.000599
Validation Loss: 0.00059638
Epoch [16/300], Train Loss: 0.000595
Validation Loss: 0.00058797
Epoch [17/300], Train Loss: 0.000596
Validation Loss: 0.00058776
Epoch [18/300], Train Loss: 0.000595
Validation Loss: 0.00058749
Epoch [19/300], Train Loss: 0.000598
Validation Loss: 0.00058638
Epoch [20/300], Train Loss: 0.000595
Validation Loss: 0.00058990
Epoch [21/300], Train Loss: 0.000592
Validation Loss: 0.00058555
Epoch [22/300], Train Loss: 0.000591
Validation Loss: 0.00058669
Epoch [23/300], Train Loss: 0.000591
Validation Loss: 0.00058400
Epoch [24/300], Train Loss: 0.000591
Validation Loss: 0.00058060
Epoch [25/300], Train Loss: 0.000589
Validation Loss: 0.00058802
Epoch [26/300], Train Loss: 0.000591
Validation Loss: 0.00057854
Epoch [27/300], Train Loss: 0.000587
Validation Loss: 0.00057819
Epoch [28/300], Train Loss: 0.000585
Validation Loss: 0.00057464
Epoch [29/300], Train Loss: 0.000585
Validation Loss: 0.00057555
Epoch [30/300], Train Loss: 0.000583
Validation Loss: 0.00057227
Epoch [31/300], Train Loss: 0.000585
Validation Loss: 0.00057270
Epoch [32/300], Train Loss: 0.000579
Validation Loss: 0.00056937
Epoch [33/300], Train Loss: 0.000580
Validation Loss: 0.00057500
Epoch [34/300], Train Loss: 0.000578
Validation Loss: 0.00057861
Epoch [35/300], Train Loss: 0.000582
Validation Loss: 0.00057193
Epoch [36/300], Train Loss: 0.000579
Validation Loss: 0.00057321
Epoch [37/300], Train Loss: 0.000574
Validation Loss: 0.00056863
Epoch [38/300], Train Loss: 0.000571
Validation Loss: 0.00056536
Epoch [39/300], Train Loss: 0.000570
Validation Loss: 0.00055970
Epoch [40/300], Train Loss: 0.000570
Validation Loss: 0.00056505
Epoch [41/300], Train Loss: 0.000572
Validation Loss: 0.00056430
Epoch [42/300], Train Loss: 0.000570
Validation Loss: 0.00055717
Epoch [43/300], Train Loss: 0.000562
Validation Loss: 0.00055407
Epoch [44/300], Train Loss: 0.000559
Validation Loss: 0.00055505
Epoch [45/300], Train Loss: 0.000559
Validation Loss: 0.00055217
Epoch [46/300], Train Loss: 0.000553
Validation Loss: 0.00054701
Epoch [47/300], Train Loss: 0.000553
Validation Loss: 0.00054811
Epoch [48/300], Train Loss: 0.000551
Validation Loss: 0.00054590
Epoch [49/300], Train Loss: 0.000547
Validation Loss: 0.00055391
Epoch [50/300], Train Loss: 0.000545
Validation Loss: 0.00054631
Epoch [51/300], Train Loss: 0.000539
Validation Loss: 0.00053870
Epoch [52/300], Train Loss: 0.000533
Validation Loss: 0.00053185
Epoch [53/300], Train Loss: 0.000537
Validation Loss: 0.00053193
Epoch [54/300], Train Loss: 0.000527
Validation Loss: 0.00052464
Epoch [55/300], Train Loss: 0.000524
Validation Loss: 0.00053190
Epoch [56/300], Train Loss: 0.000526
Validation Loss: 0.00052620
Epoch [57/300], Train Loss: 0.000522
Validation Loss: 0.00053284
Epoch [58/300], Train Loss: 0.000529
Validation Loss: 0.00052885
Epoch [59/300], Train Loss: 0.000517
Validation Loss: 0.00051718
Epoch [60/300], Train Loss: 0.000515
Validation Loss: 0.00050895
Epoch [61/300], Train Loss: 0.000518
Validation Loss: 0.00051259
Epoch [62/300], Train Loss: 0.000506
Validation Loss: 0.00050377
Epoch [63/300], Train Loss: 0.000542
Validation Loss: 0.00053223
Epoch [64/300], Train Loss: 0.000529
Validation Loss: 0.00052009
Epoch [65/300], Train Loss: 0.000518
Validation Loss: 0.00052357
Epoch [66/300], Train Loss: 0.000516
Validation Loss: 0.00051569
Epoch [67/300], Train Loss: 0.000514
Validation Loss: 0.00051397
Epoch [68/300], Train Loss: 0.000508
Validation Loss: 0.00051122
Epoch [69/300], Train Loss: 0.000509
Validation Loss: 0.00051195
Epoch [70/300], Train Loss: 0.000508
Validation Loss: 0.00050036
Epoch [71/300], Train Loss: 0.000503
Validation Loss: 0.00050511
Epoch [72/300], Train Loss: 0.000500
Validation Loss: 0.00049249
Epoch [73/300], Train Loss: 0.000503
Validation Loss: 0.00048973
Epoch [74/300], Train Loss: 0.000504
Validation Loss: 0.00050712
Epoch [75/300], Train Loss: 0.000496
Validation Loss: 0.00050024
Epoch [76/300], Train Loss: 0.000496
Validation Loss: 0.00050174
Epoch [77/300], Train Loss: 0.000488
Validation Loss: 0.00048407
Epoch [78/300], Train Loss: 0.000545
Validation Loss: 0.00056332
Epoch [79/300], Train Loss: 0.000552
Validation Loss: 0.00056469
Epoch [80/300], Train Loss: 0.000553
Validation Loss: 0.00053205
Epoch [81/300], Train Loss: 0.000537
Validation Loss: 0.00051989
Epoch [82/300], Train Loss: 0.000539
Validation Loss: 0.00058339
Epoch [83/300], Train Loss: 0.000562
Validation Loss: 0.00053956
Epoch [84/300], Train Loss: 0.000538
Validation Loss: 0.00053323
Epoch [85/300], Train Loss: 0.000531
Validation Loss: 0.00053102
Epoch [86/300], Train Loss: 0.000522
Validation Loss: 0.00051940
Epoch [87/300], Train Loss: 0.000519
Validation Loss: 0.00051842
Early stopping triggered

Evaluating model for: Fridge
Run 64/72 completed in 2209.18 seconds with: {'MAE': np.float32(33.57475), 'MSE': np.float32(1726.8623), 'RMSE': np.float32(41.555534), 'SAE': np.float32(0.02170442), 'NDE': np.float32(0.72434694)}

Run 65/72: hidden=512, seq_len=720, stride=0.25, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000892
Validation Loss: 0.00066908
Epoch [2/300], Train Loss: 0.000668
Validation Loss: 0.00065981
Epoch [3/300], Train Loss: 0.000656
Validation Loss: 0.00065603
Epoch [4/300], Train Loss: 0.000651
Validation Loss: 0.00064922
Epoch [5/300], Train Loss: 0.000645
Validation Loss: 0.00063948
Epoch [6/300], Train Loss: 0.000639
Validation Loss: 0.00063177
Epoch [7/300], Train Loss: 0.000618
Validation Loss: 0.00061922
Epoch [8/300], Train Loss: 0.000618
Validation Loss: 0.00062217
Epoch [9/300], Train Loss: 0.000612
Validation Loss: 0.00061109
Epoch [10/300], Train Loss: 0.000610
Validation Loss: 0.00061517
Epoch [11/300], Train Loss: 0.000606
Validation Loss: 0.00060986
Epoch [12/300], Train Loss: 0.000609
Validation Loss: 0.00060287
Epoch [13/300], Train Loss: 0.000609
Validation Loss: 0.00060564
Epoch [14/300], Train Loss: 0.000607
Validation Loss: 0.00060637
Epoch [15/300], Train Loss: 0.000610
Validation Loss: 0.00061947
Epoch [16/300], Train Loss: 0.000608
Validation Loss: 0.00060177
Epoch [17/300], Train Loss: 0.000605
Validation Loss: 0.00060087
Epoch [18/300], Train Loss: 0.000595
Validation Loss: 0.00060075
Epoch [19/300], Train Loss: 0.000598
Validation Loss: 0.00060034
Epoch [20/300], Train Loss: 0.000595
Validation Loss: 0.00059823
Epoch [21/300], Train Loss: 0.000597
Validation Loss: 0.00059550
Epoch [22/300], Train Loss: 0.000597
Validation Loss: 0.00059720
Epoch [23/300], Train Loss: 0.000606
Validation Loss: 0.00059486
Epoch [24/300], Train Loss: 0.000598
Validation Loss: 0.00059955
Epoch [25/300], Train Loss: 0.000596
Validation Loss: 0.00059293
Epoch [26/300], Train Loss: 0.000595
Validation Loss: 0.00059664
Epoch [27/300], Train Loss: 0.000598
Validation Loss: 0.00061020
Epoch [28/300], Train Loss: 0.000603
Validation Loss: 0.00059128
Epoch [29/300], Train Loss: 0.000596
Validation Loss: 0.00059037
Epoch [30/300], Train Loss: 0.000599
Validation Loss: 0.00059101
Epoch [31/300], Train Loss: 0.000598
Validation Loss: 0.00058812
Epoch [32/300], Train Loss: 0.000592
Validation Loss: 0.00060404
Epoch [33/300], Train Loss: 0.000595
Validation Loss: 0.00058841
Epoch [34/300], Train Loss: 0.000594
Validation Loss: 0.00058671
Epoch [35/300], Train Loss: 0.000593
Validation Loss: 0.00058676
Epoch [36/300], Train Loss: 0.000596
Validation Loss: 0.00058793
Epoch [37/300], Train Loss: 0.000589
Validation Loss: 0.00058563
Epoch [38/300], Train Loss: 0.000591
Validation Loss: 0.00058813
Epoch [39/300], Train Loss: 0.000591
Validation Loss: 0.00058892
Epoch [40/300], Train Loss: 0.000586
Validation Loss: 0.00058383
Epoch [41/300], Train Loss: 0.000588
Validation Loss: 0.00058011
Epoch [42/300], Train Loss: 0.000586
Validation Loss: 0.00057931
Epoch [43/300], Train Loss: 0.000582
Validation Loss: 0.00057822
Epoch [44/300], Train Loss: 0.000584
Validation Loss: 0.00057951
Epoch [45/300], Train Loss: 0.000585
Validation Loss: 0.00058420
Epoch [46/300], Train Loss: 0.000584
Validation Loss: 0.00057596
Epoch [47/300], Train Loss: 0.000578
Validation Loss: 0.00057350
Epoch [48/300], Train Loss: 0.000582
Validation Loss: 0.00057101
Epoch [49/300], Train Loss: 0.000576
Validation Loss: 0.00057137
Epoch [50/300], Train Loss: 0.000576
Validation Loss: 0.00058258
Epoch [51/300], Train Loss: 0.000574
Validation Loss: 0.00057080
Epoch [52/300], Train Loss: 0.000574
Validation Loss: 0.00057246
Epoch [53/300], Train Loss: 0.000576
Validation Loss: 0.00056847
Epoch [54/300], Train Loss: 0.000568
Validation Loss: 0.00056448
Epoch [55/300], Train Loss: 0.000571
Validation Loss: 0.00056457
Epoch [56/300], Train Loss: 0.000566
Validation Loss: 0.00056691
Epoch [57/300], Train Loss: 0.000563
Validation Loss: 0.00055980
Epoch [58/300], Train Loss: 0.000570
Validation Loss: 0.00055986
Epoch [59/300], Train Loss: 0.000567
Validation Loss: 0.00055805
Epoch [60/300], Train Loss: 0.000571
Validation Loss: 0.00055638
Epoch [61/300], Train Loss: 0.000566
Validation Loss: 0.00055862
Epoch [62/300], Train Loss: 0.000565
Validation Loss: 0.00055618
Epoch [63/300], Train Loss: 0.000564
Validation Loss: 0.00055223
Epoch [64/300], Train Loss: 0.000558
Validation Loss: 0.00055824
Epoch [65/300], Train Loss: 0.000561
Validation Loss: 0.00054762
Epoch [66/300], Train Loss: 0.000559
Validation Loss: 0.00055728
Epoch [67/300], Train Loss: 0.000560
Validation Loss: 0.00054431
Epoch [68/300], Train Loss: 0.000557
Validation Loss: 0.00054469
Epoch [69/300], Train Loss: 0.000557
Validation Loss: 0.00054415
Epoch [70/300], Train Loss: 0.000549
Validation Loss: 0.00054596
Epoch [71/300], Train Loss: 0.000550
Validation Loss: 0.00056133
Epoch [72/300], Train Loss: 0.000557
Validation Loss: 0.00054227
Epoch [73/300], Train Loss: 0.000553
Validation Loss: 0.00056365
Epoch [74/300], Train Loss: 0.000557
Validation Loss: 0.00054609
Epoch [75/300], Train Loss: 0.000556
Validation Loss: 0.00054107
Epoch [76/300], Train Loss: 0.000542
Validation Loss: 0.00053731
Epoch [77/300], Train Loss: 0.000544
Validation Loss: 0.00055236
Epoch [78/300], Train Loss: 0.000542
Validation Loss: 0.00052821
Epoch [79/300], Train Loss: 0.000536
Validation Loss: 0.00052408
Epoch [80/300], Train Loss: 0.000534
Validation Loss: 0.00052585
Epoch [81/300], Train Loss: 0.000528
Validation Loss: 0.00050755
Epoch [82/300], Train Loss: 0.000524
Validation Loss: 0.00052831
Epoch [83/300], Train Loss: 0.000526
Validation Loss: 0.00050660
Epoch [84/300], Train Loss: 0.000517
Validation Loss: 0.00050027
Epoch [85/300], Train Loss: 0.000513
Validation Loss: 0.00050133
Epoch [86/300], Train Loss: 0.000517
Validation Loss: 0.00049559
Epoch [87/300], Train Loss: 0.000517
Validation Loss: 0.00051120
Epoch [88/300], Train Loss: 0.000516
Validation Loss: 0.00049052
Epoch [89/300], Train Loss: 0.000504
Validation Loss: 0.00048999
Epoch [90/300], Train Loss: 0.000509
Validation Loss: 0.00049186
Epoch [91/300], Train Loss: 0.000505
Validation Loss: 0.00048678
Epoch [92/300], Train Loss: 0.000510
Validation Loss: 0.00048741
Epoch [93/300], Train Loss: 0.000504
Validation Loss: 0.00050512
Epoch [94/300], Train Loss: 0.000556
Validation Loss: 0.00052164
Epoch [95/300], Train Loss: 0.000518
Validation Loss: 0.00049392
Epoch [96/300], Train Loss: 0.000506
Validation Loss: 0.00049744
Epoch [97/300], Train Loss: 0.000505
Validation Loss: 0.00048970
Epoch [98/300], Train Loss: 0.000503
Validation Loss: 0.00047948
Epoch [99/300], Train Loss: 0.000498
Validation Loss: 0.00048037
Epoch [100/300], Train Loss: 0.000507
Validation Loss: 0.00047626
Epoch [101/300], Train Loss: 0.000497
Validation Loss: 0.00047545
Epoch [102/300], Train Loss: 0.000495
Validation Loss: 0.00047473
Epoch [103/300], Train Loss: 0.000491
Validation Loss: 0.00047724
Epoch [104/300], Train Loss: 0.000489
Validation Loss: 0.00046997
Epoch [105/300], Train Loss: 0.000488
Validation Loss: 0.00046768
Epoch [106/300], Train Loss: 0.000488
Validation Loss: 0.00047093
Epoch [107/300], Train Loss: 0.000486
Validation Loss: 0.00046734
Epoch [108/300], Train Loss: 0.000481
Validation Loss: 0.00046682
Epoch [109/300], Train Loss: 0.000481
Validation Loss: 0.00046015
Epoch [110/300], Train Loss: 0.000486
Validation Loss: 0.00045853
Epoch [111/300], Train Loss: 0.000479
Validation Loss: 0.00045590
Epoch [112/300], Train Loss: 0.000474
Validation Loss: 0.00046128
Epoch [113/300], Train Loss: 0.000481
Validation Loss: 0.00046925
Epoch [114/300], Train Loss: 0.000482
Validation Loss: 0.00051245
Epoch [115/300], Train Loss: 0.000492
Validation Loss: 0.00045667
Epoch [116/300], Train Loss: 0.000475
Validation Loss: 0.00044947
Epoch [117/300], Train Loss: 0.000471
Validation Loss: 0.00046108
Epoch [118/300], Train Loss: 0.000470
Validation Loss: 0.00046173
Epoch [119/300], Train Loss: 0.000473
Validation Loss: 0.00044619
Epoch [120/300], Train Loss: 0.000471
Validation Loss: 0.00045044
Epoch [121/300], Train Loss: 0.000472
Validation Loss: 0.00045565
Epoch [122/300], Train Loss: 0.000479
Validation Loss: 0.00044104
Epoch [123/300], Train Loss: 0.000473
Validation Loss: 0.00044312
Epoch [124/300], Train Loss: 0.000468
Validation Loss: 0.00044180
Epoch [125/300], Train Loss: 0.000460
Validation Loss: 0.00044132
Epoch [126/300], Train Loss: 0.000459
Validation Loss: 0.00044592
Epoch [127/300], Train Loss: 0.000458
Validation Loss: 0.00045010
Epoch [128/300], Train Loss: 0.000464
Validation Loss: 0.00045300
Epoch [129/300], Train Loss: 0.000455
Validation Loss: 0.00043000
Epoch [130/300], Train Loss: 0.000458
Validation Loss: 0.00043623
Epoch [131/300], Train Loss: 0.000464
Validation Loss: 0.00044395
Epoch [132/300], Train Loss: 0.000458
Validation Loss: 0.00043541
Epoch [133/300], Train Loss: 0.000459
Validation Loss: 0.00042862
Epoch [134/300], Train Loss: 0.000451
Validation Loss: 0.00042993
Epoch [135/300], Train Loss: 0.000448
Validation Loss: 0.00044049
Epoch [136/300], Train Loss: 0.000453
Validation Loss: 0.00042736
Epoch [137/300], Train Loss: 0.000449
Validation Loss: 0.00043193
Epoch [138/300], Train Loss: 0.000446
Validation Loss: 0.00049545
Epoch [139/300], Train Loss: 0.000499
Validation Loss: 0.00045478
Epoch [140/300], Train Loss: 0.000455
Validation Loss: 0.00042484
Epoch [141/300], Train Loss: 0.000450
Validation Loss: 0.00042472
Epoch [142/300], Train Loss: 0.000452
Validation Loss: 0.00042880
Epoch [143/300], Train Loss: 0.000449
Validation Loss: 0.00041971
Epoch [144/300], Train Loss: 0.000442
Validation Loss: 0.00041733
Epoch [145/300], Train Loss: 0.000446
Validation Loss: 0.00041569
Epoch [146/300], Train Loss: 0.000437
Validation Loss: 0.00041428
Epoch [147/300], Train Loss: 0.000440
Validation Loss: 0.00041462
Epoch [148/300], Train Loss: 0.000436
Validation Loss: 0.00041535
Epoch [149/300], Train Loss: 0.000440
Validation Loss: 0.00041290
Epoch [150/300], Train Loss: 0.000437
Validation Loss: 0.00041482
Epoch [151/300], Train Loss: 0.000440
Validation Loss: 0.00041191
Epoch [152/300], Train Loss: 0.000433
Validation Loss: 0.00041569
Epoch [153/300], Train Loss: 0.000434
Validation Loss: 0.00040666
Epoch [154/300], Train Loss: 0.000430
Validation Loss: 0.00040669
Epoch [155/300], Train Loss: 0.000432
Validation Loss: 0.00041013
Epoch [156/300], Train Loss: 0.000436
Validation Loss: 0.00041049
Epoch [157/300], Train Loss: 0.000444
Validation Loss: 0.00042201
Epoch [158/300], Train Loss: 0.000430
Validation Loss: 0.00040975
Epoch [159/300], Train Loss: 0.000429
Validation Loss: 0.00054386
Epoch [160/300], Train Loss: 0.000537
Validation Loss: 0.00048764
Epoch [161/300], Train Loss: 0.000498
Validation Loss: 0.00047088
Epoch [162/300], Train Loss: 0.000480
Validation Loss: 0.00045481
Epoch [163/300], Train Loss: 0.000467
Validation Loss: 0.00043332
Early stopping triggered

Evaluating model for: Fridge
Run 65/72 completed in 3264.47 seconds with: {'MAE': np.float32(30.763517), 'MSE': np.float32(1500.7765), 'RMSE': np.float32(38.739857), 'SAE': np.float32(0.016398842), 'NDE': np.float32(0.6694714)}

Run 66/72: hidden=512, seq_len=720, stride=0.25, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000685
Validation Loss: 0.00066079
Epoch [2/300], Train Loss: 0.000663
Validation Loss: 0.00065651
Epoch [3/300], Train Loss: 0.000651
Validation Loss: 0.00064784
Epoch [4/300], Train Loss: 0.000636
Validation Loss: 0.00063347
Epoch [5/300], Train Loss: 0.000619
Validation Loss: 0.00062189
Epoch [6/300], Train Loss: 0.000618
Validation Loss: 0.00062471
Epoch [7/300], Train Loss: 0.000600
Validation Loss: 0.00060933
Epoch [8/300], Train Loss: 0.000609
Validation Loss: 0.00063232
Epoch [9/300], Train Loss: 0.000608
Validation Loss: 0.00060438
Epoch [10/300], Train Loss: 0.000604
Validation Loss: 0.00060604
Epoch [11/300], Train Loss: 0.000600
Validation Loss: 0.00060433
Epoch [12/300], Train Loss: 0.000606
Validation Loss: 0.00059959
Epoch [13/300], Train Loss: 0.000604
Validation Loss: 0.00059981
Epoch [14/300], Train Loss: 0.000603
Validation Loss: 0.00059872
Epoch [15/300], Train Loss: 0.000604
Validation Loss: 0.00060114
Epoch [16/300], Train Loss: 0.000601
Validation Loss: 0.00060440
Epoch [17/300], Train Loss: 0.000601
Validation Loss: 0.00059576
Epoch [18/300], Train Loss: 0.000590
Validation Loss: 0.00059395
Epoch [19/300], Train Loss: 0.000593
Validation Loss: 0.00059519
Epoch [20/300], Train Loss: 0.000591
Validation Loss: 0.00059857
Epoch [21/300], Train Loss: 0.000592
Validation Loss: 0.00058927
Epoch [22/300], Train Loss: 0.000592
Validation Loss: 0.00058860
Epoch [23/300], Train Loss: 0.000598
Validation Loss: 0.00059245
Epoch [24/300], Train Loss: 0.000593
Validation Loss: 0.00060336
Epoch [25/300], Train Loss: 0.000592
Validation Loss: 0.00058444
Epoch [26/300], Train Loss: 0.000586
Validation Loss: 0.00058233
Epoch [27/300], Train Loss: 0.000591
Validation Loss: 0.00060710
Epoch [28/300], Train Loss: 0.000593
Validation Loss: 0.00059384
Epoch [29/300], Train Loss: 0.000590
Validation Loss: 0.00057820
Epoch [30/300], Train Loss: 0.000588
Validation Loss: 0.00057862
Epoch [31/300], Train Loss: 0.000587
Validation Loss: 0.00058823
Epoch [32/300], Train Loss: 0.000581
Validation Loss: 0.00061026
Epoch [33/300], Train Loss: 0.000589
Validation Loss: 0.00058130
Epoch [34/300], Train Loss: 0.000579
Validation Loss: 0.00057531
Epoch [35/300], Train Loss: 0.000579
Validation Loss: 0.00060250
Epoch [36/300], Train Loss: 0.000587
Validation Loss: 0.00056849
Epoch [37/300], Train Loss: 0.000573
Validation Loss: 0.00056549
Epoch [38/300], Train Loss: 0.000573
Validation Loss: 0.00060732
Epoch [39/300], Train Loss: 0.000578
Validation Loss: 0.00056151
Epoch [40/300], Train Loss: 0.000566
Validation Loss: 0.00056568
Epoch [41/300], Train Loss: 0.000566
Validation Loss: 0.00056091
Epoch [42/300], Train Loss: 0.000564
Validation Loss: 0.00055340
Epoch [43/300], Train Loss: 0.000560
Validation Loss: 0.00056187
Epoch [44/300], Train Loss: 0.000554
Validation Loss: 0.00059549
Epoch [45/300], Train Loss: 0.000560
Validation Loss: 0.00054355
Epoch [46/300], Train Loss: 0.000547
Validation Loss: 0.00053532
Epoch [47/300], Train Loss: 0.000540
Validation Loss: 0.00055984
Epoch [48/300], Train Loss: 0.000542
Validation Loss: 0.00052603
Epoch [49/300], Train Loss: 0.000527
Validation Loss: 0.00051586
Epoch [50/300], Train Loss: 0.000525
Validation Loss: 0.00051594
Epoch [51/300], Train Loss: 0.000518
Validation Loss: 0.00051840
Epoch [52/300], Train Loss: 0.000521
Validation Loss: 0.00051070
Epoch [53/300], Train Loss: 0.000530
Validation Loss: 0.00063708
Epoch [54/300], Train Loss: 0.000523
Validation Loss: 0.00049940
Epoch [55/300], Train Loss: 0.000508
Validation Loss: 0.00051371
Epoch [56/300], Train Loss: 0.000508
Validation Loss: 0.00048757
Epoch [57/300], Train Loss: 0.000493
Validation Loss: 0.00049050
Epoch [58/300], Train Loss: 0.000497
Validation Loss: 0.00049972
Epoch [59/300], Train Loss: 0.000498
Validation Loss: 0.00049057
Epoch [60/300], Train Loss: 0.000495
Validation Loss: 0.00050192
Epoch [61/300], Train Loss: 0.000499
Validation Loss: 0.00050651
Epoch [62/300], Train Loss: 0.000501
Validation Loss: 0.00052829
Epoch [63/300], Train Loss: 0.000490
Validation Loss: 0.00048443
Epoch [64/300], Train Loss: 0.000487
Validation Loss: 0.00048905
Epoch [65/300], Train Loss: 0.000486
Validation Loss: 0.00048015
Epoch [66/300], Train Loss: 0.000478
Validation Loss: 0.00047512
Epoch [67/300], Train Loss: 0.000477
Validation Loss: 0.00045993
Epoch [68/300], Train Loss: 0.000477
Validation Loss: 0.00049130
Epoch [69/300], Train Loss: 0.000485
Validation Loss: 0.00046022
Epoch [70/300], Train Loss: 0.000468
Validation Loss: 0.00046186
Epoch [71/300], Train Loss: 0.000462
Validation Loss: 0.00048482
Epoch [72/300], Train Loss: 0.000467
Validation Loss: 0.00044544
Epoch [73/300], Train Loss: 0.000464
Validation Loss: 0.00065812
Epoch [74/300], Train Loss: 0.000547
Validation Loss: 0.00053268
Epoch [75/300], Train Loss: 0.000499
Validation Loss: 0.00048705
Epoch [76/300], Train Loss: 0.000472
Validation Loss: 0.00047858
Epoch [77/300], Train Loss: 0.000461
Validation Loss: 0.00050232
Epoch [78/300], Train Loss: 0.000466
Validation Loss: 0.00048551
Epoch [79/300], Train Loss: 0.000462
Validation Loss: 0.00043852
Epoch [80/300], Train Loss: 0.000454
Validation Loss: 0.00056166
Epoch [81/300], Train Loss: 0.000485
Validation Loss: 0.00046147
Epoch [82/300], Train Loss: 0.000456
Validation Loss: 0.00049788
Epoch [83/300], Train Loss: 0.000458
Validation Loss: 0.00043347
Epoch [84/300], Train Loss: 0.000447
Validation Loss: 0.00045219
Epoch [85/300], Train Loss: 0.000444
Validation Loss: 0.00043235
Epoch [86/300], Train Loss: 0.000446
Validation Loss: 0.00049144
Epoch [87/300], Train Loss: 0.000454
Validation Loss: 0.00045633
Epoch [88/300], Train Loss: 0.000448
Validation Loss: 0.00051240
Epoch [89/300], Train Loss: 0.000452
Validation Loss: 0.00045535
Epoch [90/300], Train Loss: 0.000453
Validation Loss: 0.00044133
Epoch [91/300], Train Loss: 0.000450
Validation Loss: 0.00043293
Epoch [92/300], Train Loss: 0.000434
Validation Loss: 0.00042582
Epoch [93/300], Train Loss: 0.000430
Validation Loss: 0.00046316
Epoch [94/300], Train Loss: 0.000439
Validation Loss: 0.00043240
Epoch [95/300], Train Loss: 0.000436
Validation Loss: 0.00043513
Epoch [96/300], Train Loss: 0.000430
Validation Loss: 0.00043141
Epoch [97/300], Train Loss: 0.000429
Validation Loss: 0.00047907
Epoch [98/300], Train Loss: 0.000436
Validation Loss: 0.00041337
Epoch [99/300], Train Loss: 0.000433
Validation Loss: 0.00041435
Epoch [100/300], Train Loss: 0.000427
Validation Loss: 0.00044852
Epoch [101/300], Train Loss: 0.000433
Validation Loss: 0.00041599
Epoch [102/300], Train Loss: 0.000427
Validation Loss: 0.00041998
Epoch [103/300], Train Loss: 0.000420
Validation Loss: 0.00046740
Epoch [104/300], Train Loss: 0.000422
Validation Loss: 0.00042442
Epoch [105/300], Train Loss: 0.000422
Validation Loss: 0.00040631
Epoch [106/300], Train Loss: 0.000420
Validation Loss: 0.00040668
Epoch [107/300], Train Loss: 0.000421
Validation Loss: 0.00040534
Epoch [108/300], Train Loss: 0.000416
Validation Loss: 0.00040302
Epoch [109/300], Train Loss: 0.000419
Validation Loss: 0.00043572
Epoch [110/300], Train Loss: 0.000431
Validation Loss: 0.00040329
Epoch [111/300], Train Loss: 0.000417
Validation Loss: 0.00041128
Epoch [112/300], Train Loss: 0.000413
Validation Loss: 0.00040452
Epoch [113/300], Train Loss: 0.000417
Validation Loss: 0.00040813
Epoch [114/300], Train Loss: 0.000416
Validation Loss: 0.00041308
Epoch [115/300], Train Loss: 0.000414
Validation Loss: 0.00039655
Epoch [116/300], Train Loss: 0.000418
Validation Loss: 0.00042984
Epoch [117/300], Train Loss: 0.000420
Validation Loss: 0.00042791
Epoch [118/300], Train Loss: 0.000410
Validation Loss: 0.00039731
Epoch [119/300], Train Loss: 0.000414
Validation Loss: 0.00040039
Epoch [120/300], Train Loss: 0.000408
Validation Loss: 0.00054195
Epoch [121/300], Train Loss: 0.000467
Validation Loss: 0.00041293
Epoch [122/300], Train Loss: 0.000422
Validation Loss: 0.00040471
Epoch [123/300], Train Loss: 0.000419
Validation Loss: 0.00040679
Epoch [124/300], Train Loss: 0.000416
Validation Loss: 0.00040375
Epoch [125/300], Train Loss: 0.000424
Validation Loss: 0.00039811
Early stopping triggered

Evaluating model for: Fridge
Run 66/72 completed in 3183.03 seconds with: {'MAE': np.float32(27.660276), 'MSE': np.float32(1350.1128), 'RMSE': np.float32(36.74388), 'SAE': np.float32(0.009357102), 'NDE': np.float32(0.6349786)}

Run 67/72: hidden=512, seq_len=720, stride=0.25, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000927
Validation Loss: 0.00067015
Epoch [2/300], Train Loss: 0.000671
Validation Loss: 0.00066453
Epoch [3/300], Train Loss: 0.000662
Validation Loss: 0.00066482
Epoch [4/300], Train Loss: 0.000662
Validation Loss: 0.00066415
Epoch [5/300], Train Loss: 0.000663
Validation Loss: 0.00066351
Epoch [6/300], Train Loss: 0.000665
Validation Loss: 0.00066263
Epoch [7/300], Train Loss: 0.000652
Validation Loss: 0.00066169
Epoch [8/300], Train Loss: 0.000658
Validation Loss: 0.00065496
Epoch [9/300], Train Loss: 0.000643
Validation Loss: 0.00063312
Epoch [10/300], Train Loss: 0.000627
Validation Loss: 0.00062237
Epoch [11/300], Train Loss: 0.000619
Validation Loss: 0.00062272
Epoch [12/300], Train Loss: 0.000618
Validation Loss: 0.00061171
Epoch [13/300], Train Loss: 0.000613
Validation Loss: 0.00060744
Epoch [14/300], Train Loss: 0.000610
Validation Loss: 0.00060188
Epoch [15/300], Train Loss: 0.000611
Validation Loss: 0.00060895
Epoch [16/300], Train Loss: 0.000606
Validation Loss: 0.00060386
Epoch [17/300], Train Loss: 0.000605
Validation Loss: 0.00060022
Epoch [18/300], Train Loss: 0.000594
Validation Loss: 0.00059658
Epoch [19/300], Train Loss: 0.000597
Validation Loss: 0.00060018
Epoch [20/300], Train Loss: 0.000594
Validation Loss: 0.00059876
Epoch [21/300], Train Loss: 0.000595
Validation Loss: 0.00059237
Epoch [22/300], Train Loss: 0.000597
Validation Loss: 0.00059182
Epoch [23/300], Train Loss: 0.000604
Validation Loss: 0.00059151
Epoch [24/300], Train Loss: 0.000597
Validation Loss: 0.00059635
Epoch [25/300], Train Loss: 0.000593
Validation Loss: 0.00058860
Epoch [26/300], Train Loss: 0.000592
Validation Loss: 0.00058576
Epoch [27/300], Train Loss: 0.000596
Validation Loss: 0.00058704
Epoch [28/300], Train Loss: 0.000594
Validation Loss: 0.00060375
Epoch [29/300], Train Loss: 0.000595
Validation Loss: 0.00058229
Epoch [30/300], Train Loss: 0.000593
Validation Loss: 0.00057997
Epoch [31/300], Train Loss: 0.000591
Validation Loss: 0.00058641
Epoch [32/300], Train Loss: 0.000585
Validation Loss: 0.00059544
Epoch [33/300], Train Loss: 0.000589
Validation Loss: 0.00057795
Epoch [34/300], Train Loss: 0.000583
Validation Loss: 0.00057926
Epoch [35/300], Train Loss: 0.000585
Validation Loss: 0.00058196
Epoch [36/300], Train Loss: 0.000589
Validation Loss: 0.00057272
Epoch [37/300], Train Loss: 0.000579
Validation Loss: 0.00057074
Epoch [38/300], Train Loss: 0.000578
Validation Loss: 0.00058633
Epoch [39/300], Train Loss: 0.000579
Validation Loss: 0.00056562
Epoch [40/300], Train Loss: 0.000571
Validation Loss: 0.00056236
Epoch [41/300], Train Loss: 0.000571
Validation Loss: 0.00057228
Epoch [42/300], Train Loss: 0.000570
Validation Loss: 0.00059448
Epoch [43/300], Train Loss: 0.000572
Validation Loss: 0.00057249
Epoch [44/300], Train Loss: 0.000565
Validation Loss: 0.00057448
Epoch [45/300], Train Loss: 0.000568
Validation Loss: 0.00055105
Epoch [46/300], Train Loss: 0.000558
Validation Loss: 0.00054554
Epoch [47/300], Train Loss: 0.000554
Validation Loss: 0.00054404
Epoch [48/300], Train Loss: 0.000557
Validation Loss: 0.00054229
Epoch [49/300], Train Loss: 0.000554
Validation Loss: 0.00057435
Epoch [50/300], Train Loss: 0.000559
Validation Loss: 0.00056156
Epoch [51/300], Train Loss: 0.000549
Validation Loss: 0.00053223
Epoch [52/300], Train Loss: 0.000542
Validation Loss: 0.00053662
Epoch [53/300], Train Loss: 0.000552
Validation Loss: 0.00063534
Epoch [54/300], Train Loss: 0.000550
Validation Loss: 0.00052302
Epoch [55/300], Train Loss: 0.000526
Validation Loss: 0.00051599
Epoch [56/300], Train Loss: 0.000515
Validation Loss: 0.00049608
Epoch [57/300], Train Loss: 0.000499
Validation Loss: 0.00049192
Epoch [58/300], Train Loss: 0.000495
Validation Loss: 0.00049255
Epoch [59/300], Train Loss: 0.000503
Validation Loss: 0.00050382
Epoch [60/300], Train Loss: 0.000525
Validation Loss: 0.00049526
Epoch [61/300], Train Loss: 0.000507
Validation Loss: 0.00052987
Epoch [62/300], Train Loss: 0.000504
Validation Loss: 0.00049228
Epoch [63/300], Train Loss: 0.000492
Validation Loss: 0.00048901
Epoch [64/300], Train Loss: 0.000489
Validation Loss: 0.00051176
Epoch [65/300], Train Loss: 0.000489
Validation Loss: 0.00055809
Epoch [66/300], Train Loss: 0.000515
Validation Loss: 0.00049377
Epoch [67/300], Train Loss: 0.000494
Validation Loss: 0.00047008
Epoch [68/300], Train Loss: 0.000479
Validation Loss: 0.00047057
Epoch [69/300], Train Loss: 0.000479
Validation Loss: 0.00055404
Epoch [70/300], Train Loss: 0.000524
Validation Loss: 0.00047267
Epoch [71/300], Train Loss: 0.000475
Validation Loss: 0.00047383
Epoch [72/300], Train Loss: 0.000475
Validation Loss: 0.00046727
Epoch [73/300], Train Loss: 0.000472
Validation Loss: 0.00045837
Epoch [74/300], Train Loss: 0.000473
Validation Loss: 0.00047069
Epoch [75/300], Train Loss: 0.000480
Validation Loss: 0.00048123
Epoch [76/300], Train Loss: 0.000465
Validation Loss: 0.00047929
Epoch [77/300], Train Loss: 0.000465
Validation Loss: 0.00048230
Epoch [78/300], Train Loss: 0.000467
Validation Loss: 0.00048505
Epoch [79/300], Train Loss: 0.000471
Validation Loss: 0.00044516
Epoch [80/300], Train Loss: 0.000461
Validation Loss: 0.00048265
Epoch [81/300], Train Loss: 0.000462
Validation Loss: 0.00044439
Epoch [82/300], Train Loss: 0.000455
Validation Loss: 0.00051362
Epoch [83/300], Train Loss: 0.000470
Validation Loss: 0.00044383
Epoch [84/300], Train Loss: 0.000452
Validation Loss: 0.00044091
Epoch [85/300], Train Loss: 0.000456
Validation Loss: 0.00045800
Epoch [86/300], Train Loss: 0.000453
Validation Loss: 0.00044285
Epoch [87/300], Train Loss: 0.000457
Validation Loss: 0.00045902
Epoch [88/300], Train Loss: 0.000454
Validation Loss: 0.00044067
Epoch [89/300], Train Loss: 0.000439
Validation Loss: 0.00045862
Epoch [90/300], Train Loss: 0.000449
Validation Loss: 0.00044074
Epoch [91/300], Train Loss: 0.000454
Validation Loss: 0.00042921
Epoch [92/300], Train Loss: 0.000444
Validation Loss: 0.00043191
Epoch [93/300], Train Loss: 0.000436
Validation Loss: 0.00043255
Epoch [94/300], Train Loss: 0.000438
Validation Loss: 0.00042830
Epoch [95/300], Train Loss: 0.000444
Validation Loss: 0.00046349
Epoch [96/300], Train Loss: 0.000441
Validation Loss: 0.00044309
Epoch [97/300], Train Loss: 0.000442
Validation Loss: 0.00042705
Epoch [98/300], Train Loss: 0.000435
Validation Loss: 0.00042196
Epoch [99/300], Train Loss: 0.000433
Validation Loss: 0.00043430
Epoch [100/300], Train Loss: 0.000442
Validation Loss: 0.00042640
Epoch [101/300], Train Loss: 0.000429
Validation Loss: 0.00041822
Epoch [102/300], Train Loss: 0.000427
Validation Loss: 0.00041862
Epoch [103/300], Train Loss: 0.000430
Validation Loss: 0.00044300
Epoch [104/300], Train Loss: 0.000430
Validation Loss: 0.00043230
Epoch [105/300], Train Loss: 0.000427
Validation Loss: 0.00041410
Epoch [106/300], Train Loss: 0.000427
Validation Loss: 0.00042056
Epoch [107/300], Train Loss: 0.000426
Validation Loss: 0.00041145
Epoch [108/300], Train Loss: 0.000420
Validation Loss: 0.00041763
Epoch [109/300], Train Loss: 0.000424
Validation Loss: 0.00041631
Epoch [110/300], Train Loss: 0.000428
Validation Loss: 0.00043531
Epoch [111/300], Train Loss: 0.000424
Validation Loss: 0.00041605
Epoch [112/300], Train Loss: 0.000420
Validation Loss: 0.00041628
Epoch [113/300], Train Loss: 0.000424
Validation Loss: 0.00040616
Epoch [114/300], Train Loss: 0.000417
Validation Loss: 0.00040664
Epoch [115/300], Train Loss: 0.000417
Validation Loss: 0.00040727
Epoch [116/300], Train Loss: 0.000425
Validation Loss: 0.00044091
Epoch [117/300], Train Loss: 0.000426
Validation Loss: 0.00040656
Epoch [118/300], Train Loss: 0.000411
Validation Loss: 0.00041553
Epoch [119/300], Train Loss: 0.000418
Validation Loss: 0.00040433
Epoch [120/300], Train Loss: 0.000418
Validation Loss: 0.00042562
Epoch [121/300], Train Loss: 0.000421
Validation Loss: 0.00040694
Epoch [122/300], Train Loss: 0.000411
Validation Loss: 0.00041847
Epoch [123/300], Train Loss: 0.000415
Validation Loss: 0.00040516
Epoch [124/300], Train Loss: 0.000415
Validation Loss: 0.00039917
Epoch [125/300], Train Loss: 0.000408
Validation Loss: 0.00040454
Epoch [126/300], Train Loss: 0.000409
Validation Loss: 0.00040141
Epoch [127/300], Train Loss: 0.000404
Validation Loss: 0.00039510
Epoch [128/300], Train Loss: 0.000409
Validation Loss: 0.00039428
Epoch [129/300], Train Loss: 0.000407
Validation Loss: 0.00039379
Epoch [130/300], Train Loss: 0.000399
Validation Loss: 0.00039687
Epoch [131/300], Train Loss: 0.000406
Validation Loss: 0.00040726
Epoch [132/300], Train Loss: 0.000409
Validation Loss: 0.00040650
Epoch [133/300], Train Loss: 0.000404
Validation Loss: 0.00039900
Epoch [134/300], Train Loss: 0.000400
Validation Loss: 0.00039943
Epoch [135/300], Train Loss: 0.000397
Validation Loss: 0.00038817
Epoch [136/300], Train Loss: 0.000398
Validation Loss: 0.00038935
Epoch [137/300], Train Loss: 0.000400
Validation Loss: 0.00038331
Epoch [138/300], Train Loss: 0.000389
Validation Loss: 0.00039518
Epoch [139/300], Train Loss: 0.000392
Validation Loss: 0.00037958
Epoch [140/300], Train Loss: 0.000390
Validation Loss: 0.00038834
Epoch [141/300], Train Loss: 0.000392
Validation Loss: 0.00039494
Epoch [142/300], Train Loss: 0.000395
Validation Loss: 0.00049687
Epoch [143/300], Train Loss: 0.000448
Validation Loss: 0.00041253
Epoch [144/300], Train Loss: 0.000416
Validation Loss: 0.00039618
Epoch [145/300], Train Loss: 0.000411
Validation Loss: 0.00039103
Epoch [146/300], Train Loss: 0.000399
Validation Loss: 0.00039763
Epoch [147/300], Train Loss: 0.000398
Validation Loss: 0.00038880
Epoch [148/300], Train Loss: 0.000401
Validation Loss: 0.00039303
Epoch [149/300], Train Loss: 0.000398
Validation Loss: 0.00038607
Early stopping triggered

Evaluating model for: Fridge
Run 67/72 completed in 4618.18 seconds with: {'MAE': np.float32(27.73891), 'MSE': np.float32(1331.3351), 'RMSE': np.float32(36.487465), 'SAE': np.float32(0.1032725), 'NDE': np.float32(0.6305474)}

Run 68/72: hidden=512, seq_len=720, stride=0.25, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 6726 windows

Epoch [1/300], Train Loss: 0.000663
Validation Loss: 0.00066219
Epoch [2/300], Train Loss: 0.000664
Validation Loss: 0.00066504
Epoch [3/300], Train Loss: 0.000658
Validation Loss: 0.00066175
Epoch [4/300], Train Loss: 0.000656
Validation Loss: 0.00065858
Epoch [5/300], Train Loss: 0.000651
Validation Loss: 0.00064780
Epoch [6/300], Train Loss: 0.000636
Validation Loss: 0.00062573
Epoch [7/300], Train Loss: 0.000610
Validation Loss: 0.00064710
Epoch [8/300], Train Loss: 0.000619
Validation Loss: 0.00063647
Epoch [9/300], Train Loss: 0.000614
Validation Loss: 0.00061691
Epoch [10/300], Train Loss: 0.000613
Validation Loss: 0.00061421
Epoch [11/300], Train Loss: 0.000608
Validation Loss: 0.00061272
Epoch [12/300], Train Loss: 0.000609
Validation Loss: 0.00061862
Epoch [13/300], Train Loss: 0.000609
Validation Loss: 0.00060210
Epoch [14/300], Train Loss: 0.000605
Validation Loss: 0.00060564
Epoch [15/300], Train Loss: 0.000607
Validation Loss: 0.00060696
Epoch [16/300], Train Loss: 0.000602
Validation Loss: 0.00061079
Epoch [17/300], Train Loss: 0.000603
Validation Loss: 0.00059959
Epoch [18/300], Train Loss: 0.000591
Validation Loss: 0.00059991
Epoch [19/300], Train Loss: 0.000595
Validation Loss: 0.00059818
Epoch [20/300], Train Loss: 0.000592
Validation Loss: 0.00060061
Epoch [21/300], Train Loss: 0.000593
Validation Loss: 0.00059167
Epoch [22/300], Train Loss: 0.000593
Validation Loss: 0.00059168
Epoch [23/300], Train Loss: 0.000600
Validation Loss: 0.00059151
Epoch [24/300], Train Loss: 0.000596
Validation Loss: 0.00062007
Epoch [25/300], Train Loss: 0.000600
Validation Loss: 0.00058977
Epoch [26/300], Train Loss: 0.000591
Validation Loss: 0.00058860
Epoch [27/300], Train Loss: 0.000598
Validation Loss: 0.00059202
Epoch [28/300], Train Loss: 0.000596
Validation Loss: 0.00060663
Epoch [29/300], Train Loss: 0.000595
Validation Loss: 0.00058518
Epoch [30/300], Train Loss: 0.000594
Validation Loss: 0.00058288
Epoch [31/300], Train Loss: 0.000593
Validation Loss: 0.00059261
Epoch [32/300], Train Loss: 0.000588
Validation Loss: 0.00060955
Epoch [33/300], Train Loss: 0.000592
Validation Loss: 0.00058115
Epoch [34/300], Train Loss: 0.000585
Validation Loss: 0.00057971
Epoch [35/300], Train Loss: 0.000586
Validation Loss: 0.00059037
Epoch [36/300], Train Loss: 0.000588
Validation Loss: 0.00057807
Epoch [37/300], Train Loss: 0.000581
Validation Loss: 0.00057942
Epoch [38/300], Train Loss: 0.000581
Validation Loss: 0.00058045
Epoch [39/300], Train Loss: 0.000581
Validation Loss: 0.00058215
Epoch [40/300], Train Loss: 0.000575
Validation Loss: 0.00058799
Epoch [41/300], Train Loss: 0.000579
Validation Loss: 0.00057028
Epoch [42/300], Train Loss: 0.000575
Validation Loss: 0.00057710
Epoch [43/300], Train Loss: 0.000573
Validation Loss: 0.00060336
Epoch [44/300], Train Loss: 0.000575
Validation Loss: 0.00057425
Epoch [45/300], Train Loss: 0.000575
Validation Loss: 0.00056265
Epoch [46/300], Train Loss: 0.000568
Validation Loss: 0.00056023
Epoch [47/300], Train Loss: 0.000565
Validation Loss: 0.00055687
Epoch [48/300], Train Loss: 0.000566
Validation Loss: 0.00055861
Epoch [49/300], Train Loss: 0.000565
Validation Loss: 0.00057547
Epoch [50/300], Train Loss: 0.000562
Validation Loss: 0.00055361
Epoch [51/300], Train Loss: 0.000557
Validation Loss: 0.00055107
Epoch [52/300], Train Loss: 0.000555
Validation Loss: 0.00057511
Epoch [53/300], Train Loss: 0.000577
Validation Loss: 0.00056387
Epoch [54/300], Train Loss: 0.000555
Validation Loss: 0.00058165
Epoch [55/300], Train Loss: 0.000554
Validation Loss: 0.00055462
Epoch [56/300], Train Loss: 0.000542
Validation Loss: 0.00053011
Epoch [57/300], Train Loss: 0.000530
Validation Loss: 0.00052544
Epoch [58/300], Train Loss: 0.000530
Validation Loss: 0.00054205
Epoch [59/300], Train Loss: 0.000531
Validation Loss: 0.00050012
Epoch [60/300], Train Loss: 0.000517
Validation Loss: 0.00050018
Epoch [61/300], Train Loss: 0.000505
Validation Loss: 0.00055469
Epoch [62/300], Train Loss: 0.000535
Validation Loss: 0.00051359
Epoch [63/300], Train Loss: 0.000507
Validation Loss: 0.00051605
Epoch [64/300], Train Loss: 0.000507
Validation Loss: 0.00049169
Epoch [65/300], Train Loss: 0.000489
Validation Loss: 0.00049444
Epoch [66/300], Train Loss: 0.000494
Validation Loss: 0.00048764
Epoch [67/300], Train Loss: 0.000487
Validation Loss: 0.00048735
Epoch [68/300], Train Loss: 0.000494
Validation Loss: 0.00047753
Epoch [69/300], Train Loss: 0.000487
Validation Loss: 0.00061118
Epoch [70/300], Train Loss: 0.000558
Validation Loss: 0.00053136
Epoch [71/300], Train Loss: 0.000534
Validation Loss: 0.00069280
Epoch [72/300], Train Loss: 0.000564
Validation Loss: 0.00052363
Epoch [73/300], Train Loss: 0.000525
Validation Loss: 0.00052818
Epoch [74/300], Train Loss: 0.000516
Validation Loss: 0.00052026
Epoch [75/300], Train Loss: 0.000513
Validation Loss: 0.00049471
Epoch [76/300], Train Loss: 0.000493
Validation Loss: 0.00047715
Epoch [77/300], Train Loss: 0.000485
Validation Loss: 0.00053134
Epoch [78/300], Train Loss: 0.000487
Validation Loss: 0.00047663
Epoch [79/300], Train Loss: 0.000472
Validation Loss: 0.00047801
Epoch [80/300], Train Loss: 0.000481
Validation Loss: 0.00048585
Epoch [81/300], Train Loss: 0.000470
Validation Loss: 0.00046502
Epoch [82/300], Train Loss: 0.000467
Validation Loss: 0.00046703
Epoch [83/300], Train Loss: 0.000469
Validation Loss: 0.00048400
Epoch [84/300], Train Loss: 0.000465
Validation Loss: 0.00045595
Epoch [85/300], Train Loss: 0.000461
Validation Loss: 0.00050168
Epoch [86/300], Train Loss: 0.000474
Validation Loss: 0.00045517
Epoch [87/300], Train Loss: 0.000462
Validation Loss: 0.00045778
Epoch [88/300], Train Loss: 0.000458
Validation Loss: 0.00044979
Epoch [89/300], Train Loss: 0.000453
Validation Loss: 0.00044894
Epoch [90/300], Train Loss: 0.000455
Validation Loss: 0.00045451
Epoch [91/300], Train Loss: 0.000465
Validation Loss: 0.00045261
Epoch [92/300], Train Loss: 0.000453
Validation Loss: 0.00044279
Epoch [93/300], Train Loss: 0.000444
Validation Loss: 0.00044475
Epoch [94/300], Train Loss: 0.000449
Validation Loss: 0.00044572
Epoch [95/300], Train Loss: 0.000453
Validation Loss: 0.00044838
Epoch [96/300], Train Loss: 0.000451
Validation Loss: 0.00044561
Epoch [97/300], Train Loss: 0.000445
Validation Loss: 0.00048391
Epoch [98/300], Train Loss: 0.000446
Validation Loss: 0.00049341
Epoch [99/300], Train Loss: 0.000448
Validation Loss: 0.00045066
Epoch [100/300], Train Loss: 0.000447
Validation Loss: 0.00045638
Epoch [101/300], Train Loss: 0.000445
Validation Loss: 0.00044403
Epoch [102/300], Train Loss: 0.000444
Validation Loss: 0.00047472
Early stopping triggered

Evaluating model for: Fridge
Run 68/72 completed in 4085.72 seconds with: {'MAE': np.float32(30.34598), 'MSE': np.float32(1610.9578), 'RMSE': np.float32(40.136738), 'SAE': np.float32(0.05816946), 'NDE': np.float32(0.6936115)}

Run 69/72: hidden=512, seq_len=720, stride=0.5, num_layers=2
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.000678
Validation Loss: 0.00066764
Epoch [2/300], Train Loss: 0.000657
Validation Loss: 0.00066353
Epoch [3/300], Train Loss: 0.000654
Validation Loss: 0.00066054
Epoch [4/300], Train Loss: 0.000652
Validation Loss: 0.00065731
Epoch [5/300], Train Loss: 0.000642
Validation Loss: 0.00065331
Epoch [6/300], Train Loss: 0.000636
Validation Loss: 0.00065225
Epoch [7/300], Train Loss: 0.000626
Validation Loss: 0.00064730
Epoch [8/300], Train Loss: 0.000619
Validation Loss: 0.00062907
Epoch [9/300], Train Loss: 0.000610
Validation Loss: 0.00061853
Epoch [10/300], Train Loss: 0.000604
Validation Loss: 0.00062053
Epoch [11/300], Train Loss: 0.000601
Validation Loss: 0.00061128
Epoch [12/300], Train Loss: 0.000603
Validation Loss: 0.00061336
Epoch [13/300], Train Loss: 0.000599
Validation Loss: 0.00060800
Epoch [14/300], Train Loss: 0.000597
Validation Loss: 0.00060788
Epoch [15/300], Train Loss: 0.000596
Validation Loss: 0.00061788
Epoch [16/300], Train Loss: 0.000599
Validation Loss: 0.00061509
Epoch [17/300], Train Loss: 0.000599
Validation Loss: 0.00060453
Epoch [18/300], Train Loss: 0.000589
Validation Loss: 0.00060747
Epoch [19/300], Train Loss: 0.000593
Validation Loss: 0.00060386
Epoch [20/300], Train Loss: 0.000587
Validation Loss: 0.00060310
Epoch [21/300], Train Loss: 0.000590
Validation Loss: 0.00060174
Epoch [22/300], Train Loss: 0.000589
Validation Loss: 0.00060366
Epoch [23/300], Train Loss: 0.000589
Validation Loss: 0.00060182
Epoch [24/300], Train Loss: 0.000591
Validation Loss: 0.00060793
Epoch [25/300], Train Loss: 0.000589
Validation Loss: 0.00059982
Epoch [26/300], Train Loss: 0.000584
Validation Loss: 0.00059739
Epoch [27/300], Train Loss: 0.000585
Validation Loss: 0.00059539
Epoch [28/300], Train Loss: 0.000583
Validation Loss: 0.00060401
Epoch [29/300], Train Loss: 0.000584
Validation Loss: 0.00059459
Epoch [30/300], Train Loss: 0.000585
Validation Loss: 0.00059682
Epoch [31/300], Train Loss: 0.000579
Validation Loss: 0.00059345
Epoch [32/300], Train Loss: 0.000582
Validation Loss: 0.00059951
Epoch [33/300], Train Loss: 0.000581
Validation Loss: 0.00059290
Epoch [34/300], Train Loss: 0.000580
Validation Loss: 0.00059473
Epoch [35/300], Train Loss: 0.000578
Validation Loss: 0.00059863
Epoch [36/300], Train Loss: 0.000577
Validation Loss: 0.00059431
Epoch [37/300], Train Loss: 0.000577
Validation Loss: 0.00059188
Epoch [38/300], Train Loss: 0.000578
Validation Loss: 0.00058420
Epoch [39/300], Train Loss: 0.000571
Validation Loss: 0.00060787
Epoch [40/300], Train Loss: 0.000577
Validation Loss: 0.00059110
Epoch [41/300], Train Loss: 0.000574
Validation Loss: 0.00058451
Epoch [42/300], Train Loss: 0.000571
Validation Loss: 0.00058344
Epoch [43/300], Train Loss: 0.000572
Validation Loss: 0.00058856
Epoch [44/300], Train Loss: 0.000577
Validation Loss: 0.00059667
Epoch [45/300], Train Loss: 0.000573
Validation Loss: 0.00058397
Epoch [46/300], Train Loss: 0.000567
Validation Loss: 0.00058291
Epoch [47/300], Train Loss: 0.000571
Validation Loss: 0.00058004
Epoch [48/300], Train Loss: 0.000568
Validation Loss: 0.00057974
Epoch [49/300], Train Loss: 0.000567
Validation Loss: 0.00058177
Epoch [50/300], Train Loss: 0.000573
Validation Loss: 0.00057998
Epoch [51/300], Train Loss: 0.000570
Validation Loss: 0.00058193
Epoch [52/300], Train Loss: 0.000568
Validation Loss: 0.00057708
Epoch [53/300], Train Loss: 0.000565
Validation Loss: 0.00057816
Epoch [54/300], Train Loss: 0.000564
Validation Loss: 0.00057539
Epoch [55/300], Train Loss: 0.000567
Validation Loss: 0.00057414
Epoch [56/300], Train Loss: 0.000563
Validation Loss: 0.00057676
Epoch [57/300], Train Loss: 0.000567
Validation Loss: 0.00057505
Epoch [58/300], Train Loss: 0.000564
Validation Loss: 0.00057112
Epoch [59/300], Train Loss: 0.000561
Validation Loss: 0.00057337
Epoch [60/300], Train Loss: 0.000566
Validation Loss: 0.00057063
Epoch [61/300], Train Loss: 0.000558
Validation Loss: 0.00057046
Epoch [62/300], Train Loss: 0.000563
Validation Loss: 0.00057615
Epoch [63/300], Train Loss: 0.000565
Validation Loss: 0.00057902
Epoch [64/300], Train Loss: 0.000563
Validation Loss: 0.00057178
Epoch [65/300], Train Loss: 0.000558
Validation Loss: 0.00056881
Epoch [66/300], Train Loss: 0.000558
Validation Loss: 0.00056735
Epoch [67/300], Train Loss: 0.000559
Validation Loss: 0.00056833
Epoch [68/300], Train Loss: 0.000556
Validation Loss: 0.00056670
Epoch [69/300], Train Loss: 0.000562
Validation Loss: 0.00056501
Epoch [70/300], Train Loss: 0.000555
Validation Loss: 0.00056481
Epoch [71/300], Train Loss: 0.000557
Validation Loss: 0.00056373
Epoch [72/300], Train Loss: 0.000558
Validation Loss: 0.00056794
Epoch [73/300], Train Loss: 0.000557
Validation Loss: 0.00056154
Epoch [74/300], Train Loss: 0.000557
Validation Loss: 0.00056293
Epoch [75/300], Train Loss: 0.000555
Validation Loss: 0.00056031
Epoch [76/300], Train Loss: 0.000552
Validation Loss: 0.00055986
Epoch [77/300], Train Loss: 0.000555
Validation Loss: 0.00055804
Epoch [78/300], Train Loss: 0.000555
Validation Loss: 0.00055708
Epoch [79/300], Train Loss: 0.000551
Validation Loss: 0.00055367
Epoch [80/300], Train Loss: 0.000553
Validation Loss: 0.00055485
Epoch [81/300], Train Loss: 0.000553
Validation Loss: 0.00056248
Epoch [82/300], Train Loss: 0.000547
Validation Loss: 0.00055221
Epoch [83/300], Train Loss: 0.000550
Validation Loss: 0.00055270
Epoch [84/300], Train Loss: 0.000547
Validation Loss: 0.00054788
Epoch [85/300], Train Loss: 0.000546
Validation Loss: 0.00054728
Epoch [86/300], Train Loss: 0.000543
Validation Loss: 0.00054118
Epoch [87/300], Train Loss: 0.000531
Validation Loss: 0.00056061
Epoch [88/300], Train Loss: 0.000541
Validation Loss: 0.00053499
Epoch [89/300], Train Loss: 0.000534
Validation Loss: 0.00054549
Epoch [90/300], Train Loss: 0.000542
Validation Loss: 0.00053602
Epoch [91/300], Train Loss: 0.000536
Validation Loss: 0.00052777
Epoch [92/300], Train Loss: 0.000525
Validation Loss: 0.00051689
Epoch [93/300], Train Loss: 0.000525
Validation Loss: 0.00053289
Epoch [94/300], Train Loss: 0.000520
Validation Loss: 0.00052985
Epoch [95/300], Train Loss: 0.000529
Validation Loss: 0.00051558
Epoch [96/300], Train Loss: 0.000524
Validation Loss: 0.00053142
Epoch [97/300], Train Loss: 0.000523
Validation Loss: 0.00050634
Epoch [98/300], Train Loss: 0.000515
Validation Loss: 0.00050777
Epoch [99/300], Train Loss: 0.000510
Validation Loss: 0.00050351
Epoch [100/300], Train Loss: 0.000512
Validation Loss: 0.00050592
Epoch [101/300], Train Loss: 0.000514
Validation Loss: 0.00054746
Epoch [102/300], Train Loss: 0.000523
Validation Loss: 0.00051341
Epoch [103/300], Train Loss: 0.000522
Validation Loss: 0.00051948
Epoch [104/300], Train Loss: 0.000511
Validation Loss: 0.00050293
Epoch [105/300], Train Loss: 0.000519
Validation Loss: 0.00049937
Epoch [106/300], Train Loss: 0.000509
Validation Loss: 0.00053141
Epoch [107/300], Train Loss: 0.000513
Validation Loss: 0.00049717
Epoch [108/300], Train Loss: 0.000505
Validation Loss: 0.00049261
Epoch [109/300], Train Loss: 0.000497
Validation Loss: 0.00049837
Epoch [110/300], Train Loss: 0.000501
Validation Loss: 0.00052004
Epoch [111/300], Train Loss: 0.000520
Validation Loss: 0.00050462
Epoch [112/300], Train Loss: 0.000505
Validation Loss: 0.00049682
Epoch [113/300], Train Loss: 0.000501
Validation Loss: 0.00048920
Epoch [114/300], Train Loss: 0.000501
Validation Loss: 0.00049582
Epoch [115/300], Train Loss: 0.000500
Validation Loss: 0.00048983
Epoch [116/300], Train Loss: 0.000509
Validation Loss: 0.00050431
Epoch [117/300], Train Loss: 0.000504
Validation Loss: 0.00049164
Epoch [118/300], Train Loss: 0.000502
Validation Loss: 0.00048876
Epoch [119/300], Train Loss: 0.000497
Validation Loss: 0.00049697
Epoch [120/300], Train Loss: 0.000503
Validation Loss: 0.00048818
Epoch [121/300], Train Loss: 0.000494
Validation Loss: 0.00048789
Epoch [122/300], Train Loss: 0.000499
Validation Loss: 0.00049484
Epoch [123/300], Train Loss: 0.000505
Validation Loss: 0.00049111
Epoch [124/300], Train Loss: 0.000499
Validation Loss: 0.00048412
Epoch [125/300], Train Loss: 0.000494
Validation Loss: 0.00048639
Epoch [126/300], Train Loss: 0.000495
Validation Loss: 0.00048498
Epoch [127/300], Train Loss: 0.000492
Validation Loss: 0.00048280
Epoch [128/300], Train Loss: 0.000493
Validation Loss: 0.00048593
Epoch [129/300], Train Loss: 0.000493
Validation Loss: 0.00050324
Epoch [130/300], Train Loss: 0.000495
Validation Loss: 0.00048864
Epoch [131/300], Train Loss: 0.000489
Validation Loss: 0.00047670
Epoch [132/300], Train Loss: 0.000491
Validation Loss: 0.00051373
Epoch [133/300], Train Loss: 0.000498
Validation Loss: 0.00047968
Epoch [134/300], Train Loss: 0.000488
Validation Loss: 0.00047870
Epoch [135/300], Train Loss: 0.000488
Validation Loss: 0.00047657
Epoch [136/300], Train Loss: 0.000486
Validation Loss: 0.00047568
Epoch [137/300], Train Loss: 0.000487
Validation Loss: 0.00049208
Epoch [138/300], Train Loss: 0.000496
Validation Loss: 0.00048186
Epoch [139/300], Train Loss: 0.000486
Validation Loss: 0.00047722
Epoch [140/300], Train Loss: 0.000487
Validation Loss: 0.00047404
Epoch [141/300], Train Loss: 0.000486
Validation Loss: 0.00047255
Epoch [142/300], Train Loss: 0.000489
Validation Loss: 0.00047097
Epoch [143/300], Train Loss: 0.000487
Validation Loss: 0.00047194
Epoch [144/300], Train Loss: 0.000487
Validation Loss: 0.00046973
Epoch [145/300], Train Loss: 0.000483
Validation Loss: 0.00047465
Epoch [146/300], Train Loss: 0.000485
Validation Loss: 0.00048358
Epoch [147/300], Train Loss: 0.000495
Validation Loss: 0.00047230
Epoch [148/300], Train Loss: 0.000486
Validation Loss: 0.00047308
Epoch [149/300], Train Loss: 0.000485
Validation Loss: 0.00047210
Epoch [150/300], Train Loss: 0.000480
Validation Loss: 0.00046994
Epoch [151/300], Train Loss: 0.000479
Validation Loss: 0.00046536
Epoch [152/300], Train Loss: 0.000483
Validation Loss: 0.00047194
Epoch [153/300], Train Loss: 0.000479
Validation Loss: 0.00046874
Epoch [154/300], Train Loss: 0.000484
Validation Loss: 0.00046755
Epoch [155/300], Train Loss: 0.000480
Validation Loss: 0.00046441
Epoch [156/300], Train Loss: 0.000476
Validation Loss: 0.00046223
Epoch [157/300], Train Loss: 0.000480
Validation Loss: 0.00048371
Epoch [158/300], Train Loss: 0.000488
Validation Loss: 0.00047185
Epoch [159/300], Train Loss: 0.000479
Validation Loss: 0.00046808
Epoch [160/300], Train Loss: 0.000475
Validation Loss: 0.00046139
Epoch [161/300], Train Loss: 0.000473
Validation Loss: 0.00046039
Epoch [162/300], Train Loss: 0.000472
Validation Loss: 0.00045934
Epoch [163/300], Train Loss: 0.000474
Validation Loss: 0.00045852
Epoch [164/300], Train Loss: 0.000472
Validation Loss: 0.00046458
Epoch [165/300], Train Loss: 0.000476
Validation Loss: 0.00047500
Epoch [166/300], Train Loss: 0.000473
Validation Loss: 0.00046500
Epoch [167/300], Train Loss: 0.000472
Validation Loss: 0.00045816
Epoch [168/300], Train Loss: 0.000469
Validation Loss: 0.00046141
Epoch [169/300], Train Loss: 0.000476
Validation Loss: 0.00045974
Epoch [170/300], Train Loss: 0.000471
Validation Loss: 0.00045654
Epoch [171/300], Train Loss: 0.000474
Validation Loss: 0.00045946
Epoch [172/300], Train Loss: 0.000467
Validation Loss: 0.00045958
Epoch [173/300], Train Loss: 0.000470
Validation Loss: 0.00046323
Epoch [174/300], Train Loss: 0.000470
Validation Loss: 0.00045649
Epoch [175/300], Train Loss: 0.000471
Validation Loss: 0.00046108
Epoch [176/300], Train Loss: 0.000469
Validation Loss: 0.00045582
Epoch [177/300], Train Loss: 0.000470
Validation Loss: 0.00045382
Epoch [178/300], Train Loss: 0.000468
Validation Loss: 0.00045322
Epoch [179/300], Train Loss: 0.000468
Validation Loss: 0.00045247
Epoch [180/300], Train Loss: 0.000467
Validation Loss: 0.00046158
Epoch [181/300], Train Loss: 0.000467
Validation Loss: 0.00045387
Epoch [182/300], Train Loss: 0.000464
Validation Loss: 0.00046274
Epoch [183/300], Train Loss: 0.000470
Validation Loss: 0.00045534
Epoch [184/300], Train Loss: 0.000467
Validation Loss: 0.00045292
Epoch [185/300], Train Loss: 0.000469
Validation Loss: 0.00045141
Epoch [186/300], Train Loss: 0.000464
Validation Loss: 0.00045525
Epoch [187/300], Train Loss: 0.000464
Validation Loss: 0.00045042
Epoch [188/300], Train Loss: 0.000466
Validation Loss: 0.00045625
Epoch [189/300], Train Loss: 0.000467
Validation Loss: 0.00044947
Epoch [190/300], Train Loss: 0.000464
Validation Loss: 0.00044843
Epoch [191/300], Train Loss: 0.000464
Validation Loss: 0.00044787
Epoch [192/300], Train Loss: 0.000466
Validation Loss: 0.00045233
Epoch [193/300], Train Loss: 0.000462
Validation Loss: 0.00044687
Epoch [194/300], Train Loss: 0.000460
Validation Loss: 0.00044810
Epoch [195/300], Train Loss: 0.000458
Validation Loss: 0.00044839
Epoch [196/300], Train Loss: 0.000458
Validation Loss: 0.00044408
Epoch [197/300], Train Loss: 0.000454
Validation Loss: 0.00044880
Epoch [198/300], Train Loss: 0.000456
Validation Loss: 0.00044614
Epoch [199/300], Train Loss: 0.000456
Validation Loss: 0.00044624
Epoch [200/300], Train Loss: 0.000458
Validation Loss: 0.00044520
Epoch [201/300], Train Loss: 0.000457
Validation Loss: 0.00044820
Epoch [202/300], Train Loss: 0.000462
Validation Loss: 0.00044800
Epoch [203/300], Train Loss: 0.000460
Validation Loss: 0.00044575
Epoch [204/300], Train Loss: 0.000456
Validation Loss: 0.00044471
Epoch [205/300], Train Loss: 0.000459
Validation Loss: 0.00044378
Epoch [206/300], Train Loss: 0.000455
Validation Loss: 0.00045191
Epoch [207/300], Train Loss: 0.000461
Validation Loss: 0.00044607
Epoch [208/300], Train Loss: 0.000454
Validation Loss: 0.00044315
Epoch [209/300], Train Loss: 0.000458
Validation Loss: 0.00044659
Epoch [210/300], Train Loss: 0.000458
Validation Loss: 0.00044398
Epoch [211/300], Train Loss: 0.000452
Validation Loss: 0.00043942
Epoch [212/300], Train Loss: 0.000456
Validation Loss: 0.00045736
Epoch [213/300], Train Loss: 0.000455
Validation Loss: 0.00044863
Epoch [214/300], Train Loss: 0.000457
Validation Loss: 0.00044102
Epoch [215/300], Train Loss: 0.000454
Validation Loss: 0.00044205
Epoch [216/300], Train Loss: 0.000453
Validation Loss: 0.00043940
Epoch [217/300], Train Loss: 0.000452
Validation Loss: 0.00044646
Epoch [218/300], Train Loss: 0.000453
Validation Loss: 0.00044727
Epoch [219/300], Train Loss: 0.000453
Validation Loss: 0.00043908
Epoch [220/300], Train Loss: 0.000446
Validation Loss: 0.00044033
Epoch [221/300], Train Loss: 0.000448
Validation Loss: 0.00043863
Epoch [222/300], Train Loss: 0.000450
Validation Loss: 0.00043664
Epoch [223/300], Train Loss: 0.000450
Validation Loss: 0.00044262
Epoch [224/300], Train Loss: 0.000454
Validation Loss: 0.00044327
Epoch [225/300], Train Loss: 0.000451
Validation Loss: 0.00043779
Epoch [226/300], Train Loss: 0.000453
Validation Loss: 0.00044089
Epoch [227/300], Train Loss: 0.000449
Validation Loss: 0.00043772
Epoch [228/300], Train Loss: 0.000456
Validation Loss: 0.00044402
Epoch [229/300], Train Loss: 0.000447
Validation Loss: 0.00043379
Epoch [230/300], Train Loss: 0.000446
Validation Loss: 0.00043503
Epoch [231/300], Train Loss: 0.000443
Validation Loss: 0.00043494
Epoch [232/300], Train Loss: 0.000445
Validation Loss: 0.00043858
Epoch [233/300], Train Loss: 0.000445
Validation Loss: 0.00043965
Epoch [234/300], Train Loss: 0.000451
Validation Loss: 0.00043410
Epoch [235/300], Train Loss: 0.000443
Validation Loss: 0.00043047
Epoch [236/300], Train Loss: 0.000445
Validation Loss: 0.00043533
Epoch [237/300], Train Loss: 0.000446
Validation Loss: 0.00043418
Epoch [238/300], Train Loss: 0.000446
Validation Loss: 0.00043532
Epoch [239/300], Train Loss: 0.000444
Validation Loss: 0.00043356
Epoch [240/300], Train Loss: 0.000440
Validation Loss: 0.00042885
Epoch [241/300], Train Loss: 0.000445
Validation Loss: 0.00042998
Epoch [242/300], Train Loss: 0.000439
Validation Loss: 0.00042885
Epoch [243/300], Train Loss: 0.000438
Validation Loss: 0.00042921
Epoch [244/300], Train Loss: 0.000443
Validation Loss: 0.00046212
Epoch [245/300], Train Loss: 0.000454
Validation Loss: 0.00043295
Epoch [246/300], Train Loss: 0.000443
Validation Loss: 0.00042908
Epoch [247/300], Train Loss: 0.000439
Validation Loss: 0.00042776
Epoch [248/300], Train Loss: 0.000444
Validation Loss: 0.00042997
Epoch [249/300], Train Loss: 0.000439
Validation Loss: 0.00043110
Epoch [250/300], Train Loss: 0.000436
Validation Loss: 0.00042911
Epoch [251/300], Train Loss: 0.000436
Validation Loss: 0.00043526
Epoch [252/300], Train Loss: 0.000437
Validation Loss: 0.00042486
Epoch [253/300], Train Loss: 0.000436
Validation Loss: 0.00042509
Epoch [254/300], Train Loss: 0.000440
Validation Loss: 0.00042424
Epoch [255/300], Train Loss: 0.000434
Validation Loss: 0.00042511
Epoch [256/300], Train Loss: 0.000436
Validation Loss: 0.00042553
Epoch [257/300], Train Loss: 0.000430
Validation Loss: 0.00042330
Epoch [258/300], Train Loss: 0.000434
Validation Loss: 0.00043062
Epoch [259/300], Train Loss: 0.000437
Validation Loss: 0.00043004
Epoch [260/300], Train Loss: 0.000437
Validation Loss: 0.00042348
Epoch [261/300], Train Loss: 0.000435
Validation Loss: 0.00042074
Epoch [262/300], Train Loss: 0.000431
Validation Loss: 0.00042190
Epoch [263/300], Train Loss: 0.000434
Validation Loss: 0.00042403
Epoch [264/300], Train Loss: 0.000437
Validation Loss: 0.00042783
Epoch [265/300], Train Loss: 0.000433
Validation Loss: 0.00041887
Epoch [266/300], Train Loss: 0.000430
Validation Loss: 0.00042236
Epoch [267/300], Train Loss: 0.000430
Validation Loss: 0.00042207
Epoch [268/300], Train Loss: 0.000433
Validation Loss: 0.00042648
Epoch [269/300], Train Loss: 0.000434
Validation Loss: 0.00042338
Epoch [270/300], Train Loss: 0.000431
Validation Loss: 0.00042311
Epoch [271/300], Train Loss: 0.000433
Validation Loss: 0.00042186
Epoch [272/300], Train Loss: 0.000437
Validation Loss: 0.00043611
Epoch [273/300], Train Loss: 0.000435
Validation Loss: 0.00042036
Epoch [274/300], Train Loss: 0.000432
Validation Loss: 0.00042204
Epoch [275/300], Train Loss: 0.000434
Validation Loss: 0.00041788
Epoch [276/300], Train Loss: 0.000425
Validation Loss: 0.00041628
Epoch [277/300], Train Loss: 0.000425
Validation Loss: 0.00042002
Epoch [278/300], Train Loss: 0.000428
Validation Loss: 0.00041755
Epoch [279/300], Train Loss: 0.000427
Validation Loss: 0.00042457
Epoch [280/300], Train Loss: 0.000430
Validation Loss: 0.00041565
Epoch [281/300], Train Loss: 0.000423
Validation Loss: 0.00042089
Epoch [282/300], Train Loss: 0.000426
Validation Loss: 0.00041488
Epoch [283/300], Train Loss: 0.000426
Validation Loss: 0.00042384
Epoch [284/300], Train Loss: 0.000426
Validation Loss: 0.00042710
Epoch [285/300], Train Loss: 0.000429
Validation Loss: 0.00041441
Epoch [286/300], Train Loss: 0.000426
Validation Loss: 0.00041505
Epoch [287/300], Train Loss: 0.000427
Validation Loss: 0.00042023
Epoch [288/300], Train Loss: 0.000423
Validation Loss: 0.00041484
Epoch [289/300], Train Loss: 0.000423
Validation Loss: 0.00041548
Epoch [290/300], Train Loss: 0.000421
Validation Loss: 0.00041210
Epoch [291/300], Train Loss: 0.000424
Validation Loss: 0.00041134
Epoch [292/300], Train Loss: 0.000424
Validation Loss: 0.00041610
Epoch [293/300], Train Loss: 0.000426
Validation Loss: 0.00040982
Epoch [294/300], Train Loss: 0.000422
Validation Loss: 0.00041053
Epoch [295/300], Train Loss: 0.000417
Validation Loss: 0.00041220
Epoch [296/300], Train Loss: 0.000421
Validation Loss: 0.00040954
Epoch [297/300], Train Loss: 0.000420
Validation Loss: 0.00040827
Epoch [298/300], Train Loss: 0.000417
Validation Loss: 0.00041769
Epoch [299/300], Train Loss: 0.000418
Validation Loss: 0.00040903
Epoch [300/300], Train Loss: 0.000427
Validation Loss: 0.00040815

Evaluating model for: Fridge
Run 69/72 completed in 2945.32 seconds with: {'MAE': np.float32(28.557281), 'MSE': np.float32(1409.4998), 'RMSE': np.float32(37.543304), 'SAE': np.float32(0.016903546), 'NDE': np.float32(0.6505994)}

Run 70/72: hidden=512, seq_len=720, stride=0.5, num_layers=3
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.000661
Validation Loss: 0.00066621
Epoch [2/300], Train Loss: 0.000656
Validation Loss: 0.00066396
Epoch [3/300], Train Loss: 0.000655
Validation Loss: 0.00066218
Epoch [4/300], Train Loss: 0.000653
Validation Loss: 0.00066037
Epoch [5/300], Train Loss: 0.000643
Validation Loss: 0.00065627
Epoch [6/300], Train Loss: 0.000634
Validation Loss: 0.00064435
Epoch [7/300], Train Loss: 0.000620
Validation Loss: 0.00064127
Epoch [8/300], Train Loss: 0.000613
Validation Loss: 0.00062113
Epoch [9/300], Train Loss: 0.000608
Validation Loss: 0.00062104
Epoch [10/300], Train Loss: 0.000605
Validation Loss: 0.00063518
Epoch [11/300], Train Loss: 0.000606
Validation Loss: 0.00061720
Epoch [12/300], Train Loss: 0.000607
Validation Loss: 0.00061783
Epoch [13/300], Train Loss: 0.000603
Validation Loss: 0.00061570
Epoch [14/300], Train Loss: 0.000603
Validation Loss: 0.00061962
Epoch [15/300], Train Loss: 0.000603
Validation Loss: 0.00062566
Epoch [16/300], Train Loss: 0.000604
Validation Loss: 0.00061585
Epoch [17/300], Train Loss: 0.000602
Validation Loss: 0.00061323
Epoch [18/300], Train Loss: 0.000593
Validation Loss: 0.00061353
Epoch [19/300], Train Loss: 0.000600
Validation Loss: 0.00061538
Epoch [20/300], Train Loss: 0.000594
Validation Loss: 0.00061083
Epoch [21/300], Train Loss: 0.000597
Validation Loss: 0.00060939
Epoch [22/300], Train Loss: 0.000597
Validation Loss: 0.00061157
Epoch [23/300], Train Loss: 0.000595
Validation Loss: 0.00060904
Epoch [24/300], Train Loss: 0.000598
Validation Loss: 0.00061023
Epoch [25/300], Train Loss: 0.000593
Validation Loss: 0.00060741
Epoch [26/300], Train Loss: 0.000591
Validation Loss: 0.00060684
Epoch [27/300], Train Loss: 0.000593
Validation Loss: 0.00061198
Epoch [28/300], Train Loss: 0.000593
Validation Loss: 0.00061780
Epoch [29/300], Train Loss: 0.000594
Validation Loss: 0.00060795
Epoch [30/300], Train Loss: 0.000593
Validation Loss: 0.00061099
Epoch [31/300], Train Loss: 0.000589
Validation Loss: 0.00060385
Epoch [32/300], Train Loss: 0.000590
Validation Loss: 0.00060630
Epoch [33/300], Train Loss: 0.000588
Validation Loss: 0.00060245
Epoch [34/300], Train Loss: 0.000591
Validation Loss: 0.00061064
Epoch [35/300], Train Loss: 0.000588
Validation Loss: 0.00060869
Epoch [36/300], Train Loss: 0.000587
Validation Loss: 0.00060046
Epoch [37/300], Train Loss: 0.000586
Validation Loss: 0.00060525
Epoch [38/300], Train Loss: 0.000588
Validation Loss: 0.00059528
Epoch [39/300], Train Loss: 0.000582
Validation Loss: 0.00061458
Epoch [40/300], Train Loss: 0.000589
Validation Loss: 0.00059558
Epoch [41/300], Train Loss: 0.000584
Validation Loss: 0.00060349
Epoch [42/300], Train Loss: 0.000581
Validation Loss: 0.00059449
Epoch [43/300], Train Loss: 0.000580
Validation Loss: 0.00059281
Epoch [44/300], Train Loss: 0.000585
Validation Loss: 0.00059833
Epoch [45/300], Train Loss: 0.000581
Validation Loss: 0.00059262
Epoch [46/300], Train Loss: 0.000576
Validation Loss: 0.00059037
Epoch [47/300], Train Loss: 0.000578
Validation Loss: 0.00058918
Epoch [48/300], Train Loss: 0.000576
Validation Loss: 0.00058972
Epoch [49/300], Train Loss: 0.000575
Validation Loss: 0.00058953
Epoch [50/300], Train Loss: 0.000579
Validation Loss: 0.00058803
Epoch [51/300], Train Loss: 0.000576
Validation Loss: 0.00058524
Epoch [52/300], Train Loss: 0.000573
Validation Loss: 0.00057929
Epoch [53/300], Train Loss: 0.000568
Validation Loss: 0.00058275
Epoch [54/300], Train Loss: 0.000567
Validation Loss: 0.00057669
Epoch [55/300], Train Loss: 0.000569
Validation Loss: 0.00057251
Epoch [56/300], Train Loss: 0.000564
Validation Loss: 0.00057272
Epoch [57/300], Train Loss: 0.000566
Validation Loss: 0.00057130
Epoch [58/300], Train Loss: 0.000564
Validation Loss: 0.00056712
Epoch [59/300], Train Loss: 0.000562
Validation Loss: 0.00056570
Epoch [60/300], Train Loss: 0.000565
Validation Loss: 0.00057395
Epoch [61/300], Train Loss: 0.000561
Validation Loss: 0.00056384
Epoch [62/300], Train Loss: 0.000563
Validation Loss: 0.00056423
Epoch [63/300], Train Loss: 0.000561
Validation Loss: 0.00056067
Epoch [64/300], Train Loss: 0.000556
Validation Loss: 0.00056137
Epoch [65/300], Train Loss: 0.000553
Validation Loss: 0.00055679
Epoch [66/300], Train Loss: 0.000553
Validation Loss: 0.00055897
Epoch [67/300], Train Loss: 0.000557
Validation Loss: 0.00055663
Epoch [68/300], Train Loss: 0.000553
Validation Loss: 0.00055918
Epoch [69/300], Train Loss: 0.000556
Validation Loss: 0.00056173
Epoch [70/300], Train Loss: 0.000554
Validation Loss: 0.00055645
Epoch [71/300], Train Loss: 0.000552
Validation Loss: 0.00055258
Epoch [72/300], Train Loss: 0.000553
Validation Loss: 0.00055335
Epoch [73/300], Train Loss: 0.000553
Validation Loss: 0.00055513
Epoch [74/300], Train Loss: 0.000553
Validation Loss: 0.00055793
Epoch [75/300], Train Loss: 0.000553
Validation Loss: 0.00055322
Epoch [76/300], Train Loss: 0.000550
Validation Loss: 0.00054846
Epoch [77/300], Train Loss: 0.000549
Validation Loss: 0.00054884
Epoch [78/300], Train Loss: 0.000547
Validation Loss: 0.00054785
Epoch [79/300], Train Loss: 0.000545
Validation Loss: 0.00054561
Epoch [80/300], Train Loss: 0.000549
Validation Loss: 0.00054402
Epoch [81/300], Train Loss: 0.000546
Validation Loss: 0.00054959
Epoch [82/300], Train Loss: 0.000545
Validation Loss: 0.00054329
Epoch [83/300], Train Loss: 0.000544
Validation Loss: 0.00054130
Epoch [84/300], Train Loss: 0.000541
Validation Loss: 0.00053971
Epoch [85/300], Train Loss: 0.000545
Validation Loss: 0.00055030
Epoch [86/300], Train Loss: 0.000544
Validation Loss: 0.00053833
Epoch [87/300], Train Loss: 0.000536
Validation Loss: 0.00054346
Epoch [88/300], Train Loss: 0.000539
Validation Loss: 0.00053587
Epoch [89/300], Train Loss: 0.000538
Validation Loss: 0.00054851
Epoch [90/300], Train Loss: 0.000539
Validation Loss: 0.00053547
Epoch [91/300], Train Loss: 0.000541
Validation Loss: 0.00053945
Epoch [92/300], Train Loss: 0.000538
Validation Loss: 0.00054020
Epoch [93/300], Train Loss: 0.000536
Validation Loss: 0.00053315
Epoch [94/300], Train Loss: 0.000532
Validation Loss: 0.00053288
Epoch [95/300], Train Loss: 0.000536
Validation Loss: 0.00053244
Epoch [96/300], Train Loss: 0.000532
Validation Loss: 0.00052972
Epoch [97/300], Train Loss: 0.000532
Validation Loss: 0.00053866
Epoch [98/300], Train Loss: 0.000532
Validation Loss: 0.00052944
Epoch [99/300], Train Loss: 0.000535
Validation Loss: 0.00053156
Epoch [100/300], Train Loss: 0.000534
Validation Loss: 0.00053779
Epoch [101/300], Train Loss: 0.000534
Validation Loss: 0.00053696
Epoch [102/300], Train Loss: 0.000534
Validation Loss: 0.00052314
Epoch [103/300], Train Loss: 0.000528
Validation Loss: 0.00052087
Epoch [104/300], Train Loss: 0.000524
Validation Loss: 0.00052518
Epoch [105/300], Train Loss: 0.000530
Validation Loss: 0.00051719
Epoch [106/300], Train Loss: 0.000528
Validation Loss: 0.00052570
Epoch [107/300], Train Loss: 0.000522
Validation Loss: 0.00051830
Epoch [108/300], Train Loss: 0.000521
Validation Loss: 0.00051874
Epoch [109/300], Train Loss: 0.000517
Validation Loss: 0.00051463
Epoch [110/300], Train Loss: 0.000518
Validation Loss: 0.00051326
Epoch [111/300], Train Loss: 0.000516
Validation Loss: 0.00052149
Epoch [112/300], Train Loss: 0.000519
Validation Loss: 0.00050735
Epoch [113/300], Train Loss: 0.000513
Validation Loss: 0.00051606
Epoch [114/300], Train Loss: 0.000516
Validation Loss: 0.00052026
Epoch [115/300], Train Loss: 0.000525
Validation Loss: 0.00051577
Epoch [116/300], Train Loss: 0.000515
Validation Loss: 0.00050640
Epoch [117/300], Train Loss: 0.000510
Validation Loss: 0.00050361
Epoch [118/300], Train Loss: 0.000517
Validation Loss: 0.00051662
Epoch [119/300], Train Loss: 0.000510
Validation Loss: 0.00050430
Epoch [120/300], Train Loss: 0.000506
Validation Loss: 0.00050358
Epoch [121/300], Train Loss: 0.000507
Validation Loss: 0.00052920
Epoch [122/300], Train Loss: 0.000513
Validation Loss: 0.00050316
Epoch [123/300], Train Loss: 0.000510
Validation Loss: 0.00051283
Epoch [124/300], Train Loss: 0.000515
Validation Loss: 0.00050010
Epoch [125/300], Train Loss: 0.000508
Validation Loss: 0.00049328
Epoch [126/300], Train Loss: 0.000502
Validation Loss: 0.00049869
Epoch [127/300], Train Loss: 0.000502
Validation Loss: 0.00049315
Epoch [128/300], Train Loss: 0.000503
Validation Loss: 0.00048798
Epoch [129/300], Train Loss: 0.000497
Validation Loss: 0.00049183
Epoch [130/300], Train Loss: 0.000501
Validation Loss: 0.00048690
Epoch [131/300], Train Loss: 0.000502
Validation Loss: 0.00049044
Epoch [132/300], Train Loss: 0.000498
Validation Loss: 0.00048370
Epoch [133/300], Train Loss: 0.000497
Validation Loss: 0.00049431
Epoch [134/300], Train Loss: 0.000489
Validation Loss: 0.00048209
Epoch [135/300], Train Loss: 0.000497
Validation Loss: 0.00049084
Epoch [136/300], Train Loss: 0.000495
Validation Loss: 0.00048142
Epoch [137/300], Train Loss: 0.000495
Validation Loss: 0.00048703
Epoch [138/300], Train Loss: 0.000492
Validation Loss: 0.00047966
Epoch [139/300], Train Loss: 0.000503
Validation Loss: 0.00049115
Epoch [140/300], Train Loss: 0.000500
Validation Loss: 0.00048547
Epoch [141/300], Train Loss: 0.000489
Validation Loss: 0.00048262
Epoch [142/300], Train Loss: 0.000482
Validation Loss: 0.00047805
Epoch [143/300], Train Loss: 0.000485
Validation Loss: 0.00048467
Epoch [144/300], Train Loss: 0.000507
Validation Loss: 0.00048562
Epoch [145/300], Train Loss: 0.000491
Validation Loss: 0.00048429
Epoch [146/300], Train Loss: 0.000500
Validation Loss: 0.00049019
Epoch [147/300], Train Loss: 0.000494
Validation Loss: 0.00049869
Epoch [148/300], Train Loss: 0.000489
Validation Loss: 0.00049010
Epoch [149/300], Train Loss: 0.000485
Validation Loss: 0.00047521
Epoch [150/300], Train Loss: 0.000488
Validation Loss: 0.00048494
Epoch [151/300], Train Loss: 0.000485
Validation Loss: 0.00049036
Epoch [152/300], Train Loss: 0.000489
Validation Loss: 0.00047621
Epoch [153/300], Train Loss: 0.000483
Validation Loss: 0.00047245
Epoch [154/300], Train Loss: 0.000494
Validation Loss: 0.00050902
Epoch [155/300], Train Loss: 0.000493
Validation Loss: 0.00047560
Epoch [156/300], Train Loss: 0.000480
Validation Loss: 0.00047587
Epoch [157/300], Train Loss: 0.000485
Validation Loss: 0.00048340
Epoch [158/300], Train Loss: 0.000490
Validation Loss: 0.00047867
Epoch [159/300], Train Loss: 0.000480
Validation Loss: 0.00047173
Epoch [160/300], Train Loss: 0.000480
Validation Loss: 0.00048539
Epoch [161/300], Train Loss: 0.000487
Validation Loss: 0.00048692
Epoch [162/300], Train Loss: 0.000486
Validation Loss: 0.00047516
Epoch [163/300], Train Loss: 0.000481
Validation Loss: 0.00047058
Epoch [164/300], Train Loss: 0.000504
Validation Loss: 0.00052581
Epoch [165/300], Train Loss: 0.000519
Validation Loss: 0.00050785
Epoch [166/300], Train Loss: 0.000508
Validation Loss: 0.00047820
Epoch [167/300], Train Loss: 0.000486
Validation Loss: 0.00047550
Epoch [168/300], Train Loss: 0.000477
Validation Loss: 0.00047181
Epoch [169/300], Train Loss: 0.000479
Validation Loss: 0.00047466
Epoch [170/300], Train Loss: 0.000479
Validation Loss: 0.00047227
Epoch [171/300], Train Loss: 0.000476
Validation Loss: 0.00047031
Epoch [172/300], Train Loss: 0.000471
Validation Loss: 0.00048336
Epoch [173/300], Train Loss: 0.000490
Validation Loss: 0.00048369
Epoch [174/300], Train Loss: 0.000481
Validation Loss: 0.00046970
Epoch [175/300], Train Loss: 0.000477
Validation Loss: 0.00047515
Epoch [176/300], Train Loss: 0.000491
Validation Loss: 0.00047617
Epoch [177/300], Train Loss: 0.000482
Validation Loss: 0.00049974
Epoch [178/300], Train Loss: 0.000494
Validation Loss: 0.00047354
Epoch [179/300], Train Loss: 0.000474
Validation Loss: 0.00046709
Epoch [180/300], Train Loss: 0.000469
Validation Loss: 0.00046553
Epoch [181/300], Train Loss: 0.000469
Validation Loss: 0.00046973
Epoch [182/300], Train Loss: 0.000477
Validation Loss: 0.00047074
Epoch [183/300], Train Loss: 0.000472
Validation Loss: 0.00046573
Epoch [184/300], Train Loss: 0.000467
Validation Loss: 0.00046697
Epoch [185/300], Train Loss: 0.000471
Validation Loss: 0.00046407
Epoch [186/300], Train Loss: 0.000476
Validation Loss: 0.00046981
Epoch [187/300], Train Loss: 0.000468
Validation Loss: 0.00046285
Epoch [188/300], Train Loss: 0.000470
Validation Loss: 0.00046303
Epoch [189/300], Train Loss: 0.000521
Validation Loss: 0.00062089
Epoch [190/300], Train Loss: 0.000595
Validation Loss: 0.00055241
Epoch [191/300], Train Loss: 0.000551
Validation Loss: 0.00053519
Epoch [192/300], Train Loss: 0.000537
Validation Loss: 0.00052603
Epoch [193/300], Train Loss: 0.000530
Validation Loss: 0.00051827
Epoch [194/300], Train Loss: 0.000523
Validation Loss: 0.00051665
Epoch [195/300], Train Loss: 0.000516
Validation Loss: 0.00051165
Epoch [196/300], Train Loss: 0.000518
Validation Loss: 0.00050718
Epoch [197/300], Train Loss: 0.000507
Validation Loss: 0.00050421
Early stopping triggered

Evaluating model for: Fridge
Run 70/72 completed in 2666.33 seconds with: {'MAE': np.float32(33.13187), 'MSE': np.float32(1715.0471), 'RMSE': np.float32(41.413128), 'SAE': np.float32(0.013146123), 'NDE': np.float32(0.71766067)}

Run 71/72: hidden=512, seq_len=720, stride=0.5, num_layers=4
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.000670
Validation Loss: 0.00066991
Epoch [2/300], Train Loss: 0.000659
Validation Loss: 0.00066670
Epoch [3/300], Train Loss: 0.000659
Validation Loss: 0.00066582
Epoch [4/300], Train Loss: 0.000661
Validation Loss: 0.00066533
Epoch [5/300], Train Loss: 0.000657
Validation Loss: 0.00066474
Epoch [6/300], Train Loss: 0.000657
Validation Loss: 0.00066283
Epoch [7/300], Train Loss: 0.000649
Validation Loss: 0.00065814
Epoch [8/300], Train Loss: 0.000634
Validation Loss: 0.00063625
Epoch [9/300], Train Loss: 0.000621
Validation Loss: 0.00063016
Epoch [10/300], Train Loss: 0.000614
Validation Loss: 0.00062601
Epoch [11/300], Train Loss: 0.000610
Validation Loss: 0.00062021
Epoch [12/300], Train Loss: 0.000611
Validation Loss: 0.00062358
Epoch [13/300], Train Loss: 0.000606
Validation Loss: 0.00061654
Epoch [14/300], Train Loss: 0.000604
Validation Loss: 0.00061859
Epoch [15/300], Train Loss: 0.000603
Validation Loss: 0.00062482
Epoch [16/300], Train Loss: 0.000604
Validation Loss: 0.00062147
Epoch [17/300], Train Loss: 0.000604
Validation Loss: 0.00061498
Epoch [18/300], Train Loss: 0.000595
Validation Loss: 0.00061292
Epoch [19/300], Train Loss: 0.000601
Validation Loss: 0.00061432
Epoch [20/300], Train Loss: 0.000595
Validation Loss: 0.00061124
Epoch [21/300], Train Loss: 0.000597
Validation Loss: 0.00060706
Epoch [22/300], Train Loss: 0.000596
Validation Loss: 0.00061447
Epoch [23/300], Train Loss: 0.000596
Validation Loss: 0.00060639
Epoch [24/300], Train Loss: 0.000596
Validation Loss: 0.00061226
Epoch [25/300], Train Loss: 0.000595
Validation Loss: 0.00060356
Epoch [26/300], Train Loss: 0.000592
Validation Loss: 0.00060818
Epoch [27/300], Train Loss: 0.000594
Validation Loss: 0.00061105
Epoch [28/300], Train Loss: 0.000592
Validation Loss: 0.00061728
Epoch [29/300], Train Loss: 0.000592
Validation Loss: 0.00060491
Epoch [30/300], Train Loss: 0.000593
Validation Loss: 0.00060249
Epoch [31/300], Train Loss: 0.000586
Validation Loss: 0.00060530
Epoch [32/300], Train Loss: 0.000589
Validation Loss: 0.00060607
Epoch [33/300], Train Loss: 0.000586
Validation Loss: 0.00059598
Epoch [34/300], Train Loss: 0.000587
Validation Loss: 0.00060878
Epoch [35/300], Train Loss: 0.000585
Validation Loss: 0.00059449
Epoch [36/300], Train Loss: 0.000583
Validation Loss: 0.00059368
Epoch [37/300], Train Loss: 0.000583
Validation Loss: 0.00060382
Epoch [38/300], Train Loss: 0.000585
Validation Loss: 0.00059053
Epoch [39/300], Train Loss: 0.000578
Validation Loss: 0.00060434
Epoch [40/300], Train Loss: 0.000580
Validation Loss: 0.00059642
Epoch [41/300], Train Loss: 0.000583
Validation Loss: 0.00059060
Epoch [42/300], Train Loss: 0.000578
Validation Loss: 0.00059111
Epoch [43/300], Train Loss: 0.000578
Validation Loss: 0.00058825
Epoch [44/300], Train Loss: 0.000581
Validation Loss: 0.00060612
Epoch [45/300], Train Loss: 0.000578
Validation Loss: 0.00059051
Epoch [46/300], Train Loss: 0.000574
Validation Loss: 0.00058617
Epoch [47/300], Train Loss: 0.000576
Validation Loss: 0.00058639
Epoch [48/300], Train Loss: 0.000575
Validation Loss: 0.00059668
Epoch [49/300], Train Loss: 0.000575
Validation Loss: 0.00059451
Epoch [50/300], Train Loss: 0.000579
Validation Loss: 0.00058695
Epoch [51/300], Train Loss: 0.000576
Validation Loss: 0.00058091
Epoch [52/300], Train Loss: 0.000573
Validation Loss: 0.00058967
Epoch [53/300], Train Loss: 0.000574
Validation Loss: 0.00058837
Epoch [54/300], Train Loss: 0.000569
Validation Loss: 0.00058165
Epoch [55/300], Train Loss: 0.000573
Validation Loss: 0.00057875
Epoch [56/300], Train Loss: 0.000568
Validation Loss: 0.00058462
Epoch [57/300], Train Loss: 0.000570
Validation Loss: 0.00058058
Epoch [58/300], Train Loss: 0.000569
Validation Loss: 0.00057740
Epoch [59/300], Train Loss: 0.000566
Validation Loss: 0.00057726
Epoch [60/300], Train Loss: 0.000569
Validation Loss: 0.00057252
Epoch [61/300], Train Loss: 0.000560
Validation Loss: 0.00057591
Epoch [62/300], Train Loss: 0.000567
Validation Loss: 0.00057453
Epoch [63/300], Train Loss: 0.000566
Validation Loss: 0.00057197
Epoch [64/300], Train Loss: 0.000561
Validation Loss: 0.00057101
Epoch [65/300], Train Loss: 0.000558
Validation Loss: 0.00056929
Epoch [66/300], Train Loss: 0.000563
Validation Loss: 0.00056481
Epoch [67/300], Train Loss: 0.000561
Validation Loss: 0.00056868
Epoch [68/300], Train Loss: 0.000569
Validation Loss: 0.00056911
Epoch [69/300], Train Loss: 0.000570
Validation Loss: 0.00057694
Epoch [70/300], Train Loss: 0.000560
Validation Loss: 0.00056300
Epoch [71/300], Train Loss: 0.000557
Validation Loss: 0.00056156
Epoch [72/300], Train Loss: 0.000557
Validation Loss: 0.00056034
Epoch [73/300], Train Loss: 0.000556
Validation Loss: 0.00055910
Epoch [74/300], Train Loss: 0.000557
Validation Loss: 0.00056308
Epoch [75/300], Train Loss: 0.000560
Validation Loss: 0.00055742
Epoch [76/300], Train Loss: 0.000552
Validation Loss: 0.00055252
Epoch [77/300], Train Loss: 0.000551
Validation Loss: 0.00055536
Epoch [78/300], Train Loss: 0.000550
Validation Loss: 0.00055038
Epoch [79/300], Train Loss: 0.000548
Validation Loss: 0.00055685
Epoch [80/300], Train Loss: 0.000552
Validation Loss: 0.00054477
Epoch [81/300], Train Loss: 0.000547
Validation Loss: 0.00054760
Epoch [82/300], Train Loss: 0.000547
Validation Loss: 0.00054420
Epoch [83/300], Train Loss: 0.000547
Validation Loss: 0.00054392
Epoch [84/300], Train Loss: 0.000543
Validation Loss: 0.00054802
Epoch [85/300], Train Loss: 0.000547
Validation Loss: 0.00056067
Epoch [86/300], Train Loss: 0.000539
Validation Loss: 0.00054632
Epoch [87/300], Train Loss: 0.000536
Validation Loss: 0.00054295
Epoch [88/300], Train Loss: 0.000542
Validation Loss: 0.00054725
Epoch [89/300], Train Loss: 0.000545
Validation Loss: 0.00057606
Epoch [90/300], Train Loss: 0.000549
Validation Loss: 0.00054403
Epoch [91/300], Train Loss: 0.000541
Validation Loss: 0.00053913
Epoch [92/300], Train Loss: 0.000534
Validation Loss: 0.00053184
Epoch [93/300], Train Loss: 0.000537
Validation Loss: 0.00053960
Epoch [94/300], Train Loss: 0.000531
Validation Loss: 0.00053230
Epoch [95/300], Train Loss: 0.000532
Validation Loss: 0.00052539
Epoch [96/300], Train Loss: 0.000531
Validation Loss: 0.00052477
Epoch [97/300], Train Loss: 0.000538
Validation Loss: 0.00052798
Epoch [98/300], Train Loss: 0.000525
Validation Loss: 0.00053696
Epoch [99/300], Train Loss: 0.000526
Validation Loss: 0.00051742
Epoch [100/300], Train Loss: 0.000521
Validation Loss: 0.00052403
Epoch [101/300], Train Loss: 0.000527
Validation Loss: 0.00054276
Epoch [102/300], Train Loss: 0.000523
Validation Loss: 0.00051922
Epoch [103/300], Train Loss: 0.000519
Validation Loss: 0.00051500
Epoch [104/300], Train Loss: 0.000512
Validation Loss: 0.00049938
Epoch [105/300], Train Loss: 0.000521
Validation Loss: 0.00052628
Epoch [106/300], Train Loss: 0.000515
Validation Loss: 0.00051477
Epoch [107/300], Train Loss: 0.000511
Validation Loss: 0.00051231
Epoch [108/300], Train Loss: 0.000526
Validation Loss: 0.00053999
Epoch [109/300], Train Loss: 0.000516
Validation Loss: 0.00050295
Epoch [110/300], Train Loss: 0.000503
Validation Loss: 0.00049343
Epoch [111/300], Train Loss: 0.000503
Validation Loss: 0.00049008
Epoch [112/300], Train Loss: 0.000496
Validation Loss: 0.00050889
Epoch [113/300], Train Loss: 0.000504
Validation Loss: 0.00051787
Epoch [114/300], Train Loss: 0.000510
Validation Loss: 0.00050167
Epoch [115/300], Train Loss: 0.000505
Validation Loss: 0.00049983
Epoch [116/300], Train Loss: 0.000497
Validation Loss: 0.00048999
Epoch [117/300], Train Loss: 0.000493
Validation Loss: 0.00050366
Epoch [118/300], Train Loss: 0.000505
Validation Loss: 0.00049191
Epoch [119/300], Train Loss: 0.000497
Validation Loss: 0.00050824
Epoch [120/300], Train Loss: 0.000489
Validation Loss: 0.00048242
Epoch [121/300], Train Loss: 0.000490
Validation Loss: 0.00048273
Epoch [122/300], Train Loss: 0.000485
Validation Loss: 0.00049328
Epoch [123/300], Train Loss: 0.000502
Validation Loss: 0.00048703
Epoch [124/300], Train Loss: 0.000489
Validation Loss: 0.00047504
Epoch [125/300], Train Loss: 0.000486
Validation Loss: 0.00048213
Epoch [126/300], Train Loss: 0.000487
Validation Loss: 0.00047621
Epoch [127/300], Train Loss: 0.000489
Validation Loss: 0.00048120
Epoch [128/300], Train Loss: 0.000482
Validation Loss: 0.00047888
Epoch [129/300], Train Loss: 0.000483
Validation Loss: 0.00049604
Epoch [130/300], Train Loss: 0.000489
Validation Loss: 0.00047670
Epoch [131/300], Train Loss: 0.000479
Validation Loss: 0.00047202
Epoch [132/300], Train Loss: 0.000482
Validation Loss: 0.00048694
Epoch [133/300], Train Loss: 0.000485
Validation Loss: 0.00048002
Epoch [134/300], Train Loss: 0.000484
Validation Loss: 0.00049631
Epoch [135/300], Train Loss: 0.000485
Validation Loss: 0.00046921
Epoch [136/300], Train Loss: 0.000480
Validation Loss: 0.00048119
Epoch [137/300], Train Loss: 0.000482
Validation Loss: 0.00048430
Epoch [138/300], Train Loss: 0.000482
Validation Loss: 0.00047714
Epoch [139/300], Train Loss: 0.000476
Validation Loss: 0.00046977
Epoch [140/300], Train Loss: 0.000476
Validation Loss: 0.00046888
Epoch [141/300], Train Loss: 0.000475
Validation Loss: 0.00046338
Epoch [142/300], Train Loss: 0.000469
Validation Loss: 0.00046944
Epoch [143/300], Train Loss: 0.000479
Validation Loss: 0.00046379
Epoch [144/300], Train Loss: 0.000478
Validation Loss: 0.00046414
Epoch [145/300], Train Loss: 0.000475
Validation Loss: 0.00046492
Epoch [146/300], Train Loss: 0.000470
Validation Loss: 0.00046128
Epoch [147/300], Train Loss: 0.000472
Validation Loss: 0.00046259
Epoch [148/300], Train Loss: 0.000470
Validation Loss: 0.00047181
Epoch [149/300], Train Loss: 0.000474
Validation Loss: 0.00046462
Epoch [150/300], Train Loss: 0.000469
Validation Loss: 0.00046049
Epoch [151/300], Train Loss: 0.000467
Validation Loss: 0.00046795
Epoch [152/300], Train Loss: 0.000476
Validation Loss: 0.00046002
Epoch [153/300], Train Loss: 0.000468
Validation Loss: 0.00047816
Epoch [154/300], Train Loss: 0.000471
Validation Loss: 0.00046658
Epoch [155/300], Train Loss: 0.000469
Validation Loss: 0.00048148
Epoch [156/300], Train Loss: 0.000471
Validation Loss: 0.00046341
Epoch [157/300], Train Loss: 0.000472
Validation Loss: 0.00045781
Epoch [158/300], Train Loss: 0.000472
Validation Loss: 0.00048574
Epoch [159/300], Train Loss: 0.000472
Validation Loss: 0.00045947
Epoch [160/300], Train Loss: 0.000464
Validation Loss: 0.00045435
Epoch [161/300], Train Loss: 0.000468
Validation Loss: 0.00046276
Epoch [162/300], Train Loss: 0.000466
Validation Loss: 0.00045723
Epoch [163/300], Train Loss: 0.000467
Validation Loss: 0.00045869
Epoch [164/300], Train Loss: 0.000463
Validation Loss: 0.00046110
Epoch [165/300], Train Loss: 0.000463
Validation Loss: 0.00046008
Epoch [166/300], Train Loss: 0.000463
Validation Loss: 0.00045165
Epoch [167/300], Train Loss: 0.000465
Validation Loss: 0.00046084
Epoch [168/300], Train Loss: 0.000457
Validation Loss: 0.00045357
Epoch [169/300], Train Loss: 0.000462
Validation Loss: 0.00045043
Epoch [170/300], Train Loss: 0.000462
Validation Loss: 0.00044928
Epoch [171/300], Train Loss: 0.000461
Validation Loss: 0.00044980
Epoch [172/300], Train Loss: 0.000456
Validation Loss: 0.00046610
Epoch [173/300], Train Loss: 0.000461
Validation Loss: 0.00046527
Epoch [174/300], Train Loss: 0.000465
Validation Loss: 0.00044938
Epoch [175/300], Train Loss: 0.000462
Validation Loss: 0.00044938
Epoch [176/300], Train Loss: 0.000457
Validation Loss: 0.00046035
Epoch [177/300], Train Loss: 0.000457
Validation Loss: 0.00044942
Epoch [178/300], Train Loss: 0.000460
Validation Loss: 0.00045028
Epoch [179/300], Train Loss: 0.000459
Validation Loss: 0.00044971
Epoch [180/300], Train Loss: 0.000455
Validation Loss: 0.00045717
Early stopping triggered

Evaluating model for: Fridge
Run 71/72 completed in 2985.07 seconds with: {'MAE': np.float32(29.27344), 'MSE': np.float32(1559.6571), 'RMSE': np.float32(39.492493), 'SAE': np.float32(0.09921304), 'NDE': np.float32(0.6843773)}

Run 72/72: hidden=512, seq_len=720, stride=0.5, num_layers=5
Training and evaluating model for: Fridge
Dataset length: 3386 windows

Epoch [1/300], Train Loss: 0.001063
Validation Loss: 0.00069295
Epoch [2/300], Train Loss: 0.000678
Validation Loss: 0.00067038
Epoch [3/300], Train Loss: 0.000666
Validation Loss: 0.00067092
Epoch [4/300], Train Loss: 0.000667
Validation Loss: 0.00066997
Epoch [5/300], Train Loss: 0.000665
Validation Loss: 0.00067002
Epoch [6/300], Train Loss: 0.000666
Validation Loss: 0.00066989
Epoch [7/300], Train Loss: 0.000665
Validation Loss: 0.00066968
Epoch [8/300], Train Loss: 0.000665
Validation Loss: 0.00066936
Epoch [9/300], Train Loss: 0.000666
Validation Loss: 0.00066962
Epoch [10/300], Train Loss: 0.000660
Validation Loss: 0.00066885
Epoch [11/300], Train Loss: 0.000662
Validation Loss: 0.00066866
Epoch [12/300], Train Loss: 0.000666
Validation Loss: 0.00066801
Epoch [13/300], Train Loss: 0.000661
Validation Loss: 0.00066566
Epoch [14/300], Train Loss: 0.000657
Validation Loss: 0.00066361
Epoch [15/300], Train Loss: 0.000652
Validation Loss: 0.00065753
Epoch [16/300], Train Loss: 0.000638
Validation Loss: 0.00064277
Epoch [17/300], Train Loss: 0.000631
Validation Loss: 0.00064104
Epoch [18/300], Train Loss: 0.000620
Validation Loss: 0.00063591
Epoch [19/300], Train Loss: 0.000623
Validation Loss: 0.00063491
Epoch [20/300], Train Loss: 0.000613
Validation Loss: 0.00063127
Epoch [21/300], Train Loss: 0.000615
Validation Loss: 0.00062566
Epoch [22/300], Train Loss: 0.000613
Validation Loss: 0.00063154
Epoch [23/300], Train Loss: 0.000615
Validation Loss: 0.00062864
Epoch [24/300], Train Loss: 0.000617
Validation Loss: 0.00062154
Epoch [25/300], Train Loss: 0.000608
Validation Loss: 0.00062529
Epoch [26/300], Train Loss: 0.000608
Validation Loss: 0.00061854
Epoch [27/300], Train Loss: 0.000608
Validation Loss: 0.00062510
Epoch [28/300], Train Loss: 0.000606
Validation Loss: 0.00061744
Epoch [29/300], Train Loss: 0.000607
Validation Loss: 0.00061976
Epoch [30/300], Train Loss: 0.000609
Validation Loss: 0.00063116
Epoch [31/300], Train Loss: 0.000607
Validation Loss: 0.00062322
Epoch [32/300], Train Loss: 0.000607
Validation Loss: 0.00062519
Epoch [33/300], Train Loss: 0.000604
Validation Loss: 0.00061590
Epoch [34/300], Train Loss: 0.000605
Validation Loss: 0.00062421
Epoch [35/300], Train Loss: 0.000605
Validation Loss: 0.00062510
Epoch [36/300], Train Loss: 0.000602
Validation Loss: 0.00061498
Epoch [37/300], Train Loss: 0.000602
Validation Loss: 0.00061555
Epoch [38/300], Train Loss: 0.000605
Validation Loss: 0.00061380
Epoch [39/300], Train Loss: 0.000598
Validation Loss: 0.00062558
Epoch [40/300], Train Loss: 0.000601
Validation Loss: 0.00061653
Epoch [41/300], Train Loss: 0.000601
Validation Loss: 0.00061519
Epoch [42/300], Train Loss: 0.000599
Validation Loss: 0.00061326
Epoch [43/300], Train Loss: 0.000600
Validation Loss: 0.00061465
Epoch [44/300], Train Loss: 0.000604
Validation Loss: 0.00061983
Epoch [45/300], Train Loss: 0.000600
Validation Loss: 0.00061194
Epoch [46/300], Train Loss: 0.000596
Validation Loss: 0.00061175
Epoch [47/300], Train Loss: 0.000598
Validation Loss: 0.00061004
Epoch [48/300], Train Loss: 0.000597
Validation Loss: 0.00061041
Epoch [49/300], Train Loss: 0.000596
Validation Loss: 0.00061070
Epoch [50/300], Train Loss: 0.000601
Validation Loss: 0.00061281
Epoch [51/300], Train Loss: 0.000599
Validation Loss: 0.00061014
Epoch [52/300], Train Loss: 0.000598
Validation Loss: 0.00060703
Epoch [53/300], Train Loss: 0.000594
Validation Loss: 0.00060964
Epoch [54/300], Train Loss: 0.000593
Validation Loss: 0.00060751
Epoch [55/300], Train Loss: 0.000596
Validation Loss: 0.00060564
Epoch [56/300], Train Loss: 0.000592
Validation Loss: 0.00060547
Epoch [57/300], Train Loss: 0.000595
Validation Loss: 0.00060893
Epoch [58/300], Train Loss: 0.000596
Validation Loss: 0.00060507
Epoch [59/300], Train Loss: 0.000593
Validation Loss: 0.00060483
Epoch [60/300], Train Loss: 0.000596
Validation Loss: 0.00060398
Epoch [61/300], Train Loss: 0.000589
Validation Loss: 0.00060635
Epoch [62/300], Train Loss: 0.000594
Validation Loss: 0.00060766
Epoch [63/300], Train Loss: 0.000595
Validation Loss: 0.00060514
Epoch [64/300], Train Loss: 0.000591
Validation Loss: 0.00060341
Epoch [65/300], Train Loss: 0.000589
Validation Loss: 0.00060253
Epoch [66/300], Train Loss: 0.000592
Validation Loss: 0.00060387
Epoch [67/300], Train Loss: 0.000592
Validation Loss: 0.00060772
Epoch [68/300], Train Loss: 0.000587
Validation Loss: 0.00060128
Epoch [69/300], Train Loss: 0.000592
Validation Loss: 0.00060163
Epoch [70/300], Train Loss: 0.000588
Validation Loss: 0.00060029
Epoch [71/300], Train Loss: 0.000588
Validation Loss: 0.00060107
Epoch [72/300], Train Loss: 0.000592
Validation Loss: 0.00060585
Epoch [73/300], Train Loss: 0.000590
Validation Loss: 0.00059883
Epoch [74/300], Train Loss: 0.000589
Validation Loss: 0.00059869
Epoch [75/300], Train Loss: 0.000589
Validation Loss: 0.00059927
Epoch [76/300], Train Loss: 0.000588
Validation Loss: 0.00059770
Epoch [77/300], Train Loss: 0.000587
Validation Loss: 0.00059789
Epoch [78/300], Train Loss: 0.000589
Validation Loss: 0.00059737
Epoch [79/300], Train Loss: 0.000586
Validation Loss: 0.00059696
Epoch [80/300], Train Loss: 0.000587
Validation Loss: 0.00059613
Epoch [81/300], Train Loss: 0.000587
Validation Loss: 0.00060297
Epoch [82/300], Train Loss: 0.000584
Validation Loss: 0.00059507
Epoch [83/300], Train Loss: 0.000587
Validation Loss: 0.00059283
Epoch [84/300], Train Loss: 0.000579
Validation Loss: 0.00059360
Epoch [85/300], Train Loss: 0.000584
Validation Loss: 0.00060062
Epoch [86/300], Train Loss: 0.000582
Validation Loss: 0.00059195
Epoch [87/300], Train Loss: 0.000578
Validation Loss: 0.00059458
Epoch [88/300], Train Loss: 0.000582
Validation Loss: 0.00059084
Epoch [89/300], Train Loss: 0.000582
Validation Loss: 0.00058979
Epoch [90/300], Train Loss: 0.000585
Validation Loss: 0.00059279
Epoch [91/300], Train Loss: 0.000583
Validation Loss: 0.00058813
Epoch [92/300], Train Loss: 0.000579
Validation Loss: 0.00058861
Epoch [93/300], Train Loss: 0.000578
Validation Loss: 0.00058567
Epoch [94/300], Train Loss: 0.000573
Validation Loss: 0.00058693
Epoch [95/300], Train Loss: 0.000578
Validation Loss: 0.00058537
Epoch [96/300], Train Loss: 0.000575
Validation Loss: 0.00058403
Epoch [97/300], Train Loss: 0.000575
Validation Loss: 0.00057759
Epoch [98/300], Train Loss: 0.000570
Validation Loss: 0.00058345
Epoch [99/300], Train Loss: 0.000573
Validation Loss: 0.00057424
Epoch [100/300], Train Loss: 0.000569
Validation Loss: 0.00057058
Epoch [101/300], Train Loss: 0.000572
Validation Loss: 0.00059016
Epoch [102/300], Train Loss: 0.000570
Validation Loss: 0.00057015
Epoch [103/300], Train Loss: 0.000569
Validation Loss: 0.00056735
Epoch [104/300], Train Loss: 0.000565
Validation Loss: 0.00056341
Epoch [105/300], Train Loss: 0.000567
Validation Loss: 0.00056413
Epoch [106/300], Train Loss: 0.000563
Validation Loss: 0.00056758
Epoch [107/300], Train Loss: 0.000564
Validation Loss: 0.00056533
Epoch [108/300], Train Loss: 0.000561
Validation Loss: 0.00056349
Epoch [109/300], Train Loss: 0.000558
Validation Loss: 0.00056023
Epoch [110/300], Train Loss: 0.000560
Validation Loss: 0.00056186
Epoch [111/300], Train Loss: 0.000561
Validation Loss: 0.00056081
Epoch [112/300], Train Loss: 0.000559
Validation Loss: 0.00055842
Epoch [113/300], Train Loss: 0.000559
Validation Loss: 0.00056696
Epoch [114/300], Train Loss: 0.000564
Validation Loss: 0.00056169
Epoch [115/300], Train Loss: 0.000564
Validation Loss: 0.00056044
Epoch [116/300], Train Loss: 0.000559
Validation Loss: 0.00055481
Epoch [117/300], Train Loss: 0.000556
Validation Loss: 0.00055739
Epoch [118/300], Train Loss: 0.000564
Validation Loss: 0.00055599
Epoch [119/300], Train Loss: 0.000559
Validation Loss: 0.00056175
Epoch [120/300], Train Loss: 0.000556
Validation Loss: 0.00055346
Epoch [121/300], Train Loss: 0.000553
Validation Loss: 0.00055301
Epoch [122/300], Train Loss: 0.000553
Validation Loss: 0.00054886
Epoch [123/300], Train Loss: 0.000557
Validation Loss: 0.00054971
Epoch [124/300], Train Loss: 0.000555
Validation Loss: 0.00055265
Epoch [125/300], Train Loss: 0.000555
Validation Loss: 0.00056813
Epoch [126/300], Train Loss: 0.000556
Validation Loss: 0.00055671
Epoch [127/300], Train Loss: 0.000552
Validation Loss: 0.00054607
Epoch [128/300], Train Loss: 0.000549
Validation Loss: 0.00054899
Epoch [129/300], Train Loss: 0.000550
Validation Loss: 0.00054769
Epoch [130/300], Train Loss: 0.000547
Validation Loss: 0.00054973
Epoch [131/300], Train Loss: 0.000545
Validation Loss: 0.00054358
Epoch [132/300], Train Loss: 0.000545
Validation Loss: 0.00053937
Epoch [133/300], Train Loss: 0.000542
Validation Loss: 0.00053877
Epoch [134/300], Train Loss: 0.000540
Validation Loss: 0.00053454
Epoch [135/300], Train Loss: 0.000541
Validation Loss: 0.00053039
Epoch [136/300], Train Loss: 0.000543
Validation Loss: 0.00053028
Epoch [137/300], Train Loss: 0.000537
Validation Loss: 0.00052646
Epoch [138/300], Train Loss: 0.000538
Validation Loss: 0.00052559
Epoch [139/300], Train Loss: 0.000533
Validation Loss: 0.00052589
Epoch [140/300], Train Loss: 0.000536
Validation Loss: 0.00054662
Epoch [141/300], Train Loss: 0.000541
Validation Loss: 0.00052813
Epoch [142/300], Train Loss: 0.000531
Validation Loss: 0.00053012
Epoch [143/300], Train Loss: 0.000539
Validation Loss: 0.00053213
Epoch [144/300], Train Loss: 0.000531
Validation Loss: 0.00051847
Epoch [145/300], Train Loss: 0.000535
Validation Loss: 0.00051934
Epoch [146/300], Train Loss: 0.000531
Validation Loss: 0.00051736
Epoch [147/300], Train Loss: 0.000526
Validation Loss: 0.00051697
Epoch [148/300], Train Loss: 0.000527
Validation Loss: 0.00051424
Epoch [149/300], Train Loss: 0.000524
Validation Loss: 0.00051258
Epoch [150/300], Train Loss: 0.000527
Validation Loss: 0.00051701
Epoch [151/300], Train Loss: 0.000524
Validation Loss: 0.00051434
Epoch [152/300], Train Loss: 0.000526
Validation Loss: 0.00051261
Epoch [153/300], Train Loss: 0.000524
Validation Loss: 0.00051244
Epoch [154/300], Train Loss: 0.000524
Validation Loss: 0.00052254
Epoch [155/300], Train Loss: 0.000526
Validation Loss: 0.00051228
Epoch [156/300], Train Loss: 0.000519
Validation Loss: 0.00050736
Epoch [157/300], Train Loss: 0.000522
Validation Loss: 0.00050900
Epoch [158/300], Train Loss: 0.000517
Validation Loss: 0.00051022
Epoch [159/300], Train Loss: 0.000516
Validation Loss: 0.00050930
Epoch [160/300], Train Loss: 0.000518
Validation Loss: 0.00053082
Epoch [161/300], Train Loss: 0.000519
Validation Loss: 0.00050517
Epoch [162/300], Train Loss: 0.000511
Validation Loss: 0.00050121
Epoch [163/300], Train Loss: 0.000516
Validation Loss: 0.00050173
Epoch [164/300], Train Loss: 0.000513
Validation Loss: 0.00050445
Epoch [165/300], Train Loss: 0.000512
Validation Loss: 0.00049535
Epoch [166/300], Train Loss: 0.000511
Validation Loss: 0.00050259
Epoch [167/300], Train Loss: 0.000508
Validation Loss: 0.00049808
Epoch [168/300], Train Loss: 0.000505
Validation Loss: 0.00049799
Epoch [169/300], Train Loss: 0.000512
Validation Loss: 0.00049361
Epoch [170/300], Train Loss: 0.000510
Validation Loss: 0.00049309
Epoch [171/300], Train Loss: 0.000510
Validation Loss: 0.00049564
Epoch [172/300], Train Loss: 0.000503
Validation Loss: 0.00049181
Epoch [173/300], Train Loss: 0.000502
Validation Loss: 0.00048525
Epoch [174/300], Train Loss: 0.000500
Validation Loss: 0.00049079
Epoch [175/300], Train Loss: 0.000506
Validation Loss: 0.00050330
Epoch [176/300], Train Loss: 0.000508
Validation Loss: 0.00049148
Epoch [177/300], Train Loss: 0.000501
Validation Loss: 0.00049753
Epoch [178/300], Train Loss: 0.000503
Validation Loss: 0.00048907
Epoch [179/300], Train Loss: 0.000501
Validation Loss: 0.00048006
Epoch [180/300], Train Loss: 0.000500
Validation Loss: 0.00048769
Epoch [181/300], Train Loss: 0.000503
Validation Loss: 0.00048377
Epoch [182/300], Train Loss: 0.000496
Validation Loss: 0.00049577
Epoch [183/300], Train Loss: 0.000499
Validation Loss: 0.00048553
Epoch [184/300], Train Loss: 0.000494
Validation Loss: 0.00047911
Epoch [185/300], Train Loss: 0.000492
Validation Loss: 0.00048055
Epoch [186/300], Train Loss: 0.000488
Validation Loss: 0.00047610
Epoch [187/300], Train Loss: 0.000486
Validation Loss: 0.00047336
Epoch [188/300], Train Loss: 0.000487
Validation Loss: 0.00048163
Epoch [189/300], Train Loss: 0.000488
Validation Loss: 0.00047692
Epoch [190/300], Train Loss: 0.000488
Validation Loss: 0.00047298
Epoch [191/300], Train Loss: 0.000486
Validation Loss: 0.00046943
Epoch [192/300], Train Loss: 0.000485
Validation Loss: 0.00048003
Epoch [193/300], Train Loss: 0.000488
Validation Loss: 0.00047267
Epoch [194/300], Train Loss: 0.000484
Validation Loss: 0.00046872
Epoch [195/300], Train Loss: 0.000481
Validation Loss: 0.00047008
Epoch [196/300], Train Loss: 0.000483
Validation Loss: 0.00046515
Epoch [197/300], Train Loss: 0.000480
Validation Loss: 0.00047310
Epoch [198/300], Train Loss: 0.000479
Validation Loss: 0.00047014
Epoch [199/300], Train Loss: 0.000480
Validation Loss: 0.00049059
Epoch [200/300], Train Loss: 0.000488
Validation Loss: 0.00046571
Epoch [201/300], Train Loss: 0.000479
Validation Loss: 0.00046588
Epoch [202/300], Train Loss: 0.000479
Validation Loss: 0.00046861
Epoch [203/300], Train Loss: 0.000481
Validation Loss: 0.00046523
Epoch [204/300], Train Loss: 0.000476
Validation Loss: 0.00046446
Epoch [205/300], Train Loss: 0.000478
Validation Loss: 0.00046259
Epoch [206/300], Train Loss: 0.000474
Validation Loss: 0.00047206
Epoch [207/300], Train Loss: 0.000485
Validation Loss: 0.00047120
Epoch [208/300], Train Loss: 0.000475
Validation Loss: 0.00045933
Epoch [209/300], Train Loss: 0.000479
Validation Loss: 0.00046318
Epoch [210/300], Train Loss: 0.000482
Validation Loss: 0.00046229
Epoch [211/300], Train Loss: 0.000476
Validation Loss: 0.00045939
Epoch [212/300], Train Loss: 0.000477
Validation Loss: 0.00047176
Epoch [213/300], Train Loss: 0.000476
Validation Loss: 0.00047609
Epoch [214/300], Train Loss: 0.000481
Validation Loss: 0.00046185
Epoch [215/300], Train Loss: 0.000470
Validation Loss: 0.00046284
Epoch [216/300], Train Loss: 0.000476
Validation Loss: 0.00046032
Epoch [217/300], Train Loss: 0.000474
Validation Loss: 0.00046314
Epoch [218/300], Train Loss: 0.000473
Validation Loss: 0.00046272
Early stopping triggered

Evaluating model for: Fridge
Run 72/72 completed in 4628.22 seconds with: {'MAE': np.float32(31.326706), 'MSE': np.float32(1601.7787), 'RMSE': np.float32(40.02223), 'SAE': np.float32(0.074597865), 'NDE': np.float32(0.69355744)}
    hidden_size  seq_length  stride  num_layers  eval_result
48          512         120    0.25           2    13.657178
26          256         120    0.25           4    14.614090
53          512         120    0.50           3    16.108637
1           128         120    0.25           3    16.858461
25          256         120    0.25           3    16.906137
..          ...         ...     ...         ...          ...
21          128         720    0.50           3    36.173969
11          128         360    0.25           5    36.267353
40          256         720    0.25           2    36.275188
23          128         720    0.50           5    36.464767
16          128         720    0.25           2    36.643623

[72 rows x 5 columns]
