Using device: cuda

Run 1/144: hidden=64, seq_len=120, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.005035
Validation Loss: 0.00479652
Epoch [2/200], Train Loss: 0.004782
Validation Loss: 0.00475514
Epoch [3/200], Train Loss: 0.004757
Validation Loss: 0.00474579
Epoch [4/200], Train Loss: 0.004747
Validation Loss: 0.00474346
Epoch [5/200], Train Loss: 0.004732
Validation Loss: 0.00472927
Epoch [6/200], Train Loss: 0.004721
Validation Loss: 0.00471583
Epoch [7/200], Train Loss: 0.004705
Validation Loss: 0.00469416
Epoch [8/200], Train Loss: 0.004691
Validation Loss: 0.00468270
Epoch [9/200], Train Loss: 0.004673
Validation Loss: 0.00464727
Epoch [10/200], Train Loss: 0.004645
Validation Loss: 0.00460815
Epoch [11/200], Train Loss: 0.004609
Validation Loss: 0.00454693
Epoch [12/200], Train Loss: 0.004527
Validation Loss: 0.00441472
Epoch [13/200], Train Loss: 0.004335
Validation Loss: 0.00408340
Epoch [14/200], Train Loss: 0.004053
Validation Loss: 0.00379489
Epoch [15/200], Train Loss: 0.003665
Validation Loss: 0.00335501
Epoch [16/200], Train Loss: 0.003234
Validation Loss: 0.00291054
Epoch [17/200], Train Loss: 0.002851
Validation Loss: 0.00261559
Epoch [18/200], Train Loss: 0.002631
Validation Loss: 0.00238745
Epoch [19/200], Train Loss: 0.002395
Validation Loss: 0.00223667
Epoch [20/200], Train Loss: 0.002265
Validation Loss: 0.00218369
Epoch [21/200], Train Loss: 0.002154
Validation Loss: 0.00204386
Epoch [22/200], Train Loss: 0.002075
Validation Loss: 0.00195110
Epoch [23/200], Train Loss: 0.001987
Validation Loss: 0.00195758
Epoch [24/200], Train Loss: 0.001920
Validation Loss: 0.00186938
Epoch [25/200], Train Loss: 0.001858
Validation Loss: 0.00181454
Epoch [26/200], Train Loss: 0.001799
Validation Loss: 0.00173424
Epoch [27/200], Train Loss: 0.001749
Validation Loss: 0.00171080
Epoch [28/200], Train Loss: 0.001693
Validation Loss: 0.00165268
Epoch [29/200], Train Loss: 0.001653
Validation Loss: 0.00165976
Epoch [30/200], Train Loss: 0.001606
Validation Loss: 0.00161412
Epoch [31/200], Train Loss: 0.001564
Validation Loss: 0.00154639
Epoch [32/200], Train Loss: 0.001529
Validation Loss: 0.00150878
Epoch [33/200], Train Loss: 0.001489
Validation Loss: 0.00150710
Epoch [34/200], Train Loss: 0.001458
Validation Loss: 0.00144496
Epoch [35/200], Train Loss: 0.001434
Validation Loss: 0.00141911
Epoch [36/200], Train Loss: 0.001396
Validation Loss: 0.00138826
Epoch [37/200], Train Loss: 0.001365
Validation Loss: 0.00136630
Epoch [38/200], Train Loss: 0.001344
Validation Loss: 0.00136207
Epoch [39/200], Train Loss: 0.001309
Validation Loss: 0.00133554
Epoch [40/200], Train Loss: 0.001293
Validation Loss: 0.00130553
Epoch [41/200], Train Loss: 0.001271
Validation Loss: 0.00128111
Epoch [42/200], Train Loss: 0.001241
Validation Loss: 0.00127125
Epoch [43/200], Train Loss: 0.001215
Validation Loss: 0.00130255
Epoch [44/200], Train Loss: 0.001196
Validation Loss: 0.00122535
Epoch [45/200], Train Loss: 0.001183
Validation Loss: 0.00121837
Epoch [46/200], Train Loss: 0.001154
Validation Loss: 0.00119718
Epoch [47/200], Train Loss: 0.001144
Validation Loss: 0.00119877
Epoch [48/200], Train Loss: 0.001122
Validation Loss: 0.00119327
Epoch [49/200], Train Loss: 0.001112
Validation Loss: 0.00116434
Epoch [50/200], Train Loss: 0.001097
Validation Loss: 0.00114437
Epoch [51/200], Train Loss: 0.001087
Validation Loss: 0.00114056
Epoch [52/200], Train Loss: 0.001070
Validation Loss: 0.00111938
Epoch [53/200], Train Loss: 0.001063
Validation Loss: 0.00111051
Epoch [54/200], Train Loss: 0.001070
Validation Loss: 0.00110417
Epoch [55/200], Train Loss: 0.001043
Validation Loss: 0.00156471
Epoch [56/200], Train Loss: 0.001100
Validation Loss: 0.00110618
Epoch [57/200], Train Loss: 0.001014
Validation Loss: 0.00107171
Epoch [58/200], Train Loss: 0.001002
Validation Loss: 0.00105477
Epoch [59/200], Train Loss: 0.000996
Validation Loss: 0.00111689
Epoch [60/200], Train Loss: 0.000982
Validation Loss: 0.00103182
Epoch [61/200], Train Loss: 0.000973
Validation Loss: 0.00101769
Epoch [62/200], Train Loss: 0.000969
Validation Loss: 0.00100492
Epoch [63/200], Train Loss: 0.000954
Validation Loss: 0.00103184
Epoch [64/200], Train Loss: 0.000944
Validation Loss: 0.00101466
Epoch [65/200], Train Loss: 0.000933
Validation Loss: 0.00100472
Epoch [66/200], Train Loss: 0.000921
Validation Loss: 0.00096287
Epoch [67/200], Train Loss: 0.000906
Validation Loss: 0.00097206
Epoch [68/200], Train Loss: 0.000908
Validation Loss: 0.00095565
Epoch [69/200], Train Loss: 0.000894
Validation Loss: 0.00092774
Epoch [70/200], Train Loss: 0.000887
Validation Loss: 0.00094068
Epoch [71/200], Train Loss: 0.000875
Validation Loss: 0.00091537
Epoch [72/200], Train Loss: 0.000864
Validation Loss: 0.00090865
Epoch [73/200], Train Loss: 0.000859
Validation Loss: 0.00088353
Epoch [74/200], Train Loss: 0.000845
Validation Loss: 0.00088895
Epoch [75/200], Train Loss: 0.000834
Validation Loss: 0.00087702
Epoch [76/200], Train Loss: 0.000822
Validation Loss: 0.00086038
Epoch [77/200], Train Loss: 0.000815
Validation Loss: 0.00083992
Epoch [78/200], Train Loss: 0.000812
Validation Loss: 0.00084023
Epoch [79/200], Train Loss: 0.000806
Validation Loss: 0.00085292
Epoch [80/200], Train Loss: 0.000792
Validation Loss: 0.00084477
Epoch [81/200], Train Loss: 0.000792
Validation Loss: 0.00083396
Epoch [82/200], Train Loss: 0.000785
Validation Loss: 0.00079971
Epoch [83/200], Train Loss: 0.000782
Validation Loss: 0.00081762
Epoch [84/200], Train Loss: 0.000767
Validation Loss: 0.00079663
Epoch [85/200], Train Loss: 0.000764
Validation Loss: 0.00077868
Epoch [86/200], Train Loss: 0.000759
Validation Loss: 0.00078400
Epoch [87/200], Train Loss: 0.000755
Validation Loss: 0.00078203
Epoch [88/200], Train Loss: 0.000746
Validation Loss: 0.00076892
Epoch [89/200], Train Loss: 0.000743
Validation Loss: 0.00076950
Epoch [90/200], Train Loss: 0.000742
Validation Loss: 0.00077585
Epoch [91/200], Train Loss: 0.000733
Validation Loss: 0.00074202
Epoch [92/200], Train Loss: 0.000727
Validation Loss: 0.00073495
Epoch [93/200], Train Loss: 0.000726
Validation Loss: 0.00073945
Epoch [94/200], Train Loss: 0.000719
Validation Loss: 0.00074818
Epoch [95/200], Train Loss: 0.000719
Validation Loss: 0.00074390
Epoch [96/200], Train Loss: 0.000704
Validation Loss: 0.00071864
Epoch [97/200], Train Loss: 0.000712
Validation Loss: 0.00071386
Epoch [98/200], Train Loss: 0.000705
Validation Loss: 0.00070618
Epoch [99/200], Train Loss: 0.000700
Validation Loss: 0.00073525
Epoch [100/200], Train Loss: 0.000697
Validation Loss: 0.00071070
Epoch [101/200], Train Loss: 0.000691
Validation Loss: 0.00070290
Epoch [102/200], Train Loss: 0.000690
Validation Loss: 0.00068955
Epoch [103/200], Train Loss: 0.000687
Validation Loss: 0.00069380
Epoch [104/200], Train Loss: 0.000680
Validation Loss: 0.00069451
Epoch [105/200], Train Loss: 0.000677
Validation Loss: 0.00068976
Epoch [106/200], Train Loss: 0.000674
Validation Loss: 0.00067187
Epoch [107/200], Train Loss: 0.000670
Validation Loss: 0.00067336
Epoch [108/200], Train Loss: 0.000668
Validation Loss: 0.00068150
Epoch [109/200], Train Loss: 0.000664
Validation Loss: 0.00067338
Epoch [110/200], Train Loss: 0.000663
Validation Loss: 0.00066340
Epoch [111/200], Train Loss: 0.000653
Validation Loss: 0.00065829
Epoch [112/200], Train Loss: 0.000653
Validation Loss: 0.00065175
Epoch [113/200], Train Loss: 0.000650
Validation Loss: 0.00064376
Epoch [114/200], Train Loss: 0.000648
Validation Loss: 0.00067366
Epoch [115/200], Train Loss: 0.000651
Validation Loss: 0.00066392
Epoch [116/200], Train Loss: 0.000643
Validation Loss: 0.00064197
Epoch [117/200], Train Loss: 0.000641
Validation Loss: 0.00063685
Epoch [118/200], Train Loss: 0.000642
Validation Loss: 0.00064006
Epoch [119/200], Train Loss: 0.000636
Validation Loss: 0.00063755
Epoch [120/200], Train Loss: 0.000631
Validation Loss: 0.00064448
Epoch [121/200], Train Loss: 0.000631
Validation Loss: 0.00062770
Epoch [122/200], Train Loss: 0.000625
Validation Loss: 0.00062331
Epoch [123/200], Train Loss: 0.000628
Validation Loss: 0.00064732
Epoch [124/200], Train Loss: 0.000624
Validation Loss: 0.00062111
Epoch [125/200], Train Loss: 0.000617
Validation Loss: 0.00061533
Epoch [126/200], Train Loss: 0.000610
Validation Loss: 0.00062049
Epoch [127/200], Train Loss: 0.000614
Validation Loss: 0.00061477
Epoch [128/200], Train Loss: 0.000608
Validation Loss: 0.00061278
Epoch [129/200], Train Loss: 0.000604
Validation Loss: 0.00061286
Epoch [130/200], Train Loss: 0.000605
Validation Loss: 0.00060593
Epoch [131/200], Train Loss: 0.000607
Validation Loss: 0.00060874
Epoch [132/200], Train Loss: 0.000597
Validation Loss: 0.00063513
Epoch [133/200], Train Loss: 0.000621
Validation Loss: 0.00062765
Epoch [134/200], Train Loss: 0.000613
Validation Loss: 0.00060942
Epoch [135/200], Train Loss: 0.000596
Validation Loss: 0.00059071
Epoch [136/200], Train Loss: 0.000591
Validation Loss: 0.00059940
Epoch [137/200], Train Loss: 0.000587
Validation Loss: 0.00059087
Epoch [138/200], Train Loss: 0.000589
Validation Loss: 0.00059291
Epoch [139/200], Train Loss: 0.000587
Validation Loss: 0.00059366
Epoch [140/200], Train Loss: 0.000591
Validation Loss: 0.00058720
Epoch [141/200], Train Loss: 0.000582
Validation Loss: 0.00058353
Epoch [142/200], Train Loss: 0.000582
Validation Loss: 0.00057770
Epoch [143/200], Train Loss: 0.000580
Validation Loss: 0.00057775
Epoch [144/200], Train Loss: 0.000578
Validation Loss: 0.00057462
Epoch [145/200], Train Loss: 0.000577
Validation Loss: 0.00056963
Epoch [146/200], Train Loss: 0.000574
Validation Loss: 0.00058133
Epoch [147/200], Train Loss: 0.000575
Validation Loss: 0.00058645
Epoch [148/200], Train Loss: 0.000574
Validation Loss: 0.00057601
Epoch [149/200], Train Loss: 0.000570
Validation Loss: 0.00056652
Epoch [150/200], Train Loss: 0.000569
Validation Loss: 0.00056817
Epoch [151/200], Train Loss: 0.000567
Validation Loss: 0.00057054
Epoch [152/200], Train Loss: 0.000562
Validation Loss: 0.00056841
Epoch [153/200], Train Loss: 0.000566
Validation Loss: 0.00056605
Epoch [154/200], Train Loss: 0.000560
Validation Loss: 0.00056552
Epoch [155/200], Train Loss: 0.000557
Validation Loss: 0.00056379
Epoch [156/200], Train Loss: 0.000559
Validation Loss: 0.00055883
Epoch [157/200], Train Loss: 0.000560
Validation Loss: 0.00056340
Epoch [158/200], Train Loss: 0.000557
Validation Loss: 0.00056261
Epoch [159/200], Train Loss: 0.000564
Validation Loss: 0.00056316
Epoch [160/200], Train Loss: 0.000557
Validation Loss: 0.00055666
Epoch [161/200], Train Loss: 0.000553
Validation Loss: 0.00055184
Epoch [162/200], Train Loss: 0.000548
Validation Loss: 0.00054644
Epoch [163/200], Train Loss: 0.000551
Validation Loss: 0.00056409
Epoch [164/200], Train Loss: 0.000548
Validation Loss: 0.00054814
Epoch [165/200], Train Loss: 0.000546
Validation Loss: 0.00054343
Epoch [166/200], Train Loss: 0.000547
Validation Loss: 0.00054401
Epoch [167/200], Train Loss: 0.000546
Validation Loss: 0.00054872
Epoch [168/200], Train Loss: 0.000545
Validation Loss: 0.00054530
Epoch [169/200], Train Loss: 0.000544
Validation Loss: 0.00054220
Epoch [170/200], Train Loss: 0.000549
Validation Loss: 0.00053689
Epoch [171/200], Train Loss: 0.000541
Validation Loss: 0.00054439
Epoch [172/200], Train Loss: 0.000539
Validation Loss: 0.00053859
Epoch [173/200], Train Loss: 0.000540
Validation Loss: 0.00052854
Epoch [174/200], Train Loss: 0.000539
Validation Loss: 0.00052973
Epoch [175/200], Train Loss: 0.000534
Validation Loss: 0.00053568
Epoch [176/200], Train Loss: 0.000533
Validation Loss: 0.00052894
Epoch [177/200], Train Loss: 0.000535
Validation Loss: 0.00053965
Epoch [178/200], Train Loss: 0.000535
Validation Loss: 0.00052566
Epoch [179/200], Train Loss: 0.000531
Validation Loss: 0.00052900
Epoch [180/200], Train Loss: 0.000531
Validation Loss: 0.00053082
Epoch [181/200], Train Loss: 0.000531
Validation Loss: 0.00052536
Epoch [182/200], Train Loss: 0.000527
Validation Loss: 0.00052302
Epoch [183/200], Train Loss: 0.000529
Validation Loss: 0.00051997
Epoch [184/200], Train Loss: 0.000523
Validation Loss: 0.00051940
Epoch [185/200], Train Loss: 0.000530
Validation Loss: 0.00052189
Epoch [186/200], Train Loss: 0.000523
Validation Loss: 0.00051853
Epoch [187/200], Train Loss: 0.000526
Validation Loss: 0.00051276
Epoch [188/200], Train Loss: 0.000525
Validation Loss: 0.00051584
Epoch [189/200], Train Loss: 0.000522
Validation Loss: 0.00051041
Epoch [190/200], Train Loss: 0.000521
Validation Loss: 0.00051175
Epoch [191/200], Train Loss: 0.000523
Validation Loss: 0.00050993
Epoch [192/200], Train Loss: 0.000517
Validation Loss: 0.00051644
Epoch [193/200], Train Loss: 0.000521
Validation Loss: 0.00051481
Epoch [194/200], Train Loss: 0.000520
Validation Loss: 0.00051620
Epoch [195/200], Train Loss: 0.000519
Validation Loss: 0.00051240
Epoch [196/200], Train Loss: 0.000516
Validation Loss: 0.00051203
Epoch [197/200], Train Loss: 0.000518
Validation Loss: 0.00051149
Epoch [198/200], Train Loss: 0.000515
Validation Loss: 0.00050913
Epoch [199/200], Train Loss: 0.000518
Validation Loss: 0.00050748
Epoch [200/200], Train Loss: 0.000514
Validation Loss: 0.00050723

Evaluating model for: Lamp
Run 1/144 completed in 4710.53 seconds with: {'MAE': np.float32(0.49243608), 'MSE': np.float32(17.342342), 'RMSE': np.float32(4.164414), 'SAE': np.float32(0.010027549), 'NDE': np.float32(0.3198416)}

Run 2/144: hidden=64, seq_len=120, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.005224
Validation Loss: 0.00485409
Epoch [2/200], Train Loss: 0.004834
Validation Loss: 0.00476897
Epoch [3/200], Train Loss: 0.004775
Validation Loss: 0.00474895
Epoch [4/200], Train Loss: 0.004758
Validation Loss: 0.00473919
Epoch [5/200], Train Loss: 0.004752
Validation Loss: 0.00473610
Epoch [6/200], Train Loss: 0.004740
Validation Loss: 0.00473751
Epoch [7/200], Train Loss: 0.004737
Validation Loss: 0.00472617
Epoch [8/200], Train Loss: 0.004737
Validation Loss: 0.00472582
Epoch [9/200], Train Loss: 0.004726
Validation Loss: 0.00472070
Epoch [10/200], Train Loss: 0.004727
Validation Loss: 0.00471723
Epoch [11/200], Train Loss: 0.004725
Validation Loss: 0.00471802
Epoch [12/200], Train Loss: 0.004723
Validation Loss: 0.00471400
Epoch [13/200], Train Loss: 0.004720
Validation Loss: 0.00471446
Epoch [14/200], Train Loss: 0.004713
Validation Loss: 0.00471133
Epoch [15/200], Train Loss: 0.004717
Validation Loss: 0.00471514
Epoch [16/200], Train Loss: 0.004713
Validation Loss: 0.00471356
Epoch [17/200], Train Loss: 0.004712
Validation Loss: 0.00471007
Epoch [18/200], Train Loss: 0.004709
Validation Loss: 0.00471435
Epoch [19/200], Train Loss: 0.004708
Validation Loss: 0.00470195
Epoch [20/200], Train Loss: 0.004705
Validation Loss: 0.00471037
Epoch [21/200], Train Loss: 0.004699
Validation Loss: 0.00470662
Epoch [22/200], Train Loss: 0.004694
Validation Loss: 0.00468972
Epoch [23/200], Train Loss: 0.004689
Validation Loss: 0.00467520
Epoch [24/200], Train Loss: 0.004673
Validation Loss: 0.00465909
Epoch [25/200], Train Loss: 0.004661
Validation Loss: 0.00464229
Epoch [26/200], Train Loss: 0.004645
Validation Loss: 0.00465245
Epoch [27/200], Train Loss: 0.004612
Validation Loss: 0.00457091
Epoch [28/200], Train Loss: 0.004548
Validation Loss: 0.00446245
Epoch [29/200], Train Loss: 0.004394
Validation Loss: 0.00419611
Epoch [30/200], Train Loss: 0.004111
Validation Loss: 0.00395057
Epoch [31/200], Train Loss: 0.003727
Validation Loss: 0.00328255
Epoch [32/200], Train Loss: 0.003187
Validation Loss: 0.00266001
Epoch [33/200], Train Loss: 0.002671
Validation Loss: 0.00234470
Epoch [34/200], Train Loss: 0.002353
Validation Loss: 0.00217146
Epoch [35/200], Train Loss: 0.002158
Validation Loss: 0.00199279
Epoch [36/200], Train Loss: 0.002004
Validation Loss: 0.00184399
Epoch [37/200], Train Loss: 0.001876
Validation Loss: 0.00173300
Epoch [38/200], Train Loss: 0.001769
Validation Loss: 0.00165284
Epoch [39/200], Train Loss: 0.001684
Validation Loss: 0.00159062
Epoch [40/200], Train Loss: 0.001610
Validation Loss: 0.00151653
Epoch [41/200], Train Loss: 0.001533
Validation Loss: 0.00145506
Epoch [42/200], Train Loss: 0.001477
Validation Loss: 0.00140439
Epoch [43/200], Train Loss: 0.001422
Validation Loss: 0.00139389
Epoch [44/200], Train Loss: 0.001365
Validation Loss: 0.00134707
Epoch [45/200], Train Loss: 0.001330
Validation Loss: 0.00130036
Epoch [46/200], Train Loss: 0.001285
Validation Loss: 0.00128369
Epoch [47/200], Train Loss: 0.001259
Validation Loss: 0.00123333
Epoch [48/200], Train Loss: 0.001218
Validation Loss: 0.00124832
Epoch [49/200], Train Loss: 0.001199
Validation Loss: 0.00120410
Epoch [50/200], Train Loss: 0.001171
Validation Loss: 0.00118907
Epoch [51/200], Train Loss: 0.001139
Validation Loss: 0.00114863
Epoch [52/200], Train Loss: 0.001124
Validation Loss: 0.00118956
Epoch [53/200], Train Loss: 0.001097
Validation Loss: 0.00114075
Epoch [54/200], Train Loss: 0.001092
Validation Loss: 0.00113840
Epoch [55/200], Train Loss: 0.001149
Validation Loss: 0.00112388
Epoch [56/200], Train Loss: 0.001073
Validation Loss: 0.00108365
Epoch [57/200], Train Loss: 0.001046
Validation Loss: 0.00107867
Epoch [58/200], Train Loss: 0.001028
Validation Loss: 0.00111284
Epoch [59/200], Train Loss: 0.001007
Validation Loss: 0.00105783
Epoch [60/200], Train Loss: 0.000989
Validation Loss: 0.00101279
Epoch [61/200], Train Loss: 0.000967
Validation Loss: 0.00099207
Epoch [62/200], Train Loss: 0.000954
Validation Loss: 0.00096405
Epoch [63/200], Train Loss: 0.000934
Validation Loss: 0.00095818
Epoch [64/200], Train Loss: 0.000921
Validation Loss: 0.00093207
Epoch [65/200], Train Loss: 0.000897
Validation Loss: 0.00091936
Epoch [66/200], Train Loss: 0.000917
Validation Loss: 0.00089145
Epoch [67/200], Train Loss: 0.000869
Validation Loss: 0.00087887
Epoch [68/200], Train Loss: 0.000850
Validation Loss: 0.00085741
Epoch [69/200], Train Loss: 0.000839
Validation Loss: 0.00083941
Epoch [70/200], Train Loss: 0.000829
Validation Loss: 0.00083630
Epoch [71/200], Train Loss: 0.000817
Validation Loss: 0.00082590
Epoch [72/200], Train Loss: 0.000804
Validation Loss: 0.00081411
Epoch [73/200], Train Loss: 0.000798
Validation Loss: 0.00082012
Epoch [74/200], Train Loss: 0.000789
Validation Loss: 0.00078113
Epoch [75/200], Train Loss: 0.000776
Validation Loss: 0.00078080
Epoch [76/200], Train Loss: 0.000767
Validation Loss: 0.00077781
Epoch [77/200], Train Loss: 0.000751
Validation Loss: 0.00076321
Epoch [78/200], Train Loss: 0.000764
Validation Loss: 0.00075049
Epoch [79/200], Train Loss: 0.000745
Validation Loss: 0.00074600
Epoch [80/200], Train Loss: 0.000745
Validation Loss: 0.00073368
Epoch [81/200], Train Loss: 0.000733
Validation Loss: 0.00072849
Epoch [82/200], Train Loss: 0.000729
Validation Loss: 0.00072552
Epoch [83/200], Train Loss: 0.000727
Validation Loss: 0.00072017
Epoch [84/200], Train Loss: 0.000713
Validation Loss: 0.00071749
Epoch [85/200], Train Loss: 0.000709
Validation Loss: 0.00070253
Epoch [86/200], Train Loss: 0.000700
Validation Loss: 0.00071530
Epoch [87/200], Train Loss: 0.000697
Validation Loss: 0.00068781
Epoch [88/200], Train Loss: 0.000689
Validation Loss: 0.00070105
Epoch [89/200], Train Loss: 0.000687
Validation Loss: 0.00068571
Epoch [90/200], Train Loss: 0.000679
Validation Loss: 0.00067093
Epoch [91/200], Train Loss: 0.000675
Validation Loss: 0.00067455
Epoch [92/200], Train Loss: 0.000669
Validation Loss: 0.00066817
Epoch [93/200], Train Loss: 0.000666
Validation Loss: 0.00066414
Epoch [94/200], Train Loss: 0.000661
Validation Loss: 0.00065794
Epoch [95/200], Train Loss: 0.000659
Validation Loss: 0.00064809
Epoch [96/200], Train Loss: 0.000651
Validation Loss: 0.00063625
Epoch [97/200], Train Loss: 0.000649
Validation Loss: 0.00063441
Epoch [98/200], Train Loss: 0.000643
Validation Loss: 0.00063589
Epoch [99/200], Train Loss: 0.000639
Validation Loss: 0.00062720
Epoch [100/200], Train Loss: 0.000659
Validation Loss: 0.00064083
Epoch [101/200], Train Loss: 0.000637
Validation Loss: 0.00063593
Epoch [102/200], Train Loss: 0.000625
Validation Loss: 0.00062003
Epoch [103/200], Train Loss: 0.000622
Validation Loss: 0.00063389
Epoch [104/200], Train Loss: 0.000618
Validation Loss: 0.00062192
Epoch [105/200], Train Loss: 0.000614
Validation Loss: 0.00062316
Epoch [106/200], Train Loss: 0.000610
Validation Loss: 0.00059011
Epoch [107/200], Train Loss: 0.000609
Validation Loss: 0.00060693
Epoch [108/200], Train Loss: 0.000603
Validation Loss: 0.00059347
Epoch [109/200], Train Loss: 0.000602
Validation Loss: 0.00060941
Epoch [110/200], Train Loss: 0.000597
Validation Loss: 0.00058466
Epoch [111/200], Train Loss: 0.000596
Validation Loss: 0.00058188
Epoch [112/200], Train Loss: 0.000594
Validation Loss: 0.00057355
Epoch [113/200], Train Loss: 0.000588
Validation Loss: 0.00059058
Epoch [114/200], Train Loss: 0.000587
Validation Loss: 0.00056714
Epoch [115/200], Train Loss: 0.000582
Validation Loss: 0.00058150
Epoch [116/200], Train Loss: 0.000580
Validation Loss: 0.00058057
Epoch [117/200], Train Loss: 0.000578
Validation Loss: 0.00058207
Epoch [118/200], Train Loss: 0.000576
Validation Loss: 0.00057145
Epoch [119/200], Train Loss: 0.000571
Validation Loss: 0.00056392
Epoch [120/200], Train Loss: 0.000575
Validation Loss: 0.00055299
Epoch [121/200], Train Loss: 0.000559
Validation Loss: 0.00055133
Epoch [122/200], Train Loss: 0.000563
Validation Loss: 0.00055104
Epoch [123/200], Train Loss: 0.000566
Validation Loss: 0.00054941
Epoch [124/200], Train Loss: 0.000560
Validation Loss: 0.00054017
Epoch [125/200], Train Loss: 0.000558
Validation Loss: 0.00054081
Epoch [126/200], Train Loss: 0.000557
Validation Loss: 0.00054079
Epoch [127/200], Train Loss: 0.000551
Validation Loss: 0.00054480
Epoch [128/200], Train Loss: 0.000549
Validation Loss: 0.00056209
Epoch [129/200], Train Loss: 0.000549
Validation Loss: 0.00053547
Epoch [130/200], Train Loss: 0.000550
Validation Loss: 0.00055926
Epoch [131/200], Train Loss: 0.000545
Validation Loss: 0.00052931
Epoch [132/200], Train Loss: 0.000540
Validation Loss: 0.00052954
Epoch [133/200], Train Loss: 0.000540
Validation Loss: 0.00052407
Epoch [134/200], Train Loss: 0.000540
Validation Loss: 0.00054397
Epoch [135/200], Train Loss: 0.000534
Validation Loss: 0.00051346
Epoch [136/200], Train Loss: 0.000534
Validation Loss: 0.00052237
Epoch [137/200], Train Loss: 0.000529
Validation Loss: 0.00051124
Epoch [138/200], Train Loss: 0.000533
Validation Loss: 0.00051779
Epoch [139/200], Train Loss: 0.000528
Validation Loss: 0.00051305
Epoch [140/200], Train Loss: 0.000523
Validation Loss: 0.00050895
Epoch [141/200], Train Loss: 0.000526
Validation Loss: 0.00051508
Epoch [142/200], Train Loss: 0.000524
Validation Loss: 0.00050691
Epoch [143/200], Train Loss: 0.000527
Validation Loss: 0.00054357
Epoch [144/200], Train Loss: 0.000525
Validation Loss: 0.00050123
Epoch [145/200], Train Loss: 0.000516
Validation Loss: 0.00051002
Epoch [146/200], Train Loss: 0.000513
Validation Loss: 0.00050200
Epoch [147/200], Train Loss: 0.000513
Validation Loss: 0.00052421
Epoch [148/200], Train Loss: 0.000518
Validation Loss: 0.00049993
Epoch [149/200], Train Loss: 0.000515
Validation Loss: 0.00052080
Epoch [150/200], Train Loss: 0.000511
Validation Loss: 0.00050558
Epoch [151/200], Train Loss: 0.000514
Validation Loss: 0.00049305
Epoch [152/200], Train Loss: 0.000509
Validation Loss: 0.00050716
Epoch [153/200], Train Loss: 0.000508
Validation Loss: 0.00048970
Epoch [154/200], Train Loss: 0.000505
Validation Loss: 0.00051019
Epoch [155/200], Train Loss: 0.000506
Validation Loss: 0.00049869
Epoch [156/200], Train Loss: 0.000509
Validation Loss: 0.00048833
Epoch [157/200], Train Loss: 0.000505
Validation Loss: 0.00050344
Epoch [158/200], Train Loss: 0.000502
Validation Loss: 0.00048604
Epoch [159/200], Train Loss: 0.000499
Validation Loss: 0.00049267
Epoch [160/200], Train Loss: 0.000500
Validation Loss: 0.00049245
Epoch [161/200], Train Loss: 0.000496
Validation Loss: 0.00048945
Epoch [162/200], Train Loss: 0.000499
Validation Loss: 0.00047981
Epoch [163/200], Train Loss: 0.000494
Validation Loss: 0.00049070
Epoch [164/200], Train Loss: 0.000492
Validation Loss: 0.00048901
Epoch [165/200], Train Loss: 0.000495
Validation Loss: 0.00047758
Epoch [166/200], Train Loss: 0.000496
Validation Loss: 0.00047434
Epoch [167/200], Train Loss: 0.000488
Validation Loss: 0.00047094
Epoch [168/200], Train Loss: 0.000486
Validation Loss: 0.00047128
Epoch [169/200], Train Loss: 0.000486
Validation Loss: 0.00047501
Epoch [170/200], Train Loss: 0.000491
Validation Loss: 0.00048415
Epoch [171/200], Train Loss: 0.000487
Validation Loss: 0.00047982
Epoch [172/200], Train Loss: 0.000489
Validation Loss: 0.00048166
Epoch [173/200], Train Loss: 0.000481
Validation Loss: 0.00046953
Epoch [174/200], Train Loss: 0.000483
Validation Loss: 0.00046659
Epoch [175/200], Train Loss: 0.000487
Validation Loss: 0.00046474
Epoch [176/200], Train Loss: 0.000485
Validation Loss: 0.00046624
Epoch [177/200], Train Loss: 0.000481
Validation Loss: 0.00047096
Epoch [178/200], Train Loss: 0.000480
Validation Loss: 0.00046680
Epoch [179/200], Train Loss: 0.000481
Validation Loss: 0.00046323
Epoch [180/200], Train Loss: 0.000479
Validation Loss: 0.00046385
Epoch [181/200], Train Loss: 0.000475
Validation Loss: 0.00046339
Epoch [182/200], Train Loss: 0.000474
Validation Loss: 0.00046546
Epoch [183/200], Train Loss: 0.000474
Validation Loss: 0.00047112
Epoch [184/200], Train Loss: 0.000476
Validation Loss: 0.00046400
Epoch [185/200], Train Loss: 0.000472
Validation Loss: 0.00045650
Epoch [186/200], Train Loss: 0.000475
Validation Loss: 0.00045769
Epoch [187/200], Train Loss: 0.000472
Validation Loss: 0.00046086
Epoch [188/200], Train Loss: 0.000471
Validation Loss: 0.00046126
Epoch [189/200], Train Loss: 0.000467
Validation Loss: 0.00045225
Epoch [190/200], Train Loss: 0.000472
Validation Loss: 0.00045140
Epoch [191/200], Train Loss: 0.000467
Validation Loss: 0.00045278
Epoch [192/200], Train Loss: 0.000467
Validation Loss: 0.00045529
Epoch [193/200], Train Loss: 0.000467
Validation Loss: 0.00044887
Epoch [194/200], Train Loss: 0.000466
Validation Loss: 0.00045946
Epoch [195/200], Train Loss: 0.000464
Validation Loss: 0.00045862
Epoch [196/200], Train Loss: 0.000466
Validation Loss: 0.00045138
Epoch [197/200], Train Loss: 0.000465
Validation Loss: 0.00045311
Epoch [198/200], Train Loss: 0.000463
Validation Loss: 0.00044604
Epoch [199/200], Train Loss: 0.000466
Validation Loss: 0.00045643
Epoch [200/200], Train Loss: 0.000464
Validation Loss: 0.00044999

Evaluating model for: Lamp
Run 2/144 completed in 4856.19 seconds with: {'MAE': np.float32(0.42067048), 'MSE': np.float32(16.087282), 'RMSE': np.float32(4.0108953), 'SAE': np.float32(0.02382945), 'NDE': np.float32(0.30805042)}

Run 3/144: hidden=64, seq_len=120, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.006250
Validation Loss: 0.00488341
Epoch [2/200], Train Loss: 0.004894
Validation Loss: 0.00482912
Epoch [3/200], Train Loss: 0.004849
Validation Loss: 0.00479365
Epoch [4/200], Train Loss: 0.004818
Validation Loss: 0.00477308
Epoch [5/200], Train Loss: 0.004795
Validation Loss: 0.00476388
Epoch [6/200], Train Loss: 0.004779
Validation Loss: 0.00474557
Epoch [7/200], Train Loss: 0.004768
Validation Loss: 0.00473975
Epoch [8/200], Train Loss: 0.004761
Validation Loss: 0.00473760
Epoch [9/200], Train Loss: 0.004751
Validation Loss: 0.00473457
Epoch [10/200], Train Loss: 0.004745
Validation Loss: 0.00473126
Epoch [11/200], Train Loss: 0.004744
Validation Loss: 0.00472456
Epoch [12/200], Train Loss: 0.004741
Validation Loss: 0.00472278
Epoch [13/200], Train Loss: 0.004738
Validation Loss: 0.00472803
Epoch [14/200], Train Loss: 0.004732
Validation Loss: 0.00472076
Epoch [15/200], Train Loss: 0.004733
Validation Loss: 0.00471767
Epoch [16/200], Train Loss: 0.004726
Validation Loss: 0.00472396
Epoch [17/200], Train Loss: 0.004721
Validation Loss: 0.00471352
Epoch [18/200], Train Loss: 0.004721
Validation Loss: 0.00472346
Epoch [19/200], Train Loss: 0.004720
Validation Loss: 0.00472168
Epoch [20/200], Train Loss: 0.004717
Validation Loss: 0.00471191
Epoch [21/200], Train Loss: 0.004714
Validation Loss: 0.00470861
Epoch [22/200], Train Loss: 0.004711
Validation Loss: 0.00471092
Epoch [23/200], Train Loss: 0.004715
Validation Loss: 0.00470556
Epoch [24/200], Train Loss: 0.004707
Validation Loss: 0.00470313
Epoch [25/200], Train Loss: 0.004709
Validation Loss: 0.00470718
Epoch [26/200], Train Loss: 0.004704
Validation Loss: 0.00470152
Epoch [27/200], Train Loss: 0.004700
Validation Loss: 0.00469750
Epoch [28/200], Train Loss: 0.004700
Validation Loss: 0.00469446
Epoch [29/200], Train Loss: 0.004699
Validation Loss: 0.00469516
Epoch [30/200], Train Loss: 0.004695
Validation Loss: 0.00469098
Epoch [31/200], Train Loss: 0.004694
Validation Loss: 0.00469248
Epoch [32/200], Train Loss: 0.004685
Validation Loss: 0.00468399
Epoch [33/200], Train Loss: 0.004687
Validation Loss: 0.00468679
Epoch [34/200], Train Loss: 0.004684
Validation Loss: 0.00467574
Epoch [35/200], Train Loss: 0.004681
Validation Loss: 0.00467787
Epoch [36/200], Train Loss: 0.004682
Validation Loss: 0.00467533
Epoch [37/200], Train Loss: 0.004675
Validation Loss: 0.00466610
Epoch [38/200], Train Loss: 0.004671
Validation Loss: 0.00466138
Epoch [39/200], Train Loss: 0.004668
Validation Loss: 0.00464556
Epoch [40/200], Train Loss: 0.004659
Validation Loss: 0.00465374
Epoch [41/200], Train Loss: 0.004661
Validation Loss: 0.00464247
Epoch [42/200], Train Loss: 0.004650
Validation Loss: 0.00463413
Epoch [43/200], Train Loss: 0.004638
Validation Loss: 0.00462811
Epoch [44/200], Train Loss: 0.004629
Validation Loss: 0.00460223
Epoch [45/200], Train Loss: 0.004619
Validation Loss: 0.00461725
Epoch [46/200], Train Loss: 0.004606
Validation Loss: 0.00457830
Epoch [47/200], Train Loss: 0.004587
Validation Loss: 0.00453369
Epoch [48/200], Train Loss: 0.004548
Validation Loss: 0.00444623
Epoch [49/200], Train Loss: 0.004469
Validation Loss: 0.00428528
Epoch [50/200], Train Loss: 0.004315
Validation Loss: 0.00396927
Epoch [51/200], Train Loss: 0.004012
Validation Loss: 0.00348989
Epoch [52/200], Train Loss: 0.003542
Validation Loss: 0.00302418
Epoch [53/200], Train Loss: 0.003088
Validation Loss: 0.00282500
Epoch [54/200], Train Loss: 0.002786
Validation Loss: 0.00246800
Epoch [55/200], Train Loss: 0.002558
Validation Loss: 0.00236828
Epoch [56/200], Train Loss: 0.002386
Validation Loss: 0.00212797
Epoch [57/200], Train Loss: 0.002251
Validation Loss: 0.00206099
Epoch [58/200], Train Loss: 0.002134
Validation Loss: 0.00191691
Epoch [59/200], Train Loss: 0.002027
Validation Loss: 0.00186551
Epoch [60/200], Train Loss: 0.001930
Validation Loss: 0.00172678
Epoch [61/200], Train Loss: 0.001844
Validation Loss: 0.00167036
Epoch [62/200], Train Loss: 0.001767
Validation Loss: 0.00166410
Epoch [63/200], Train Loss: 0.001702
Validation Loss: 0.00159377
Epoch [64/200], Train Loss: 0.001656
Validation Loss: 0.00155512
Epoch [65/200], Train Loss: 0.001598
Validation Loss: 0.00148559
Epoch [66/200], Train Loss: 0.001544
Validation Loss: 0.00150900
Epoch [67/200], Train Loss: 0.001494
Validation Loss: 0.00140465
Epoch [68/200], Train Loss: 0.001450
Validation Loss: 0.00144352
Epoch [69/200], Train Loss: 0.001407
Validation Loss: 0.00132511
Epoch [70/200], Train Loss: 0.001375
Validation Loss: 0.00130981
Epoch [71/200], Train Loss: 0.001328
Validation Loss: 0.00134786
Epoch [72/200], Train Loss: 0.001293
Validation Loss: 0.00122766
Epoch [73/200], Train Loss: 0.001265
Validation Loss: 0.00121608
Epoch [74/200], Train Loss: 0.001231
Validation Loss: 0.00116174
Epoch [75/200], Train Loss: 0.001201
Validation Loss: 0.00113386
Epoch [76/200], Train Loss: 0.001157
Validation Loss: 0.00121213
Epoch [77/200], Train Loss: 0.001131
Validation Loss: 0.00109078
Epoch [78/200], Train Loss: 0.001098
Validation Loss: 0.00103583
Epoch [79/200], Train Loss: 0.001074
Validation Loss: 0.00103449
Epoch [80/200], Train Loss: 0.001038
Validation Loss: 0.00096310
Epoch [81/200], Train Loss: 0.001019
Validation Loss: 0.00097533
Epoch [82/200], Train Loss: 0.000996
Validation Loss: 0.00099754
Epoch [83/200], Train Loss: 0.000976
Validation Loss: 0.00091535
Epoch [84/200], Train Loss: 0.000948
Validation Loss: 0.00088004
Epoch [85/200], Train Loss: 0.000937
Validation Loss: 0.00087287
Epoch [86/200], Train Loss: 0.000918
Validation Loss: 0.00085317
Epoch [87/200], Train Loss: 0.000910
Validation Loss: 0.00085583
Epoch [88/200], Train Loss: 0.000894
Validation Loss: 0.00082619
Epoch [89/200], Train Loss: 0.000860
Validation Loss: 0.00080899
Epoch [90/200], Train Loss: 0.000856
Validation Loss: 0.00079442
Epoch [91/200], Train Loss: 0.000851
Validation Loss: 0.00079373
Epoch [92/200], Train Loss: 0.000830
Validation Loss: 0.00078044
Epoch [93/200], Train Loss: 0.000826
Validation Loss: 0.00077169
Epoch [94/200], Train Loss: 0.000825
Validation Loss: 0.00078644
Epoch [95/200], Train Loss: 0.000793
Validation Loss: 0.00075382
Epoch [96/200], Train Loss: 0.000792
Validation Loss: 0.00075084
Epoch [97/200], Train Loss: 0.000793
Validation Loss: 0.00073479
Epoch [98/200], Train Loss: 0.000779
Validation Loss: 0.00073322
Epoch [99/200], Train Loss: 0.000766
Validation Loss: 0.00073089
Epoch [100/200], Train Loss: 0.000763
Validation Loss: 0.00072636
Epoch [101/200], Train Loss: 0.000756
Validation Loss: 0.00071646
Epoch [102/200], Train Loss: 0.000749
Validation Loss: 0.00070862
Epoch [103/200], Train Loss: 0.000742
Validation Loss: 0.00070596
Epoch [104/200], Train Loss: 0.000737
Validation Loss: 0.00069921
Epoch [105/200], Train Loss: 0.000730
Validation Loss: 0.00069163
Epoch [106/200], Train Loss: 0.000720
Validation Loss: 0.00071539
Epoch [107/200], Train Loss: 0.000713
Validation Loss: 0.00067443
Epoch [108/200], Train Loss: 0.000712
Validation Loss: 0.00068388
Epoch [109/200], Train Loss: 0.000708
Validation Loss: 0.00066777
Epoch [110/200], Train Loss: 0.000699
Validation Loss: 0.00065096
Epoch [111/200], Train Loss: 0.000691
Validation Loss: 0.00067159
Epoch [112/200], Train Loss: 0.000691
Validation Loss: 0.00073516
Epoch [113/200], Train Loss: 0.000688
Validation Loss: 0.00063802
Epoch [114/200], Train Loss: 0.000676
Validation Loss: 0.00064782
Epoch [115/200], Train Loss: 0.000682
Validation Loss: 0.00062935
Epoch [116/200], Train Loss: 0.000684
Validation Loss: 0.00066842
Epoch [117/200], Train Loss: 0.000668
Validation Loss: 0.00061648
Epoch [118/200], Train Loss: 0.000660
Validation Loss: 0.00062665
Epoch [119/200], Train Loss: 0.000671
Validation Loss: 0.00062242
Epoch [120/200], Train Loss: 0.000657
Validation Loss: 0.00060299
Epoch [121/200], Train Loss: 0.000645
Validation Loss: 0.00060270
Epoch [122/200], Train Loss: 0.000642
Validation Loss: 0.00059606
Epoch [123/200], Train Loss: 0.000642
Validation Loss: 0.00060042
Epoch [124/200], Train Loss: 0.000643
Validation Loss: 0.00059372
Epoch [125/200], Train Loss: 0.000628
Validation Loss: 0.00061150
Epoch [126/200], Train Loss: 0.000630
Validation Loss: 0.00058631
Epoch [127/200], Train Loss: 0.000626
Validation Loss: 0.00063410
Epoch [128/200], Train Loss: 0.000632
Validation Loss: 0.00058797
Epoch [129/200], Train Loss: 0.000627
Validation Loss: 0.00059460
Epoch [130/200], Train Loss: 0.000625
Validation Loss: 0.00057530
Epoch [131/200], Train Loss: 0.000622
Validation Loss: 0.00056808
Epoch [132/200], Train Loss: 0.000609
Validation Loss: 0.00057028
Epoch [133/200], Train Loss: 0.000614
Validation Loss: 0.00058172
Epoch [134/200], Train Loss: 0.000616
Validation Loss: 0.00056646
Epoch [135/200], Train Loss: 0.000604
Validation Loss: 0.00056933
Epoch [136/200], Train Loss: 0.000608
Validation Loss: 0.00056282
Epoch [137/200], Train Loss: 0.000604
Validation Loss: 0.00055794
Epoch [138/200], Train Loss: 0.000599
Validation Loss: 0.00055198
Epoch [139/200], Train Loss: 0.000601
Validation Loss: 0.00056130
Epoch [140/200], Train Loss: 0.000590
Validation Loss: 0.00055658
Epoch [141/200], Train Loss: 0.000590
Validation Loss: 0.00054731
Epoch [142/200], Train Loss: 0.000592
Validation Loss: 0.00054488
Epoch [143/200], Train Loss: 0.000587
Validation Loss: 0.00054897
Epoch [144/200], Train Loss: 0.000585
Validation Loss: 0.00054163
Epoch [145/200], Train Loss: 0.000588
Validation Loss: 0.00054586
Epoch [146/200], Train Loss: 0.000578
Validation Loss: 0.00057514
Epoch [147/200], Train Loss: 0.000588
Validation Loss: 0.00055178
Epoch [148/200], Train Loss: 0.000573
Validation Loss: 0.00054037
Epoch [149/200], Train Loss: 0.000574
Validation Loss: 0.00054052
Epoch [150/200], Train Loss: 0.000603
Validation Loss: 0.00064527
Epoch [151/200], Train Loss: 0.000595
Validation Loss: 0.00057738
Epoch [152/200], Train Loss: 0.000574
Validation Loss: 0.00053672
Epoch [153/200], Train Loss: 0.000563
Validation Loss: 0.00053345
Epoch [154/200], Train Loss: 0.000564
Validation Loss: 0.00052526
Epoch [155/200], Train Loss: 0.000561
Validation Loss: 0.00051562
Epoch [156/200], Train Loss: 0.000563
Validation Loss: 0.00051824
Epoch [157/200], Train Loss: 0.000559
Validation Loss: 0.00052611
Epoch [158/200], Train Loss: 0.000554
Validation Loss: 0.00051327
Epoch [159/200], Train Loss: 0.000558
Validation Loss: 0.00051425
Epoch [160/200], Train Loss: 0.000561
Validation Loss: 0.00053040
Epoch [161/200], Train Loss: 0.000552
Validation Loss: 0.00050978
Epoch [162/200], Train Loss: 0.000549
Validation Loss: 0.00053550
Epoch [163/200], Train Loss: 0.000554
Validation Loss: 0.00052794
Epoch [164/200], Train Loss: 0.000546
Validation Loss: 0.00050441
Epoch [165/200], Train Loss: 0.000546
Validation Loss: 0.00051345
Epoch [166/200], Train Loss: 0.000538
Validation Loss: 0.00051715
Epoch [167/200], Train Loss: 0.000545
Validation Loss: 0.00051079
Epoch [168/200], Train Loss: 0.000542
Validation Loss: 0.00051430
Epoch [169/200], Train Loss: 0.000542
Validation Loss: 0.00049776
Epoch [170/200], Train Loss: 0.000533
Validation Loss: 0.00050756
Epoch [171/200], Train Loss: 0.000540
Validation Loss: 0.00050456
Epoch [172/200], Train Loss: 0.000531
Validation Loss: 0.00049364
Epoch [173/200], Train Loss: 0.000531
Validation Loss: 0.00048813
Epoch [174/200], Train Loss: 0.000530
Validation Loss: 0.00048990
Epoch [175/200], Train Loss: 0.000526
Validation Loss: 0.00048809
Epoch [176/200], Train Loss: 0.000534
Validation Loss: 0.00049571
Epoch [177/200], Train Loss: 0.000529
Validation Loss: 0.00049147
Epoch [178/200], Train Loss: 0.000527
Validation Loss: 0.00049097
Epoch [179/200], Train Loss: 0.000529
Validation Loss: 0.00049275
Epoch [180/200], Train Loss: 0.000526
Validation Loss: 0.00048785
Epoch [181/200], Train Loss: 0.000524
Validation Loss: 0.00048679
Epoch [182/200], Train Loss: 0.000520
Validation Loss: 0.00047907
Epoch [183/200], Train Loss: 0.000524
Validation Loss: 0.00047916
Epoch [184/200], Train Loss: 0.000516
Validation Loss: 0.00048116
Epoch [185/200], Train Loss: 0.000515
Validation Loss: 0.00052711
Epoch [186/200], Train Loss: 0.000517
Validation Loss: 0.00047573
Epoch [187/200], Train Loss: 0.000510
Validation Loss: 0.00047657
Epoch [188/200], Train Loss: 0.000517
Validation Loss: 0.00047480
Epoch [189/200], Train Loss: 0.000510
Validation Loss: 0.00048059
Epoch [190/200], Train Loss: 0.000511
Validation Loss: 0.00047925
Epoch [191/200], Train Loss: 0.000511
Validation Loss: 0.00047604
Epoch [192/200], Train Loss: 0.000500
Validation Loss: 0.00046766
Epoch [193/200], Train Loss: 0.000510
Validation Loss: 0.00048496
Epoch [194/200], Train Loss: 0.000508
Validation Loss: 0.00046867
Epoch [195/200], Train Loss: 0.000506
Validation Loss: 0.00047199
Epoch [196/200], Train Loss: 0.000499
Validation Loss: 0.00046686
Epoch [197/200], Train Loss: 0.000503
Validation Loss: 0.00046144
Epoch [198/200], Train Loss: 0.000503
Validation Loss: 0.00046334
Epoch [199/200], Train Loss: 0.000499
Validation Loss: 0.00045795
Epoch [200/200], Train Loss: 0.000498
Validation Loss: 0.00045692

Evaluating model for: Lamp
Run 3/144 completed in 4837.07 seconds with: {'MAE': np.float32(0.42186397), 'MSE': np.float32(16.704294), 'RMSE': np.float32(4.0870886), 'SAE': np.float32(0.015380235), 'NDE': np.float32(0.31390262)}

Run 4/144: hidden=64, seq_len=120, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004865
Validation Loss: 0.00482628
Epoch [2/200], Train Loss: 0.004754
Validation Loss: 0.00473492
Epoch [3/200], Train Loss: 0.004730
Validation Loss: 0.00472753
Epoch [4/200], Train Loss: 0.004718
Validation Loss: 0.00472466
Epoch [5/200], Train Loss: 0.004714
Validation Loss: 0.00471922
Epoch [6/200], Train Loss: 0.004711
Validation Loss: 0.00472969
Epoch [7/200], Train Loss: 0.004710
Validation Loss: 0.00471515
Epoch [8/200], Train Loss: 0.004708
Validation Loss: 0.00471983
Epoch [9/200], Train Loss: 0.004702
Validation Loss: 0.00471100
Epoch [10/200], Train Loss: 0.004707
Validation Loss: 0.00471070
Epoch [11/200], Train Loss: 0.004709
Validation Loss: 0.00471210
Epoch [12/200], Train Loss: 0.004704
Validation Loss: 0.00471355
Epoch [13/200], Train Loss: 0.004704
Validation Loss: 0.00470988
Epoch [14/200], Train Loss: 0.004703
Validation Loss: 0.00471148
Epoch [15/200], Train Loss: 0.004700
Validation Loss: 0.00471100
Epoch [16/200], Train Loss: 0.004702
Validation Loss: 0.00471476
Epoch [17/200], Train Loss: 0.004699
Validation Loss: 0.00471432
Epoch [18/200], Train Loss: 0.004699
Validation Loss: 0.00471357
Epoch [19/200], Train Loss: 0.004700
Validation Loss: 0.00470737
Epoch [20/200], Train Loss: 0.004698
Validation Loss: 0.00471420
Epoch [21/200], Train Loss: 0.004698
Validation Loss: 0.00471201
Epoch [22/200], Train Loss: 0.004700
Validation Loss: 0.00471256
Epoch [23/200], Train Loss: 0.004702
Validation Loss: 0.00470878
Epoch [24/200], Train Loss: 0.004697
Validation Loss: 0.00470601
Epoch [25/200], Train Loss: 0.004698
Validation Loss: 0.00470620
Epoch [26/200], Train Loss: 0.004696
Validation Loss: 0.00470178
Epoch [27/200], Train Loss: 0.004691
Validation Loss: 0.00469814
Epoch [28/200], Train Loss: 0.004692
Validation Loss: 0.00469559
Epoch [29/200], Train Loss: 0.004681
Validation Loss: 0.00469617
Epoch [30/200], Train Loss: 0.004682
Validation Loss: 0.00469019
Epoch [31/200], Train Loss: 0.004674
Validation Loss: 0.00468961
Epoch [32/200], Train Loss: 0.004671
Validation Loss: 0.00467991
Epoch [33/200], Train Loss: 0.004665
Validation Loss: 0.00467792
Epoch [34/200], Train Loss: 0.004666
Validation Loss: 0.00467658
Epoch [35/200], Train Loss: 0.004665
Validation Loss: 0.00466170
Epoch [36/200], Train Loss: 0.004654
Validation Loss: 0.00464964
Epoch [37/200], Train Loss: 0.004647
Validation Loss: 0.00465550
Epoch [38/200], Train Loss: 0.004639
Validation Loss: 0.00463228
Epoch [39/200], Train Loss: 0.004626
Validation Loss: 0.00461062
Epoch [40/200], Train Loss: 0.004598
Validation Loss: 0.00455859
Epoch [41/200], Train Loss: 0.004528
Validation Loss: 0.00442694
Epoch [42/200], Train Loss: 0.004352
Validation Loss: 0.00408844
Epoch [43/200], Train Loss: 0.003956
Validation Loss: 0.00340761
Epoch [44/200], Train Loss: 0.003285
Validation Loss: 0.00268698
Epoch [45/200], Train Loss: 0.002729
Validation Loss: 0.00224532
Epoch [46/200], Train Loss: 0.002406
Validation Loss: 0.00206666
Epoch [47/200], Train Loss: 0.002200
Validation Loss: 0.00192777
Epoch [48/200], Train Loss: 0.002041
Validation Loss: 0.00178955
Epoch [49/200], Train Loss: 0.001901
Validation Loss: 0.00173142
Epoch [50/200], Train Loss: 0.001801
Validation Loss: 0.00164991
Epoch [51/200], Train Loss: 0.001699
Validation Loss: 0.00155563
Epoch [52/200], Train Loss: 0.001611
Validation Loss: 0.00148596
Epoch [53/200], Train Loss: 0.001535
Validation Loss: 0.00142443
Epoch [54/200], Train Loss: 0.001472
Validation Loss: 0.00137454
Epoch [55/200], Train Loss: 0.001402
Validation Loss: 0.00131914
Epoch [56/200], Train Loss: 0.001349
Validation Loss: 0.00127981
Epoch [57/200], Train Loss: 0.001289
Validation Loss: 0.00126018
Epoch [58/200], Train Loss: 0.001264
Validation Loss: 0.00120116
Epoch [59/200], Train Loss: 0.001215
Validation Loss: 0.00116588
Epoch [60/200], Train Loss: 0.001190
Validation Loss: 0.00118288
Epoch [61/200], Train Loss: 0.001133
Validation Loss: 0.00108851
Epoch [62/200], Train Loss: 0.001100
Validation Loss: 0.00106990
Epoch [63/200], Train Loss: 0.001082
Validation Loss: 0.00113753
Epoch [64/200], Train Loss: 0.001052
Validation Loss: 0.00101855
Epoch [65/200], Train Loss: 0.001021
Validation Loss: 0.00098059
Epoch [66/200], Train Loss: 0.001000
Validation Loss: 0.00096449
Epoch [67/200], Train Loss: 0.000968
Validation Loss: 0.00093272
Epoch [68/200], Train Loss: 0.000941
Validation Loss: 0.00090081
Epoch [69/200], Train Loss: 0.000902
Validation Loss: 0.00084810
Epoch [70/200], Train Loss: 0.000868
Validation Loss: 0.00082040
Epoch [71/200], Train Loss: 0.000846
Validation Loss: 0.00079856
Epoch [72/200], Train Loss: 0.000815
Validation Loss: 0.00078037
Epoch [73/200], Train Loss: 0.000801
Validation Loss: 0.00075761
Epoch [74/200], Train Loss: 0.000788
Validation Loss: 0.00075676
Epoch [75/200], Train Loss: 0.000772
Validation Loss: 0.00073330
Epoch [76/200], Train Loss: 0.000754
Validation Loss: 0.00071014
Epoch [77/200], Train Loss: 0.000749
Validation Loss: 0.00070068
Epoch [78/200], Train Loss: 0.000728
Validation Loss: 0.00069305
Epoch [79/200], Train Loss: 0.000727
Validation Loss: 0.00068428
Epoch [80/200], Train Loss: 0.000717
Validation Loss: 0.00066998
Epoch [81/200], Train Loss: 0.000703
Validation Loss: 0.00068200
Epoch [82/200], Train Loss: 0.000732
Validation Loss: 0.00066065
Epoch [83/200], Train Loss: 0.000681
Validation Loss: 0.00065499
Epoch [84/200], Train Loss: 0.000673
Validation Loss: 0.00064867
Epoch [85/200], Train Loss: 0.000672
Validation Loss: 0.00066350
Epoch [86/200], Train Loss: 0.000663
Validation Loss: 0.00062968
Epoch [87/200], Train Loss: 0.000649
Validation Loss: 0.00063145
Epoch [88/200], Train Loss: 0.000666
Validation Loss: 0.00063876
Epoch [89/200], Train Loss: 0.000644
Validation Loss: 0.00061412
Epoch [90/200], Train Loss: 0.000626
Validation Loss: 0.00062695
Epoch [91/200], Train Loss: 0.000628
Validation Loss: 0.00060080
Epoch [92/200], Train Loss: 0.000618
Validation Loss: 0.00059890
Epoch [93/200], Train Loss: 0.000613
Validation Loss: 0.00059237
Epoch [94/200], Train Loss: 0.000612
Validation Loss: 0.00058468
Epoch [95/200], Train Loss: 0.000606
Validation Loss: 0.00058735
Epoch [96/200], Train Loss: 0.000608
Validation Loss: 0.00058799
Epoch [97/200], Train Loss: 0.000600
Validation Loss: 0.00058061
Epoch [98/200], Train Loss: 0.000596
Validation Loss: 0.00059032
Epoch [99/200], Train Loss: 0.000595
Validation Loss: 0.00057596
Epoch [100/200], Train Loss: 0.000590
Validation Loss: 0.00058449
Epoch [101/200], Train Loss: 0.000587
Validation Loss: 0.00056959
Epoch [102/200], Train Loss: 0.000576
Validation Loss: 0.00056657
Epoch [103/200], Train Loss: 0.000571
Validation Loss: 0.00056418
Epoch [104/200], Train Loss: 0.000572
Validation Loss: 0.00056805
Epoch [105/200], Train Loss: 0.000566
Validation Loss: 0.00055193
Epoch [106/200], Train Loss: 0.000562
Validation Loss: 0.00055474
Epoch [107/200], Train Loss: 0.000554
Validation Loss: 0.00054098
Epoch [108/200], Train Loss: 0.000557
Validation Loss: 0.00054555
Epoch [109/200], Train Loss: 0.000553
Validation Loss: 0.00054503
Epoch [110/200], Train Loss: 0.000546
Validation Loss: 0.00053808
Epoch [111/200], Train Loss: 0.000545
Validation Loss: 0.00054281
Epoch [112/200], Train Loss: 0.000540
Validation Loss: 0.00053181
Epoch [113/200], Train Loss: 0.000548
Validation Loss: 0.00053171
Epoch [114/200], Train Loss: 0.000538
Validation Loss: 0.00053806
Epoch [115/200], Train Loss: 0.000538
Validation Loss: 0.00052774
Epoch [116/200], Train Loss: 0.000540
Validation Loss: 0.00052041
Epoch [117/200], Train Loss: 0.000530
Validation Loss: 0.00051357
Epoch [118/200], Train Loss: 0.000520
Validation Loss: 0.00050801
Epoch [119/200], Train Loss: 0.000524
Validation Loss: 0.00051203
Epoch [120/200], Train Loss: 0.000522
Validation Loss: 0.00051085
Epoch [121/200], Train Loss: 0.000521
Validation Loss: 0.00050458
Epoch [122/200], Train Loss: 0.000511
Validation Loss: 0.00049929
Epoch [123/200], Train Loss: 0.000512
Validation Loss: 0.00051849
Epoch [124/200], Train Loss: 0.000508
Validation Loss: 0.00050456
Epoch [125/200], Train Loss: 0.000506
Validation Loss: 0.00049215
Epoch [126/200], Train Loss: 0.000498
Validation Loss: 0.00049348
Epoch [127/200], Train Loss: 0.000497
Validation Loss: 0.00049156
Epoch [128/200], Train Loss: 0.000503
Validation Loss: 0.00049915
Epoch [129/200], Train Loss: 0.000496
Validation Loss: 0.00049063
Epoch [130/200], Train Loss: 0.000492
Validation Loss: 0.00049451
Epoch [131/200], Train Loss: 0.000493
Validation Loss: 0.00048295
Epoch [132/200], Train Loss: 0.000491
Validation Loss: 0.00048295
Epoch [133/200], Train Loss: 0.000485
Validation Loss: 0.00047709
Epoch [134/200], Train Loss: 0.000484
Validation Loss: 0.00047576
Epoch [135/200], Train Loss: 0.000483
Validation Loss: 0.00048818
Epoch [136/200], Train Loss: 0.000482
Validation Loss: 0.00046875
Epoch [137/200], Train Loss: 0.000480
Validation Loss: 0.00049101
Epoch [138/200], Train Loss: 0.000475
Validation Loss: 0.00045987
Epoch [139/200], Train Loss: 0.000471
Validation Loss: 0.00046799
Epoch [140/200], Train Loss: 0.000500
Validation Loss: 0.00048902
Epoch [141/200], Train Loss: 0.000475
Validation Loss: 0.00046003
Epoch [142/200], Train Loss: 0.000472
Validation Loss: 0.00046455
Epoch [143/200], Train Loss: 0.000467
Validation Loss: 0.00045711
Epoch [144/200], Train Loss: 0.000468
Validation Loss: 0.00046529
Epoch [145/200], Train Loss: 0.000464
Validation Loss: 0.00045433
Epoch [146/200], Train Loss: 0.000466
Validation Loss: 0.00045507
Epoch [147/200], Train Loss: 0.000462
Validation Loss: 0.00045576
Epoch [148/200], Train Loss: 0.000464
Validation Loss: 0.00044808
Epoch [149/200], Train Loss: 0.000459
Validation Loss: 0.00044743
Epoch [150/200], Train Loss: 0.000461
Validation Loss: 0.00044307
Epoch [151/200], Train Loss: 0.000451
Validation Loss: 0.00044799
Epoch [152/200], Train Loss: 0.000453
Validation Loss: 0.00044956
Epoch [153/200], Train Loss: 0.000450
Validation Loss: 0.00044026
Epoch [154/200], Train Loss: 0.000458
Validation Loss: 0.00044311
Epoch [155/200], Train Loss: 0.000453
Validation Loss: 0.00043668
Epoch [156/200], Train Loss: 0.000446
Validation Loss: 0.00044472
Epoch [157/200], Train Loss: 0.000448
Validation Loss: 0.00043183
Epoch [158/200], Train Loss: 0.000443
Validation Loss: 0.00044149
Epoch [159/200], Train Loss: 0.000448
Validation Loss: 0.00042928
Epoch [160/200], Train Loss: 0.000447
Validation Loss: 0.00043660
Epoch [161/200], Train Loss: 0.000438
Validation Loss: 0.00043410
Epoch [162/200], Train Loss: 0.000442
Validation Loss: 0.00042845
Epoch [163/200], Train Loss: 0.000442
Validation Loss: 0.00042424
Epoch [164/200], Train Loss: 0.000459
Validation Loss: 0.00043555
Epoch [165/200], Train Loss: 0.000442
Validation Loss: 0.00043107
Epoch [166/200], Train Loss: 0.000438
Validation Loss: 0.00043128
Epoch [167/200], Train Loss: 0.000439
Validation Loss: 0.00042500
Epoch [168/200], Train Loss: 0.000431
Validation Loss: 0.00043044
Epoch [169/200], Train Loss: 0.000432
Validation Loss: 0.00042105
Epoch [170/200], Train Loss: 0.000436
Validation Loss: 0.00043070
Epoch [171/200], Train Loss: 0.000432
Validation Loss: 0.00042021
Epoch [172/200], Train Loss: 0.000426
Validation Loss: 0.00042273
Epoch [173/200], Train Loss: 0.000428
Validation Loss: 0.00042154
Epoch [174/200], Train Loss: 0.000452
Validation Loss: 0.00043634
Epoch [175/200], Train Loss: 0.000430
Validation Loss: 0.00041682
Epoch [176/200], Train Loss: 0.000428
Validation Loss: 0.00042186
Epoch [177/200], Train Loss: 0.000421
Validation Loss: 0.00041772
Epoch [178/200], Train Loss: 0.000423
Validation Loss: 0.00041650
Epoch [179/200], Train Loss: 0.000420
Validation Loss: 0.00041785
Epoch [180/200], Train Loss: 0.000422
Validation Loss: 0.00041876
Epoch [181/200], Train Loss: 0.000421
Validation Loss: 0.00041969
Epoch [182/200], Train Loss: 0.000424
Validation Loss: 0.00041948
Epoch [183/200], Train Loss: 0.000420
Validation Loss: 0.00041054
Epoch [184/200], Train Loss: 0.000424
Validation Loss: 0.00041329
Epoch [185/200], Train Loss: 0.000417
Validation Loss: 0.00041949
Epoch [186/200], Train Loss: 0.000418
Validation Loss: 0.00040748
Epoch [187/200], Train Loss: 0.000412
Validation Loss: 0.00041054
Epoch [188/200], Train Loss: 0.000413
Validation Loss: 0.00040583
Epoch [189/200], Train Loss: 0.000415
Validation Loss: 0.00040924
Epoch [190/200], Train Loss: 0.000410
Validation Loss: 0.00040836
Epoch [191/200], Train Loss: 0.000414
Validation Loss: 0.00040223
Epoch [192/200], Train Loss: 0.000415
Validation Loss: 0.00040817
Epoch [193/200], Train Loss: 0.000413
Validation Loss: 0.00040162
Epoch [194/200], Train Loss: 0.000410
Validation Loss: 0.00041109
Epoch [195/200], Train Loss: 0.000411
Validation Loss: 0.00041161
Epoch [196/200], Train Loss: 0.000412
Validation Loss: 0.00040186
Epoch [197/200], Train Loss: 0.000406
Validation Loss: 0.00040551
Epoch [198/200], Train Loss: 0.000409
Validation Loss: 0.00040205
Epoch [199/200], Train Loss: 0.000410
Validation Loss: 0.00040441
Epoch [200/200], Train Loss: 0.000406
Validation Loss: 0.00039422

Evaluating model for: Lamp
Run 4/144 completed in 4909.78 seconds with: {'MAE': np.float32(0.3570955), 'MSE': np.float32(13.714319), 'RMSE': np.float32(3.703285), 'SAE': np.float32(0.008093051), 'NDE': np.float32(0.284425)}

Run 5/144: hidden=64, seq_len=120, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.005364
Validation Loss: 0.00457346
Epoch [2/200], Train Loss: 0.004847
Validation Loss: 0.00455487
Epoch [3/200], Train Loss: 0.004808
Validation Loss: 0.00453399
Epoch [4/200], Train Loss: 0.004802
Validation Loss: 0.00451740
Epoch [5/200], Train Loss: 0.004800
Validation Loss: 0.00450932
Epoch [6/200], Train Loss: 0.004774
Validation Loss: 0.00450568
Epoch [7/200], Train Loss: 0.004794
Validation Loss: 0.00450276
Epoch [8/200], Train Loss: 0.004751
Validation Loss: 0.00450575
Epoch [9/200], Train Loss: 0.004769
Validation Loss: 0.00449695
Epoch [10/200], Train Loss: 0.004770
Validation Loss: 0.00449685
Epoch [11/200], Train Loss: 0.004751
Validation Loss: 0.00448880
Epoch [12/200], Train Loss: 0.004743
Validation Loss: 0.00448428
Epoch [13/200], Train Loss: 0.004748
Validation Loss: 0.00447657
Epoch [14/200], Train Loss: 0.004740
Validation Loss: 0.00447108
Epoch [15/200], Train Loss: 0.004724
Validation Loss: 0.00446422
Epoch [16/200], Train Loss: 0.004729
Validation Loss: 0.00445701
Epoch [17/200], Train Loss: 0.004728
Validation Loss: 0.00445247
Epoch [18/200], Train Loss: 0.004721
Validation Loss: 0.00444662
Epoch [19/200], Train Loss: 0.004719
Validation Loss: 0.00444593
Epoch [20/200], Train Loss: 0.004706
Validation Loss: 0.00443617
Epoch [21/200], Train Loss: 0.004702
Validation Loss: 0.00443107
Epoch [22/200], Train Loss: 0.004704
Validation Loss: 0.00441812
Epoch [23/200], Train Loss: 0.004686
Validation Loss: 0.00441249
Epoch [24/200], Train Loss: 0.004680
Validation Loss: 0.00440373
Epoch [25/200], Train Loss: 0.004685
Validation Loss: 0.00438670
Epoch [26/200], Train Loss: 0.004646
Validation Loss: 0.00437030
Epoch [27/200], Train Loss: 0.004638
Validation Loss: 0.00436872
Epoch [28/200], Train Loss: 0.004648
Validation Loss: 0.00432942
Epoch [29/200], Train Loss: 0.004607
Validation Loss: 0.00430032
Epoch [30/200], Train Loss: 0.004578
Validation Loss: 0.00427182
Epoch [31/200], Train Loss: 0.004562
Validation Loss: 0.00422824
Epoch [32/200], Train Loss: 0.004508
Validation Loss: 0.00417621
Epoch [33/200], Train Loss: 0.004457
Validation Loss: 0.00411502
Epoch [34/200], Train Loss: 0.004390
Validation Loss: 0.00399314
Epoch [35/200], Train Loss: 0.004289
Validation Loss: 0.00387657
Epoch [36/200], Train Loss: 0.004204
Validation Loss: 0.00375872
Epoch [37/200], Train Loss: 0.004081
Validation Loss: 0.00363956
Epoch [38/200], Train Loss: 0.003969
Validation Loss: 0.00356723
Epoch [39/200], Train Loss: 0.003867
Validation Loss: 0.00337964
Epoch [40/200], Train Loss: 0.003714
Validation Loss: 0.00321323
Epoch [41/200], Train Loss: 0.003578
Validation Loss: 0.00305456
Epoch [42/200], Train Loss: 0.003426
Validation Loss: 0.00298326
Epoch [43/200], Train Loss: 0.003276
Validation Loss: 0.00277938
Epoch [44/200], Train Loss: 0.003142
Validation Loss: 0.00275374
Epoch [45/200], Train Loss: 0.003012
Validation Loss: 0.00251859
Epoch [46/200], Train Loss: 0.002865
Validation Loss: 0.00243812
Epoch [47/200], Train Loss: 0.002752
Validation Loss: 0.00237917
Epoch [48/200], Train Loss: 0.002686
Validation Loss: 0.00226419
Epoch [49/200], Train Loss: 0.002606
Validation Loss: 0.00227562
Epoch [50/200], Train Loss: 0.002558
Validation Loss: 0.00213105
Epoch [51/200], Train Loss: 0.002483
Validation Loss: 0.00212280
Epoch [52/200], Train Loss: 0.002425
Validation Loss: 0.00206692
Epoch [53/200], Train Loss: 0.002361
Validation Loss: 0.00200171
Epoch [54/200], Train Loss: 0.002322
Validation Loss: 0.00196984
Epoch [55/200], Train Loss: 0.002269
Validation Loss: 0.00191935
Epoch [56/200], Train Loss: 0.002240
Validation Loss: 0.00187551
Epoch [57/200], Train Loss: 0.002184
Validation Loss: 0.00185513
Epoch [58/200], Train Loss: 0.002162
Validation Loss: 0.00181140
Epoch [59/200], Train Loss: 0.002138
Validation Loss: 0.00179543
Epoch [60/200], Train Loss: 0.002100
Validation Loss: 0.00179444
Epoch [61/200], Train Loss: 0.002089
Validation Loss: 0.00175315
Epoch [62/200], Train Loss: 0.002048
Validation Loss: 0.00172082
Epoch [63/200], Train Loss: 0.002023
Validation Loss: 0.00171045
Epoch [64/200], Train Loss: 0.002004
Validation Loss: 0.00168464
Epoch [65/200], Train Loss: 0.001969
Validation Loss: 0.00167650
Epoch [66/200], Train Loss: 0.001959
Validation Loss: 0.00169460
Epoch [67/200], Train Loss: 0.001928
Validation Loss: 0.00164169
Epoch [68/200], Train Loss: 0.001914
Validation Loss: 0.00163116
Epoch [69/200], Train Loss: 0.001884
Validation Loss: 0.00162413
Epoch [70/200], Train Loss: 0.001877
Validation Loss: 0.00157725
Epoch [71/200], Train Loss: 0.001859
Validation Loss: 0.00158550
Epoch [72/200], Train Loss: 0.001841
Validation Loss: 0.00157176
Epoch [73/200], Train Loss: 0.001817
Validation Loss: 0.00156143
Epoch [74/200], Train Loss: 0.001796
Validation Loss: 0.00154972
Epoch [75/200], Train Loss: 0.001778
Validation Loss: 0.00155172
Epoch [76/200], Train Loss: 0.001773
Validation Loss: 0.00152712
Epoch [77/200], Train Loss: 0.001739
Validation Loss: 0.00150258
Epoch [78/200], Train Loss: 0.001729
Validation Loss: 0.00148971
Epoch [79/200], Train Loss: 0.001712
Validation Loss: 0.00146939
Epoch [80/200], Train Loss: 0.001705
Validation Loss: 0.00145784
Epoch [81/200], Train Loss: 0.001695
Validation Loss: 0.00145067
Epoch [82/200], Train Loss: 0.001674
Validation Loss: 0.00144689
Epoch [83/200], Train Loss: 0.001675
Validation Loss: 0.00143355
Epoch [84/200], Train Loss: 0.001660
Validation Loss: 0.00141310
Epoch [85/200], Train Loss: 0.001645
Validation Loss: 0.00140611
Epoch [86/200], Train Loss: 0.001625
Validation Loss: 0.00139875
Epoch [87/200], Train Loss: 0.001628
Validation Loss: 0.00138846
Epoch [88/200], Train Loss: 0.001598
Validation Loss: 0.00138640
Epoch [89/200], Train Loss: 0.001598
Validation Loss: 0.00136229
Epoch [90/200], Train Loss: 0.001579
Validation Loss: 0.00138126
Epoch [91/200], Train Loss: 0.001568
Validation Loss: 0.00135317
Epoch [92/200], Train Loss: 0.001548
Validation Loss: 0.00134112
Epoch [93/200], Train Loss: 0.001541
Validation Loss: 0.00133746
Epoch [94/200], Train Loss: 0.001527
Validation Loss: 0.00133084
Epoch [95/200], Train Loss: 0.001525
Validation Loss: 0.00134525
Epoch [96/200], Train Loss: 0.001511
Validation Loss: 0.00131354
Epoch [97/200], Train Loss: 0.001515
Validation Loss: 0.00129920
Epoch [98/200], Train Loss: 0.001499
Validation Loss: 0.00129484
Epoch [99/200], Train Loss: 0.001484
Validation Loss: 0.00130904
Epoch [100/200], Train Loss: 0.001484
Validation Loss: 0.00129008
Epoch [101/200], Train Loss: 0.001473
Validation Loss: 0.00127208
Epoch [102/200], Train Loss: 0.001456
Validation Loss: 0.00127843
Epoch [103/200], Train Loss: 0.001434
Validation Loss: 0.00125594
Epoch [104/200], Train Loss: 0.001446
Validation Loss: 0.00126238
Epoch [105/200], Train Loss: 0.001432
Validation Loss: 0.00123310
Epoch [106/200], Train Loss: 0.001412
Validation Loss: 0.00122768
Epoch [107/200], Train Loss: 0.001409
Validation Loss: 0.00123268
Epoch [108/200], Train Loss: 0.001397
Validation Loss: 0.00123270
Epoch [109/200], Train Loss: 0.001412
Validation Loss: 0.00121879
Epoch [110/200], Train Loss: 0.001398
Validation Loss: 0.00121507
Epoch [111/200], Train Loss: 0.001392
Validation Loss: 0.00121237
Epoch [112/200], Train Loss: 0.001375
Validation Loss: 0.00120161
Epoch [113/200], Train Loss: 0.001367
Validation Loss: 0.00118280
Epoch [114/200], Train Loss: 0.001361
Validation Loss: 0.00118025
Epoch [115/200], Train Loss: 0.001365
Validation Loss: 0.00117507
Epoch [116/200], Train Loss: 0.001358
Validation Loss: 0.00118166
Epoch [117/200], Train Loss: 0.001334
Validation Loss: 0.00116636
Epoch [118/200], Train Loss: 0.001344
Validation Loss: 0.00115499
Epoch [119/200], Train Loss: 0.001338
Validation Loss: 0.00114467
Epoch [120/200], Train Loss: 0.001314
Validation Loss: 0.00115371
Epoch [121/200], Train Loss: 0.001314
Validation Loss: 0.00115416
Epoch [122/200], Train Loss: 0.001304
Validation Loss: 0.00114463
Epoch [123/200], Train Loss: 0.001321
Validation Loss: 0.00112593
Epoch [124/200], Train Loss: 0.001297
Validation Loss: 0.00112996
Epoch [125/200], Train Loss: 0.001289
Validation Loss: 0.00113980
Epoch [126/200], Train Loss: 0.001297
Validation Loss: 0.00112329
Epoch [127/200], Train Loss: 0.001268
Validation Loss: 0.00110600
Epoch [128/200], Train Loss: 0.001283
Validation Loss: 0.00110359
Epoch [129/200], Train Loss: 0.001262
Validation Loss: 0.00111070
Epoch [130/200], Train Loss: 0.001266
Validation Loss: 0.00109755
Epoch [131/200], Train Loss: 0.001258
Validation Loss: 0.00108650
Epoch [132/200], Train Loss: 0.001250
Validation Loss: 0.00109147
Epoch [133/200], Train Loss: 0.001251
Validation Loss: 0.00108114
Epoch [134/200], Train Loss: 0.001239
Validation Loss: 0.00107886
Epoch [135/200], Train Loss: 0.001240
Validation Loss: 0.00107036
Epoch [136/200], Train Loss: 0.001230
Validation Loss: 0.00109022
Epoch [137/200], Train Loss: 0.001220
Validation Loss: 0.00107219
Epoch [138/200], Train Loss: 0.001219
Validation Loss: 0.00106077
Epoch [139/200], Train Loss: 0.001219
Validation Loss: 0.00105789
Epoch [140/200], Train Loss: 0.001209
Validation Loss: 0.00105798
Epoch [141/200], Train Loss: 0.001204
Validation Loss: 0.00105205
Epoch [142/200], Train Loss: 0.001202
Validation Loss: 0.00104320
Epoch [143/200], Train Loss: 0.001194
Validation Loss: 0.00103599
Epoch [144/200], Train Loss: 0.001188
Validation Loss: 0.00103885
Epoch [145/200], Train Loss: 0.001191
Validation Loss: 0.00103000
Epoch [146/200], Train Loss: 0.001188
Validation Loss: 0.00103262
Epoch [147/200], Train Loss: 0.001186
Validation Loss: 0.00104262
Epoch [148/200], Train Loss: 0.001177
Validation Loss: 0.00102603
Epoch [149/200], Train Loss: 0.001180
Validation Loss: 0.00101876
Epoch [150/200], Train Loss: 0.001169
Validation Loss: 0.00101655
Epoch [151/200], Train Loss: 0.001173
Validation Loss: 0.00101114
Epoch [152/200], Train Loss: 0.001156
Validation Loss: 0.00101007
Epoch [153/200], Train Loss: 0.001157
Validation Loss: 0.00100665
Epoch [154/200], Train Loss: 0.001149
Validation Loss: 0.00100000
Epoch [155/200], Train Loss: 0.001153
Validation Loss: 0.00100159
Epoch [156/200], Train Loss: 0.001148
Validation Loss: 0.00100631
Epoch [157/200], Train Loss: 0.001138
Validation Loss: 0.00099581
Epoch [158/200], Train Loss: 0.001147
Validation Loss: 0.00099054
Epoch [159/200], Train Loss: 0.001149
Validation Loss: 0.00099483
Epoch [160/200], Train Loss: 0.001139
Validation Loss: 0.00098278
Epoch [161/200], Train Loss: 0.001133
Validation Loss: 0.00098114
Epoch [162/200], Train Loss: 0.001136
Validation Loss: 0.00097873
Epoch [163/200], Train Loss: 0.001129
Validation Loss: 0.00097390
Epoch [164/200], Train Loss: 0.001127
Validation Loss: 0.00098564
Epoch [165/200], Train Loss: 0.001114
Validation Loss: 0.00097427
Epoch [166/200], Train Loss: 0.001117
Validation Loss: 0.00097235
Epoch [167/200], Train Loss: 0.001122
Validation Loss: 0.00096553
Epoch [168/200], Train Loss: 0.001106
Validation Loss: 0.00096209
Epoch [169/200], Train Loss: 0.001103
Validation Loss: 0.00096658
Epoch [170/200], Train Loss: 0.001109
Validation Loss: 0.00095919
Epoch [171/200], Train Loss: 0.001108
Validation Loss: 0.00095874
Epoch [172/200], Train Loss: 0.001105
Validation Loss: 0.00095764
Epoch [173/200], Train Loss: 0.001098
Validation Loss: 0.00095271
Epoch [174/200], Train Loss: 0.001098
Validation Loss: 0.00095403
Epoch [175/200], Train Loss: 0.001093
Validation Loss: 0.00094896
Epoch [176/200], Train Loss: 0.001084
Validation Loss: 0.00096758
Epoch [177/200], Train Loss: 0.001094
Validation Loss: 0.00094405
Epoch [178/200], Train Loss: 0.001088
Validation Loss: 0.00094582
Epoch [179/200], Train Loss: 0.001078
Validation Loss: 0.00094032
Epoch [180/200], Train Loss: 0.001080
Validation Loss: 0.00093663
Epoch [181/200], Train Loss: 0.001080
Validation Loss: 0.00094344
Epoch [182/200], Train Loss: 0.001072
Validation Loss: 0.00093289
Epoch [183/200], Train Loss: 0.001077
Validation Loss: 0.00093276
Epoch [184/200], Train Loss: 0.001065
Validation Loss: 0.00093240
Epoch [185/200], Train Loss: 0.001062
Validation Loss: 0.00092962
Epoch [186/200], Train Loss: 0.001057
Validation Loss: 0.00093057
Epoch [187/200], Train Loss: 0.001059
Validation Loss: 0.00093393
Epoch [188/200], Train Loss: 0.001067
Validation Loss: 0.00092314
Epoch [189/200], Train Loss: 0.001067
Validation Loss: 0.00092381
Epoch [190/200], Train Loss: 0.001060
Validation Loss: 0.00092305
Epoch [191/200], Train Loss: 0.001055
Validation Loss: 0.00092087
Epoch [192/200], Train Loss: 0.001064
Validation Loss: 0.00091817
Epoch [193/200], Train Loss: 0.001048
Validation Loss: 0.00091776
Epoch [194/200], Train Loss: 0.001036
Validation Loss: 0.00091964
Epoch [195/200], Train Loss: 0.001051
Validation Loss: 0.00091053
Epoch [196/200], Train Loss: 0.001038
Validation Loss: 0.00091012
Epoch [197/200], Train Loss: 0.001038
Validation Loss: 0.00090735
Epoch [198/200], Train Loss: 0.001038
Validation Loss: 0.00091262
Epoch [199/200], Train Loss: 0.001044
Validation Loss: 0.00090426
Epoch [200/200], Train Loss: 0.001040
Validation Loss: 0.00091136

Evaluating model for: Lamp
Run 5/144 completed in 1900.83 seconds with: {'MAE': np.float32(0.79016155), 'MSE': np.float32(31.337732), 'RMSE': np.float32(5.5980115), 'SAE': np.float32(0.040399794), 'NDE': np.float32(0.42789587)}

Run 6/144: hidden=64, seq_len=120, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.005758
Validation Loss: 0.00462060
Epoch [2/200], Train Loss: 0.004898
Validation Loss: 0.00460817
Epoch [3/200], Train Loss: 0.004903
Validation Loss: 0.00459295
Epoch [4/200], Train Loss: 0.004896
Validation Loss: 0.00456524
Epoch [5/200], Train Loss: 0.004823
Validation Loss: 0.00452535
Epoch [6/200], Train Loss: 0.004799
Validation Loss: 0.00451616
Epoch [7/200], Train Loss: 0.004786
Validation Loss: 0.00451017
Epoch [8/200], Train Loss: 0.004789
Validation Loss: 0.00450357
Epoch [9/200], Train Loss: 0.004811
Validation Loss: 0.00449978
Epoch [10/200], Train Loss: 0.004800
Validation Loss: 0.00449473
Epoch [11/200], Train Loss: 0.004779
Validation Loss: 0.00449122
Epoch [12/200], Train Loss: 0.004757
Validation Loss: 0.00448943
Epoch [13/200], Train Loss: 0.004782
Validation Loss: 0.00448991
Epoch [14/200], Train Loss: 0.004772
Validation Loss: 0.00448500
Epoch [15/200], Train Loss: 0.004780
Validation Loss: 0.00448242
Epoch [16/200], Train Loss: 0.004748
Validation Loss: 0.00448126
Epoch [17/200], Train Loss: 0.004772
Validation Loss: 0.00448088
Epoch [18/200], Train Loss: 0.004754
Validation Loss: 0.00448473
Epoch [19/200], Train Loss: 0.004745
Validation Loss: 0.00448165
Epoch [20/200], Train Loss: 0.004764
Validation Loss: 0.00447629
Epoch [21/200], Train Loss: 0.004752
Validation Loss: 0.00447658
Epoch [22/200], Train Loss: 0.004753
Validation Loss: 0.00447505
Epoch [23/200], Train Loss: 0.004754
Validation Loss: 0.00447323
Epoch [24/200], Train Loss: 0.004742
Validation Loss: 0.00447501
Epoch [25/200], Train Loss: 0.004762
Validation Loss: 0.00447522
Epoch [26/200], Train Loss: 0.004765
Validation Loss: 0.00447272
Epoch [27/200], Train Loss: 0.004744
Validation Loss: 0.00447342
Epoch [28/200], Train Loss: 0.004752
Validation Loss: 0.00447599
Epoch [29/200], Train Loss: 0.004749
Validation Loss: 0.00447734
Epoch [30/200], Train Loss: 0.004736
Validation Loss: 0.00446787
Epoch [31/200], Train Loss: 0.004731
Validation Loss: 0.00446712
Epoch [32/200], Train Loss: 0.004749
Validation Loss: 0.00446898
Epoch [33/200], Train Loss: 0.004732
Validation Loss: 0.00447362
Epoch [34/200], Train Loss: 0.004739
Validation Loss: 0.00446860
Epoch [35/200], Train Loss: 0.004729
Validation Loss: 0.00446413
Epoch [36/200], Train Loss: 0.004721
Validation Loss: 0.00446627
Epoch [37/200], Train Loss: 0.004724
Validation Loss: 0.00446300
Epoch [38/200], Train Loss: 0.004734
Validation Loss: 0.00446257
Epoch [39/200], Train Loss: 0.004739
Validation Loss: 0.00446048
Epoch [40/200], Train Loss: 0.004726
Validation Loss: 0.00446157
Epoch [41/200], Train Loss: 0.004727
Validation Loss: 0.00446030
Epoch [42/200], Train Loss: 0.004724
Validation Loss: 0.00445963
Epoch [43/200], Train Loss: 0.004732
Validation Loss: 0.00446007
Epoch [44/200], Train Loss: 0.004732
Validation Loss: 0.00445721
Epoch [45/200], Train Loss: 0.004724
Validation Loss: 0.00445678
Epoch [46/200], Train Loss: 0.004720
Validation Loss: 0.00446049
Epoch [47/200], Train Loss: 0.004719
Validation Loss: 0.00445623
Epoch [48/200], Train Loss: 0.004734
Validation Loss: 0.00445483
Epoch [49/200], Train Loss: 0.004733
Validation Loss: 0.00445607
Epoch [50/200], Train Loss: 0.004724
Validation Loss: 0.00445429
Epoch [51/200], Train Loss: 0.004748
Validation Loss: 0.00445280
Epoch [52/200], Train Loss: 0.004731
Validation Loss: 0.00445381
Epoch [53/200], Train Loss: 0.004757
Validation Loss: 0.00445256
Epoch [54/200], Train Loss: 0.004735
Validation Loss: 0.00444947
Epoch [55/200], Train Loss: 0.004719
Validation Loss: 0.00444937
Epoch [56/200], Train Loss: 0.004717
Validation Loss: 0.00444659
Epoch [57/200], Train Loss: 0.004708
Validation Loss: 0.00444741
Epoch [58/200], Train Loss: 0.004709
Validation Loss: 0.00444050
Epoch [59/200], Train Loss: 0.004718
Validation Loss: 0.00443696
Epoch [60/200], Train Loss: 0.004719
Validation Loss: 0.00443251
Epoch [61/200], Train Loss: 0.004685
Validation Loss: 0.00443275
Epoch [62/200], Train Loss: 0.004688
Validation Loss: 0.00442331
Epoch [63/200], Train Loss: 0.004699
Validation Loss: 0.00442336
Epoch [64/200], Train Loss: 0.004675
Validation Loss: 0.00441455
Epoch [65/200], Train Loss: 0.004684
Validation Loss: 0.00441003
Epoch [66/200], Train Loss: 0.004661
Validation Loss: 0.00440473
Epoch [67/200], Train Loss: 0.004665
Validation Loss: 0.00439818
Epoch [68/200], Train Loss: 0.004661
Validation Loss: 0.00438713
Epoch [69/200], Train Loss: 0.004673
Validation Loss: 0.00437937
Epoch [70/200], Train Loss: 0.004665
Validation Loss: 0.00437254
Epoch [71/200], Train Loss: 0.004634
Validation Loss: 0.00435970
Epoch [72/200], Train Loss: 0.004638
Validation Loss: 0.00434411
Epoch [73/200], Train Loss: 0.004619
Validation Loss: 0.00432813
Epoch [74/200], Train Loss: 0.004584
Validation Loss: 0.00430676
Epoch [75/200], Train Loss: 0.004557
Validation Loss: 0.00427686
Epoch [76/200], Train Loss: 0.004517
Validation Loss: 0.00422341
Epoch [77/200], Train Loss: 0.004502
Validation Loss: 0.00417633
Epoch [78/200], Train Loss: 0.004446
Validation Loss: 0.00411166
Epoch [79/200], Train Loss: 0.004392
Validation Loss: 0.00402591
Epoch [80/200], Train Loss: 0.004311
Validation Loss: 0.00393768
Epoch [81/200], Train Loss: 0.004270
Validation Loss: 0.00387345
Epoch [82/200], Train Loss: 0.004163
Validation Loss: 0.00377251
Epoch [83/200], Train Loss: 0.004103
Validation Loss: 0.00373760
Epoch [84/200], Train Loss: 0.003991
Validation Loss: 0.00354979
Epoch [85/200], Train Loss: 0.003900
Validation Loss: 0.00341512
Epoch [86/200], Train Loss: 0.003787
Validation Loss: 0.00330882
Epoch [87/200], Train Loss: 0.003644
Validation Loss: 0.00313381
Epoch [88/200], Train Loss: 0.003505
Validation Loss: 0.00296437
Epoch [89/200], Train Loss: 0.003336
Validation Loss: 0.00281220
Epoch [90/200], Train Loss: 0.003176
Validation Loss: 0.00266802
Epoch [91/200], Train Loss: 0.003005
Validation Loss: 0.00254520
Epoch [92/200], Train Loss: 0.002910
Validation Loss: 0.00240455
Epoch [93/200], Train Loss: 0.002770
Validation Loss: 0.00232061
Epoch [94/200], Train Loss: 0.002684
Validation Loss: 0.00221290
Epoch [95/200], Train Loss: 0.002569
Validation Loss: 0.00215178
Epoch [96/200], Train Loss: 0.002510
Validation Loss: 0.00209226
Epoch [97/200], Train Loss: 0.002443
Validation Loss: 0.00200251
Epoch [98/200], Train Loss: 0.002373
Validation Loss: 0.00193752
Epoch [99/200], Train Loss: 0.002311
Validation Loss: 0.00193554
Epoch [100/200], Train Loss: 0.002255
Validation Loss: 0.00191887
Epoch [101/200], Train Loss: 0.002212
Validation Loss: 0.00185883
Epoch [102/200], Train Loss: 0.002173
Validation Loss: 0.00180868
Epoch [103/200], Train Loss: 0.002133
Validation Loss: 0.00175483
Epoch [104/200], Train Loss: 0.002099
Validation Loss: 0.00171687
Epoch [105/200], Train Loss: 0.002082
Validation Loss: 0.00168724
Epoch [106/200], Train Loss: 0.002026
Validation Loss: 0.00166165
Epoch [107/200], Train Loss: 0.001986
Validation Loss: 0.00164018
Epoch [108/200], Train Loss: 0.001984
Validation Loss: 0.00163522
Epoch [109/200], Train Loss: 0.001936
Validation Loss: 0.00159912
Epoch [110/200], Train Loss: 0.001908
Validation Loss: 0.00160031
Epoch [111/200], Train Loss: 0.001876
Validation Loss: 0.00155201
Epoch [112/200], Train Loss: 0.001861
Validation Loss: 0.00157574
Epoch [113/200], Train Loss: 0.001852
Validation Loss: 0.00153324
Epoch [114/200], Train Loss: 0.001807
Validation Loss: 0.00151045
Epoch [115/200], Train Loss: 0.001794
Validation Loss: 0.00147235
Epoch [116/200], Train Loss: 0.001778
Validation Loss: 0.00147582
Epoch [117/200], Train Loss: 0.001747
Validation Loss: 0.00144953
Epoch [118/200], Train Loss: 0.001732
Validation Loss: 0.00149140
Epoch [119/200], Train Loss: 0.001698
Validation Loss: 0.00141247
Epoch [120/200], Train Loss: 0.001687
Validation Loss: 0.00140548
Epoch [121/200], Train Loss: 0.001669
Validation Loss: 0.00138733
Epoch [122/200], Train Loss: 0.001650
Validation Loss: 0.00137034
Epoch [123/200], Train Loss: 0.001624
Validation Loss: 0.00136237
Epoch [124/200], Train Loss: 0.001616
Validation Loss: 0.00134869
Epoch [125/200], Train Loss: 0.001593
Validation Loss: 0.00133873
Epoch [126/200], Train Loss: 0.001589
Validation Loss: 0.00132909
Epoch [127/200], Train Loss: 0.001562
Validation Loss: 0.00131162
Epoch [128/200], Train Loss: 0.001550
Validation Loss: 0.00132108
Epoch [129/200], Train Loss: 0.001545
Validation Loss: 0.00130717
Epoch [130/200], Train Loss: 0.001528
Validation Loss: 0.00128131
Epoch [131/200], Train Loss: 0.001511
Validation Loss: 0.00127273
Epoch [132/200], Train Loss: 0.001485
Validation Loss: 0.00126380
Epoch [133/200], Train Loss: 0.001479
Validation Loss: 0.00125328
Epoch [134/200], Train Loss: 0.001484
Validation Loss: 0.00124475
Epoch [135/200], Train Loss: 0.001480
Validation Loss: 0.00123936
Epoch [136/200], Train Loss: 0.001450
Validation Loss: 0.00121610
Epoch [137/200], Train Loss: 0.001447
Validation Loss: 0.00121160
Epoch [138/200], Train Loss: 0.001428
Validation Loss: 0.00120770
Epoch [139/200], Train Loss: 0.001414
Validation Loss: 0.00119336
Epoch [140/200], Train Loss: 0.001425
Validation Loss: 0.00118951
Epoch [141/200], Train Loss: 0.001401
Validation Loss: 0.00118582
Epoch [142/200], Train Loss: 0.001387
Validation Loss: 0.00118190
Epoch [143/200], Train Loss: 0.001374
Validation Loss: 0.00117087
Epoch [144/200], Train Loss: 0.001372
Validation Loss: 0.00116214
Epoch [145/200], Train Loss: 0.001361
Validation Loss: 0.00115565
Epoch [146/200], Train Loss: 0.001344
Validation Loss: 0.00115386
Epoch [147/200], Train Loss: 0.001349
Validation Loss: 0.00114426
Epoch [148/200], Train Loss: 0.001337
Validation Loss: 0.00113615
Epoch [149/200], Train Loss: 0.001337
Validation Loss: 0.00114034
Epoch [150/200], Train Loss: 0.001315
Validation Loss: 0.00111703
Epoch [151/200], Train Loss: 0.001313
Validation Loss: 0.00112103
Epoch [152/200], Train Loss: 0.001308
Validation Loss: 0.00111820
Epoch [153/200], Train Loss: 0.001298
Validation Loss: 0.00111229
Epoch [154/200], Train Loss: 0.001293
Validation Loss: 0.00109852
Epoch [155/200], Train Loss: 0.001291
Validation Loss: 0.00109914
Epoch [156/200], Train Loss: 0.001278
Validation Loss: 0.00109381
Epoch [157/200], Train Loss: 0.001271
Validation Loss: 0.00109195
Epoch [158/200], Train Loss: 0.001270
Validation Loss: 0.00108204
Epoch [159/200], Train Loss: 0.001268
Validation Loss: 0.00108054
Epoch [160/200], Train Loss: 0.001251
Validation Loss: 0.00107494
Epoch [161/200], Train Loss: 0.001243
Validation Loss: 0.00108507
Epoch [162/200], Train Loss: 0.001242
Validation Loss: 0.00106189
Epoch [163/200], Train Loss: 0.001257
Validation Loss: 0.00105627
Epoch [164/200], Train Loss: 0.001228
Validation Loss: 0.00105704
Epoch [165/200], Train Loss: 0.001243
Validation Loss: 0.00105806
Epoch [166/200], Train Loss: 0.001232
Validation Loss: 0.00105063
Epoch [167/200], Train Loss: 0.001229
Validation Loss: 0.00104362
Epoch [168/200], Train Loss: 0.001216
Validation Loss: 0.00104705
Epoch [169/200], Train Loss: 0.001200
Validation Loss: 0.00103386
Epoch [170/200], Train Loss: 0.001197
Validation Loss: 0.00103853
Epoch [171/200], Train Loss: 0.001203
Validation Loss: 0.00102583
Epoch [172/200], Train Loss: 0.001195
Validation Loss: 0.00103189
Epoch [173/200], Train Loss: 0.001193
Validation Loss: 0.00101868
Epoch [174/200], Train Loss: 0.001200
Validation Loss: 0.00103824
Epoch [175/200], Train Loss: 0.001185
Validation Loss: 0.00101173
Epoch [176/200], Train Loss: 0.001183
Validation Loss: 0.00100819
Epoch [177/200], Train Loss: 0.001177
Validation Loss: 0.00102287
Epoch [178/200], Train Loss: 0.001178
Validation Loss: 0.00100344
Epoch [179/200], Train Loss: 0.001172
Validation Loss: 0.00099899
Epoch [180/200], Train Loss: 0.001169
Validation Loss: 0.00100201
Epoch [181/200], Train Loss: 0.001167
Validation Loss: 0.00100221
Epoch [182/200], Train Loss: 0.001162
Validation Loss: 0.00098823
Epoch [183/200], Train Loss: 0.001176
Validation Loss: 0.00100109
Epoch [184/200], Train Loss: 0.001150
Validation Loss: 0.00098834
Epoch [185/200], Train Loss: 0.001153
Validation Loss: 0.00098063
Epoch [186/200], Train Loss: 0.001139
Validation Loss: 0.00098621
Epoch [187/200], Train Loss: 0.001149
Validation Loss: 0.00098231
Epoch [188/200], Train Loss: 0.001141
Validation Loss: 0.00097364
Epoch [189/200], Train Loss: 0.001147
Validation Loss: 0.00097590
Epoch [190/200], Train Loss: 0.001145
Validation Loss: 0.00097643
Epoch [191/200], Train Loss: 0.001132
Validation Loss: 0.00096973
Epoch [192/200], Train Loss: 0.001129
Validation Loss: 0.00097003
Epoch [193/200], Train Loss: 0.001120
Validation Loss: 0.00097934
Epoch [194/200], Train Loss: 0.001127
Validation Loss: 0.00096831
Epoch [195/200], Train Loss: 0.001123
Validation Loss: 0.00096209
Epoch [196/200], Train Loss: 0.001110
Validation Loss: 0.00095108
Epoch [197/200], Train Loss: 0.001116
Validation Loss: 0.00095106
Epoch [198/200], Train Loss: 0.001106
Validation Loss: 0.00095312
Epoch [199/200], Train Loss: 0.001113
Validation Loss: 0.00096140
Epoch [200/200], Train Loss: 0.001103
Validation Loss: 0.00094047

Evaluating model for: Lamp
Run 6/144 completed in 1913.56 seconds with: {'MAE': np.float32(0.825352), 'MSE': np.float32(32.993183), 'RMSE': np.float32(5.7439694), 'SAE': np.float32(0.0013161489), 'NDE': np.float32(0.43905264)}

Run 7/144: hidden=64, seq_len=120, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.008263
Validation Loss: 0.00475458
Epoch [2/200], Train Loss: 0.004969
Validation Loss: 0.00463244
Epoch [3/200], Train Loss: 0.004928
Validation Loss: 0.00461963
Epoch [4/200], Train Loss: 0.004916
Validation Loss: 0.00460011
Epoch [5/200], Train Loss: 0.004913
Validation Loss: 0.00457506
Epoch [6/200], Train Loss: 0.004862
Validation Loss: 0.00456309
Epoch [7/200], Train Loss: 0.004850
Validation Loss: 0.00454910
Epoch [8/200], Train Loss: 0.004847
Validation Loss: 0.00454279
Epoch [9/200], Train Loss: 0.004838
Validation Loss: 0.00453238
Epoch [10/200], Train Loss: 0.004846
Validation Loss: 0.00452565
Epoch [11/200], Train Loss: 0.004806
Validation Loss: 0.00451659
Epoch [12/200], Train Loss: 0.004803
Validation Loss: 0.00451077
Epoch [13/200], Train Loss: 0.004819
Validation Loss: 0.00450427
Epoch [14/200], Train Loss: 0.004803
Validation Loss: 0.00450352
Epoch [15/200], Train Loss: 0.004804
Validation Loss: 0.00449760
Epoch [16/200], Train Loss: 0.004785
Validation Loss: 0.00449310
Epoch [17/200], Train Loss: 0.004809
Validation Loss: 0.00449229
Epoch [18/200], Train Loss: 0.004788
Validation Loss: 0.00449994
Epoch [19/200], Train Loss: 0.004791
Validation Loss: 0.00448681
Epoch [20/200], Train Loss: 0.004800
Validation Loss: 0.00448450
Epoch [21/200], Train Loss: 0.004770
Validation Loss: 0.00448520
Epoch [22/200], Train Loss: 0.004772
Validation Loss: 0.00448539
Epoch [23/200], Train Loss: 0.004779
Validation Loss: 0.00448245
Epoch [24/200], Train Loss: 0.004765
Validation Loss: 0.00448537
Epoch [25/200], Train Loss: 0.004776
Validation Loss: 0.00447812
Epoch [26/200], Train Loss: 0.004750
Validation Loss: 0.00447940
Epoch [27/200], Train Loss: 0.004763
Validation Loss: 0.00447473
Epoch [28/200], Train Loss: 0.004766
Validation Loss: 0.00447436
Epoch [29/200], Train Loss: 0.004755
Validation Loss: 0.00448300
Epoch [30/200], Train Loss: 0.004758
Validation Loss: 0.00447536
Epoch [31/200], Train Loss: 0.004754
Validation Loss: 0.00447273
Epoch [32/200], Train Loss: 0.004745
Validation Loss: 0.00447240
Epoch [33/200], Train Loss: 0.004775
Validation Loss: 0.00447029
Epoch [34/200], Train Loss: 0.004772
Validation Loss: 0.00447287
Epoch [35/200], Train Loss: 0.004746
Validation Loss: 0.00447073
Epoch [36/200], Train Loss: 0.004747
Validation Loss: 0.00446586
Epoch [37/200], Train Loss: 0.004757
Validation Loss: 0.00446685
Epoch [38/200], Train Loss: 0.004762
Validation Loss: 0.00446648
Epoch [39/200], Train Loss: 0.004765
Validation Loss: 0.00446581
Epoch [40/200], Train Loss: 0.004744
Validation Loss: 0.00446444
Epoch [41/200], Train Loss: 0.004756
Validation Loss: 0.00446447
Epoch [42/200], Train Loss: 0.004755
Validation Loss: 0.00446756
Epoch [43/200], Train Loss: 0.004743
Validation Loss: 0.00446586
Epoch [44/200], Train Loss: 0.004745
Validation Loss: 0.00446739
Epoch [45/200], Train Loss: 0.004765
Validation Loss: 0.00446823
Epoch [46/200], Train Loss: 0.004741
Validation Loss: 0.00446752
Epoch [47/200], Train Loss: 0.004762
Validation Loss: 0.00446173
Epoch [48/200], Train Loss: 0.004763
Validation Loss: 0.00445995
Epoch [49/200], Train Loss: 0.004736
Validation Loss: 0.00446401
Epoch [50/200], Train Loss: 0.004752
Validation Loss: 0.00445975
Epoch [51/200], Train Loss: 0.004724
Validation Loss: 0.00446026
Epoch [52/200], Train Loss: 0.004730
Validation Loss: 0.00446289
Epoch [53/200], Train Loss: 0.004738
Validation Loss: 0.00446302
Epoch [54/200], Train Loss: 0.004737
Validation Loss: 0.00446009
Epoch [55/200], Train Loss: 0.004737
Validation Loss: 0.00445794
Epoch [56/200], Train Loss: 0.004747
Validation Loss: 0.00445659
Epoch [57/200], Train Loss: 0.004735
Validation Loss: 0.00445610
Epoch [58/200], Train Loss: 0.004720
Validation Loss: 0.00445825
Epoch [59/200], Train Loss: 0.004729
Validation Loss: 0.00445739
Epoch [60/200], Train Loss: 0.004737
Validation Loss: 0.00445508
Epoch [61/200], Train Loss: 0.004740
Validation Loss: 0.00445911
Epoch [62/200], Train Loss: 0.004745
Validation Loss: 0.00445435
Epoch [63/200], Train Loss: 0.004756
Validation Loss: 0.00445478
Epoch [64/200], Train Loss: 0.004732
Validation Loss: 0.00445617
Epoch [65/200], Train Loss: 0.004723
Validation Loss: 0.00445807
Epoch [66/200], Train Loss: 0.004743
Validation Loss: 0.00445123
Epoch [67/200], Train Loss: 0.004726
Validation Loss: 0.00445134
Epoch [68/200], Train Loss: 0.004715
Validation Loss: 0.00445230
Epoch [69/200], Train Loss: 0.004716
Validation Loss: 0.00445210
Epoch [70/200], Train Loss: 0.004726
Validation Loss: 0.00445189
Epoch [71/200], Train Loss: 0.004742
Validation Loss: 0.00445468
Epoch [72/200], Train Loss: 0.004736
Validation Loss: 0.00445228
Epoch [73/200], Train Loss: 0.004720
Validation Loss: 0.00444954
Epoch [74/200], Train Loss: 0.004726
Validation Loss: 0.00445227
Epoch [75/200], Train Loss: 0.004737
Validation Loss: 0.00444815
Epoch [76/200], Train Loss: 0.004718
Validation Loss: 0.00444798
Epoch [77/200], Train Loss: 0.004738
Validation Loss: 0.00444686
Epoch [78/200], Train Loss: 0.004721
Validation Loss: 0.00444864
Epoch [79/200], Train Loss: 0.004714
Validation Loss: 0.00444874
Epoch [80/200], Train Loss: 0.004729
Validation Loss: 0.00444750
Epoch [81/200], Train Loss: 0.004716
Validation Loss: 0.00445089
Epoch [82/200], Train Loss: 0.004715
Validation Loss: 0.00444926
Epoch [83/200], Train Loss: 0.004712
Validation Loss: 0.00444495
Epoch [84/200], Train Loss: 0.004719
Validation Loss: 0.00444407
Epoch [85/200], Train Loss: 0.004723
Validation Loss: 0.00444228
Epoch [86/200], Train Loss: 0.004709
Validation Loss: 0.00444173
Epoch [87/200], Train Loss: 0.004730
Validation Loss: 0.00444548
Epoch [88/200], Train Loss: 0.004715
Validation Loss: 0.00444269
Epoch [89/200], Train Loss: 0.004716
Validation Loss: 0.00443989
Epoch [90/200], Train Loss: 0.004709
Validation Loss: 0.00443949
Epoch [91/200], Train Loss: 0.004713
Validation Loss: 0.00443915
Epoch [92/200], Train Loss: 0.004698
Validation Loss: 0.00443789
Epoch [93/200], Train Loss: 0.004729
Validation Loss: 0.00443840
Epoch [94/200], Train Loss: 0.004727
Validation Loss: 0.00443808
Epoch [95/200], Train Loss: 0.004709
Validation Loss: 0.00443579
Epoch [96/200], Train Loss: 0.004710
Validation Loss: 0.00443833
Epoch [97/200], Train Loss: 0.004695
Validation Loss: 0.00443339
Epoch [98/200], Train Loss: 0.004715
Validation Loss: 0.00443743
Epoch [99/200], Train Loss: 0.004713
Validation Loss: 0.00443282
Epoch [100/200], Train Loss: 0.004712
Validation Loss: 0.00443552
Epoch [101/200], Train Loss: 0.004717
Validation Loss: 0.00443020
Epoch [102/200], Train Loss: 0.004716
Validation Loss: 0.00443170
Epoch [103/200], Train Loss: 0.004708
Validation Loss: 0.00443052
Epoch [104/200], Train Loss: 0.004707
Validation Loss: 0.00442884
Epoch [105/200], Train Loss: 0.004697
Validation Loss: 0.00444700
Epoch [106/200], Train Loss: 0.004707
Validation Loss: 0.00442543
Epoch [107/200], Train Loss: 0.004695
Validation Loss: 0.00442565
Epoch [108/200], Train Loss: 0.004676
Validation Loss: 0.00443650
Epoch [109/200], Train Loss: 0.004681
Validation Loss: 0.00442626
Epoch [110/200], Train Loss: 0.004692
Validation Loss: 0.00443475
Epoch [111/200], Train Loss: 0.004686
Validation Loss: 0.00442286
Epoch [112/200], Train Loss: 0.004679
Validation Loss: 0.00442276
Epoch [113/200], Train Loss: 0.004670
Validation Loss: 0.00442467
Epoch [114/200], Train Loss: 0.004686
Validation Loss: 0.00442067
Epoch [115/200], Train Loss: 0.004684
Validation Loss: 0.00442154
Epoch [116/200], Train Loss: 0.004680
Validation Loss: 0.00442302
Epoch [117/200], Train Loss: 0.004697
Validation Loss: 0.00441869
Epoch [118/200], Train Loss: 0.004664
Validation Loss: 0.00441880
Epoch [119/200], Train Loss: 0.004670
Validation Loss: 0.00441649
Epoch [120/200], Train Loss: 0.004689
Validation Loss: 0.00441490
Epoch [121/200], Train Loss: 0.004680
Validation Loss: 0.00441574
Epoch [122/200], Train Loss: 0.004670
Validation Loss: 0.00441601
Epoch [123/200], Train Loss: 0.004668
Validation Loss: 0.00441328
Epoch [124/200], Train Loss: 0.004661
Validation Loss: 0.00441406
Epoch [125/200], Train Loss: 0.004665
Validation Loss: 0.00441530
Epoch [126/200], Train Loss: 0.004684
Validation Loss: 0.00441015
Epoch [127/200], Train Loss: 0.004665
Validation Loss: 0.00440804
Epoch [128/200], Train Loss: 0.004661
Validation Loss: 0.00441052
Epoch [129/200], Train Loss: 0.004671
Validation Loss: 0.00440572
Epoch [130/200], Train Loss: 0.004666
Validation Loss: 0.00440599
Epoch [131/200], Train Loss: 0.004648
Validation Loss: 0.00440771
Epoch [132/200], Train Loss: 0.004667
Validation Loss: 0.00440731
Epoch [133/200], Train Loss: 0.004646
Validation Loss: 0.00440158
Epoch [134/200], Train Loss: 0.004671
Validation Loss: 0.00440117
Epoch [135/200], Train Loss: 0.004645
Validation Loss: 0.00440497
Epoch [136/200], Train Loss: 0.004653
Validation Loss: 0.00439861
Epoch [137/200], Train Loss: 0.004668
Validation Loss: 0.00439557
Epoch [138/200], Train Loss: 0.004666
Validation Loss: 0.00439497
Epoch [139/200], Train Loss: 0.004655
Validation Loss: 0.00439388
Epoch [140/200], Train Loss: 0.004646
Validation Loss: 0.00441412
Epoch [141/200], Train Loss: 0.004661
Validation Loss: 0.00439163
Epoch [142/200], Train Loss: 0.004653
Validation Loss: 0.00438843
Epoch [143/200], Train Loss: 0.004662
Validation Loss: 0.00438734
Epoch [144/200], Train Loss: 0.004639
Validation Loss: 0.00438633
Epoch [145/200], Train Loss: 0.004663
Validation Loss: 0.00438300
Epoch [146/200], Train Loss: 0.004640
Validation Loss: 0.00438063
Epoch [147/200], Train Loss: 0.004659
Validation Loss: 0.00437855
Epoch [148/200], Train Loss: 0.004647
Validation Loss: 0.00437123
Epoch [149/200], Train Loss: 0.004631
Validation Loss: 0.00436719
Epoch [150/200], Train Loss: 0.004626
Validation Loss: 0.00436481
Epoch [151/200], Train Loss: 0.004612
Validation Loss: 0.00436423
Epoch [152/200], Train Loss: 0.004623
Validation Loss: 0.00435353
Epoch [153/200], Train Loss: 0.004615
Validation Loss: 0.00435725
Epoch [154/200], Train Loss: 0.004611
Validation Loss: 0.00434841
Epoch [155/200], Train Loss: 0.004637
Validation Loss: 0.00433721
Epoch [156/200], Train Loss: 0.004617
Validation Loss: 0.00432300
Epoch [157/200], Train Loss: 0.004594
Validation Loss: 0.00430472
Epoch [158/200], Train Loss: 0.004583
Validation Loss: 0.00429331
Epoch [159/200], Train Loss: 0.004587
Validation Loss: 0.00427556
Epoch [160/200], Train Loss: 0.004558
Validation Loss: 0.00426201
Epoch [161/200], Train Loss: 0.004562
Validation Loss: 0.00425483
Epoch [162/200], Train Loss: 0.004537
Validation Loss: 0.00423345
Epoch [163/200], Train Loss: 0.004526
Validation Loss: 0.00422776
Epoch [164/200], Train Loss: 0.004519
Validation Loss: 0.00420095
Epoch [165/200], Train Loss: 0.004510
Validation Loss: 0.00417184
Epoch [166/200], Train Loss: 0.004495
Validation Loss: 0.00415727
Epoch [167/200], Train Loss: 0.004476
Validation Loss: 0.00417050
Epoch [168/200], Train Loss: 0.004444
Validation Loss: 0.00411458
Epoch [169/200], Train Loss: 0.004443
Validation Loss: 0.00410665
Epoch [170/200], Train Loss: 0.004406
Validation Loss: 0.00408499
Epoch [171/200], Train Loss: 0.004416
Validation Loss: 0.00403720
Epoch [172/200], Train Loss: 0.004375
Validation Loss: 0.00403866
Epoch [173/200], Train Loss: 0.004363
Validation Loss: 0.00398768
Epoch [174/200], Train Loss: 0.004340
Validation Loss: 0.00396159
Epoch [175/200], Train Loss: 0.004313
Validation Loss: 0.00399110
Epoch [176/200], Train Loss: 0.004279
Validation Loss: 0.00399370
Epoch [177/200], Train Loss: 0.004267
Validation Loss: 0.00388501
Epoch [178/200], Train Loss: 0.004220
Validation Loss: 0.00395808
Epoch [179/200], Train Loss: 0.004188
Validation Loss: 0.00380317
Epoch [180/200], Train Loss: 0.004169
Validation Loss: 0.00381913
Epoch [181/200], Train Loss: 0.004130
Validation Loss: 0.00370341
Epoch [182/200], Train Loss: 0.004132
Validation Loss: 0.00363379
Epoch [183/200], Train Loss: 0.004084
Validation Loss: 0.00361241
Epoch [184/200], Train Loss: 0.004049
Validation Loss: 0.00365892
Epoch [185/200], Train Loss: 0.004003
Validation Loss: 0.00353160
Epoch [186/200], Train Loss: 0.003995
Validation Loss: 0.00350629
Epoch [187/200], Train Loss: 0.003945
Validation Loss: 0.00344525
Epoch [188/200], Train Loss: 0.003920
Validation Loss: 0.00342404
Epoch [189/200], Train Loss: 0.003896
Validation Loss: 0.00345800
Epoch [190/200], Train Loss: 0.003818
Validation Loss: 0.00336672
Epoch [191/200], Train Loss: 0.003802
Validation Loss: 0.00333834
Epoch [192/200], Train Loss: 0.003752
Validation Loss: 0.00330197
Epoch [193/200], Train Loss: 0.003726
Validation Loss: 0.00336802
Epoch [194/200], Train Loss: 0.003708
Validation Loss: 0.00317778
Epoch [195/200], Train Loss: 0.003657
Validation Loss: 0.00312605
Epoch [196/200], Train Loss: 0.003645
Validation Loss: 0.00307746
Epoch [197/200], Train Loss: 0.003599
Validation Loss: 0.00311739
Epoch [198/200], Train Loss: 0.003578
Validation Loss: 0.00300743
Epoch [199/200], Train Loss: 0.003539
Validation Loss: 0.00305554
Epoch [200/200], Train Loss: 0.003524
Validation Loss: 0.00306251

Evaluating model for: Lamp
Run 7/144 completed in 1909.63 seconds with: {'MAE': np.float32(1.7690266), 'MSE': np.float32(108.139854), 'RMSE': np.float32(10.399032), 'SAE': np.float32(0.28770253), 'NDE': np.float32(0.79487264)}

Run 8/144: hidden=64, seq_len=120, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.004955
Validation Loss: 0.00458500
Epoch [2/200], Train Loss: 0.004859
Validation Loss: 0.00457968
Epoch [3/200], Train Loss: 0.004819
Validation Loss: 0.00454493
Epoch [4/200], Train Loss: 0.004781
Validation Loss: 0.00449598
Epoch [5/200], Train Loss: 0.004748
Validation Loss: 0.00449287
Epoch [6/200], Train Loss: 0.004752
Validation Loss: 0.00449035
Epoch [7/200], Train Loss: 0.004749
Validation Loss: 0.00448761
Epoch [8/200], Train Loss: 0.004733
Validation Loss: 0.00448697
Epoch [9/200], Train Loss: 0.004752
Validation Loss: 0.00448560
Epoch [10/200], Train Loss: 0.004726
Validation Loss: 0.00448448
Epoch [11/200], Train Loss: 0.004753
Validation Loss: 0.00448195
Epoch [12/200], Train Loss: 0.004722
Validation Loss: 0.00447942
Epoch [13/200], Train Loss: 0.004749
Validation Loss: 0.00447936
Epoch [14/200], Train Loss: 0.004717
Validation Loss: 0.00447433
Epoch [15/200], Train Loss: 0.004738
Validation Loss: 0.00447882
Epoch [16/200], Train Loss: 0.004727
Validation Loss: 0.00447383
Epoch [17/200], Train Loss: 0.004739
Validation Loss: 0.00447160
Epoch [18/200], Train Loss: 0.004731
Validation Loss: 0.00447041
Epoch [19/200], Train Loss: 0.004722
Validation Loss: 0.00447331
Epoch [20/200], Train Loss: 0.004733
Validation Loss: 0.00447050
Epoch [21/200], Train Loss: 0.004719
Validation Loss: 0.00446742
Epoch [22/200], Train Loss: 0.004727
Validation Loss: 0.00446891
Epoch [23/200], Train Loss: 0.004730
Validation Loss: 0.00447133
Epoch [24/200], Train Loss: 0.004729
Validation Loss: 0.00446686
Epoch [25/200], Train Loss: 0.004721
Validation Loss: 0.00446359
Epoch [26/200], Train Loss: 0.004739
Validation Loss: 0.00446627
Epoch [27/200], Train Loss: 0.004732
Validation Loss: 0.00447009
Epoch [28/200], Train Loss: 0.004719
Validation Loss: 0.00446372
Epoch [29/200], Train Loss: 0.004728
Validation Loss: 0.00446323
Epoch [30/200], Train Loss: 0.004731
Validation Loss: 0.00446293
Epoch [31/200], Train Loss: 0.004734
Validation Loss: 0.00446776
Epoch [32/200], Train Loss: 0.004735
Validation Loss: 0.00446471
Epoch [33/200], Train Loss: 0.004705
Validation Loss: 0.00446068
Epoch [34/200], Train Loss: 0.004713
Validation Loss: 0.00446388
Epoch [35/200], Train Loss: 0.004717
Validation Loss: 0.00446811
Epoch [36/200], Train Loss: 0.004712
Validation Loss: 0.00446011
Epoch [37/200], Train Loss: 0.004715
Validation Loss: 0.00446161
Epoch [38/200], Train Loss: 0.004717
Validation Loss: 0.00445808
Epoch [39/200], Train Loss: 0.004726
Validation Loss: 0.00445914
Epoch [40/200], Train Loss: 0.004742
Validation Loss: 0.00446099
Epoch [41/200], Train Loss: 0.004731
Validation Loss: 0.00445959
Epoch [42/200], Train Loss: 0.004730
Validation Loss: 0.00445998
Epoch [43/200], Train Loss: 0.004710
Validation Loss: 0.00445746
Epoch [44/200], Train Loss: 0.004735
Validation Loss: 0.00445946
Epoch [45/200], Train Loss: 0.004718
Validation Loss: 0.00446189
Epoch [46/200], Train Loss: 0.004744
Validation Loss: 0.00445776
Epoch [47/200], Train Loss: 0.004720
Validation Loss: 0.00446556
Epoch [48/200], Train Loss: 0.004706
Validation Loss: 0.00445877
Epoch [49/200], Train Loss: 0.004727
Validation Loss: 0.00445716
Epoch [50/200], Train Loss: 0.004718
Validation Loss: 0.00445824
Epoch [51/200], Train Loss: 0.004703
Validation Loss: 0.00445760
Epoch [52/200], Train Loss: 0.004732
Validation Loss: 0.00446047
Epoch [53/200], Train Loss: 0.004715
Validation Loss: 0.00445927
Epoch [54/200], Train Loss: 0.004711
Validation Loss: 0.00445767
Epoch [55/200], Train Loss: 0.004719
Validation Loss: 0.00445622
Epoch [56/200], Train Loss: 0.004738
Validation Loss: 0.00445824
Epoch [57/200], Train Loss: 0.004728
Validation Loss: 0.00445667
Epoch [58/200], Train Loss: 0.004740
Validation Loss: 0.00445661
Epoch [59/200], Train Loss: 0.004725
Validation Loss: 0.00445667
Epoch [60/200], Train Loss: 0.004708
Validation Loss: 0.00445771
Epoch [61/200], Train Loss: 0.004715
Validation Loss: 0.00445519
Epoch [62/200], Train Loss: 0.004716
Validation Loss: 0.00445346
Epoch [63/200], Train Loss: 0.004710
Validation Loss: 0.00445553
Epoch [64/200], Train Loss: 0.004717
Validation Loss: 0.00445685
Epoch [65/200], Train Loss: 0.004723
Validation Loss: 0.00445582
Epoch [66/200], Train Loss: 0.004735
Validation Loss: 0.00445678
Epoch [67/200], Train Loss: 0.004729
Validation Loss: 0.00445614
Epoch [68/200], Train Loss: 0.004714
Validation Loss: 0.00445234
Epoch [69/200], Train Loss: 0.004713
Validation Loss: 0.00445334
Epoch [70/200], Train Loss: 0.004717
Validation Loss: 0.00445620
Epoch [71/200], Train Loss: 0.004733
Validation Loss: 0.00445410
Epoch [72/200], Train Loss: 0.004704
Validation Loss: 0.00445034
Epoch [73/200], Train Loss: 0.004722
Validation Loss: 0.00445669
Epoch [74/200], Train Loss: 0.004711
Validation Loss: 0.00445391
Epoch [75/200], Train Loss: 0.004712
Validation Loss: 0.00445356
Epoch [76/200], Train Loss: 0.004725
Validation Loss: 0.00445245
Epoch [77/200], Train Loss: 0.004726
Validation Loss: 0.00445498
Epoch [78/200], Train Loss: 0.004712
Validation Loss: 0.00444910
Epoch [79/200], Train Loss: 0.004718
Validation Loss: 0.00445160
Epoch [80/200], Train Loss: 0.004710
Validation Loss: 0.00444889
Epoch [81/200], Train Loss: 0.004707
Validation Loss: 0.00444724
Epoch [82/200], Train Loss: 0.004700
Validation Loss: 0.00444691
Epoch [83/200], Train Loss: 0.004703
Validation Loss: 0.00444706
Epoch [84/200], Train Loss: 0.004725
Validation Loss: 0.00444856
Epoch [85/200], Train Loss: 0.004713
Validation Loss: 0.00444730
Epoch [86/200], Train Loss: 0.004697
Validation Loss: 0.00444557
Epoch [87/200], Train Loss: 0.004691
Validation Loss: 0.00444456
Epoch [88/200], Train Loss: 0.004711
Validation Loss: 0.00444635
Epoch [89/200], Train Loss: 0.004709
Validation Loss: 0.00444696
Epoch [90/200], Train Loss: 0.004731
Validation Loss: 0.00444963
Epoch [91/200], Train Loss: 0.004696
Validation Loss: 0.00444233
Epoch [92/200], Train Loss: 0.004695
Validation Loss: 0.00444070
Epoch [93/200], Train Loss: 0.004680
Validation Loss: 0.00444323
Epoch [94/200], Train Loss: 0.004706
Validation Loss: 0.00444001
Epoch [95/200], Train Loss: 0.004697
Validation Loss: 0.00443966
Epoch [96/200], Train Loss: 0.004691
Validation Loss: 0.00444074
Epoch [97/200], Train Loss: 0.004691
Validation Loss: 0.00444027
Epoch [98/200], Train Loss: 0.004691
Validation Loss: 0.00443663
Epoch [99/200], Train Loss: 0.004685
Validation Loss: 0.00443775
Epoch [100/200], Train Loss: 0.004693
Validation Loss: 0.00443254
Epoch [101/200], Train Loss: 0.004682
Validation Loss: 0.00443007
Epoch [102/200], Train Loss: 0.004680
Validation Loss: 0.00443360
Epoch [103/200], Train Loss: 0.004670
Validation Loss: 0.00443245
Epoch [104/200], Train Loss: 0.004675
Validation Loss: 0.00442923
Epoch [105/200], Train Loss: 0.004696
Validation Loss: 0.00443772
Epoch [106/200], Train Loss: 0.004679
Validation Loss: 0.00443016
Epoch [107/200], Train Loss: 0.004684
Validation Loss: 0.00443156
Epoch [108/200], Train Loss: 0.004673
Validation Loss: 0.00442168
Epoch [109/200], Train Loss: 0.004667
Validation Loss: 0.00442355
Epoch [110/200], Train Loss: 0.004678
Validation Loss: 0.00441508
Epoch [111/200], Train Loss: 0.004664
Validation Loss: 0.00441749
Epoch [112/200], Train Loss: 0.004652
Validation Loss: 0.00440890
Epoch [113/200], Train Loss: 0.004682
Validation Loss: 0.00440286
Epoch [114/200], Train Loss: 0.004662
Validation Loss: 0.00439864
Epoch [115/200], Train Loss: 0.004636
Validation Loss: 0.00439308
Epoch [116/200], Train Loss: 0.004639
Validation Loss: 0.00438608
Epoch [117/200], Train Loss: 0.004641
Validation Loss: 0.00437970
Epoch [118/200], Train Loss: 0.004631
Validation Loss: 0.00436969
Epoch [119/200], Train Loss: 0.004645
Validation Loss: 0.00435612
Epoch [120/200], Train Loss: 0.004605
Validation Loss: 0.00435073
Epoch [121/200], Train Loss: 0.004615
Validation Loss: 0.00432169
Epoch [122/200], Train Loss: 0.004588
Validation Loss: 0.00430707
Epoch [123/200], Train Loss: 0.004573
Validation Loss: 0.00427360
Epoch [124/200], Train Loss: 0.004542
Validation Loss: 0.00427402
Epoch [125/200], Train Loss: 0.004520
Validation Loss: 0.00420514
Epoch [126/200], Train Loss: 0.004486
Validation Loss: 0.00418992
Epoch [127/200], Train Loss: 0.004456
Validation Loss: 0.00412952
Epoch [128/200], Train Loss: 0.004399
Validation Loss: 0.00406346
Epoch [129/200], Train Loss: 0.004363
Validation Loss: 0.00400079
Epoch [130/200], Train Loss: 0.004324
Validation Loss: 0.00394271
Epoch [131/200], Train Loss: 0.004241
Validation Loss: 0.00389018
Epoch [132/200], Train Loss: 0.004153
Validation Loss: 0.00379542
Epoch [133/200], Train Loss: 0.004069
Validation Loss: 0.00362953
Epoch [134/200], Train Loss: 0.003996
Validation Loss: 0.00348380
Epoch [135/200], Train Loss: 0.003922
Validation Loss: 0.00332046
Epoch [136/200], Train Loss: 0.003747
Validation Loss: 0.00319831
Epoch [137/200], Train Loss: 0.003637
Validation Loss: 0.00316409
Epoch [138/200], Train Loss: 0.003518
Validation Loss: 0.00302681
Epoch [139/200], Train Loss: 0.003391
Validation Loss: 0.00280360
Epoch [140/200], Train Loss: 0.003312
Validation Loss: 0.00274690
Epoch [141/200], Train Loss: 0.003182
Validation Loss: 0.00262621
Epoch [142/200], Train Loss: 0.003082
Validation Loss: 0.00252620
Epoch [143/200], Train Loss: 0.002997
Validation Loss: 0.00251712
Epoch [144/200], Train Loss: 0.002954
Validation Loss: 0.00239026
Epoch [145/200], Train Loss: 0.002830
Validation Loss: 0.00233798
Epoch [146/200], Train Loss: 0.002764
Validation Loss: 0.00226070
Epoch [147/200], Train Loss: 0.002705
Validation Loss: 0.00226767
Epoch [148/200], Train Loss: 0.002647
Validation Loss: 0.00224745
Epoch [149/200], Train Loss: 0.002603
Validation Loss: 0.00220092
Epoch [150/200], Train Loss: 0.002538
Validation Loss: 0.00207959
Epoch [151/200], Train Loss: 0.002478
Validation Loss: 0.00201628
Epoch [152/200], Train Loss: 0.002444
Validation Loss: 0.00202384
Epoch [153/200], Train Loss: 0.002417
Validation Loss: 0.00198661
Epoch [154/200], Train Loss: 0.002383
Validation Loss: 0.00192066
Epoch [155/200], Train Loss: 0.002334
Validation Loss: 0.00190718
Epoch [156/200], Train Loss: 0.002314
Validation Loss: 0.00189156
Epoch [157/200], Train Loss: 0.002271
Validation Loss: 0.00183078
Epoch [158/200], Train Loss: 0.002242
Validation Loss: 0.00179306
Epoch [159/200], Train Loss: 0.002207
Validation Loss: 0.00176814
Epoch [160/200], Train Loss: 0.002195
Validation Loss: 0.00174706
Epoch [161/200], Train Loss: 0.002144
Validation Loss: 0.00174159
Epoch [162/200], Train Loss: 0.002107
Validation Loss: 0.00173205
Epoch [163/200], Train Loss: 0.002080
Validation Loss: 0.00166948
Epoch [164/200], Train Loss: 0.002083
Validation Loss: 0.00172074
Epoch [165/200], Train Loss: 0.002056
Validation Loss: 0.00165129
Epoch [166/200], Train Loss: 0.002019
Validation Loss: 0.00161790
Epoch [167/200], Train Loss: 0.001989
Validation Loss: 0.00163281
Epoch [168/200], Train Loss: 0.001969
Validation Loss: 0.00160018
Epoch [169/200], Train Loss: 0.001947
Validation Loss: 0.00156212
Epoch [170/200], Train Loss: 0.001940
Validation Loss: 0.00155782
Epoch [171/200], Train Loss: 0.001896
Validation Loss: 0.00154979
Epoch [172/200], Train Loss: 0.001891
Validation Loss: 0.00151542
Epoch [173/200], Train Loss: 0.001856
Validation Loss: 0.00151379
Epoch [174/200], Train Loss: 0.001835
Validation Loss: 0.00151366
Epoch [175/200], Train Loss: 0.001834
Validation Loss: 0.00146111
Epoch [176/200], Train Loss: 0.001811
Validation Loss: 0.00144962
Epoch [177/200], Train Loss: 0.001803
Validation Loss: 0.00143161
Epoch [178/200], Train Loss: 0.001774
Validation Loss: 0.00142173
Epoch [179/200], Train Loss: 0.001773
Validation Loss: 0.00142048
Epoch [180/200], Train Loss: 0.001742
Validation Loss: 0.00142816
Epoch [181/200], Train Loss: 0.001713
Validation Loss: 0.00140557
Epoch [182/200], Train Loss: 0.001686
Validation Loss: 0.00138267
Epoch [183/200], Train Loss: 0.001671
Validation Loss: 0.00135153
Epoch [184/200], Train Loss: 0.001673
Validation Loss: 0.00134420
Epoch [185/200], Train Loss: 0.001655
Validation Loss: 0.00134713
Epoch [186/200], Train Loss: 0.001632
Validation Loss: 0.00133531
Epoch [187/200], Train Loss: 0.001617
Validation Loss: 0.00131482
Epoch [188/200], Train Loss: 0.001604
Validation Loss: 0.00130195
Epoch [189/200], Train Loss: 0.001608
Validation Loss: 0.00129976
Epoch [190/200], Train Loss: 0.001592
Validation Loss: 0.00129541
Epoch [191/200], Train Loss: 0.001560
Validation Loss: 0.00127890
Epoch [192/200], Train Loss: 0.001555
Validation Loss: 0.00125991
Epoch [193/200], Train Loss: 0.001547
Validation Loss: 0.00125396
Epoch [194/200], Train Loss: 0.001541
Validation Loss: 0.00124663
Epoch [195/200], Train Loss: 0.001509
Validation Loss: 0.00123808
Epoch [196/200], Train Loss: 0.001509
Validation Loss: 0.00121882
Epoch [197/200], Train Loss: 0.001490
Validation Loss: 0.00122530
Epoch [198/200], Train Loss: 0.001488
Validation Loss: 0.00121904
Epoch [199/200], Train Loss: 0.001477
Validation Loss: 0.00120774
Epoch [200/200], Train Loss: 0.001463
Validation Loss: 0.00119975

Evaluating model for: Lamp
Run 8/144 completed in 1943.43 seconds with: {'MAE': np.float32(0.9252153), 'MSE': np.float32(41.503296), 'RMSE': np.float32(6.442305), 'SAE': np.float32(0.057623547), 'NDE': np.float32(0.4924315)}

Run 9/144: hidden=64, seq_len=120, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.005738
Validation Loss: 0.00696996
Epoch [2/200], Train Loss: 0.004848
Validation Loss: 0.00660221
Epoch [3/200], Train Loss: 0.004774
Validation Loss: 0.00659798
Epoch [4/200], Train Loss: 0.004765
Validation Loss: 0.00658704
Epoch [5/200], Train Loss: 0.004743
Validation Loss: 0.00656519
Epoch [6/200], Train Loss: 0.004728
Validation Loss: 0.00655451
Epoch [7/200], Train Loss: 0.004736
Validation Loss: 0.00653502
Epoch [8/200], Train Loss: 0.004727
Validation Loss: 0.00652145
Epoch [9/200], Train Loss: 0.004723
Validation Loss: 0.00651854
Epoch [10/200], Train Loss: 0.004699
Validation Loss: 0.00652590
Epoch [11/200], Train Loss: 0.004716
Validation Loss: 0.00650279
Epoch [12/200], Train Loss: 0.004696
Validation Loss: 0.00651020
Epoch [13/200], Train Loss: 0.004693
Validation Loss: 0.00650829
Epoch [14/200], Train Loss: 0.004692
Validation Loss: 0.00649853
Epoch [15/200], Train Loss: 0.004696
Validation Loss: 0.00649191
Epoch [16/200], Train Loss: 0.004679
Validation Loss: 0.00649695
Epoch [17/200], Train Loss: 0.004688
Validation Loss: 0.00649727
Epoch [18/200], Train Loss: 0.004684
Validation Loss: 0.00648747
Epoch [19/200], Train Loss: 0.004689
Validation Loss: 0.00651478
Epoch [20/200], Train Loss: 0.004680
Validation Loss: 0.00649789
Epoch [21/200], Train Loss: 0.004668
Validation Loss: 0.00648935
Epoch [22/200], Train Loss: 0.004682
Validation Loss: 0.00648695
Epoch [23/200], Train Loss: 0.004684
Validation Loss: 0.00648476
Epoch [24/200], Train Loss: 0.004669
Validation Loss: 0.00647478
Epoch [25/200], Train Loss: 0.004670
Validation Loss: 0.00647561
Epoch [26/200], Train Loss: 0.004657
Validation Loss: 0.00648547
Epoch [27/200], Train Loss: 0.004665
Validation Loss: 0.00647927
Epoch [28/200], Train Loss: 0.004657
Validation Loss: 0.00648009
Epoch [29/200], Train Loss: 0.004652
Validation Loss: 0.00646613
Epoch [30/200], Train Loss: 0.004655
Validation Loss: 0.00647836
Epoch [31/200], Train Loss: 0.004640
Validation Loss: 0.00646827
Epoch [32/200], Train Loss: 0.004652
Validation Loss: 0.00645619
Epoch [33/200], Train Loss: 0.004649
Validation Loss: 0.00645688
Epoch [34/200], Train Loss: 0.004642
Validation Loss: 0.00645155
Epoch [35/200], Train Loss: 0.004647
Validation Loss: 0.00644851
Epoch [36/200], Train Loss: 0.004639
Validation Loss: 0.00645119
Epoch [37/200], Train Loss: 0.004654
Validation Loss: 0.00645486
Epoch [38/200], Train Loss: 0.004655
Validation Loss: 0.00644810
Epoch [39/200], Train Loss: 0.004634
Validation Loss: 0.00644490
Epoch [40/200], Train Loss: 0.004640
Validation Loss: 0.00645197
Epoch [41/200], Train Loss: 0.004643
Validation Loss: 0.00641906
Epoch [42/200], Train Loss: 0.004640
Validation Loss: 0.00642550
Epoch [43/200], Train Loss: 0.004626
Validation Loss: 0.00641043
Epoch [44/200], Train Loss: 0.004630
Validation Loss: 0.00640469
Epoch [45/200], Train Loss: 0.004621
Validation Loss: 0.00640909
Epoch [46/200], Train Loss: 0.004616
Validation Loss: 0.00639273
Epoch [47/200], Train Loss: 0.004614
Validation Loss: 0.00642765
Epoch [48/200], Train Loss: 0.004610
Validation Loss: 0.00638972
Epoch [49/200], Train Loss: 0.004604
Validation Loss: 0.00637817
Epoch [50/200], Train Loss: 0.004610
Validation Loss: 0.00637326
Epoch [51/200], Train Loss: 0.004601
Validation Loss: 0.00637331
Epoch [52/200], Train Loss: 0.004598
Validation Loss: 0.00637593
Epoch [53/200], Train Loss: 0.004603
Validation Loss: 0.00635663
Epoch [54/200], Train Loss: 0.004591
Validation Loss: 0.00635021
Epoch [55/200], Train Loss: 0.004601
Validation Loss: 0.00636942
Epoch [56/200], Train Loss: 0.004592
Validation Loss: 0.00633927
Epoch [57/200], Train Loss: 0.004580
Validation Loss: 0.00634407
Epoch [58/200], Train Loss: 0.004570
Validation Loss: 0.00634029
Epoch [59/200], Train Loss: 0.004575
Validation Loss: 0.00631233
Epoch [60/200], Train Loss: 0.004580
Validation Loss: 0.00630207
Epoch [61/200], Train Loss: 0.004569
Validation Loss: 0.00629289
Epoch [62/200], Train Loss: 0.004558
Validation Loss: 0.00631691
Epoch [63/200], Train Loss: 0.004549
Validation Loss: 0.00628019
Epoch [64/200], Train Loss: 0.004548
Validation Loss: 0.00627975
Epoch [65/200], Train Loss: 0.004538
Validation Loss: 0.00625108
Epoch [66/200], Train Loss: 0.004525
Validation Loss: 0.00627826
Epoch [67/200], Train Loss: 0.004522
Validation Loss: 0.00623269
Epoch [68/200], Train Loss: 0.004512
Validation Loss: 0.00623285
Epoch [69/200], Train Loss: 0.004525
Validation Loss: 0.00619991
Epoch [70/200], Train Loss: 0.004507
Validation Loss: 0.00619628
Epoch [71/200], Train Loss: 0.004492
Validation Loss: 0.00616695
Epoch [72/200], Train Loss: 0.004500
Validation Loss: 0.00620003
Epoch [73/200], Train Loss: 0.004474
Validation Loss: 0.00612946
Epoch [74/200], Train Loss: 0.004449
Validation Loss: 0.00609046
Epoch [75/200], Train Loss: 0.004434
Validation Loss: 0.00608052
Epoch [76/200], Train Loss: 0.004436
Validation Loss: 0.00606458
Epoch [77/200], Train Loss: 0.004423
Validation Loss: 0.00604340
Epoch [78/200], Train Loss: 0.004392
Validation Loss: 0.00597272
Epoch [79/200], Train Loss: 0.004364
Validation Loss: 0.00595563
Epoch [80/200], Train Loss: 0.004341
Validation Loss: 0.00591684
Epoch [81/200], Train Loss: 0.004323
Validation Loss: 0.00586925
Epoch [82/200], Train Loss: 0.004288
Validation Loss: 0.00582986
Epoch [83/200], Train Loss: 0.004277
Validation Loss: 0.00578581
Epoch [84/200], Train Loss: 0.004228
Validation Loss: 0.00572866
Epoch [85/200], Train Loss: 0.004205
Validation Loss: 0.00569297
Epoch [86/200], Train Loss: 0.004190
Validation Loss: 0.00565867
Epoch [87/200], Train Loss: 0.004159
Validation Loss: 0.00570920
Epoch [88/200], Train Loss: 0.004134
Validation Loss: 0.00558448
Epoch [89/200], Train Loss: 0.004122
Validation Loss: 0.00561660
Epoch [90/200], Train Loss: 0.004100
Validation Loss: 0.00551934
Epoch [91/200], Train Loss: 0.004038
Validation Loss: 0.00547499
Epoch [92/200], Train Loss: 0.004039
Validation Loss: 0.00544423
Epoch [93/200], Train Loss: 0.004000
Validation Loss: 0.00543890
Epoch [94/200], Train Loss: 0.003992
Validation Loss: 0.00536059
Epoch [95/200], Train Loss: 0.003945
Validation Loss: 0.00538384
Epoch [96/200], Train Loss: 0.003903
Validation Loss: 0.00531357
Epoch [97/200], Train Loss: 0.003888
Validation Loss: 0.00520177
Epoch [98/200], Train Loss: 0.003845
Validation Loss: 0.00529543
Epoch [99/200], Train Loss: 0.003818
Validation Loss: 0.00522314
Epoch [100/200], Train Loss: 0.003790
Validation Loss: 0.00508517
Epoch [101/200], Train Loss: 0.003756
Validation Loss: 0.00503280
Epoch [102/200], Train Loss: 0.003721
Validation Loss: 0.00497011
Epoch [103/200], Train Loss: 0.003677
Validation Loss: 0.00489644
Epoch [104/200], Train Loss: 0.003653
Validation Loss: 0.00487617
Epoch [105/200], Train Loss: 0.003611
Validation Loss: 0.00485421
Epoch [106/200], Train Loss: 0.003581
Validation Loss: 0.00470768
Epoch [107/200], Train Loss: 0.003536
Validation Loss: 0.00468740
Epoch [108/200], Train Loss: 0.003498
Validation Loss: 0.00463530
Epoch [109/200], Train Loss: 0.003466
Validation Loss: 0.00457980
Epoch [110/200], Train Loss: 0.003426
Validation Loss: 0.00451173
Epoch [111/200], Train Loss: 0.003391
Validation Loss: 0.00445610
Epoch [112/200], Train Loss: 0.003353
Validation Loss: 0.00440923
Epoch [113/200], Train Loss: 0.003317
Validation Loss: 0.00432647
Epoch [114/200], Train Loss: 0.003275
Validation Loss: 0.00430536
Epoch [115/200], Train Loss: 0.003240
Validation Loss: 0.00422733
Epoch [116/200], Train Loss: 0.003211
Validation Loss: 0.00420987
Epoch [117/200], Train Loss: 0.003183
Validation Loss: 0.00413261
Epoch [118/200], Train Loss: 0.003146
Validation Loss: 0.00408834
Epoch [119/200], Train Loss: 0.003098
Validation Loss: 0.00401941
Epoch [120/200], Train Loss: 0.003074
Validation Loss: 0.00399493
Epoch [121/200], Train Loss: 0.003039
Validation Loss: 0.00394056
Epoch [122/200], Train Loss: 0.003004
Validation Loss: 0.00413496
Epoch [123/200], Train Loss: 0.002991
Validation Loss: 0.00396220
Epoch [124/200], Train Loss: 0.002941
Validation Loss: 0.00390244
Epoch [125/200], Train Loss: 0.002906
Validation Loss: 0.00381109
Epoch [126/200], Train Loss: 0.002888
Validation Loss: 0.00379222
Epoch [127/200], Train Loss: 0.002856
Validation Loss: 0.00375789
Epoch [128/200], Train Loss: 0.002833
Validation Loss: 0.00370088
Epoch [129/200], Train Loss: 0.002793
Validation Loss: 0.00369615
Epoch [130/200], Train Loss: 0.002772
Validation Loss: 0.00364758
Epoch [131/200], Train Loss: 0.002751
Validation Loss: 0.00370035
Epoch [132/200], Train Loss: 0.002745
Validation Loss: 0.00361375
Epoch [133/200], Train Loss: 0.002697
Validation Loss: 0.00359803
Epoch [134/200], Train Loss: 0.002697
Validation Loss: 0.00354273
Epoch [135/200], Train Loss: 0.002668
Validation Loss: 0.00349199
Epoch [136/200], Train Loss: 0.002651
Validation Loss: 0.00348135
Epoch [137/200], Train Loss: 0.002607
Validation Loss: 0.00346733
Epoch [138/200], Train Loss: 0.002602
Validation Loss: 0.00349012
Epoch [139/200], Train Loss: 0.002586
Validation Loss: 0.00342542
Epoch [140/200], Train Loss: 0.002566
Validation Loss: 0.00338526
Epoch [141/200], Train Loss: 0.002549
Validation Loss: 0.00340200
Epoch [142/200], Train Loss: 0.002519
Validation Loss: 0.00338907
Epoch [143/200], Train Loss: 0.002523
Validation Loss: 0.00335612
Epoch [144/200], Train Loss: 0.002503
Validation Loss: 0.00328317
Epoch [145/200], Train Loss: 0.002489
Validation Loss: 0.00327939
Epoch [146/200], Train Loss: 0.002465
Validation Loss: 0.00329578
Epoch [147/200], Train Loss: 0.002439
Validation Loss: 0.00329709
Epoch [148/200], Train Loss: 0.002430
Validation Loss: 0.00331422
Epoch [149/200], Train Loss: 0.002429
Validation Loss: 0.00327351
Epoch [150/200], Train Loss: 0.002411
Validation Loss: 0.00323924
Epoch [151/200], Train Loss: 0.002389
Validation Loss: 0.00321726
Epoch [152/200], Train Loss: 0.002372
Validation Loss: 0.00319927
Epoch [153/200], Train Loss: 0.002364
Validation Loss: 0.00316958
Epoch [154/200], Train Loss: 0.002346
Validation Loss: 0.00317945
Epoch [155/200], Train Loss: 0.002345
Validation Loss: 0.00315660
Epoch [156/200], Train Loss: 0.002342
Validation Loss: 0.00311620
Epoch [157/200], Train Loss: 0.002318
Validation Loss: 0.00313861
Epoch [158/200], Train Loss: 0.002305
Validation Loss: 0.00312127
Epoch [159/200], Train Loss: 0.002303
Validation Loss: 0.00311711
Epoch [160/200], Train Loss: 0.002293
Validation Loss: 0.00313096
Epoch [161/200], Train Loss: 0.002277
Validation Loss: 0.00308378
Epoch [162/200], Train Loss: 0.002264
Validation Loss: 0.00309997
Epoch [163/200], Train Loss: 0.002264
Validation Loss: 0.00303337
Epoch [164/200], Train Loss: 0.002239
Validation Loss: 0.00302093
Epoch [165/200], Train Loss: 0.002223
Validation Loss: 0.00302193
Epoch [166/200], Train Loss: 0.002226
Validation Loss: 0.00304677
Epoch [167/200], Train Loss: 0.002214
Validation Loss: 0.00306297
Epoch [168/200], Train Loss: 0.002230
Validation Loss: 0.00294747
Epoch [169/200], Train Loss: 0.002202
Validation Loss: 0.00300438
Epoch [170/200], Train Loss: 0.002184
Validation Loss: 0.00300115
Epoch [171/200], Train Loss: 0.002182
Validation Loss: 0.00300245
Epoch [172/200], Train Loss: 0.002181
Validation Loss: 0.00299451
Epoch [173/200], Train Loss: 0.002190
Validation Loss: 0.00294512
Epoch [174/200], Train Loss: 0.002172
Validation Loss: 0.00292147
Epoch [175/200], Train Loss: 0.002153
Validation Loss: 0.00290386
Epoch [176/200], Train Loss: 0.002151
Validation Loss: 0.00288301
Epoch [177/200], Train Loss: 0.002143
Validation Loss: 0.00292274
Epoch [178/200], Train Loss: 0.002114
Validation Loss: 0.00295417
Epoch [179/200], Train Loss: 0.002124
Validation Loss: 0.00289765
Epoch [180/200], Train Loss: 0.002114
Validation Loss: 0.00287286
Epoch [181/200], Train Loss: 0.002118
Validation Loss: 0.00288380
Epoch [182/200], Train Loss: 0.002112
Validation Loss: 0.00288810
Epoch [183/200], Train Loss: 0.002102
Validation Loss: 0.00286912
Epoch [184/200], Train Loss: 0.002088
Validation Loss: 0.00288445
Epoch [185/200], Train Loss: 0.002087
Validation Loss: 0.00288432
Epoch [186/200], Train Loss: 0.002080
Validation Loss: 0.00285880
Epoch [187/200], Train Loss: 0.002069
Validation Loss: 0.00284732
Epoch [188/200], Train Loss: 0.002069
Validation Loss: 0.00280931
Epoch [189/200], Train Loss: 0.002061
Validation Loss: 0.00282599
Epoch [190/200], Train Loss: 0.002071
Validation Loss: 0.00282683
Epoch [191/200], Train Loss: 0.002047
Validation Loss: 0.00279818
Epoch [192/200], Train Loss: 0.002048
Validation Loss: 0.00281276
Epoch [193/200], Train Loss: 0.002043
Validation Loss: 0.00284719
Epoch [194/200], Train Loss: 0.002044
Validation Loss: 0.00281353
Epoch [195/200], Train Loss: 0.002038
Validation Loss: 0.00277982
Epoch [196/200], Train Loss: 0.002028
Validation Loss: 0.00276617
Epoch [197/200], Train Loss: 0.002034
Validation Loss: 0.00277771
Epoch [198/200], Train Loss: 0.002022
Validation Loss: 0.00276488
Epoch [199/200], Train Loss: 0.002010
Validation Loss: 0.00276808
Epoch [200/200], Train Loss: 0.002002
Validation Loss: 0.00275996

Evaluating model for: Lamp
Run 9/144 completed in 958.41 seconds with: {'MAE': np.float32(1.173025), 'MSE': np.float32(41.350716), 'RMSE': np.float32(6.4304523), 'SAE': np.float32(0.029440919), 'NDE': np.float32(0.5720309)}

Run 10/144: hidden=64, seq_len=120, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.006423
Validation Loss: 0.00698060
Epoch [2/200], Train Loss: 0.004950
Validation Loss: 0.00665310
Epoch [3/200], Train Loss: 0.004833
Validation Loss: 0.00664878
Epoch [4/200], Train Loss: 0.004822
Validation Loss: 0.00663928
Epoch [5/200], Train Loss: 0.004809
Validation Loss: 0.00663239
Epoch [6/200], Train Loss: 0.004806
Validation Loss: 0.00662866
Epoch [7/200], Train Loss: 0.004784
Validation Loss: 0.00661096
Epoch [8/200], Train Loss: 0.004784
Validation Loss: 0.00658484
Epoch [9/200], Train Loss: 0.004753
Validation Loss: 0.00655799
Epoch [10/200], Train Loss: 0.004740
Validation Loss: 0.00652500
Epoch [11/200], Train Loss: 0.004725
Validation Loss: 0.00651027
Epoch [12/200], Train Loss: 0.004715
Validation Loss: 0.00651456
Epoch [13/200], Train Loss: 0.004715
Validation Loss: 0.00650366
Epoch [14/200], Train Loss: 0.004714
Validation Loss: 0.00651013
Epoch [15/200], Train Loss: 0.004701
Validation Loss: 0.00650071
Epoch [16/200], Train Loss: 0.004711
Validation Loss: 0.00651342
Epoch [17/200], Train Loss: 0.004706
Validation Loss: 0.00651719
Epoch [18/200], Train Loss: 0.004699
Validation Loss: 0.00649885
Epoch [19/200], Train Loss: 0.004697
Validation Loss: 0.00650102
Epoch [20/200], Train Loss: 0.004706
Validation Loss: 0.00650132
Epoch [21/200], Train Loss: 0.004693
Validation Loss: 0.00649759
Epoch [22/200], Train Loss: 0.004687
Validation Loss: 0.00650006
Epoch [23/200], Train Loss: 0.004679
Validation Loss: 0.00649323
Epoch [24/200], Train Loss: 0.004688
Validation Loss: 0.00650727
Epoch [25/200], Train Loss: 0.004692
Validation Loss: 0.00649788
Epoch [26/200], Train Loss: 0.004681
Validation Loss: 0.00650935
Epoch [27/200], Train Loss: 0.004683
Validation Loss: 0.00648706
Epoch [28/200], Train Loss: 0.004685
Validation Loss: 0.00648854
Epoch [29/200], Train Loss: 0.004694
Validation Loss: 0.00649803
Epoch [30/200], Train Loss: 0.004673
Validation Loss: 0.00648818
Epoch [31/200], Train Loss: 0.004683
Validation Loss: 0.00649501
Epoch [32/200], Train Loss: 0.004684
Validation Loss: 0.00649080
Epoch [33/200], Train Loss: 0.004672
Validation Loss: 0.00650690
Epoch [34/200], Train Loss: 0.004673
Validation Loss: 0.00652083
Epoch [35/200], Train Loss: 0.004668
Validation Loss: 0.00649561
Epoch [36/200], Train Loss: 0.004681
Validation Loss: 0.00649071
Epoch [37/200], Train Loss: 0.004666
Validation Loss: 0.00649456
Early stopping triggered

Evaluating model for: Lamp
Run 10/144 completed in 179.89 seconds with: {'MAE': np.float32(2.4179096), 'MSE': np.float32(123.20913), 'RMSE': np.float32(11.09996), 'SAE': np.float32(0.2945902), 'NDE': np.float32(0.9874143)}

Run 11/144: hidden=64, seq_len=120, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.010302
Validation Loss: 0.00960907
Epoch [2/200], Train Loss: 0.006025
Validation Loss: 0.00687650
Epoch [3/200], Train Loss: 0.004887
Validation Loss: 0.00667450
Epoch [4/200], Train Loss: 0.004858
Validation Loss: 0.00667085
Epoch [5/200], Train Loss: 0.004856
Validation Loss: 0.00666854
Epoch [6/200], Train Loss: 0.004855
Validation Loss: 0.00665783
Epoch [7/200], Train Loss: 0.004837
Validation Loss: 0.00664500
Epoch [8/200], Train Loss: 0.004834
Validation Loss: 0.00662787
Epoch [9/200], Train Loss: 0.004826
Validation Loss: 0.00660222
Epoch [10/200], Train Loss: 0.004818
Validation Loss: 0.00658799
Epoch [11/200], Train Loss: 0.004810
Validation Loss: 0.00657793
Epoch [12/200], Train Loss: 0.004789
Validation Loss: 0.00659508
Epoch [13/200], Train Loss: 0.004785
Validation Loss: 0.00660040
Epoch [14/200], Train Loss: 0.004772
Validation Loss: 0.00656646
Epoch [15/200], Train Loss: 0.004772
Validation Loss: 0.00656885
Epoch [16/200], Train Loss: 0.004760
Validation Loss: 0.00657693
Epoch [17/200], Train Loss: 0.004758
Validation Loss: 0.00655399
Epoch [18/200], Train Loss: 0.004763
Validation Loss: 0.00653816
Epoch [19/200], Train Loss: 0.004745
Validation Loss: 0.00653122
Epoch [20/200], Train Loss: 0.004758
Validation Loss: 0.00653308
Epoch [21/200], Train Loss: 0.004741
Validation Loss: 0.00654474
Epoch [22/200], Train Loss: 0.004743
Validation Loss: 0.00653433
Epoch [23/200], Train Loss: 0.004736
Validation Loss: 0.00653237
Epoch [24/200], Train Loss: 0.004727
Validation Loss: 0.00654252
Epoch [25/200], Train Loss: 0.004725
Validation Loss: 0.00654149
Epoch [26/200], Train Loss: 0.004717
Validation Loss: 0.00654132
Epoch [27/200], Train Loss: 0.004723
Validation Loss: 0.00653314
Epoch [28/200], Train Loss: 0.004716
Validation Loss: 0.00654097
Epoch [29/200], Train Loss: 0.004720
Validation Loss: 0.00652516
Epoch [30/200], Train Loss: 0.004713
Validation Loss: 0.00654438
Epoch [31/200], Train Loss: 0.004715
Validation Loss: 0.00653784
Epoch [32/200], Train Loss: 0.004710
Validation Loss: 0.00652875
Epoch [33/200], Train Loss: 0.004707
Validation Loss: 0.00653326
Epoch [34/200], Train Loss: 0.004701
Validation Loss: 0.00652379
Epoch [35/200], Train Loss: 0.004698
Validation Loss: 0.00652957
Epoch [36/200], Train Loss: 0.004700
Validation Loss: 0.00652481
Epoch [37/200], Train Loss: 0.004687
Validation Loss: 0.00652493
Epoch [38/200], Train Loss: 0.004686
Validation Loss: 0.00652783
Epoch [39/200], Train Loss: 0.004700
Validation Loss: 0.00651336
Epoch [40/200], Train Loss: 0.004696
Validation Loss: 0.00651806
Epoch [41/200], Train Loss: 0.004685
Validation Loss: 0.00651070
Epoch [42/200], Train Loss: 0.004697
Validation Loss: 0.00651745
Epoch [43/200], Train Loss: 0.004689
Validation Loss: 0.00655115
Epoch [44/200], Train Loss: 0.004694
Validation Loss: 0.00653395
Epoch [45/200], Train Loss: 0.004684
Validation Loss: 0.00651965
Epoch [46/200], Train Loss: 0.004684
Validation Loss: 0.00652250
Epoch [47/200], Train Loss: 0.004684
Validation Loss: 0.00650947
Epoch [48/200], Train Loss: 0.004689
Validation Loss: 0.00651987
Epoch [49/200], Train Loss: 0.004680
Validation Loss: 0.00651124
Epoch [50/200], Train Loss: 0.004689
Validation Loss: 0.00651983
Epoch [51/200], Train Loss: 0.004681
Validation Loss: 0.00652924
Epoch [52/200], Train Loss: 0.004686
Validation Loss: 0.00650969
Epoch [53/200], Train Loss: 0.004678
Validation Loss: 0.00650577
Epoch [54/200], Train Loss: 0.004680
Validation Loss: 0.00650067
Epoch [55/200], Train Loss: 0.004677
Validation Loss: 0.00651294
Epoch [56/200], Train Loss: 0.004681
Validation Loss: 0.00653331
Epoch [57/200], Train Loss: 0.004695
Validation Loss: 0.00651940
Epoch [58/200], Train Loss: 0.004674
Validation Loss: 0.00651102
Epoch [59/200], Train Loss: 0.004683
Validation Loss: 0.00650435
Epoch [60/200], Train Loss: 0.004674
Validation Loss: 0.00650429
Epoch [61/200], Train Loss: 0.004679
Validation Loss: 0.00653589
Epoch [62/200], Train Loss: 0.004668
Validation Loss: 0.00651747
Epoch [63/200], Train Loss: 0.004667
Validation Loss: 0.00650137
Epoch [64/200], Train Loss: 0.004661
Validation Loss: 0.00650827
Early stopping triggered

Evaluating model for: Lamp
Run 11/144 completed in 311.34 seconds with: {'MAE': np.float32(2.3574858), 'MSE': np.float32(123.54768), 'RMSE': np.float32(11.115201), 'SAE': np.float32(0.11621357), 'NDE': np.float32(0.98877)}

Run 12/144: hidden=64, seq_len=120, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.004965
Validation Loss: 0.00664089
Epoch [2/200], Train Loss: 0.004769
Validation Loss: 0.00661891
Epoch [3/200], Train Loss: 0.004759
Validation Loss: 0.00661882
Epoch [4/200], Train Loss: 0.004766
Validation Loss: 0.00661325
Epoch [5/200], Train Loss: 0.004747
Validation Loss: 0.00660276
Epoch [6/200], Train Loss: 0.004739
Validation Loss: 0.00655678
Epoch [7/200], Train Loss: 0.004696
Validation Loss: 0.00648563
Epoch [8/200], Train Loss: 0.004683
Validation Loss: 0.00649153
Epoch [9/200], Train Loss: 0.004673
Validation Loss: 0.00648992
Epoch [10/200], Train Loss: 0.004667
Validation Loss: 0.00649484
Epoch [11/200], Train Loss: 0.004678
Validation Loss: 0.00649541
Epoch [12/200], Train Loss: 0.004673
Validation Loss: 0.00648946
Epoch [13/200], Train Loss: 0.004676
Validation Loss: 0.00649033
Epoch [14/200], Train Loss: 0.004661
Validation Loss: 0.00648120
Epoch [15/200], Train Loss: 0.004656
Validation Loss: 0.00648789
Epoch [16/200], Train Loss: 0.004673
Validation Loss: 0.00649502
Epoch [17/200], Train Loss: 0.004655
Validation Loss: 0.00648018
Epoch [18/200], Train Loss: 0.004654
Validation Loss: 0.00649946
Epoch [19/200], Train Loss: 0.004662
Validation Loss: 0.00648291
Epoch [20/200], Train Loss: 0.004660
Validation Loss: 0.00649234
Epoch [21/200], Train Loss: 0.004651
Validation Loss: 0.00649144
Epoch [22/200], Train Loss: 0.004655
Validation Loss: 0.00648201
Epoch [23/200], Train Loss: 0.004642
Validation Loss: 0.00648304
Epoch [24/200], Train Loss: 0.004645
Validation Loss: 0.00648179
Epoch [25/200], Train Loss: 0.004643
Validation Loss: 0.00648632
Epoch [26/200], Train Loss: 0.004645
Validation Loss: 0.00648864
Epoch [27/200], Train Loss: 0.004667
Validation Loss: 0.00650081
Early stopping triggered

Evaluating model for: Lamp
Run 12/144 completed in 136.04 seconds with: {'MAE': np.float32(2.151705), 'MSE': np.float32(122.81639), 'RMSE': np.float32(11.082255), 'SAE': np.float32(0.08066256), 'NDE': np.float32(0.9858391)}

Run 13/144: hidden=64, seq_len=360, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.005439
Validation Loss: 0.00475003
Epoch [2/200], Train Loss: 0.004810
Validation Loss: 0.00473077
Epoch [3/200], Train Loss: 0.004799
Validation Loss: 0.00471388
Epoch [4/200], Train Loss: 0.004772
Validation Loss: 0.00469713
Epoch [5/200], Train Loss: 0.004754
Validation Loss: 0.00468818
Epoch [6/200], Train Loss: 0.004752
Validation Loss: 0.00468411
Epoch [7/200], Train Loss: 0.004740
Validation Loss: 0.00468366
Epoch [8/200], Train Loss: 0.004730
Validation Loss: 0.00468274
Epoch [9/200], Train Loss: 0.004733
Validation Loss: 0.00467974
Epoch [10/200], Train Loss: 0.004727
Validation Loss: 0.00467795
Epoch [11/200], Train Loss: 0.004721
Validation Loss: 0.00467633
Epoch [12/200], Train Loss: 0.004730
Validation Loss: 0.00467450
Epoch [13/200], Train Loss: 0.004713
Validation Loss: 0.00466981
Epoch [14/200], Train Loss: 0.004715
Validation Loss: 0.00466745
Epoch [15/200], Train Loss: 0.004707
Validation Loss: 0.00466152
Epoch [16/200], Train Loss: 0.004706
Validation Loss: 0.00465847
Epoch [17/200], Train Loss: 0.004699
Validation Loss: 0.00465435
Epoch [18/200], Train Loss: 0.004688
Validation Loss: 0.00465067
Epoch [19/200], Train Loss: 0.004689
Validation Loss: 0.00464548
Epoch [20/200], Train Loss: 0.004679
Validation Loss: 0.00464107
Epoch [21/200], Train Loss: 0.004674
Validation Loss: 0.00463479
Epoch [22/200], Train Loss: 0.004670
Validation Loss: 0.00462692
Epoch [23/200], Train Loss: 0.004659
Validation Loss: 0.00461632
Epoch [24/200], Train Loss: 0.004658
Validation Loss: 0.00460496
Epoch [25/200], Train Loss: 0.004650
Validation Loss: 0.00459671
Epoch [26/200], Train Loss: 0.004629
Validation Loss: 0.00457733
Epoch [27/200], Train Loss: 0.004626
Validation Loss: 0.00455876
Epoch [28/200], Train Loss: 0.004591
Validation Loss: 0.00453744
Epoch [29/200], Train Loss: 0.004576
Validation Loss: 0.00451235
Epoch [30/200], Train Loss: 0.004559
Validation Loss: 0.00450571
Epoch [31/200], Train Loss: 0.004512
Validation Loss: 0.00444955
Epoch [32/200], Train Loss: 0.004476
Validation Loss: 0.00439297
Epoch [33/200], Train Loss: 0.004436
Validation Loss: 0.00434966
Epoch [34/200], Train Loss: 0.004366
Validation Loss: 0.00427280
Epoch [35/200], Train Loss: 0.004316
Validation Loss: 0.00421243
Epoch [36/200], Train Loss: 0.004276
Validation Loss: 0.00415804
Epoch [37/200], Train Loss: 0.004209
Validation Loss: 0.00409146
Epoch [38/200], Train Loss: 0.004135
Validation Loss: 0.00402273
Epoch [39/200], Train Loss: 0.004081
Validation Loss: 0.00392299
Epoch [40/200], Train Loss: 0.003967
Validation Loss: 0.00385363
Epoch [41/200], Train Loss: 0.003876
Validation Loss: 0.00374356
Epoch [42/200], Train Loss: 0.003769
Validation Loss: 0.00364438
Epoch [43/200], Train Loss: 0.003636
Validation Loss: 0.00345005
Epoch [44/200], Train Loss: 0.003477
Validation Loss: 0.00332211
Epoch [45/200], Train Loss: 0.003316
Validation Loss: 0.00312882
Epoch [46/200], Train Loss: 0.003169
Validation Loss: 0.00300003
Epoch [47/200], Train Loss: 0.003050
Validation Loss: 0.00286733
Epoch [48/200], Train Loss: 0.002919
Validation Loss: 0.00272915
Epoch [49/200], Train Loss: 0.002800
Validation Loss: 0.00263572
Epoch [50/200], Train Loss: 0.002671
Validation Loss: 0.00254343
Epoch [51/200], Train Loss: 0.002571
Validation Loss: 0.00243878
Epoch [52/200], Train Loss: 0.002484
Validation Loss: 0.00236572
Epoch [53/200], Train Loss: 0.002404
Validation Loss: 0.00228774
Epoch [54/200], Train Loss: 0.002321
Validation Loss: 0.00223771
Epoch [55/200], Train Loss: 0.002269
Validation Loss: 0.00222419
Epoch [56/200], Train Loss: 0.002208
Validation Loss: 0.00211575
Epoch [57/200], Train Loss: 0.002155
Validation Loss: 0.00209992
Epoch [58/200], Train Loss: 0.002112
Validation Loss: 0.00202601
Epoch [59/200], Train Loss: 0.002067
Validation Loss: 0.00202947
Epoch [60/200], Train Loss: 0.002027
Validation Loss: 0.00195997
Epoch [61/200], Train Loss: 0.001994
Validation Loss: 0.00196556
Epoch [62/200], Train Loss: 0.001968
Validation Loss: 0.00190212
Epoch [63/200], Train Loss: 0.001948
Validation Loss: 0.00189702
Epoch [64/200], Train Loss: 0.001910
Validation Loss: 0.00187892
Epoch [65/200], Train Loss: 0.001880
Validation Loss: 0.00185751
Epoch [66/200], Train Loss: 0.001871
Validation Loss: 0.00182560
Epoch [67/200], Train Loss: 0.001835
Validation Loss: 0.00178836
Epoch [68/200], Train Loss: 0.001814
Validation Loss: 0.00178328
Epoch [69/200], Train Loss: 0.001794
Validation Loss: 0.00176469
Epoch [70/200], Train Loss: 0.001768
Validation Loss: 0.00174371
Epoch [71/200], Train Loss: 0.001750
Validation Loss: 0.00172939
Epoch [72/200], Train Loss: 0.001727
Validation Loss: 0.00171107
Epoch [73/200], Train Loss: 0.001720
Validation Loss: 0.00170335
Epoch [74/200], Train Loss: 0.001702
Validation Loss: 0.00168656
Epoch [75/200], Train Loss: 0.001680
Validation Loss: 0.00165332
Epoch [76/200], Train Loss: 0.001660
Validation Loss: 0.00165458
Epoch [77/200], Train Loss: 0.001646
Validation Loss: 0.00163687
Epoch [78/200], Train Loss: 0.001629
Validation Loss: 0.00161567
Epoch [79/200], Train Loss: 0.001614
Validation Loss: 0.00161883
Epoch [80/200], Train Loss: 0.001605
Validation Loss: 0.00160001
Epoch [81/200], Train Loss: 0.001590
Validation Loss: 0.00160953
Epoch [82/200], Train Loss: 0.001584
Validation Loss: 0.00157584
Epoch [83/200], Train Loss: 0.001566
Validation Loss: 0.00156872
Epoch [84/200], Train Loss: 0.001551
Validation Loss: 0.00154271
Epoch [85/200], Train Loss: 0.001538
Validation Loss: 0.00152990
Epoch [86/200], Train Loss: 0.001524
Validation Loss: 0.00152459
Epoch [87/200], Train Loss: 0.001515
Validation Loss: 0.00151071
Epoch [88/200], Train Loss: 0.001498
Validation Loss: 0.00150700
Epoch [89/200], Train Loss: 0.001485
Validation Loss: 0.00148129
Epoch [90/200], Train Loss: 0.001479
Validation Loss: 0.00149104
Epoch [91/200], Train Loss: 0.001463
Validation Loss: 0.00146232
Epoch [92/200], Train Loss: 0.001449
Validation Loss: 0.00145981
Epoch [93/200], Train Loss: 0.001444
Validation Loss: 0.00145177
Epoch [94/200], Train Loss: 0.001429
Validation Loss: 0.00144681
Epoch [95/200], Train Loss: 0.001426
Validation Loss: 0.00142275
Epoch [96/200], Train Loss: 0.001414
Validation Loss: 0.00143422
Epoch [97/200], Train Loss: 0.001411
Validation Loss: 0.00141195
Epoch [98/200], Train Loss: 0.001398
Validation Loss: 0.00139784
Epoch [99/200], Train Loss: 0.001385
Validation Loss: 0.00138950
Epoch [100/200], Train Loss: 0.001375
Validation Loss: 0.00136812
Epoch [101/200], Train Loss: 0.001362
Validation Loss: 0.00136988
Epoch [102/200], Train Loss: 0.001359
Validation Loss: 0.00136518
Epoch [103/200], Train Loss: 0.001354
Validation Loss: 0.00137826
Epoch [104/200], Train Loss: 0.001339
Validation Loss: 0.00133717
Epoch [105/200], Train Loss: 0.001329
Validation Loss: 0.00132611
Epoch [106/200], Train Loss: 0.001319
Validation Loss: 0.00131403
Epoch [107/200], Train Loss: 0.001313
Validation Loss: 0.00131146
Epoch [108/200], Train Loss: 0.001299
Validation Loss: 0.00130911
Epoch [109/200], Train Loss: 0.001295
Validation Loss: 0.00129460
Epoch [110/200], Train Loss: 0.001287
Validation Loss: 0.00129272
Epoch [111/200], Train Loss: 0.001290
Validation Loss: 0.00128103
Epoch [112/200], Train Loss: 0.001277
Validation Loss: 0.00127095
Epoch [113/200], Train Loss: 0.001263
Validation Loss: 0.00126400
Epoch [114/200], Train Loss: 0.001259
Validation Loss: 0.00125170
Epoch [115/200], Train Loss: 0.001248
Validation Loss: 0.00125096
Epoch [116/200], Train Loss: 0.001242
Validation Loss: 0.00125451
Epoch [117/200], Train Loss: 0.001236
Validation Loss: 0.00123364
Epoch [118/200], Train Loss: 0.001225
Validation Loss: 0.00122706
Epoch [119/200], Train Loss: 0.001219
Validation Loss: 0.00121662
Epoch [120/200], Train Loss: 0.001211
Validation Loss: 0.00121716
Epoch [121/200], Train Loss: 0.001207
Validation Loss: 0.00120624
Epoch [122/200], Train Loss: 0.001209
Validation Loss: 0.00121291
Epoch [123/200], Train Loss: 0.001193
Validation Loss: 0.00118703
Epoch [124/200], Train Loss: 0.001188
Validation Loss: 0.00118391
Epoch [125/200], Train Loss: 0.001183
Validation Loss: 0.00118000
Epoch [126/200], Train Loss: 0.001176
Validation Loss: 0.00118019
Epoch [127/200], Train Loss: 0.001166
Validation Loss: 0.00116046
Epoch [128/200], Train Loss: 0.001166
Validation Loss: 0.00116314
Epoch [129/200], Train Loss: 0.001162
Validation Loss: 0.00116272
Epoch [130/200], Train Loss: 0.001156
Validation Loss: 0.00115133
Epoch [131/200], Train Loss: 0.001148
Validation Loss: 0.00114712
Epoch [132/200], Train Loss: 0.001144
Validation Loss: 0.00113577
Epoch [133/200], Train Loss: 0.001131
Validation Loss: 0.00113835
Epoch [134/200], Train Loss: 0.001133
Validation Loss: 0.00112343
Epoch [135/200], Train Loss: 0.001119
Validation Loss: 0.00112064
Epoch [136/200], Train Loss: 0.001118
Validation Loss: 0.00110915
Epoch [137/200], Train Loss: 0.001117
Validation Loss: 0.00110551
Epoch [138/200], Train Loss: 0.001108
Validation Loss: 0.00110413
Epoch [139/200], Train Loss: 0.001105
Validation Loss: 0.00109609
Epoch [140/200], Train Loss: 0.001099
Validation Loss: 0.00109350
Epoch [141/200], Train Loss: 0.001097
Validation Loss: 0.00108676
Epoch [142/200], Train Loss: 0.001094
Validation Loss: 0.00108361
Epoch [143/200], Train Loss: 0.001092
Validation Loss: 0.00107903
Epoch [144/200], Train Loss: 0.001078
Validation Loss: 0.00107558
Epoch [145/200], Train Loss: 0.001078
Validation Loss: 0.00106993
Epoch [146/200], Train Loss: 0.001070
Validation Loss: 0.00106255
Epoch [147/200], Train Loss: 0.001058
Validation Loss: 0.00106176
Epoch [148/200], Train Loss: 0.001163
Validation Loss: 0.00111892
Epoch [149/200], Train Loss: 0.001082
Validation Loss: 0.00106063
Epoch [150/200], Train Loss: 0.001068
Validation Loss: 0.00107019
Epoch [151/200], Train Loss: 0.001055
Validation Loss: 0.00104914
Epoch [152/200], Train Loss: 0.001051
Validation Loss: 0.00104372
Epoch [153/200], Train Loss: 0.001046
Validation Loss: 0.00104107
Epoch [154/200], Train Loss: 0.001044
Validation Loss: 0.00103655
Epoch [155/200], Train Loss: 0.001036
Validation Loss: 0.00103549
Epoch [156/200], Train Loss: 0.001041
Validation Loss: 0.00103539
Epoch [157/200], Train Loss: 0.001031
Validation Loss: 0.00102673
Epoch [158/200], Train Loss: 0.001029
Validation Loss: 0.00102774
Epoch [159/200], Train Loss: 0.001027
Validation Loss: 0.00101704
Epoch [160/200], Train Loss: 0.001026
Validation Loss: 0.00101638
Epoch [161/200], Train Loss: 0.001015
Validation Loss: 0.00101226
Epoch [162/200], Train Loss: 0.001018
Validation Loss: 0.00101166
Epoch [163/200], Train Loss: 0.001019
Validation Loss: 0.00100883
Epoch [164/200], Train Loss: 0.001008
Validation Loss: 0.00100205
Epoch [165/200], Train Loss: 0.001005
Validation Loss: 0.00099688
Epoch [166/200], Train Loss: 0.001015
Validation Loss: 0.00102032
Epoch [167/200], Train Loss: 0.001010
Validation Loss: 0.00099728
Epoch [168/200], Train Loss: 0.000998
Validation Loss: 0.00099497
Epoch [169/200], Train Loss: 0.000998
Validation Loss: 0.00099495
Epoch [170/200], Train Loss: 0.000996
Validation Loss: 0.00098964
Epoch [171/200], Train Loss: 0.000989
Validation Loss: 0.00098743
Epoch [172/200], Train Loss: 0.000996
Validation Loss: 0.00098178
Epoch [173/200], Train Loss: 0.000993
Validation Loss: 0.00099123
Epoch [174/200], Train Loss: 0.000984
Validation Loss: 0.00097481
Epoch [175/200], Train Loss: 0.000977
Validation Loss: 0.00098032
Epoch [176/200], Train Loss: 0.000991
Validation Loss: 0.00097096
Epoch [177/200], Train Loss: 0.000975
Validation Loss: 0.00096772
Epoch [178/200], Train Loss: 0.000971
Validation Loss: 0.00097143
Epoch [179/200], Train Loss: 0.000970
Validation Loss: 0.00096308
Epoch [180/200], Train Loss: 0.000973
Validation Loss: 0.00095693
Epoch [181/200], Train Loss: 0.000974
Validation Loss: 0.00095413
Epoch [182/200], Train Loss: 0.000971
Validation Loss: 0.00095653
Epoch [183/200], Train Loss: 0.000962
Validation Loss: 0.00095238
Epoch [184/200], Train Loss: 0.000961
Validation Loss: 0.00096889
Epoch [185/200], Train Loss: 0.000964
Validation Loss: 0.00095088
Epoch [186/200], Train Loss: 0.000961
Validation Loss: 0.00094606
Epoch [187/200], Train Loss: 0.000956
Validation Loss: 0.00094696
Epoch [188/200], Train Loss: 0.000950
Validation Loss: 0.00094172
Epoch [189/200], Train Loss: 0.000944
Validation Loss: 0.00093736
Epoch [190/200], Train Loss: 0.000944
Validation Loss: 0.00093748
Epoch [191/200], Train Loss: 0.000950
Validation Loss: 0.00093469
Epoch [192/200], Train Loss: 0.000947
Validation Loss: 0.00093135
Epoch [193/200], Train Loss: 0.000944
Validation Loss: 0.00093427
Epoch [194/200], Train Loss: 0.000937
Validation Loss: 0.00092804
Epoch [195/200], Train Loss: 0.000930
Validation Loss: 0.00093024
Epoch [196/200], Train Loss: 0.000930
Validation Loss: 0.00092265
Epoch [197/200], Train Loss: 0.000927
Validation Loss: 0.00092307
Epoch [198/200], Train Loss: 0.000936
Validation Loss: 0.00094181
Epoch [199/200], Train Loss: 0.000937
Validation Loss: 0.00092733
Epoch [200/200], Train Loss: 0.000932
Validation Loss: 0.00091489

Evaluating model for: Lamp
Run 13/144 completed in 1594.20 seconds with: {'MAE': np.float32(0.8064391), 'MSE': np.float32(35.251263), 'RMSE': np.float32(5.9372773), 'SAE': np.float32(0.023042697), 'NDE': np.float32(0.43669397)}

Run 14/144: hidden=64, seq_len=360, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.005902
Validation Loss: 0.00479334
Epoch [2/200], Train Loss: 0.004851
Validation Loss: 0.00476014
Epoch [3/200], Train Loss: 0.004853
Validation Loss: 0.00475190
Epoch [4/200], Train Loss: 0.004825
Validation Loss: 0.00473584
Epoch [5/200], Train Loss: 0.004808
Validation Loss: 0.00470831
Epoch [6/200], Train Loss: 0.004768
Validation Loss: 0.00468559
Epoch [7/200], Train Loss: 0.004759
Validation Loss: 0.00468040
Epoch [8/200], Train Loss: 0.004754
Validation Loss: 0.00467805
Epoch [9/200], Train Loss: 0.004737
Validation Loss: 0.00467594
Epoch [10/200], Train Loss: 0.004742
Validation Loss: 0.00467428
Epoch [11/200], Train Loss: 0.004737
Validation Loss: 0.00468130
Epoch [12/200], Train Loss: 0.004734
Validation Loss: 0.00467422
Epoch [13/200], Train Loss: 0.004734
Validation Loss: 0.00467707
Epoch [14/200], Train Loss: 0.004717
Validation Loss: 0.00467814
Epoch [15/200], Train Loss: 0.004721
Validation Loss: 0.00467019
Epoch [16/200], Train Loss: 0.004722
Validation Loss: 0.00466787
Epoch [17/200], Train Loss: 0.004725
Validation Loss: 0.00466682
Epoch [18/200], Train Loss: 0.004727
Validation Loss: 0.00466673
Epoch [19/200], Train Loss: 0.004720
Validation Loss: 0.00467001
Epoch [20/200], Train Loss: 0.004726
Validation Loss: 0.00466545
Epoch [21/200], Train Loss: 0.004711
Validation Loss: 0.00466516
Epoch [22/200], Train Loss: 0.004707
Validation Loss: 0.00466732
Epoch [23/200], Train Loss: 0.004704
Validation Loss: 0.00466471
Epoch [24/200], Train Loss: 0.004708
Validation Loss: 0.00466901
Epoch [25/200], Train Loss: 0.004705
Validation Loss: 0.00466312
Epoch [26/200], Train Loss: 0.004703
Validation Loss: 0.00466230
Epoch [27/200], Train Loss: 0.004705
Validation Loss: 0.00466317
Epoch [28/200], Train Loss: 0.004705
Validation Loss: 0.00466358
Epoch [29/200], Train Loss: 0.004697
Validation Loss: 0.00466510
Epoch [30/200], Train Loss: 0.004699
Validation Loss: 0.00466748
Epoch [31/200], Train Loss: 0.004702
Validation Loss: 0.00466085
Epoch [32/200], Train Loss: 0.004702
Validation Loss: 0.00466114
Epoch [33/200], Train Loss: 0.004710
Validation Loss: 0.00467130
Epoch [34/200], Train Loss: 0.004700
Validation Loss: 0.00466291
Epoch [35/200], Train Loss: 0.004707
Validation Loss: 0.00466101
Epoch [36/200], Train Loss: 0.004700
Validation Loss: 0.00467703
Epoch [37/200], Train Loss: 0.004693
Validation Loss: 0.00466219
Epoch [38/200], Train Loss: 0.004706
Validation Loss: 0.00466033
Epoch [39/200], Train Loss: 0.004707
Validation Loss: 0.00465895
Epoch [40/200], Train Loss: 0.004693
Validation Loss: 0.00466197
Epoch [41/200], Train Loss: 0.004698
Validation Loss: 0.00466168
Epoch [42/200], Train Loss: 0.004709
Validation Loss: 0.00465807
Epoch [43/200], Train Loss: 0.004698
Validation Loss: 0.00465987
Epoch [44/200], Train Loss: 0.004699
Validation Loss: 0.00465998
Epoch [45/200], Train Loss: 0.004697
Validation Loss: 0.00465925
Epoch [46/200], Train Loss: 0.004698
Validation Loss: 0.00466080
Epoch [47/200], Train Loss: 0.004693
Validation Loss: 0.00466139
Epoch [48/200], Train Loss: 0.004697
Validation Loss: 0.00465647
Epoch [49/200], Train Loss: 0.004692
Validation Loss: 0.00465590
Epoch [50/200], Train Loss: 0.004695
Validation Loss: 0.00465521
Epoch [51/200], Train Loss: 0.004691
Validation Loss: 0.00465556
Epoch [52/200], Train Loss: 0.004691
Validation Loss: 0.00465395
Epoch [53/200], Train Loss: 0.004695
Validation Loss: 0.00465389
Epoch [54/200], Train Loss: 0.004700
Validation Loss: 0.00465280
Epoch [55/200], Train Loss: 0.004694
Validation Loss: 0.00465220
Epoch [56/200], Train Loss: 0.004683
Validation Loss: 0.00465124
Epoch [57/200], Train Loss: 0.004686
Validation Loss: 0.00465131
Epoch [58/200], Train Loss: 0.004685
Validation Loss: 0.00465195
Epoch [59/200], Train Loss: 0.004677
Validation Loss: 0.00464689
Epoch [60/200], Train Loss: 0.004687
Validation Loss: 0.00464470
Epoch [61/200], Train Loss: 0.004684
Validation Loss: 0.00464164
Epoch [62/200], Train Loss: 0.004673
Validation Loss: 0.00463845
Epoch [63/200], Train Loss: 0.004666
Validation Loss: 0.00463350
Epoch [64/200], Train Loss: 0.004664
Validation Loss: 0.00462806
Epoch [65/200], Train Loss: 0.004670
Validation Loss: 0.00462440
Epoch [66/200], Train Loss: 0.004669
Validation Loss: 0.00462225
Epoch [67/200], Train Loss: 0.004664
Validation Loss: 0.00461708
Epoch [68/200], Train Loss: 0.004657
Validation Loss: 0.00461272
Epoch [69/200], Train Loss: 0.004660
Validation Loss: 0.00460876
Epoch [70/200], Train Loss: 0.004647
Validation Loss: 0.00460705
Epoch [71/200], Train Loss: 0.004644
Validation Loss: 0.00460164
Epoch [72/200], Train Loss: 0.004637
Validation Loss: 0.00459995
Epoch [73/200], Train Loss: 0.004638
Validation Loss: 0.00459112
Epoch [74/200], Train Loss: 0.004628
Validation Loss: 0.00460534
Epoch [75/200], Train Loss: 0.004617
Validation Loss: 0.00457839
Epoch [76/200], Train Loss: 0.004610
Validation Loss: 0.00458126
Epoch [77/200], Train Loss: 0.004599
Validation Loss: 0.00456462
Epoch [78/200], Train Loss: 0.004595
Validation Loss: 0.00454843
Epoch [79/200], Train Loss: 0.004579
Validation Loss: 0.00453054
Epoch [80/200], Train Loss: 0.004556
Validation Loss: 0.00451044
Epoch [81/200], Train Loss: 0.004540
Validation Loss: 0.00450127
Epoch [82/200], Train Loss: 0.004503
Validation Loss: 0.00444552
Epoch [83/200], Train Loss: 0.004464
Validation Loss: 0.00438927
Epoch [84/200], Train Loss: 0.004408
Validation Loss: 0.00437764
Epoch [85/200], Train Loss: 0.004363
Validation Loss: 0.00431717
Epoch [86/200], Train Loss: 0.004313
Validation Loss: 0.00423547
Epoch [87/200], Train Loss: 0.004271
Validation Loss: 0.00419420
Epoch [88/200], Train Loss: 0.004189
Validation Loss: 0.00412050
Epoch [89/200], Train Loss: 0.004112
Validation Loss: 0.00402038
Epoch [90/200], Train Loss: 0.004026
Validation Loss: 0.00394453
Epoch [91/200], Train Loss: 0.003954
Validation Loss: 0.00382166
Epoch [92/200], Train Loss: 0.003849
Validation Loss: 0.00369242
Epoch [93/200], Train Loss: 0.003733
Validation Loss: 0.00357601
Epoch [94/200], Train Loss: 0.003621
Validation Loss: 0.00342507
Epoch [95/200], Train Loss: 0.003471
Validation Loss: 0.00329092
Epoch [96/200], Train Loss: 0.003335
Validation Loss: 0.00313232
Epoch [97/200], Train Loss: 0.003163
Validation Loss: 0.00300265
Epoch [98/200], Train Loss: 0.003002
Validation Loss: 0.00281040
Epoch [99/200], Train Loss: 0.002857
Validation Loss: 0.00272605
Epoch [100/200], Train Loss: 0.002734
Validation Loss: 0.00259937
Epoch [101/200], Train Loss: 0.002640
Validation Loss: 0.00253406
Epoch [102/200], Train Loss: 0.002547
Validation Loss: 0.00246931
Epoch [103/200], Train Loss: 0.002463
Validation Loss: 0.00234452
Epoch [104/200], Train Loss: 0.002386
Validation Loss: 0.00236647
Epoch [105/200], Train Loss: 0.002330
Validation Loss: 0.00226690
Epoch [106/200], Train Loss: 0.002286
Validation Loss: 0.00219684
Epoch [107/200], Train Loss: 0.002228
Validation Loss: 0.00216810
Epoch [108/200], Train Loss: 0.002176
Validation Loss: 0.00212108
Epoch [109/200], Train Loss: 0.002137
Validation Loss: 0.00207657
Epoch [110/200], Train Loss: 0.002092
Validation Loss: 0.00201390
Epoch [111/200], Train Loss: 0.002063
Validation Loss: 0.00202076
Epoch [112/200], Train Loss: 0.002032
Validation Loss: 0.00201016
Epoch [113/200], Train Loss: 0.001991
Validation Loss: 0.00196883
Epoch [114/200], Train Loss: 0.001955
Validation Loss: 0.00194441
Epoch [115/200], Train Loss: 0.001923
Validation Loss: 0.00190077
Epoch [116/200], Train Loss: 0.001892
Validation Loss: 0.00186217
Epoch [117/200], Train Loss: 0.001858
Validation Loss: 0.00182081
Epoch [118/200], Train Loss: 0.001844
Validation Loss: 0.00178579
Epoch [119/200], Train Loss: 0.001810
Validation Loss: 0.00178115
Epoch [120/200], Train Loss: 0.001784
Validation Loss: 0.00180160
Epoch [121/200], Train Loss: 0.001753
Validation Loss: 0.00173073
Epoch [122/200], Train Loss: 0.001726
Validation Loss: 0.00173945
Epoch [123/200], Train Loss: 0.001699
Validation Loss: 0.00167605
Epoch [124/200], Train Loss: 0.001674
Validation Loss: 0.00167601
Epoch [125/200], Train Loss: 0.001656
Validation Loss: 0.00164669
Epoch [126/200], Train Loss: 0.001637
Validation Loss: 0.00162281
Epoch [127/200], Train Loss: 0.001607
Validation Loss: 0.00160078
Epoch [128/200], Train Loss: 0.001583
Validation Loss: 0.00156805
Epoch [129/200], Train Loss: 0.001567
Validation Loss: 0.00155011
Epoch [130/200], Train Loss: 0.001546
Validation Loss: 0.00153332
Epoch [131/200], Train Loss: 0.001527
Validation Loss: 0.00153073
Epoch [132/200], Train Loss: 0.001511
Validation Loss: 0.00151347
Epoch [133/200], Train Loss: 0.001490
Validation Loss: 0.00147357
Epoch [134/200], Train Loss: 0.001474
Validation Loss: 0.00147309
Epoch [135/200], Train Loss: 0.001453
Validation Loss: 0.00145564
Epoch [136/200], Train Loss: 0.001436
Validation Loss: 0.00143665
Epoch [137/200], Train Loss: 0.001425
Validation Loss: 0.00142495
Epoch [138/200], Train Loss: 0.001409
Validation Loss: 0.00141147
Epoch [139/200], Train Loss: 0.001393
Validation Loss: 0.00139427
Epoch [140/200], Train Loss: 0.001382
Validation Loss: 0.00138114
Epoch [141/200], Train Loss: 0.001373
Validation Loss: 0.00137569
Epoch [142/200], Train Loss: 0.001356
Validation Loss: 0.00135773
Epoch [143/200], Train Loss: 0.001349
Validation Loss: 0.00135477
Epoch [144/200], Train Loss: 0.001327
Validation Loss: 0.00133988
Epoch [145/200], Train Loss: 0.001316
Validation Loss: 0.00133064
Epoch [146/200], Train Loss: 0.001315
Validation Loss: 0.00130447
Epoch [147/200], Train Loss: 0.001291
Validation Loss: 0.00131355
Epoch [148/200], Train Loss: 0.001290
Validation Loss: 0.00129759
Epoch [149/200], Train Loss: 0.001275
Validation Loss: 0.00127912
Epoch [150/200], Train Loss: 0.001267
Validation Loss: 0.00126444
Epoch [151/200], Train Loss: 0.001258
Validation Loss: 0.00127029
Epoch [152/200], Train Loss: 0.001251
Validation Loss: 0.00125484
Epoch [153/200], Train Loss: 0.001247
Validation Loss: 0.00124516
Epoch [154/200], Train Loss: 0.001225
Validation Loss: 0.00125510
Epoch [155/200], Train Loss: 0.001231
Validation Loss: 0.00123225
Epoch [156/200], Train Loss: 0.001213
Validation Loss: 0.00122726
Epoch [157/200], Train Loss: 0.001203
Validation Loss: 0.00120719
Epoch [158/200], Train Loss: 0.001195
Validation Loss: 0.00120260
Epoch [159/200], Train Loss: 0.001188
Validation Loss: 0.00119696
Epoch [160/200], Train Loss: 0.001183
Validation Loss: 0.00119232
Epoch [161/200], Train Loss: 0.001173
Validation Loss: 0.00117222
Epoch [162/200], Train Loss: 0.001162
Validation Loss: 0.00118473
Epoch [163/200], Train Loss: 0.001159
Validation Loss: 0.00116386
Epoch [164/200], Train Loss: 0.001155
Validation Loss: 0.00115447
Epoch [165/200], Train Loss: 0.001149
Validation Loss: 0.00115406
Epoch [166/200], Train Loss: 0.001137
Validation Loss: 0.00114855
Epoch [167/200], Train Loss: 0.001134
Validation Loss: 0.00113823
Epoch [168/200], Train Loss: 0.001128
Validation Loss: 0.00114082
Epoch [169/200], Train Loss: 0.001122
Validation Loss: 0.00113280
Epoch [170/200], Train Loss: 0.001116
Validation Loss: 0.00112023
Epoch [171/200], Train Loss: 0.001114
Validation Loss: 0.00111382
Epoch [172/200], Train Loss: 0.001103
Validation Loss: 0.00110481
Epoch [173/200], Train Loss: 0.001094
Validation Loss: 0.00110318
Epoch [174/200], Train Loss: 0.001091
Validation Loss: 0.00110020
Epoch [175/200], Train Loss: 0.001089
Validation Loss: 0.00110312
Epoch [176/200], Train Loss: 0.001082
Validation Loss: 0.00108948
Epoch [177/200], Train Loss: 0.001078
Validation Loss: 0.00108388
Epoch [178/200], Train Loss: 0.001073
Validation Loss: 0.00108109
Epoch [179/200], Train Loss: 0.001068
Validation Loss: 0.00107164
Epoch [180/200], Train Loss: 0.001068
Validation Loss: 0.00106672
Epoch [181/200], Train Loss: 0.001062
Validation Loss: 0.00106656
Epoch [182/200], Train Loss: 0.001059
Validation Loss: 0.00106589
Epoch [183/200], Train Loss: 0.001056
Validation Loss: 0.00105131
Epoch [184/200], Train Loss: 0.001042
Validation Loss: 0.00104662
Epoch [185/200], Train Loss: 0.001039
Validation Loss: 0.00105804
Epoch [186/200], Train Loss: 0.001045
Validation Loss: 0.00104749
Epoch [187/200], Train Loss: 0.001036
Validation Loss: 0.00103889
Epoch [188/200], Train Loss: 0.001034
Validation Loss: 0.00103485
Epoch [189/200], Train Loss: 0.001021
Validation Loss: 0.00102581
Epoch [190/200], Train Loss: 0.001022
Validation Loss: 0.00102628
Epoch [191/200], Train Loss: 0.001015
Validation Loss: 0.00101569
Epoch [192/200], Train Loss: 0.001019
Validation Loss: 0.00101585
Epoch [193/200], Train Loss: 0.001008
Validation Loss: 0.00101218
Epoch [194/200], Train Loss: 0.001004
Validation Loss: 0.00101002
Epoch [195/200], Train Loss: 0.001005
Validation Loss: 0.00100276
Epoch [196/200], Train Loss: 0.000997
Validation Loss: 0.00099651
Epoch [197/200], Train Loss: 0.001000
Validation Loss: 0.00099444
Epoch [198/200], Train Loss: 0.000991
Validation Loss: 0.00099422
Epoch [199/200], Train Loss: 0.000988
Validation Loss: 0.00099252
Epoch [200/200], Train Loss: 0.000984
Validation Loss: 0.00098435

Evaluating model for: Lamp
Run 14/144 completed in 1646.04 seconds with: {'MAE': np.float32(0.8185931), 'MSE': np.float32(37.192574), 'RMSE': np.float32(6.0985713), 'SAE': np.float32(0.0071011223), 'NDE': np.float32(0.4485577)}

Run 15/144: hidden=64, seq_len=360, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.008829
Validation Loss: 0.00544154
Epoch [2/200], Train Loss: 0.004961
Validation Loss: 0.00476736
Epoch [3/200], Train Loss: 0.004848
Validation Loss: 0.00475688
Epoch [4/200], Train Loss: 0.004832
Validation Loss: 0.00474399
Epoch [5/200], Train Loss: 0.004824
Validation Loss: 0.00472192
Epoch [6/200], Train Loss: 0.004802
Validation Loss: 0.00471393
Epoch [7/200], Train Loss: 0.004786
Validation Loss: 0.00470725
Epoch [8/200], Train Loss: 0.004777
Validation Loss: 0.00470573
Epoch [9/200], Train Loss: 0.004773
Validation Loss: 0.00470199
Epoch [10/200], Train Loss: 0.004767
Validation Loss: 0.00470030
Epoch [11/200], Train Loss: 0.004759
Validation Loss: 0.00469707
Epoch [12/200], Train Loss: 0.004761
Validation Loss: 0.00469472
Epoch [13/200], Train Loss: 0.004757
Validation Loss: 0.00469445
Epoch [14/200], Train Loss: 0.004754
Validation Loss: 0.00469130
Epoch [15/200], Train Loss: 0.004753
Validation Loss: 0.00469681
Epoch [16/200], Train Loss: 0.004744
Validation Loss: 0.00468796
Epoch [17/200], Train Loss: 0.004746
Validation Loss: 0.00468823
Epoch [18/200], Train Loss: 0.004741
Validation Loss: 0.00468565
Epoch [19/200], Train Loss: 0.004745
Validation Loss: 0.00468601
Epoch [20/200], Train Loss: 0.004737
Validation Loss: 0.00468972
Epoch [21/200], Train Loss: 0.004738
Validation Loss: 0.00468230
Epoch [22/200], Train Loss: 0.004736
Validation Loss: 0.00468170
Epoch [23/200], Train Loss: 0.004733
Validation Loss: 0.00467993
Epoch [24/200], Train Loss: 0.004734
Validation Loss: 0.00468439
Epoch [25/200], Train Loss: 0.004732
Validation Loss: 0.00467810
Epoch [26/200], Train Loss: 0.004739
Validation Loss: 0.00468079
Epoch [27/200], Train Loss: 0.004733
Validation Loss: 0.00467966
Epoch [28/200], Train Loss: 0.004722
Validation Loss: 0.00467644
Epoch [29/200], Train Loss: 0.004735
Validation Loss: 0.00467461
Epoch [30/200], Train Loss: 0.004727
Validation Loss: 0.00467394
Epoch [31/200], Train Loss: 0.004723
Validation Loss: 0.00467369
Epoch [32/200], Train Loss: 0.004722
Validation Loss: 0.00467323
Epoch [33/200], Train Loss: 0.004734
Validation Loss: 0.00467291
Epoch [34/200], Train Loss: 0.004728
Validation Loss: 0.00467220
Epoch [35/200], Train Loss: 0.004719
Validation Loss: 0.00467288
Epoch [36/200], Train Loss: 0.004718
Validation Loss: 0.00468255
Epoch [37/200], Train Loss: 0.004713
Validation Loss: 0.00467022
Epoch [38/200], Train Loss: 0.004723
Validation Loss: 0.00466983
Epoch [39/200], Train Loss: 0.004717
Validation Loss: 0.00466963
Epoch [40/200], Train Loss: 0.004713
Validation Loss: 0.00466933
Epoch [41/200], Train Loss: 0.004730
Validation Loss: 0.00466932
Epoch [42/200], Train Loss: 0.004722
Validation Loss: 0.00467995
Epoch [43/200], Train Loss: 0.004708
Validation Loss: 0.00466771
Epoch [44/200], Train Loss: 0.004718
Validation Loss: 0.00466999
Epoch [45/200], Train Loss: 0.004711
Validation Loss: 0.00466714
Epoch [46/200], Train Loss: 0.004717
Validation Loss: 0.00466904
Epoch [47/200], Train Loss: 0.004710
Validation Loss: 0.00466749
Epoch [48/200], Train Loss: 0.004706
Validation Loss: 0.00466728
Epoch [49/200], Train Loss: 0.004716
Validation Loss: 0.00466587
Epoch [50/200], Train Loss: 0.004706
Validation Loss: 0.00466571
Epoch [51/200], Train Loss: 0.004709
Validation Loss: 0.00466616
Epoch [52/200], Train Loss: 0.004708
Validation Loss: 0.00466496
Epoch [53/200], Train Loss: 0.004705
Validation Loss: 0.00466669
Epoch [54/200], Train Loss: 0.004705
Validation Loss: 0.00466776
Epoch [55/200], Train Loss: 0.004712
Validation Loss: 0.00466502
Epoch [56/200], Train Loss: 0.004702
Validation Loss: 0.00466428
Epoch [57/200], Train Loss: 0.004700
Validation Loss: 0.00466601
Epoch [58/200], Train Loss: 0.004695
Validation Loss: 0.00466328
Epoch [59/200], Train Loss: 0.004701
Validation Loss: 0.00466688
Epoch [60/200], Train Loss: 0.004704
Validation Loss: 0.00466415
Epoch [61/200], Train Loss: 0.004698
Validation Loss: 0.00466378
Epoch [62/200], Train Loss: 0.004697
Validation Loss: 0.00466346
Epoch [63/200], Train Loss: 0.004700
Validation Loss: 0.00466514
Epoch [64/200], Train Loss: 0.004699
Validation Loss: 0.00466255
Epoch [65/200], Train Loss: 0.004707
Validation Loss: 0.00466351
Epoch [66/200], Train Loss: 0.004699
Validation Loss: 0.00466165
Epoch [67/200], Train Loss: 0.004698
Validation Loss: 0.00466205
Epoch [68/200], Train Loss: 0.004697
Validation Loss: 0.00466164
Epoch [69/200], Train Loss: 0.004698
Validation Loss: 0.00466182
Epoch [70/200], Train Loss: 0.004702
Validation Loss: 0.00466182
Epoch [71/200], Train Loss: 0.004690
Validation Loss: 0.00466257
Epoch [72/200], Train Loss: 0.004695
Validation Loss: 0.00466055
Epoch [73/200], Train Loss: 0.004694
Validation Loss: 0.00465949
Epoch [74/200], Train Loss: 0.004698
Validation Loss: 0.00466317
Epoch [75/200], Train Loss: 0.004695
Validation Loss: 0.00465997
Epoch [76/200], Train Loss: 0.004692
Validation Loss: 0.00465945
Epoch [77/200], Train Loss: 0.004697
Validation Loss: 0.00465893
Epoch [78/200], Train Loss: 0.004692
Validation Loss: 0.00465796
Epoch [79/200], Train Loss: 0.004701
Validation Loss: 0.00465852
Epoch [80/200], Train Loss: 0.004695
Validation Loss: 0.00465710
Epoch [81/200], Train Loss: 0.004690
Validation Loss: 0.00465710
Epoch [82/200], Train Loss: 0.004682
Validation Loss: 0.00465640
Epoch [83/200], Train Loss: 0.004695
Validation Loss: 0.00465567
Epoch [84/200], Train Loss: 0.004689
Validation Loss: 0.00465579
Epoch [85/200], Train Loss: 0.004698
Validation Loss: 0.00465616
Epoch [86/200], Train Loss: 0.004696
Validation Loss: 0.00465399
Epoch [87/200], Train Loss: 0.004685
Validation Loss: 0.00465359
Epoch [88/200], Train Loss: 0.004690
Validation Loss: 0.00465367
Epoch [89/200], Train Loss: 0.004691
Validation Loss: 0.00465274
Epoch [90/200], Train Loss: 0.004686
Validation Loss: 0.00465129
Epoch [91/200], Train Loss: 0.004681
Validation Loss: 0.00465123
Epoch [92/200], Train Loss: 0.004681
Validation Loss: 0.00465032
Epoch [93/200], Train Loss: 0.004687
Validation Loss: 0.00465050
Epoch [94/200], Train Loss: 0.004679
Validation Loss: 0.00465564
Epoch [95/200], Train Loss: 0.004685
Validation Loss: 0.00464712
Epoch [96/200], Train Loss: 0.004685
Validation Loss: 0.00464692
Epoch [97/200], Train Loss: 0.004684
Validation Loss: 0.00464648
Epoch [98/200], Train Loss: 0.004684
Validation Loss: 0.00464987
Epoch [99/200], Train Loss: 0.004675
Validation Loss: 0.00464136
Epoch [100/200], Train Loss: 0.004670
Validation Loss: 0.00464028
Epoch [101/200], Train Loss: 0.004669
Validation Loss: 0.00463618
Epoch [102/200], Train Loss: 0.004672
Validation Loss: 0.00463518
Epoch [103/200], Train Loss: 0.004664
Validation Loss: 0.00464158
Epoch [104/200], Train Loss: 0.004662
Validation Loss: 0.00463088
Epoch [105/200], Train Loss: 0.004670
Validation Loss: 0.00463934
Epoch [106/200], Train Loss: 0.004666
Validation Loss: 0.00463215
Epoch [107/200], Train Loss: 0.004666
Validation Loss: 0.00462619
Epoch [108/200], Train Loss: 0.004665
Validation Loss: 0.00463221
Epoch [109/200], Train Loss: 0.004662
Validation Loss: 0.00462620
Epoch [110/200], Train Loss: 0.004661
Validation Loss: 0.00462731
Epoch [111/200], Train Loss: 0.004665
Validation Loss: 0.00463807
Epoch [112/200], Train Loss: 0.004673
Validation Loss: 0.00462505
Epoch [113/200], Train Loss: 0.004666
Validation Loss: 0.00461980
Epoch [114/200], Train Loss: 0.004662
Validation Loss: 0.00462856
Epoch [115/200], Train Loss: 0.004663
Validation Loss: 0.00462730
Epoch [116/200], Train Loss: 0.004661
Validation Loss: 0.00462574
Epoch [117/200], Train Loss: 0.004667
Validation Loss: 0.00461826
Epoch [118/200], Train Loss: 0.004661
Validation Loss: 0.00462266
Epoch [119/200], Train Loss: 0.004660
Validation Loss: 0.00461895
Epoch [120/200], Train Loss: 0.004659
Validation Loss: 0.00461716
Epoch [121/200], Train Loss: 0.004653
Validation Loss: 0.00461976
Epoch [122/200], Train Loss: 0.004663
Validation Loss: 0.00461845
Epoch [123/200], Train Loss: 0.004661
Validation Loss: 0.00461525
Epoch [124/200], Train Loss: 0.004652
Validation Loss: 0.00461757
Epoch [125/200], Train Loss: 0.004671
Validation Loss: 0.00461463
Epoch [126/200], Train Loss: 0.004660
Validation Loss: 0.00461648
Epoch [127/200], Train Loss: 0.004657
Validation Loss: 0.00461707
Epoch [128/200], Train Loss: 0.004655
Validation Loss: 0.00461253
Epoch [129/200], Train Loss: 0.004643
Validation Loss: 0.00461366
Epoch [130/200], Train Loss: 0.004654
Validation Loss: 0.00460876
Epoch [131/200], Train Loss: 0.004645
Validation Loss: 0.00461452
Epoch [132/200], Train Loss: 0.004661
Validation Loss: 0.00461229
Epoch [133/200], Train Loss: 0.004649
Validation Loss: 0.00460937
Epoch [134/200], Train Loss: 0.004650
Validation Loss: 0.00461774
Epoch [135/200], Train Loss: 0.004653
Validation Loss: 0.00461081
Epoch [136/200], Train Loss: 0.004640
Validation Loss: 0.00460930
Epoch [137/200], Train Loss: 0.004657
Validation Loss: 0.00460994
Epoch [138/200], Train Loss: 0.004649
Validation Loss: 0.00460842
Epoch [139/200], Train Loss: 0.004644
Validation Loss: 0.00460907
Epoch [140/200], Train Loss: 0.004642
Validation Loss: 0.00461475
Epoch [141/200], Train Loss: 0.004643
Validation Loss: 0.00461351
Epoch [142/200], Train Loss: 0.004643
Validation Loss: 0.00461104
Epoch [143/200], Train Loss: 0.004641
Validation Loss: 0.00460633
Epoch [144/200], Train Loss: 0.004641
Validation Loss: 0.00460914
Epoch [145/200], Train Loss: 0.004648
Validation Loss: 0.00460964
Epoch [146/200], Train Loss: 0.004646
Validation Loss: 0.00461339
Epoch [147/200], Train Loss: 0.004636
Validation Loss: 0.00460801
Epoch [148/200], Train Loss: 0.004640
Validation Loss: 0.00460723
Epoch [149/200], Train Loss: 0.004652
Validation Loss: 0.00460722
Epoch [150/200], Train Loss: 0.004638
Validation Loss: 0.00460530
Epoch [151/200], Train Loss: 0.004635
Validation Loss: 0.00460256
Epoch [152/200], Train Loss: 0.004645
Validation Loss: 0.00460410
Epoch [153/200], Train Loss: 0.004634
Validation Loss: 0.00460601
Epoch [154/200], Train Loss: 0.004641
Validation Loss: 0.00460638
Epoch [155/200], Train Loss: 0.004646
Validation Loss: 0.00460228
Epoch [156/200], Train Loss: 0.004640
Validation Loss: 0.00460075
Epoch [157/200], Train Loss: 0.004636
Validation Loss: 0.00459873
Epoch [158/200], Train Loss: 0.004634
Validation Loss: 0.00459747
Epoch [159/200], Train Loss: 0.004632
Validation Loss: 0.00460035
Epoch [160/200], Train Loss: 0.004635
Validation Loss: 0.00462506
Epoch [161/200], Train Loss: 0.004638
Validation Loss: 0.00459870
Epoch [162/200], Train Loss: 0.004640
Validation Loss: 0.00460412
Epoch [163/200], Train Loss: 0.004632
Validation Loss: 0.00459705
Epoch [164/200], Train Loss: 0.004638
Validation Loss: 0.00460263
Epoch [165/200], Train Loss: 0.004636
Validation Loss: 0.00459925
Epoch [166/200], Train Loss: 0.004634
Validation Loss: 0.00459551
Epoch [167/200], Train Loss: 0.004630
Validation Loss: 0.00459588
Epoch [168/200], Train Loss: 0.004642
Validation Loss: 0.00462348
Epoch [169/200], Train Loss: 0.004635
Validation Loss: 0.00462658
Epoch [170/200], Train Loss: 0.004632
Validation Loss: 0.00459763
Epoch [171/200], Train Loss: 0.004644
Validation Loss: 0.00459690
Epoch [172/200], Train Loss: 0.004626
Validation Loss: 0.00459967
Epoch [173/200], Train Loss: 0.004636
Validation Loss: 0.00459733
Epoch [174/200], Train Loss: 0.004626
Validation Loss: 0.00459459
Epoch [175/200], Train Loss: 0.004642
Validation Loss: 0.00459683
Epoch [176/200], Train Loss: 0.004630
Validation Loss: 0.00460529
Epoch [177/200], Train Loss: 0.004648
Validation Loss: 0.00459872
Epoch [178/200], Train Loss: 0.004645
Validation Loss: 0.00459766
Epoch [179/200], Train Loss: 0.004623
Validation Loss: 0.00459516
Epoch [180/200], Train Loss: 0.004627
Validation Loss: 0.00459320
Epoch [181/200], Train Loss: 0.004626
Validation Loss: 0.00459506
Epoch [182/200], Train Loss: 0.004645
Validation Loss: 0.00459305
Epoch [183/200], Train Loss: 0.004629
Validation Loss: 0.00459206
Epoch [184/200], Train Loss: 0.004637
Validation Loss: 0.00459670
Epoch [185/200], Train Loss: 0.004630
Validation Loss: 0.00459122
Epoch [186/200], Train Loss: 0.004626
Validation Loss: 0.00459443
Epoch [187/200], Train Loss: 0.004631
Validation Loss: 0.00461678
Epoch [188/200], Train Loss: 0.004627
Validation Loss: 0.00459797
Epoch [189/200], Train Loss: 0.004618
Validation Loss: 0.00459562
Epoch [190/200], Train Loss: 0.004632
Validation Loss: 0.00460409
Epoch [191/200], Train Loss: 0.004624
Validation Loss: 0.00459133
Epoch [192/200], Train Loss: 0.004633
Validation Loss: 0.00458883
Epoch [193/200], Train Loss: 0.004630
Validation Loss: 0.00458801
Epoch [194/200], Train Loss: 0.004628
Validation Loss: 0.00459316
Epoch [195/200], Train Loss: 0.004629
Validation Loss: 0.00458902
Epoch [196/200], Train Loss: 0.004627
Validation Loss: 0.00458915
Epoch [197/200], Train Loss: 0.004625
Validation Loss: 0.00458458
Epoch [198/200], Train Loss: 0.004629
Validation Loss: 0.00459019
Epoch [199/200], Train Loss: 0.004624
Validation Loss: 0.00459226
Epoch [200/200], Train Loss: 0.004628
Validation Loss: 0.00458855

Evaluating model for: Lamp
Run 15/144 completed in 1644.26 seconds with: {'MAE': np.float32(2.9554055), 'MSE': np.float32(175.33447), 'RMSE': np.float32(13.241392), 'SAE': np.float32(0.053979754), 'NDE': np.float32(0.9739228)}

Run 16/144: hidden=64, seq_len=360, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.004944
Validation Loss: 0.00475527
Epoch [2/200], Train Loss: 0.004808
Validation Loss: 0.00475282
Epoch [3/200], Train Loss: 0.004809
Validation Loss: 0.00474050
Epoch [4/200], Train Loss: 0.004771
Validation Loss: 0.00468481
Epoch [5/200], Train Loss: 0.004726
Validation Loss: 0.00467522
Epoch [6/200], Train Loss: 0.004713
Validation Loss: 0.00467216
Epoch [7/200], Train Loss: 0.004709
Validation Loss: 0.00467053
Epoch [8/200], Train Loss: 0.004702
Validation Loss: 0.00467493
Epoch [9/200], Train Loss: 0.004700
Validation Loss: 0.00466822
Epoch [10/200], Train Loss: 0.004705
Validation Loss: 0.00467554
Epoch [11/200], Train Loss: 0.004702
Validation Loss: 0.00466460
Epoch [12/200], Train Loss: 0.004695
Validation Loss: 0.00466488
Epoch [13/200], Train Loss: 0.004705
Validation Loss: 0.00466775
Epoch [14/200], Train Loss: 0.004700
Validation Loss: 0.00466630
Epoch [15/200], Train Loss: 0.004696
Validation Loss: 0.00466361
Epoch [16/200], Train Loss: 0.004693
Validation Loss: 0.00466517
Epoch [17/200], Train Loss: 0.004698
Validation Loss: 0.00466200
Epoch [18/200], Train Loss: 0.004688
Validation Loss: 0.00466213
Epoch [19/200], Train Loss: 0.004686
Validation Loss: 0.00466624
Epoch [20/200], Train Loss: 0.004698
Validation Loss: 0.00466088
Epoch [21/200], Train Loss: 0.004683
Validation Loss: 0.00466155
Epoch [22/200], Train Loss: 0.004687
Validation Loss: 0.00466052
Epoch [23/200], Train Loss: 0.004682
Validation Loss: 0.00465971
Epoch [24/200], Train Loss: 0.004687
Validation Loss: 0.00466029
Epoch [25/200], Train Loss: 0.004693
Validation Loss: 0.00466017
Epoch [26/200], Train Loss: 0.004693
Validation Loss: 0.00466075
Epoch [27/200], Train Loss: 0.004688
Validation Loss: 0.00465938
Epoch [28/200], Train Loss: 0.004682
Validation Loss: 0.00465895
Epoch [29/200], Train Loss: 0.004682
Validation Loss: 0.00465901
Epoch [30/200], Train Loss: 0.004685
Validation Loss: 0.00465988
Epoch [31/200], Train Loss: 0.004676
Validation Loss: 0.00466127
Epoch [32/200], Train Loss: 0.004685
Validation Loss: 0.00466101
Epoch [33/200], Train Loss: 0.004686
Validation Loss: 0.00466435
Epoch [34/200], Train Loss: 0.004691
Validation Loss: 0.00465933
Epoch [35/200], Train Loss: 0.004682
Validation Loss: 0.00465988
Epoch [36/200], Train Loss: 0.004683
Validation Loss: 0.00466083
Epoch [37/200], Train Loss: 0.004686
Validation Loss: 0.00466003
Epoch [38/200], Train Loss: 0.004678
Validation Loss: 0.00466053
Early stopping triggered

Evaluating model for: Lamp
Run 16/144 completed in 323.24 seconds with: {'MAE': np.float32(2.9452715), 'MSE': np.float32(177.54878), 'RMSE': np.float32(13.324743), 'SAE': np.float32(0.0033247627), 'NDE': np.float32(0.9800527)}

Run 17/144: hidden=64, seq_len=360, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.006235
Validation Loss: 0.00437145
Epoch [2/200], Train Loss: 0.005311
Validation Loss: 0.00386719
Epoch [3/200], Train Loss: 0.005030
Validation Loss: 0.00383189
Epoch [4/200], Train Loss: 0.004991
Validation Loss: 0.00382306
Epoch [5/200], Train Loss: 0.004973
Validation Loss: 0.00381624
Epoch [6/200], Train Loss: 0.004972
Validation Loss: 0.00381173
Epoch [7/200], Train Loss: 0.004965
Validation Loss: 0.00380568
Epoch [8/200], Train Loss: 0.004928
Validation Loss: 0.00379943
Epoch [9/200], Train Loss: 0.004923
Validation Loss: 0.00379239
Epoch [10/200], Train Loss: 0.004923
Validation Loss: 0.00378861
Epoch [11/200], Train Loss: 0.004938
Validation Loss: 0.00378372
Epoch [12/200], Train Loss: 0.004918
Validation Loss: 0.00378249
Epoch [13/200], Train Loss: 0.004947
Validation Loss: 0.00378182
Epoch [14/200], Train Loss: 0.004912
Validation Loss: 0.00378628
Epoch [15/200], Train Loss: 0.004895
Validation Loss: 0.00378027
Epoch [16/200], Train Loss: 0.004906
Validation Loss: 0.00378219
Epoch [17/200], Train Loss: 0.004869
Validation Loss: 0.00378005
Epoch [18/200], Train Loss: 0.004905
Validation Loss: 0.00378234
Epoch [19/200], Train Loss: 0.004946
Validation Loss: 0.00378019
Epoch [20/200], Train Loss: 0.004882
Validation Loss: 0.00378055
Epoch [21/200], Train Loss: 0.004887
Validation Loss: 0.00377821
Epoch [22/200], Train Loss: 0.004897
Validation Loss: 0.00377685
Epoch [23/200], Train Loss: 0.004914
Validation Loss: 0.00377710
Epoch [24/200], Train Loss: 0.004956
Validation Loss: 0.00377869
Epoch [25/200], Train Loss: 0.004960
Validation Loss: 0.00377504
Epoch [26/200], Train Loss: 0.004927
Validation Loss: 0.00377649
Epoch [27/200], Train Loss: 0.004886
Validation Loss: 0.00377486
Epoch [28/200], Train Loss: 0.004899
Validation Loss: 0.00377780
Epoch [29/200], Train Loss: 0.004898
Validation Loss: 0.00377218
Epoch [30/200], Train Loss: 0.004885
Validation Loss: 0.00377153
Epoch [31/200], Train Loss: 0.004867
Validation Loss: 0.00377035
Epoch [32/200], Train Loss: 0.004864
Validation Loss: 0.00377233
Epoch [33/200], Train Loss: 0.004883
Validation Loss: 0.00376767
Epoch [34/200], Train Loss: 0.004870
Validation Loss: 0.00376696
Epoch [35/200], Train Loss: 0.004861
Validation Loss: 0.00376551
Epoch [36/200], Train Loss: 0.004865
Validation Loss: 0.00376543
Epoch [37/200], Train Loss: 0.004881
Validation Loss: 0.00376525
Epoch [38/200], Train Loss: 0.004856
Validation Loss: 0.00376122
Epoch [39/200], Train Loss: 0.004894
Validation Loss: 0.00375907
Epoch [40/200], Train Loss: 0.004876
Validation Loss: 0.00376298
Epoch [41/200], Train Loss: 0.004879
Validation Loss: 0.00376411
Epoch [42/200], Train Loss: 0.004852
Validation Loss: 0.00375910
Epoch [43/200], Train Loss: 0.004856
Validation Loss: 0.00376059
Epoch [44/200], Train Loss: 0.004880
Validation Loss: 0.00375225
Epoch [45/200], Train Loss: 0.004863
Validation Loss: 0.00376390
Epoch [46/200], Train Loss: 0.004849
Validation Loss: 0.00375355
Epoch [47/200], Train Loss: 0.004853
Validation Loss: 0.00375272
Epoch [48/200], Train Loss: 0.004886
Validation Loss: 0.00375084
Epoch [49/200], Train Loss: 0.004877
Validation Loss: 0.00374517
Epoch [50/200], Train Loss: 0.004842
Validation Loss: 0.00374366
Epoch [51/200], Train Loss: 0.004852
Validation Loss: 0.00374239
Epoch [52/200], Train Loss: 0.004879
Validation Loss: 0.00374629
Epoch [53/200], Train Loss: 0.004834
Validation Loss: 0.00374094
Epoch [54/200], Train Loss: 0.004873
Validation Loss: 0.00373827
Epoch [55/200], Train Loss: 0.004845
Validation Loss: 0.00373692
Epoch [56/200], Train Loss: 0.004839
Validation Loss: 0.00373516
Epoch [57/200], Train Loss: 0.004864
Validation Loss: 0.00373536
Epoch [58/200], Train Loss: 0.004830
Validation Loss: 0.00373154
Epoch [59/200], Train Loss: 0.004828
Validation Loss: 0.00373959
Epoch [60/200], Train Loss: 0.004869
Validation Loss: 0.00372914
Epoch [61/200], Train Loss: 0.004873
Validation Loss: 0.00372966
Epoch [62/200], Train Loss: 0.004809
Validation Loss: 0.00372700
Epoch [63/200], Train Loss: 0.004837
Validation Loss: 0.00372103
Epoch [64/200], Train Loss: 0.004838
Validation Loss: 0.00371842
Epoch [65/200], Train Loss: 0.004811
Validation Loss: 0.00372249
Epoch [66/200], Train Loss: 0.004828
Validation Loss: 0.00371678
Epoch [67/200], Train Loss: 0.004835
Validation Loss: 0.00371256
Epoch [68/200], Train Loss: 0.004814
Validation Loss: 0.00371344
Epoch [69/200], Train Loss: 0.004841
Validation Loss: 0.00374025
Epoch [70/200], Train Loss: 0.004788
Validation Loss: 0.00370448
Epoch [71/200], Train Loss: 0.004804
Validation Loss: 0.00370874
Epoch [72/200], Train Loss: 0.004785
Validation Loss: 0.00369719
Epoch [73/200], Train Loss: 0.004801
Validation Loss: 0.00369744
Epoch [74/200], Train Loss: 0.004808
Validation Loss: 0.00368974
Epoch [75/200], Train Loss: 0.004788
Validation Loss: 0.00368833
Epoch [76/200], Train Loss: 0.004825
Validation Loss: 0.00369743
Epoch [77/200], Train Loss: 0.004784
Validation Loss: 0.00367851
Epoch [78/200], Train Loss: 0.004842
Validation Loss: 0.00370196
Epoch [79/200], Train Loss: 0.004795
Validation Loss: 0.00368308
Epoch [80/200], Train Loss: 0.004734
Validation Loss: 0.00366420
Epoch [81/200], Train Loss: 0.004767
Validation Loss: 0.00365856
Epoch [82/200], Train Loss: 0.004728
Validation Loss: 0.00366381
Epoch [83/200], Train Loss: 0.004735
Validation Loss: 0.00366079
Epoch [84/200], Train Loss: 0.004727
Validation Loss: 0.00364355
Epoch [85/200], Train Loss: 0.004706
Validation Loss: 0.00363595
Epoch [86/200], Train Loss: 0.004732
Validation Loss: 0.00363053
Epoch [87/200], Train Loss: 0.004693
Validation Loss: 0.00363104
Epoch [88/200], Train Loss: 0.004737
Validation Loss: 0.00362940
Epoch [89/200], Train Loss: 0.004687
Validation Loss: 0.00360995
Epoch [90/200], Train Loss: 0.004667
Validation Loss: 0.00361550
Epoch [91/200], Train Loss: 0.004677
Validation Loss: 0.00359590
Epoch [92/200], Train Loss: 0.004628
Validation Loss: 0.00359959
Epoch [93/200], Train Loss: 0.004637
Validation Loss: 0.00358007
Epoch [94/200], Train Loss: 0.004613
Validation Loss: 0.00357923
Epoch [95/200], Train Loss: 0.004654
Validation Loss: 0.00363743
Epoch [96/200], Train Loss: 0.004618
Validation Loss: 0.00356148
Epoch [97/200], Train Loss: 0.004599
Validation Loss: 0.00354478
Epoch [98/200], Train Loss: 0.004578
Validation Loss: 0.00354669
Epoch [99/200], Train Loss: 0.004585
Validation Loss: 0.00354615
Epoch [100/200], Train Loss: 0.004565
Validation Loss: 0.00352797
Epoch [101/200], Train Loss: 0.004546
Validation Loss: 0.00350427
Epoch [102/200], Train Loss: 0.004537
Validation Loss: 0.00351284
Epoch [103/200], Train Loss: 0.004514
Validation Loss: 0.00349285
Epoch [104/200], Train Loss: 0.004478
Validation Loss: 0.00347850
Epoch [105/200], Train Loss: 0.004456
Validation Loss: 0.00347027
Epoch [106/200], Train Loss: 0.004465
Validation Loss: 0.00346132
Epoch [107/200], Train Loss: 0.004536
Validation Loss: 0.00345304
Epoch [108/200], Train Loss: 0.004487
Validation Loss: 0.00344684
Epoch [109/200], Train Loss: 0.004392
Validation Loss: 0.00347499
Epoch [110/200], Train Loss: 0.004419
Validation Loss: 0.00342740
Epoch [111/200], Train Loss: 0.004380
Validation Loss: 0.00341744
Epoch [112/200], Train Loss: 0.004420
Validation Loss: 0.00340803
Epoch [113/200], Train Loss: 0.004368
Validation Loss: 0.00339965
Epoch [114/200], Train Loss: 0.004375
Validation Loss: 0.00338734
Epoch [115/200], Train Loss: 0.004352
Validation Loss: 0.00338474
Epoch [116/200], Train Loss: 0.004370
Validation Loss: 0.00336607
Epoch [117/200], Train Loss: 0.004324
Validation Loss: 0.00335127
Epoch [118/200], Train Loss: 0.004272
Validation Loss: 0.00335560
Epoch [119/200], Train Loss: 0.004349
Validation Loss: 0.00336735
Epoch [120/200], Train Loss: 0.004334
Validation Loss: 0.00332099
Epoch [121/200], Train Loss: 0.004237
Validation Loss: 0.00330103
Epoch [122/200], Train Loss: 0.004215
Validation Loss: 0.00328474
Epoch [123/200], Train Loss: 0.004230
Validation Loss: 0.00327806
Epoch [124/200], Train Loss: 0.004242
Validation Loss: 0.00327242
Epoch [125/200], Train Loss: 0.004208
Validation Loss: 0.00325384
Epoch [126/200], Train Loss: 0.004195
Validation Loss: 0.00324277
Epoch [127/200], Train Loss: 0.004212
Validation Loss: 0.00321954
Epoch [128/200], Train Loss: 0.004130
Validation Loss: 0.00321696
Epoch [129/200], Train Loss: 0.004161
Validation Loss: 0.00323522
Epoch [130/200], Train Loss: 0.004105
Validation Loss: 0.00317240
Epoch [131/200], Train Loss: 0.004102
Validation Loss: 0.00315704
Epoch [132/200], Train Loss: 0.004091
Validation Loss: 0.00314563
Epoch [133/200], Train Loss: 0.004055
Validation Loss: 0.00312135
Epoch [134/200], Train Loss: 0.004009
Validation Loss: 0.00311726
Epoch [135/200], Train Loss: 0.004025
Validation Loss: 0.00308903
Epoch [136/200], Train Loss: 0.004005
Validation Loss: 0.00306874
Epoch [137/200], Train Loss: 0.003944
Validation Loss: 0.00305028
Epoch [138/200], Train Loss: 0.003973
Validation Loss: 0.00304328
Epoch [139/200], Train Loss: 0.003943
Validation Loss: 0.00304778
Epoch [140/200], Train Loss: 0.003915
Validation Loss: 0.00299597
Epoch [141/200], Train Loss: 0.003882
Validation Loss: 0.00297492
Epoch [142/200], Train Loss: 0.003864
Validation Loss: 0.00295502
Epoch [143/200], Train Loss: 0.003847
Validation Loss: 0.00293191
Epoch [144/200], Train Loss: 0.003817
Validation Loss: 0.00293531
Epoch [145/200], Train Loss: 0.003803
Validation Loss: 0.00291023
Epoch [146/200], Train Loss: 0.003755
Validation Loss: 0.00286967
Epoch [147/200], Train Loss: 0.003743
Validation Loss: 0.00288591
Epoch [148/200], Train Loss: 0.003731
Validation Loss: 0.00282581
Epoch [149/200], Train Loss: 0.003683
Validation Loss: 0.00283479
Epoch [150/200], Train Loss: 0.003666
Validation Loss: 0.00278504
Epoch [151/200], Train Loss: 0.003630
Validation Loss: 0.00278446
Epoch [152/200], Train Loss: 0.003636
Validation Loss: 0.00274225
Epoch [153/200], Train Loss: 0.003580
Validation Loss: 0.00273313
Epoch [154/200], Train Loss: 0.003553
Validation Loss: 0.00271405
Epoch [155/200], Train Loss: 0.003556
Validation Loss: 0.00269139
Epoch [156/200], Train Loss: 0.003502
Validation Loss: 0.00266717
Epoch [157/200], Train Loss: 0.003509
Validation Loss: 0.00265554
Epoch [158/200], Train Loss: 0.003508
Validation Loss: 0.00266534
Epoch [159/200], Train Loss: 0.003413
Validation Loss: 0.00260569
Epoch [160/200], Train Loss: 0.003435
Validation Loss: 0.00259456
Epoch [161/200], Train Loss: 0.003408
Validation Loss: 0.00256996
Epoch [162/200], Train Loss: 0.003375
Validation Loss: 0.00255566
Epoch [163/200], Train Loss: 0.003385
Validation Loss: 0.00253933
Epoch [164/200], Train Loss: 0.003368
Validation Loss: 0.00251217
Epoch [165/200], Train Loss: 0.003286
Validation Loss: 0.00250774
Epoch [166/200], Train Loss: 0.003272
Validation Loss: 0.00250830
Epoch [167/200], Train Loss: 0.003304
Validation Loss: 0.00245717
Epoch [168/200], Train Loss: 0.003235
Validation Loss: 0.00245246
Epoch [169/200], Train Loss: 0.003219
Validation Loss: 0.00245625
Epoch [170/200], Train Loss: 0.003195
Validation Loss: 0.00241150
Epoch [171/200], Train Loss: 0.003178
Validation Loss: 0.00242750
Epoch [172/200], Train Loss: 0.003168
Validation Loss: 0.00238896
Epoch [173/200], Train Loss: 0.003136
Validation Loss: 0.00237418
Epoch [174/200], Train Loss: 0.003127
Validation Loss: 0.00236792
Epoch [175/200], Train Loss: 0.003087
Validation Loss: 0.00234411
Epoch [176/200], Train Loss: 0.003070
Validation Loss: 0.00233828
Epoch [177/200], Train Loss: 0.003039
Validation Loss: 0.00233347
Epoch [178/200], Train Loss: 0.003035
Validation Loss: 0.00230239
Epoch [179/200], Train Loss: 0.002999
Validation Loss: 0.00229627
Epoch [180/200], Train Loss: 0.002994
Validation Loss: 0.00229779
Epoch [181/200], Train Loss: 0.002995
Validation Loss: 0.00228575
Epoch [182/200], Train Loss: 0.002967
Validation Loss: 0.00228669
Epoch [183/200], Train Loss: 0.002988
Validation Loss: 0.00226422
Epoch [184/200], Train Loss: 0.002946
Validation Loss: 0.00226938
Epoch [185/200], Train Loss: 0.002899
Validation Loss: 0.00222244
Epoch [186/200], Train Loss: 0.002894
Validation Loss: 0.00221415
Epoch [187/200], Train Loss: 0.002870
Validation Loss: 0.00222943
Epoch [188/200], Train Loss: 0.002864
Validation Loss: 0.00221018
Epoch [189/200], Train Loss: 0.002883
Validation Loss: 0.00221870
Epoch [190/200], Train Loss: 0.002852
Validation Loss: 0.00217206
Epoch [191/200], Train Loss: 0.002834
Validation Loss: 0.00216057
Epoch [192/200], Train Loss: 0.002804
Validation Loss: 0.00215087
Epoch [193/200], Train Loss: 0.002785
Validation Loss: 0.00213753
Epoch [194/200], Train Loss: 0.002791
Validation Loss: 0.00213163
Epoch [195/200], Train Loss: 0.002771
Validation Loss: 0.00212263
Epoch [196/200], Train Loss: 0.002735
Validation Loss: 0.00211019
Epoch [197/200], Train Loss: 0.002731
Validation Loss: 0.00211063
Epoch [198/200], Train Loss: 0.002716
Validation Loss: 0.00209530
Epoch [199/200], Train Loss: 0.002705
Validation Loss: 0.00209593
Epoch [200/200], Train Loss: 0.002718
Validation Loss: 0.00207981

Evaluating model for: Lamp
Run 17/144 completed in 646.78 seconds with: {'MAE': np.float32(1.8105313), 'MSE': np.float32(83.781296), 'RMSE': np.float32(9.153213), 'SAE': np.float32(0.17402819), 'NDE': np.float32(0.7082995)}

Run 18/144: hidden=64, seq_len=360, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.007138
Validation Loss: 0.00493818
Epoch [2/200], Train Loss: 0.005543
Validation Loss: 0.00403623
Epoch [3/200], Train Loss: 0.005081
Validation Loss: 0.00385613
Epoch [4/200], Train Loss: 0.005021
Validation Loss: 0.00384986
Epoch [5/200], Train Loss: 0.005085
Validation Loss: 0.00384970
Epoch [6/200], Train Loss: 0.005005
Validation Loss: 0.00384780
Epoch [7/200], Train Loss: 0.005017
Validation Loss: 0.00384199
Epoch [8/200], Train Loss: 0.005031
Validation Loss: 0.00383866
Epoch [9/200], Train Loss: 0.005011
Validation Loss: 0.00383393
Epoch [10/200], Train Loss: 0.004995
Validation Loss: 0.00382591
Epoch [11/200], Train Loss: 0.005029
Validation Loss: 0.00381862
Epoch [12/200], Train Loss: 0.004947
Validation Loss: 0.00380789
Epoch [13/200], Train Loss: 0.004951
Validation Loss: 0.00379275
Epoch [14/200], Train Loss: 0.004975
Validation Loss: 0.00379234
Epoch [15/200], Train Loss: 0.004935
Validation Loss: 0.00378694
Epoch [16/200], Train Loss: 0.004958
Validation Loss: 0.00378398
Epoch [17/200], Train Loss: 0.004928
Validation Loss: 0.00378847
Epoch [18/200], Train Loss: 0.004961
Validation Loss: 0.00378276
Epoch [19/200], Train Loss: 0.004954
Validation Loss: 0.00378154
Epoch [20/200], Train Loss: 0.004893
Validation Loss: 0.00378425
Epoch [21/200], Train Loss: 0.004906
Validation Loss: 0.00378182
Epoch [22/200], Train Loss: 0.004913
Validation Loss: 0.00377851
Epoch [23/200], Train Loss: 0.004891
Validation Loss: 0.00377741
Epoch [24/200], Train Loss: 0.004940
Validation Loss: 0.00379095
Epoch [25/200], Train Loss: 0.004897
Validation Loss: 0.00377964
Epoch [26/200], Train Loss: 0.004884
Validation Loss: 0.00378153
Epoch [27/200], Train Loss: 0.004874
Validation Loss: 0.00377495
Epoch [28/200], Train Loss: 0.004906
Validation Loss: 0.00377641
Epoch [29/200], Train Loss: 0.004908
Validation Loss: 0.00377709
Epoch [30/200], Train Loss: 0.004905
Validation Loss: 0.00377812
Epoch [31/200], Train Loss: 0.004870
Validation Loss: 0.00377691
Epoch [32/200], Train Loss: 0.004906
Validation Loss: 0.00377567
Epoch [33/200], Train Loss: 0.004949
Validation Loss: 0.00377297
Epoch [34/200], Train Loss: 0.004901
Validation Loss: 0.00377409
Epoch [35/200], Train Loss: 0.004906
Validation Loss: 0.00378243
Epoch [36/200], Train Loss: 0.004905
Validation Loss: 0.00377191
Epoch [37/200], Train Loss: 0.004855
Validation Loss: 0.00377810
Epoch [38/200], Train Loss: 0.004882
Validation Loss: 0.00376869
Epoch [39/200], Train Loss: 0.004889
Validation Loss: 0.00377568
Epoch [40/200], Train Loss: 0.004974
Validation Loss: 0.00377506
Epoch [41/200], Train Loss: 0.004924
Validation Loss: 0.00376716
Epoch [42/200], Train Loss: 0.004864
Validation Loss: 0.00377414
Epoch [43/200], Train Loss: 0.004862
Validation Loss: 0.00377311
Epoch [44/200], Train Loss: 0.004932
Validation Loss: 0.00376944
Epoch [45/200], Train Loss: 0.004903
Validation Loss: 0.00376926
Epoch [46/200], Train Loss: 0.004898
Validation Loss: 0.00377782
Epoch [47/200], Train Loss: 0.004910
Validation Loss: 0.00377323
Epoch [48/200], Train Loss: 0.004896
Validation Loss: 0.00376448
Epoch [49/200], Train Loss: 0.004928
Validation Loss: 0.00376314
Epoch [50/200], Train Loss: 0.004906
Validation Loss: 0.00376376
Epoch [51/200], Train Loss: 0.004924
Validation Loss: 0.00377318
Epoch [52/200], Train Loss: 0.004889
Validation Loss: 0.00377199
Epoch [53/200], Train Loss: 0.004885
Validation Loss: 0.00376192
Epoch [54/200], Train Loss: 0.004897
Validation Loss: 0.00376600
Epoch [55/200], Train Loss: 0.004892
Validation Loss: 0.00376080
Epoch [56/200], Train Loss: 0.004881
Validation Loss: 0.00376072
Epoch [57/200], Train Loss: 0.004897
Validation Loss: 0.00376037
Epoch [58/200], Train Loss: 0.004903
Validation Loss: 0.00376970
Epoch [59/200], Train Loss: 0.004904
Validation Loss: 0.00375951
Epoch [60/200], Train Loss: 0.004866
Validation Loss: 0.00376291
Epoch [61/200], Train Loss: 0.004891
Validation Loss: 0.00376305
Epoch [62/200], Train Loss: 0.004888
Validation Loss: 0.00375927
Epoch [63/200], Train Loss: 0.004862
Validation Loss: 0.00376324
Epoch [64/200], Train Loss: 0.004875
Validation Loss: 0.00375913
Epoch [65/200], Train Loss: 0.004908
Validation Loss: 0.00376048
Epoch [66/200], Train Loss: 0.004880
Validation Loss: 0.00375869
Epoch [67/200], Train Loss: 0.004888
Validation Loss: 0.00375966
Epoch [68/200], Train Loss: 0.004864
Validation Loss: 0.00376159
Epoch [69/200], Train Loss: 0.004901
Validation Loss: 0.00375962
Epoch [70/200], Train Loss: 0.004906
Validation Loss: 0.00375678
Epoch [71/200], Train Loss: 0.004854
Validation Loss: 0.00375666
Epoch [72/200], Train Loss: 0.004924
Validation Loss: 0.00375765
Epoch [73/200], Train Loss: 0.004882
Validation Loss: 0.00375836
Epoch [74/200], Train Loss: 0.004929
Validation Loss: 0.00376124
Epoch [75/200], Train Loss: 0.004914
Validation Loss: 0.00375643
Epoch [76/200], Train Loss: 0.004885
Validation Loss: 0.00375641
Epoch [77/200], Train Loss: 0.004876
Validation Loss: 0.00376095
Epoch [78/200], Train Loss: 0.004871
Validation Loss: 0.00375873
Epoch [79/200], Train Loss: 0.004916
Validation Loss: 0.00375482
Epoch [80/200], Train Loss: 0.004861
Validation Loss: 0.00375741
Epoch [81/200], Train Loss: 0.004918
Validation Loss: 0.00375545
Epoch [82/200], Train Loss: 0.004889
Validation Loss: 0.00375743
Epoch [83/200], Train Loss: 0.004911
Validation Loss: 0.00375958
Epoch [84/200], Train Loss: 0.004898
Validation Loss: 0.00375638
Epoch [85/200], Train Loss: 0.004869
Validation Loss: 0.00375514
Epoch [86/200], Train Loss: 0.004879
Validation Loss: 0.00375617
Epoch [87/200], Train Loss: 0.004884
Validation Loss: 0.00375762
Epoch [88/200], Train Loss: 0.004841
Validation Loss: 0.00375869
Epoch [89/200], Train Loss: 0.004863
Validation Loss: 0.00375674
Early stopping triggered

Evaluating model for: Lamp
Run 18/144 completed in 288.24 seconds with: {'MAE': np.float32(2.9562156), 'MSE': np.float32(161.2923), 'RMSE': np.float32(12.70009), 'SAE': np.float32(0.17186342), 'NDE': np.float32(0.9827663)}

Run 19/144: hidden=64, seq_len=360, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.011597
Validation Loss: 0.00793481
Epoch [2/200], Train Loss: 0.007805
Validation Loss: 0.00518517
Epoch [3/200], Train Loss: 0.005754
Validation Loss: 0.00393468
Epoch [4/200], Train Loss: 0.005054
Validation Loss: 0.00387468
Epoch [5/200], Train Loss: 0.005021
Validation Loss: 0.00384799
Epoch [6/200], Train Loss: 0.005031
Validation Loss: 0.00384704
Epoch [7/200], Train Loss: 0.005025
Validation Loss: 0.00384412
Epoch [8/200], Train Loss: 0.004995
Validation Loss: 0.00383855
Epoch [9/200], Train Loss: 0.005065
Validation Loss: 0.00383566
Epoch [10/200], Train Loss: 0.005014
Validation Loss: 0.00382944
Epoch [11/200], Train Loss: 0.005002
Validation Loss: 0.00382021
Epoch [12/200], Train Loss: 0.004986
Validation Loss: 0.00381750
Epoch [13/200], Train Loss: 0.005003
Validation Loss: 0.00380642
Epoch [14/200], Train Loss: 0.004935
Validation Loss: 0.00380420
Epoch [15/200], Train Loss: 0.004991
Validation Loss: 0.00380231
Epoch [16/200], Train Loss: 0.004961
Validation Loss: 0.00380347
Epoch [17/200], Train Loss: 0.004951
Validation Loss: 0.00380000
Epoch [18/200], Train Loss: 0.004986
Validation Loss: 0.00380173
Epoch [19/200], Train Loss: 0.004979
Validation Loss: 0.00380218
Epoch [20/200], Train Loss: 0.004976
Validation Loss: 0.00379845
Epoch [21/200], Train Loss: 0.004953
Validation Loss: 0.00379609
Epoch [22/200], Train Loss: 0.004962
Validation Loss: 0.00381273
Epoch [23/200], Train Loss: 0.004942
Validation Loss: 0.00379735
Epoch [24/200], Train Loss: 0.004949
Validation Loss: 0.00379347
Epoch [25/200], Train Loss: 0.004970
Validation Loss: 0.00379591
Epoch [26/200], Train Loss: 0.004996
Validation Loss: 0.00379052
Epoch [27/200], Train Loss: 0.004950
Validation Loss: 0.00379075
Epoch [28/200], Train Loss: 0.004965
Validation Loss: 0.00379077
Epoch [29/200], Train Loss: 0.004929
Validation Loss: 0.00379134
Epoch [30/200], Train Loss: 0.004922
Validation Loss: 0.00378551
Epoch [31/200], Train Loss: 0.004947
Validation Loss: 0.00378406
Epoch [32/200], Train Loss: 0.004932
Validation Loss: 0.00378267
Epoch [33/200], Train Loss: 0.004955
Validation Loss: 0.00378298
Epoch [34/200], Train Loss: 0.004916
Validation Loss: 0.00378075
Epoch [35/200], Train Loss: 0.004918
Validation Loss: 0.00378079
Epoch [36/200], Train Loss: 0.004967
Validation Loss: 0.00377717
Epoch [37/200], Train Loss: 0.004965
Validation Loss: 0.00377664
Epoch [38/200], Train Loss: 0.004928
Validation Loss: 0.00377728
Epoch [39/200], Train Loss: 0.005003
Validation Loss: 0.00377407
Epoch [40/200], Train Loss: 0.004908
Validation Loss: 0.00377316
Epoch [41/200], Train Loss: 0.004935
Validation Loss: 0.00377488
Epoch [42/200], Train Loss: 0.004957
Validation Loss: 0.00377147
Epoch [43/200], Train Loss: 0.004937
Validation Loss: 0.00376904
Epoch [44/200], Train Loss: 0.004896
Validation Loss: 0.00377579
Epoch [45/200], Train Loss: 0.004953
Validation Loss: 0.00377227
Epoch [46/200], Train Loss: 0.004949
Validation Loss: 0.00376706
Epoch [47/200], Train Loss: 0.004920
Validation Loss: 0.00376717
Epoch [48/200], Train Loss: 0.004941
Validation Loss: 0.00376570
Epoch [49/200], Train Loss: 0.004928
Validation Loss: 0.00376591
Epoch [50/200], Train Loss: 0.004928
Validation Loss: 0.00376537
Epoch [51/200], Train Loss: 0.004875
Validation Loss: 0.00376538
Epoch [52/200], Train Loss: 0.004933
Validation Loss: 0.00376403
Epoch [53/200], Train Loss: 0.004918
Validation Loss: 0.00376344
Epoch [54/200], Train Loss: 0.004900
Validation Loss: 0.00376451
Epoch [55/200], Train Loss: 0.004927
Validation Loss: 0.00376655
Epoch [56/200], Train Loss: 0.004890
Validation Loss: 0.00376248
Epoch [57/200], Train Loss: 0.004923
Validation Loss: 0.00376196
Epoch [58/200], Train Loss: 0.004931
Validation Loss: 0.00376259
Epoch [59/200], Train Loss: 0.004921
Validation Loss: 0.00376164
Epoch [60/200], Train Loss: 0.004926
Validation Loss: 0.00376378
Epoch [61/200], Train Loss: 0.004920
Validation Loss: 0.00376077
Epoch [62/200], Train Loss: 0.004908
Validation Loss: 0.00376017
Epoch [63/200], Train Loss: 0.004914
Validation Loss: 0.00375949
Epoch [64/200], Train Loss: 0.004879
Validation Loss: 0.00376058
Epoch [65/200], Train Loss: 0.004993
Validation Loss: 0.00376020
Epoch [66/200], Train Loss: 0.004893
Validation Loss: 0.00375963
Epoch [67/200], Train Loss: 0.004911
Validation Loss: 0.00375960
Epoch [68/200], Train Loss: 0.004894
Validation Loss: 0.00375835
Epoch [69/200], Train Loss: 0.004907
Validation Loss: 0.00376107
Epoch [70/200], Train Loss: 0.004884
Validation Loss: 0.00375951
Epoch [71/200], Train Loss: 0.004894
Validation Loss: 0.00375717
Epoch [72/200], Train Loss: 0.004901
Validation Loss: 0.00375845
Epoch [73/200], Train Loss: 0.004918
Validation Loss: 0.00376396
Epoch [74/200], Train Loss: 0.004915
Validation Loss: 0.00375767
Epoch [75/200], Train Loss: 0.004917
Validation Loss: 0.00375679
Epoch [76/200], Train Loss: 0.004945
Validation Loss: 0.00376206
Epoch [77/200], Train Loss: 0.004911
Validation Loss: 0.00376061
Epoch [78/200], Train Loss: 0.004932
Validation Loss: 0.00375643
Epoch [79/200], Train Loss: 0.004943
Validation Loss: 0.00376184
Epoch [80/200], Train Loss: 0.004934
Validation Loss: 0.00375610
Epoch [81/200], Train Loss: 0.004933
Validation Loss: 0.00375921
Epoch [82/200], Train Loss: 0.004877
Validation Loss: 0.00375855
Epoch [83/200], Train Loss: 0.004895
Validation Loss: 0.00375599
Epoch [84/200], Train Loss: 0.004914
Validation Loss: 0.00375695
Epoch [85/200], Train Loss: 0.004941
Validation Loss: 0.00375722
Epoch [86/200], Train Loss: 0.004918
Validation Loss: 0.00375515
Epoch [87/200], Train Loss: 0.004920
Validation Loss: 0.00375456
Epoch [88/200], Train Loss: 0.004879
Validation Loss: 0.00375676
Epoch [89/200], Train Loss: 0.004891
Validation Loss: 0.00375920
Epoch [90/200], Train Loss: 0.004874
Validation Loss: 0.00376230
Epoch [91/200], Train Loss: 0.004872
Validation Loss: 0.00375522
Epoch [92/200], Train Loss: 0.004934
Validation Loss: 0.00375374
Epoch [93/200], Train Loss: 0.004876
Validation Loss: 0.00375412
Epoch [94/200], Train Loss: 0.004926
Validation Loss: 0.00375614
Epoch [95/200], Train Loss: 0.004901
Validation Loss: 0.00375822
Epoch [96/200], Train Loss: 0.004879
Validation Loss: 0.00375314
Epoch [97/200], Train Loss: 0.004863
Validation Loss: 0.00375313
Epoch [98/200], Train Loss: 0.004915
Validation Loss: 0.00375557
Epoch [99/200], Train Loss: 0.004925
Validation Loss: 0.00375863
Epoch [100/200], Train Loss: 0.004955
Validation Loss: 0.00375246
Epoch [101/200], Train Loss: 0.004862
Validation Loss: 0.00375265
Epoch [102/200], Train Loss: 0.004930
Validation Loss: 0.00375240
Epoch [103/200], Train Loss: 0.004906
Validation Loss: 0.00375251
Epoch [104/200], Train Loss: 0.004929
Validation Loss: 0.00375358
Epoch [105/200], Train Loss: 0.004883
Validation Loss: 0.00375650
Epoch [106/200], Train Loss: 0.004900
Validation Loss: 0.00375527
Epoch [107/200], Train Loss: 0.004947
Validation Loss: 0.00375158
Epoch [108/200], Train Loss: 0.004933
Validation Loss: 0.00375160
Epoch [109/200], Train Loss: 0.004860
Validation Loss: 0.00375476
Epoch [110/200], Train Loss: 0.004896
Validation Loss: 0.00375194
Epoch [111/200], Train Loss: 0.004885
Validation Loss: 0.00375328
Epoch [112/200], Train Loss: 0.004937
Validation Loss: 0.00375089
Epoch [113/200], Train Loss: 0.004906
Validation Loss: 0.00375062
Epoch [114/200], Train Loss: 0.004891
Validation Loss: 0.00375312
Epoch [115/200], Train Loss: 0.004940
Validation Loss: 0.00375110
Epoch [116/200], Train Loss: 0.004881
Validation Loss: 0.00375001
Epoch [117/200], Train Loss: 0.004894
Validation Loss: 0.00375231
Epoch [118/200], Train Loss: 0.004860
Validation Loss: 0.00375192
Epoch [119/200], Train Loss: 0.004887
Validation Loss: 0.00375389
Epoch [120/200], Train Loss: 0.004942
Validation Loss: 0.00375535
Epoch [121/200], Train Loss: 0.004870
Validation Loss: 0.00374975
Epoch [122/200], Train Loss: 0.004918
Validation Loss: 0.00375164
Epoch [123/200], Train Loss: 0.004884
Validation Loss: 0.00375349
Epoch [124/200], Train Loss: 0.004884
Validation Loss: 0.00375598
Epoch [125/200], Train Loss: 0.004859
Validation Loss: 0.00375246
Epoch [126/200], Train Loss: 0.004909
Validation Loss: 0.00374954
Epoch [127/200], Train Loss: 0.004873
Validation Loss: 0.00374951
Epoch [128/200], Train Loss: 0.004888
Validation Loss: 0.00375037
Epoch [129/200], Train Loss: 0.004895
Validation Loss: 0.00375025
Epoch [130/200], Train Loss: 0.004863
Validation Loss: 0.00374913
Epoch [131/200], Train Loss: 0.004899
Validation Loss: 0.00375674
Epoch [132/200], Train Loss: 0.004880
Validation Loss: 0.00375247
Epoch [133/200], Train Loss: 0.004869
Validation Loss: 0.00374990
Epoch [134/200], Train Loss: 0.004913
Validation Loss: 0.00375107
Epoch [135/200], Train Loss: 0.004863
Validation Loss: 0.00375078
Epoch [136/200], Train Loss: 0.004889
Validation Loss: 0.00374851
Epoch [137/200], Train Loss: 0.004875
Validation Loss: 0.00375045
Epoch [138/200], Train Loss: 0.004886
Validation Loss: 0.00375141
Epoch [139/200], Train Loss: 0.004877
Validation Loss: 0.00375010
Epoch [140/200], Train Loss: 0.004923
Validation Loss: 0.00374842
Epoch [141/200], Train Loss: 0.004889
Validation Loss: 0.00374845
Epoch [142/200], Train Loss: 0.004880
Validation Loss: 0.00374870
Epoch [143/200], Train Loss: 0.004928
Validation Loss: 0.00374985
Epoch [144/200], Train Loss: 0.004876
Validation Loss: 0.00374935
Epoch [145/200], Train Loss: 0.004862
Validation Loss: 0.00374884
Epoch [146/200], Train Loss: 0.004891
Validation Loss: 0.00374785
Epoch [147/200], Train Loss: 0.004849
Validation Loss: 0.00375315
Epoch [148/200], Train Loss: 0.004881
Validation Loss: 0.00374806
Epoch [149/200], Train Loss: 0.004914
Validation Loss: 0.00374819
Epoch [150/200], Train Loss: 0.004856
Validation Loss: 0.00374789
Epoch [151/200], Train Loss: 0.004934
Validation Loss: 0.00375588
Epoch [152/200], Train Loss: 0.004880
Validation Loss: 0.00374859
Epoch [153/200], Train Loss: 0.004940
Validation Loss: 0.00375305
Epoch [154/200], Train Loss: 0.004984
Validation Loss: 0.00374873
Epoch [155/200], Train Loss: 0.004881
Validation Loss: 0.00374758
Epoch [156/200], Train Loss: 0.004929
Validation Loss: 0.00374816
Epoch [157/200], Train Loss: 0.004920
Validation Loss: 0.00374851
Epoch [158/200], Train Loss: 0.004837
Validation Loss: 0.00375014
Epoch [159/200], Train Loss: 0.004901
Validation Loss: 0.00374695
Epoch [160/200], Train Loss: 0.004915
Validation Loss: 0.00374683
Epoch [161/200], Train Loss: 0.004919
Validation Loss: 0.00374766
Epoch [162/200], Train Loss: 0.004855
Validation Loss: 0.00374744
Epoch [163/200], Train Loss: 0.004900
Validation Loss: 0.00374753
Epoch [164/200], Train Loss: 0.004893
Validation Loss: 0.00374641
Epoch [165/200], Train Loss: 0.004850
Validation Loss: 0.00374679
Epoch [166/200], Train Loss: 0.004873
Validation Loss: 0.00374789
Epoch [167/200], Train Loss: 0.004861
Validation Loss: 0.00374633
Epoch [168/200], Train Loss: 0.004860
Validation Loss: 0.00374852
Epoch [169/200], Train Loss: 0.004871
Validation Loss: 0.00374626
Epoch [170/200], Train Loss: 0.004866
Validation Loss: 0.00374740
Epoch [171/200], Train Loss: 0.004870
Validation Loss: 0.00374674
Epoch [172/200], Train Loss: 0.004896
Validation Loss: 0.00375088
Epoch [173/200], Train Loss: 0.004847
Validation Loss: 0.00374608
Epoch [174/200], Train Loss: 0.004880
Validation Loss: 0.00374878
Epoch [175/200], Train Loss: 0.004878
Validation Loss: 0.00374798
Epoch [176/200], Train Loss: 0.004876
Validation Loss: 0.00374810
Epoch [177/200], Train Loss: 0.004873
Validation Loss: 0.00374782
Epoch [178/200], Train Loss: 0.004870
Validation Loss: 0.00374778
Epoch [179/200], Train Loss: 0.004870
Validation Loss: 0.00374581
Epoch [180/200], Train Loss: 0.004897
Validation Loss: 0.00374746
Epoch [181/200], Train Loss: 0.004885
Validation Loss: 0.00374845
Epoch [182/200], Train Loss: 0.004917
Validation Loss: 0.00374721
Epoch [183/200], Train Loss: 0.004900
Validation Loss: 0.00374580
Epoch [184/200], Train Loss: 0.004885
Validation Loss: 0.00374597
Epoch [185/200], Train Loss: 0.004878
Validation Loss: 0.00374637
Epoch [186/200], Train Loss: 0.004857
Validation Loss: 0.00374995
Epoch [187/200], Train Loss: 0.004879
Validation Loss: 0.00374549
Epoch [188/200], Train Loss: 0.004877
Validation Loss: 0.00374592
Epoch [189/200], Train Loss: 0.004871
Validation Loss: 0.00374626
Epoch [190/200], Train Loss: 0.004864
Validation Loss: 0.00374604
Epoch [191/200], Train Loss: 0.004867
Validation Loss: 0.00374942
Epoch [192/200], Train Loss: 0.004876
Validation Loss: 0.00374540
Epoch [193/200], Train Loss: 0.004872
Validation Loss: 0.00374507
Epoch [194/200], Train Loss: 0.004885
Validation Loss: 0.00374671
Epoch [195/200], Train Loss: 0.004868
Validation Loss: 0.00374580
Epoch [196/200], Train Loss: 0.004850
Validation Loss: 0.00374485
Epoch [197/200], Train Loss: 0.004842
Validation Loss: 0.00374565
Epoch [198/200], Train Loss: 0.004877
Validation Loss: 0.00374529
Epoch [199/200], Train Loss: 0.004854
Validation Loss: 0.00374493
Epoch [200/200], Train Loss: 0.004868
Validation Loss: 0.00374409

Evaluating model for: Lamp
Run 19/144 completed in 657.88 seconds with: {'MAE': np.float32(2.7765455), 'MSE': np.float32(161.04416), 'RMSE': np.float32(12.690317), 'SAE': np.float32(0.0093083745), 'NDE': np.float32(0.98201007)}

Run 20/144: hidden=64, seq_len=360, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.005254
Validation Loss: 0.00385772
Epoch [2/200], Train Loss: 0.005017
Validation Loss: 0.00383767
Epoch [3/200], Train Loss: 0.004981
Validation Loss: 0.00384353
Epoch [4/200], Train Loss: 0.005000
Validation Loss: 0.00383926
Epoch [5/200], Train Loss: 0.004970
Validation Loss: 0.00384025
Epoch [6/200], Train Loss: 0.004986
Validation Loss: 0.00383677
Epoch [7/200], Train Loss: 0.004981
Validation Loss: 0.00383200
Epoch [8/200], Train Loss: 0.004950
Validation Loss: 0.00382138
Epoch [9/200], Train Loss: 0.004934
Validation Loss: 0.00378876
Epoch [10/200], Train Loss: 0.004926
Validation Loss: 0.00378500
Epoch [11/200], Train Loss: 0.004886
Validation Loss: 0.00377428
Epoch [12/200], Train Loss: 0.004902
Validation Loss: 0.00377672
Epoch [13/200], Train Loss: 0.004889
Validation Loss: 0.00377438
Epoch [14/200], Train Loss: 0.004852
Validation Loss: 0.00377123
Epoch [15/200], Train Loss: 0.004905
Validation Loss: 0.00377479
Epoch [16/200], Train Loss: 0.004921
Validation Loss: 0.00376876
Epoch [17/200], Train Loss: 0.004910
Validation Loss: 0.00377163
Epoch [18/200], Train Loss: 0.004843
Validation Loss: 0.00376922
Epoch [19/200], Train Loss: 0.004883
Validation Loss: 0.00376816
Epoch [20/200], Train Loss: 0.004831
Validation Loss: 0.00377299
Epoch [21/200], Train Loss: 0.004881
Validation Loss: 0.00376561
Epoch [22/200], Train Loss: 0.004874
Validation Loss: 0.00377191
Epoch [23/200], Train Loss: 0.004892
Validation Loss: 0.00376545
Epoch [24/200], Train Loss: 0.004901
Validation Loss: 0.00376945
Epoch [25/200], Train Loss: 0.004890
Validation Loss: 0.00376304
Epoch [26/200], Train Loss: 0.004860
Validation Loss: 0.00376463
Epoch [27/200], Train Loss: 0.004859
Validation Loss: 0.00376193
Epoch [28/200], Train Loss: 0.004872
Validation Loss: 0.00376484
Epoch [29/200], Train Loss: 0.004853
Validation Loss: 0.00376157
Epoch [30/200], Train Loss: 0.004856
Validation Loss: 0.00376558
Epoch [31/200], Train Loss: 0.004857
Validation Loss: 0.00376197
Epoch [32/200], Train Loss: 0.004863
Validation Loss: 0.00376104
Epoch [33/200], Train Loss: 0.004863
Validation Loss: 0.00376765
Epoch [34/200], Train Loss: 0.004854
Validation Loss: 0.00376114
Epoch [35/200], Train Loss: 0.004894
Validation Loss: 0.00376037
Epoch [36/200], Train Loss: 0.004879
Validation Loss: 0.00376019
Epoch [37/200], Train Loss: 0.004928
Validation Loss: 0.00375897
Epoch [38/200], Train Loss: 0.004835
Validation Loss: 0.00375961
Epoch [39/200], Train Loss: 0.004872
Validation Loss: 0.00375772
Epoch [40/200], Train Loss: 0.004856
Validation Loss: 0.00375823
Epoch [41/200], Train Loss: 0.004862
Validation Loss: 0.00375819
Epoch [42/200], Train Loss: 0.004870
Validation Loss: 0.00375793
Epoch [43/200], Train Loss: 0.004875
Validation Loss: 0.00375731
Epoch [44/200], Train Loss: 0.004885
Validation Loss: 0.00375713
Epoch [45/200], Train Loss: 0.004845
Validation Loss: 0.00375685
Epoch [46/200], Train Loss: 0.004864
Validation Loss: 0.00375844
Epoch [47/200], Train Loss: 0.004895
Validation Loss: 0.00375594
Epoch [48/200], Train Loss: 0.004836
Validation Loss: 0.00375897
Epoch [49/200], Train Loss: 0.004861
Validation Loss: 0.00375637
Epoch [50/200], Train Loss: 0.004889
Validation Loss: 0.00375835
Epoch [51/200], Train Loss: 0.004910
Validation Loss: 0.00375950
Epoch [52/200], Train Loss: 0.004909
Validation Loss: 0.00375456
Epoch [53/200], Train Loss: 0.004868
Validation Loss: 0.00375515
Epoch [54/200], Train Loss: 0.004848
Validation Loss: 0.00375549
Epoch [55/200], Train Loss: 0.004855
Validation Loss: 0.00375426
Epoch [56/200], Train Loss: 0.004920
Validation Loss: 0.00375637
Epoch [57/200], Train Loss: 0.004875
Validation Loss: 0.00375350
Epoch [58/200], Train Loss: 0.004864
Validation Loss: 0.00375455
Epoch [59/200], Train Loss: 0.004866
Validation Loss: 0.00375225
Epoch [60/200], Train Loss: 0.004876
Validation Loss: 0.00375268
Epoch [61/200], Train Loss: 0.004890
Validation Loss: 0.00375313
Epoch [62/200], Train Loss: 0.004831
Validation Loss: 0.00375714
Epoch [63/200], Train Loss: 0.004877
Validation Loss: 0.00375246
Epoch [64/200], Train Loss: 0.004888
Validation Loss: 0.00375301
Epoch [65/200], Train Loss: 0.004844
Validation Loss: 0.00375220
Epoch [66/200], Train Loss: 0.004840
Validation Loss: 0.00375192
Epoch [67/200], Train Loss: 0.004891
Validation Loss: 0.00375122
Epoch [68/200], Train Loss: 0.004837
Validation Loss: 0.00375237
Epoch [69/200], Train Loss: 0.004844
Validation Loss: 0.00375150
Epoch [70/200], Train Loss: 0.004844
Validation Loss: 0.00375355
Epoch [71/200], Train Loss: 0.004868
Validation Loss: 0.00375541
Epoch [72/200], Train Loss: 0.004854
Validation Loss: 0.00375231
Epoch [73/200], Train Loss: 0.004878
Validation Loss: 0.00375749
Epoch [74/200], Train Loss: 0.004886
Validation Loss: 0.00375151
Epoch [75/200], Train Loss: 0.004855
Validation Loss: 0.00375099
Epoch [76/200], Train Loss: 0.004847
Validation Loss: 0.00375222
Epoch [77/200], Train Loss: 0.004874
Validation Loss: 0.00375089
Epoch [78/200], Train Loss: 0.004860
Validation Loss: 0.00375364
Epoch [79/200], Train Loss: 0.004910
Validation Loss: 0.00375225
Epoch [80/200], Train Loss: 0.004856
Validation Loss: 0.00375111
Epoch [81/200], Train Loss: 0.004884
Validation Loss: 0.00375019
Epoch [82/200], Train Loss: 0.004866
Validation Loss: 0.00375030
Epoch [83/200], Train Loss: 0.004844
Validation Loss: 0.00375200
Epoch [84/200], Train Loss: 0.004850
Validation Loss: 0.00374964
Epoch [85/200], Train Loss: 0.004917
Validation Loss: 0.00375330
Epoch [86/200], Train Loss: 0.004826
Validation Loss: 0.00375154
Epoch [87/200], Train Loss: 0.004855
Validation Loss: 0.00375006
Epoch [88/200], Train Loss: 0.004863
Validation Loss: 0.00374954
Epoch [89/200], Train Loss: 0.004904
Validation Loss: 0.00375180
Epoch [90/200], Train Loss: 0.004892
Validation Loss: 0.00374899
Epoch [91/200], Train Loss: 0.004858
Validation Loss: 0.00375004
Epoch [92/200], Train Loss: 0.004866
Validation Loss: 0.00374942
Epoch [93/200], Train Loss: 0.004883
Validation Loss: 0.00375039
Epoch [94/200], Train Loss: 0.004886
Validation Loss: 0.00374969
Epoch [95/200], Train Loss: 0.004869
Validation Loss: 0.00374883
Epoch [96/200], Train Loss: 0.004849
Validation Loss: 0.00374941
Epoch [97/200], Train Loss: 0.004896
Validation Loss: 0.00375133
Epoch [98/200], Train Loss: 0.004916
Validation Loss: 0.00374828
Epoch [99/200], Train Loss: 0.004850
Validation Loss: 0.00374831
Epoch [100/200], Train Loss: 0.004814
Validation Loss: 0.00374921
Epoch [101/200], Train Loss: 0.004902
Validation Loss: 0.00375099
Epoch [102/200], Train Loss: 0.004835
Validation Loss: 0.00374936
Epoch [103/200], Train Loss: 0.004897
Validation Loss: 0.00375024
Epoch [104/200], Train Loss: 0.004844
Validation Loss: 0.00374816
Epoch [105/200], Train Loss: 0.004844
Validation Loss: 0.00374787
Epoch [106/200], Train Loss: 0.004862
Validation Loss: 0.00374897
Epoch [107/200], Train Loss: 0.004854
Validation Loss: 0.00374970
Epoch [108/200], Train Loss: 0.004842
Validation Loss: 0.00374911
Epoch [109/200], Train Loss: 0.004841
Validation Loss: 0.00374759
Epoch [110/200], Train Loss: 0.004842
Validation Loss: 0.00374766
Epoch [111/200], Train Loss: 0.004841
Validation Loss: 0.00374975
Epoch [112/200], Train Loss: 0.004890
Validation Loss: 0.00374772
Epoch [113/200], Train Loss: 0.004899
Validation Loss: 0.00374751
Epoch [114/200], Train Loss: 0.004872
Validation Loss: 0.00374872
Epoch [115/200], Train Loss: 0.004901
Validation Loss: 0.00374977
Epoch [116/200], Train Loss: 0.004943
Validation Loss: 0.00374780
Epoch [117/200], Train Loss: 0.004872
Validation Loss: 0.00374707
Epoch [118/200], Train Loss: 0.004831
Validation Loss: 0.00374816
Epoch [119/200], Train Loss: 0.004839
Validation Loss: 0.00374854
Epoch [120/200], Train Loss: 0.004854
Validation Loss: 0.00374692
Epoch [121/200], Train Loss: 0.004880
Validation Loss: 0.00374788
Epoch [122/200], Train Loss: 0.004854
Validation Loss: 0.00374758
Epoch [123/200], Train Loss: 0.004873
Validation Loss: 0.00374810
Epoch [124/200], Train Loss: 0.004845
Validation Loss: 0.00374693
Epoch [125/200], Train Loss: 0.004813
Validation Loss: 0.00374923
Epoch [126/200], Train Loss: 0.004864
Validation Loss: 0.00374703
Epoch [127/200], Train Loss: 0.004872
Validation Loss: 0.00374719
Epoch [128/200], Train Loss: 0.004890
Validation Loss: 0.00374792
Epoch [129/200], Train Loss: 0.004867
Validation Loss: 0.00374615
Epoch [130/200], Train Loss: 0.004821
Validation Loss: 0.00374698
Epoch [131/200], Train Loss: 0.004902
Validation Loss: 0.00374619
Epoch [132/200], Train Loss: 0.004862
Validation Loss: 0.00374718
Epoch [133/200], Train Loss: 0.004878
Validation Loss: 0.00374756
Epoch [134/200], Train Loss: 0.004882
Validation Loss: 0.00374616
Epoch [135/200], Train Loss: 0.004885
Validation Loss: 0.00374821
Epoch [136/200], Train Loss: 0.004839
Validation Loss: 0.00374671
Epoch [137/200], Train Loss: 0.004902
Validation Loss: 0.00374510
Epoch [138/200], Train Loss: 0.004872
Validation Loss: 0.00374595
Epoch [139/200], Train Loss: 0.004846
Validation Loss: 0.00374883
Epoch [140/200], Train Loss: 0.004843
Validation Loss: 0.00374587
Epoch [141/200], Train Loss: 0.004881
Validation Loss: 0.00374518
Epoch [142/200], Train Loss: 0.004847
Validation Loss: 0.00374586
Epoch [143/200], Train Loss: 0.004881
Validation Loss: 0.00374655
Epoch [144/200], Train Loss: 0.004835
Validation Loss: 0.00374556
Epoch [145/200], Train Loss: 0.004842
Validation Loss: 0.00374543
Epoch [146/200], Train Loss: 0.004867
Validation Loss: 0.00374689
Epoch [147/200], Train Loss: 0.004882
Validation Loss: 0.00374485
Epoch [148/200], Train Loss: 0.004854
Validation Loss: 0.00374457
Epoch [149/200], Train Loss: 0.004837
Validation Loss: 0.00374490
Epoch [150/200], Train Loss: 0.004870
Validation Loss: 0.00374558
Epoch [151/200], Train Loss: 0.004853
Validation Loss: 0.00374631
Epoch [152/200], Train Loss: 0.004869
Validation Loss: 0.00374716
Epoch [153/200], Train Loss: 0.004874
Validation Loss: 0.00374446
Epoch [154/200], Train Loss: 0.004834
Validation Loss: 0.00374574
Epoch [155/200], Train Loss: 0.004914
Validation Loss: 0.00374412
Epoch [156/200], Train Loss: 0.004886
Validation Loss: 0.00374385
Epoch [157/200], Train Loss: 0.004861
Validation Loss: 0.00374483
Epoch [158/200], Train Loss: 0.004848
Validation Loss: 0.00374436
Epoch [159/200], Train Loss: 0.004884
Validation Loss: 0.00374306
Epoch [160/200], Train Loss: 0.004879
Validation Loss: 0.00374415
Epoch [161/200], Train Loss: 0.004851
Validation Loss: 0.00374389
Epoch [162/200], Train Loss: 0.004890
Validation Loss: 0.00374427
Epoch [163/200], Train Loss: 0.004894
Validation Loss: 0.00374370
Epoch [164/200], Train Loss: 0.004858
Validation Loss: 0.00374555
Epoch [165/200], Train Loss: 0.004922
Validation Loss: 0.00374393
Epoch [166/200], Train Loss: 0.004860
Validation Loss: 0.00374431
Epoch [167/200], Train Loss: 0.004840
Validation Loss: 0.00374354
Epoch [168/200], Train Loss: 0.004846
Validation Loss: 0.00374317
Epoch [169/200], Train Loss: 0.004855
Validation Loss: 0.00374424
Early stopping triggered

Evaluating model for: Lamp
Run 20/144 completed in 579.80 seconds with: {'MAE': np.float32(2.8545432), 'MSE': np.float32(160.97487), 'RMSE': np.float32(12.687587), 'SAE': np.float32(0.10684341), 'NDE': np.float32(0.9817984)}

Run 21/144: hidden=64, seq_len=360, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.006282
Validation Loss: 0.00661482
Epoch [2/200], Train Loss: 0.005620
Validation Loss: 0.00603021
Epoch [3/200], Train Loss: 0.005140
Validation Loss: 0.00564474
Epoch [4/200], Train Loss: 0.004860
Validation Loss: 0.00543663
Epoch [5/200], Train Loss: 0.004750
Validation Loss: 0.00536414
Epoch [6/200], Train Loss: 0.004713
Validation Loss: 0.00535278
Epoch [7/200], Train Loss: 0.004735
Validation Loss: 0.00534934
Epoch [8/200], Train Loss: 0.004714
Validation Loss: 0.00534760
Epoch [9/200], Train Loss: 0.004703
Validation Loss: 0.00534602
Epoch [10/200], Train Loss: 0.004721
Validation Loss: 0.00534160
Epoch [11/200], Train Loss: 0.004704
Validation Loss: 0.00533638
Epoch [12/200], Train Loss: 0.004721
Validation Loss: 0.00533298
Epoch [13/200], Train Loss: 0.004716
Validation Loss: 0.00532792
Epoch [14/200], Train Loss: 0.004686
Validation Loss: 0.00532323
Epoch [15/200], Train Loss: 0.004694
Validation Loss: 0.00531833
Epoch [16/200], Train Loss: 0.004711
Validation Loss: 0.00531519
Epoch [17/200], Train Loss: 0.004691
Validation Loss: 0.00530967
Epoch [18/200], Train Loss: 0.004683
Validation Loss: 0.00530493
Epoch [19/200], Train Loss: 0.004683
Validation Loss: 0.00530128
Epoch [20/200], Train Loss: 0.004668
Validation Loss: 0.00529767
Epoch [21/200], Train Loss: 0.004652
Validation Loss: 0.00529297
Epoch [22/200], Train Loss: 0.004672
Validation Loss: 0.00529023
Epoch [23/200], Train Loss: 0.004687
Validation Loss: 0.00528810
Epoch [24/200], Train Loss: 0.004668
Validation Loss: 0.00528370
Epoch [25/200], Train Loss: 0.004664
Validation Loss: 0.00528233
Epoch [26/200], Train Loss: 0.004668
Validation Loss: 0.00528089
Epoch [27/200], Train Loss: 0.004651
Validation Loss: 0.00527730
Epoch [28/200], Train Loss: 0.004675
Validation Loss: 0.00528090
Epoch [29/200], Train Loss: 0.004652
Validation Loss: 0.00527638
Epoch [30/200], Train Loss: 0.004667
Validation Loss: 0.00527576
Epoch [31/200], Train Loss: 0.004655
Validation Loss: 0.00527590
Epoch [32/200], Train Loss: 0.004676
Validation Loss: 0.00527537
Epoch [33/200], Train Loss: 0.004652
Validation Loss: 0.00527401
Epoch [34/200], Train Loss: 0.004654
Validation Loss: 0.00527381
Epoch [35/200], Train Loss: 0.004673
Validation Loss: 0.00527376
Epoch [36/200], Train Loss: 0.004647
Validation Loss: 0.00527213
Epoch [37/200], Train Loss: 0.004663
Validation Loss: 0.00527432
Epoch [38/200], Train Loss: 0.004662
Validation Loss: 0.00527222
Epoch [39/200], Train Loss: 0.004643
Validation Loss: 0.00527186
Epoch [40/200], Train Loss: 0.004649
Validation Loss: 0.00527184
Epoch [41/200], Train Loss: 0.004660
Validation Loss: 0.00527109
Epoch [42/200], Train Loss: 0.004639
Validation Loss: 0.00527042
Epoch [43/200], Train Loss: 0.004679
Validation Loss: 0.00527037
Epoch [44/200], Train Loss: 0.004653
Validation Loss: 0.00526915
Epoch [45/200], Train Loss: 0.004663
Validation Loss: 0.00526988
Epoch [46/200], Train Loss: 0.004647
Validation Loss: 0.00526797
Epoch [47/200], Train Loss: 0.004661
Validation Loss: 0.00527151
Epoch [48/200], Train Loss: 0.004640
Validation Loss: 0.00526699
Epoch [49/200], Train Loss: 0.004650
Validation Loss: 0.00526991
Epoch [50/200], Train Loss: 0.004648
Validation Loss: 0.00526764
Epoch [51/200], Train Loss: 0.004657
Validation Loss: 0.00526640
Epoch [52/200], Train Loss: 0.004641
Validation Loss: 0.00526711
Epoch [53/200], Train Loss: 0.004638
Validation Loss: 0.00526613
Epoch [54/200], Train Loss: 0.004639
Validation Loss: 0.00526605
Epoch [55/200], Train Loss: 0.004646
Validation Loss: 0.00526584
Epoch [56/200], Train Loss: 0.004644
Validation Loss: 0.00526433
Epoch [57/200], Train Loss: 0.004634
Validation Loss: 0.00526437
Epoch [58/200], Train Loss: 0.004665
Validation Loss: 0.00526497
Epoch [59/200], Train Loss: 0.004656
Validation Loss: 0.00526308
Epoch [60/200], Train Loss: 0.004657
Validation Loss: 0.00526330
Epoch [61/200], Train Loss: 0.004630
Validation Loss: 0.00526161
Epoch [62/200], Train Loss: 0.004634
Validation Loss: 0.00526377
Epoch [63/200], Train Loss: 0.004653
Validation Loss: 0.00526214
Epoch [64/200], Train Loss: 0.004638
Validation Loss: 0.00525988
Epoch [65/200], Train Loss: 0.004654
Validation Loss: 0.00526247
Epoch [66/200], Train Loss: 0.004641
Validation Loss: 0.00525985
Epoch [67/200], Train Loss: 0.004635
Validation Loss: 0.00525939
Epoch [68/200], Train Loss: 0.004626
Validation Loss: 0.00525836
Epoch [69/200], Train Loss: 0.004644
Validation Loss: 0.00525889
Epoch [70/200], Train Loss: 0.004630
Validation Loss: 0.00525602
Epoch [71/200], Train Loss: 0.004623
Validation Loss: 0.00525865
Epoch [72/200], Train Loss: 0.004643
Validation Loss: 0.00525557
Epoch [73/200], Train Loss: 0.004629
Validation Loss: 0.00525489
Epoch [74/200], Train Loss: 0.004620
Validation Loss: 0.00525633
Epoch [75/200], Train Loss: 0.004628
Validation Loss: 0.00525273
Epoch [76/200], Train Loss: 0.004614
Validation Loss: 0.00525458
Epoch [77/200], Train Loss: 0.004621
Validation Loss: 0.00525173
Epoch [78/200], Train Loss: 0.004645
Validation Loss: 0.00525424
Epoch [79/200], Train Loss: 0.004620
Validation Loss: 0.00525018
Epoch [80/200], Train Loss: 0.004653
Validation Loss: 0.00525316
Epoch [81/200], Train Loss: 0.004631
Validation Loss: 0.00524953
Epoch [82/200], Train Loss: 0.004634
Validation Loss: 0.00524802
Epoch [83/200], Train Loss: 0.004615
Validation Loss: 0.00524926
Epoch [84/200], Train Loss: 0.004619
Validation Loss: 0.00524657
Epoch [85/200], Train Loss: 0.004661
Validation Loss: 0.00524828
Epoch [86/200], Train Loss: 0.004618
Validation Loss: 0.00524381
Epoch [87/200], Train Loss: 0.004624
Validation Loss: 0.00524850
Epoch [88/200], Train Loss: 0.004615
Validation Loss: 0.00524342
Epoch [89/200], Train Loss: 0.004661
Validation Loss: 0.00524538
Epoch [90/200], Train Loss: 0.004606
Validation Loss: 0.00524141
Epoch [91/200], Train Loss: 0.004618
Validation Loss: 0.00524375
Epoch [92/200], Train Loss: 0.004625
Validation Loss: 0.00524076
Epoch [93/200], Train Loss: 0.004627
Validation Loss: 0.00524175
Epoch [94/200], Train Loss: 0.004610
Validation Loss: 0.00523990
Epoch [95/200], Train Loss: 0.004593
Validation Loss: 0.00523897
Epoch [96/200], Train Loss: 0.004615
Validation Loss: 0.00524138
Epoch [97/200], Train Loss: 0.004627
Validation Loss: 0.00523842
Epoch [98/200], Train Loss: 0.004609
Validation Loss: 0.00523817
Epoch [99/200], Train Loss: 0.004622
Validation Loss: 0.00523877
Epoch [100/200], Train Loss: 0.004602
Validation Loss: 0.00523616
Epoch [101/200], Train Loss: 0.004618
Validation Loss: 0.00523717
Epoch [102/200], Train Loss: 0.004627
Validation Loss: 0.00523576
Epoch [103/200], Train Loss: 0.004614
Validation Loss: 0.00523381
Epoch [104/200], Train Loss: 0.004614
Validation Loss: 0.00523662
Epoch [105/200], Train Loss: 0.004619
Validation Loss: 0.00523295
Epoch [106/200], Train Loss: 0.004614
Validation Loss: 0.00523466
Epoch [107/200], Train Loss: 0.004598
Validation Loss: 0.00523264
Epoch [108/200], Train Loss: 0.004625
Validation Loss: 0.00523253
Epoch [109/200], Train Loss: 0.004600
Validation Loss: 0.00523214
Epoch [110/200], Train Loss: 0.004588
Validation Loss: 0.00523161
Epoch [111/200], Train Loss: 0.004634
Validation Loss: 0.00523129
Epoch [112/200], Train Loss: 0.004598
Validation Loss: 0.00522938
Epoch [113/200], Train Loss: 0.004599
Validation Loss: 0.00523321
Epoch [114/200], Train Loss: 0.004596
Validation Loss: 0.00522782
Epoch [115/200], Train Loss: 0.004593
Validation Loss: 0.00522952
Epoch [116/200], Train Loss: 0.004609
Validation Loss: 0.00522849
Epoch [117/200], Train Loss: 0.004589
Validation Loss: 0.00522685
Epoch [118/200], Train Loss: 0.004590
Validation Loss: 0.00522639
Epoch [119/200], Train Loss: 0.004618
Validation Loss: 0.00522833
Epoch [120/200], Train Loss: 0.004613
Validation Loss: 0.00522463
Epoch [121/200], Train Loss: 0.004616
Validation Loss: 0.00522516
Epoch [122/200], Train Loss: 0.004607
Validation Loss: 0.00522504
Epoch [123/200], Train Loss: 0.004579
Validation Loss: 0.00522364
Epoch [124/200], Train Loss: 0.004607
Validation Loss: 0.00522369
Epoch [125/200], Train Loss: 0.004596
Validation Loss: 0.00522272
Epoch [126/200], Train Loss: 0.004597
Validation Loss: 0.00522345
Epoch [127/200], Train Loss: 0.004592
Validation Loss: 0.00522123
Epoch [128/200], Train Loss: 0.004616
Validation Loss: 0.00522300
Epoch [129/200], Train Loss: 0.004604
Validation Loss: 0.00522047
Epoch [130/200], Train Loss: 0.004594
Validation Loss: 0.00522030
Epoch [131/200], Train Loss: 0.004619
Validation Loss: 0.00522145
Epoch [132/200], Train Loss: 0.004606
Validation Loss: 0.00521717
Epoch [133/200], Train Loss: 0.004602
Validation Loss: 0.00522023
Epoch [134/200], Train Loss: 0.004623
Validation Loss: 0.00521772
Epoch [135/200], Train Loss: 0.004595
Validation Loss: 0.00521603
Epoch [136/200], Train Loss: 0.004595
Validation Loss: 0.00521849
Epoch [137/200], Train Loss: 0.004605
Validation Loss: 0.00521694
Epoch [138/200], Train Loss: 0.004599
Validation Loss: 0.00521514
Epoch [139/200], Train Loss: 0.004594
Validation Loss: 0.00521422
Epoch [140/200], Train Loss: 0.004576
Validation Loss: 0.00521423
Epoch [141/200], Train Loss: 0.004605
Validation Loss: 0.00521569
Epoch [142/200], Train Loss: 0.004593
Validation Loss: 0.00521098
Epoch [143/200], Train Loss: 0.004598
Validation Loss: 0.00521575
Epoch [144/200], Train Loss: 0.004572
Validation Loss: 0.00520978
Epoch [145/200], Train Loss: 0.004595
Validation Loss: 0.00521428
Epoch [146/200], Train Loss: 0.004604
Validation Loss: 0.00520945
Epoch [147/200], Train Loss: 0.004589
Validation Loss: 0.00520872
Epoch [148/200], Train Loss: 0.004600
Validation Loss: 0.00521199
Epoch [149/200], Train Loss: 0.004619
Validation Loss: 0.00520804
Epoch [150/200], Train Loss: 0.004593
Validation Loss: 0.00520869
Epoch [151/200], Train Loss: 0.004594
Validation Loss: 0.00520691
Epoch [152/200], Train Loss: 0.004564
Validation Loss: 0.00520503
Epoch [153/200], Train Loss: 0.004592
Validation Loss: 0.00520704
Epoch [154/200], Train Loss: 0.004579
Validation Loss: 0.00520472
Epoch [155/200], Train Loss: 0.004581
Validation Loss: 0.00520351
Epoch [156/200], Train Loss: 0.004560
Validation Loss: 0.00520510
Epoch [157/200], Train Loss: 0.004584
Validation Loss: 0.00520365
Epoch [158/200], Train Loss: 0.004574
Validation Loss: 0.00520106
Epoch [159/200], Train Loss: 0.004605
Validation Loss: 0.00520121
Epoch [160/200], Train Loss: 0.004572
Validation Loss: 0.00519988
Epoch [161/200], Train Loss: 0.004581
Validation Loss: 0.00520003
Epoch [162/200], Train Loss: 0.004581
Validation Loss: 0.00519795
Epoch [163/200], Train Loss: 0.004578
Validation Loss: 0.00519968
Epoch [164/200], Train Loss: 0.004578
Validation Loss: 0.00519797
Epoch [165/200], Train Loss: 0.004600
Validation Loss: 0.00519807
Epoch [166/200], Train Loss: 0.004571
Validation Loss: 0.00519484
Epoch [167/200], Train Loss: 0.004568
Validation Loss: 0.00519642
Epoch [168/200], Train Loss: 0.004598
Validation Loss: 0.00519499
Epoch [169/200], Train Loss: 0.004560
Validation Loss: 0.00519159
Epoch [170/200], Train Loss: 0.004581
Validation Loss: 0.00519548
Epoch [171/200], Train Loss: 0.004585
Validation Loss: 0.00519171
Epoch [172/200], Train Loss: 0.004581
Validation Loss: 0.00519126
Epoch [173/200], Train Loss: 0.004598
Validation Loss: 0.00518975
Epoch [174/200], Train Loss: 0.004582
Validation Loss: 0.00519022
Epoch [175/200], Train Loss: 0.004596
Validation Loss: 0.00518798
Epoch [176/200], Train Loss: 0.004591
Validation Loss: 0.00518905
Epoch [177/200], Train Loss: 0.004549
Validation Loss: 0.00518484
Epoch [178/200], Train Loss: 0.004590
Validation Loss: 0.00518967
Epoch [179/200], Train Loss: 0.004570
Validation Loss: 0.00518445
Epoch [180/200], Train Loss: 0.004581
Validation Loss: 0.00518506
Epoch [181/200], Train Loss: 0.004579
Validation Loss: 0.00518299
Epoch [182/200], Train Loss: 0.004565
Validation Loss: 0.00518398
Epoch [183/200], Train Loss: 0.004549
Validation Loss: 0.00518179
Epoch [184/200], Train Loss: 0.004580
Validation Loss: 0.00518203
Epoch [185/200], Train Loss: 0.004582
Validation Loss: 0.00518142
Epoch [186/200], Train Loss: 0.004598
Validation Loss: 0.00517836
Epoch [187/200], Train Loss: 0.004572
Validation Loss: 0.00517818
Epoch [188/200], Train Loss: 0.004609
Validation Loss: 0.00517943
Epoch [189/200], Train Loss: 0.004558
Validation Loss: 0.00517540
Epoch [190/200], Train Loss: 0.004555
Validation Loss: 0.00517776
Epoch [191/200], Train Loss: 0.004555
Validation Loss: 0.00517449
Epoch [192/200], Train Loss: 0.004574
Validation Loss: 0.00517512
Epoch [193/200], Train Loss: 0.004581
Validation Loss: 0.00517110
Epoch [194/200], Train Loss: 0.004567
Validation Loss: 0.00517260
Epoch [195/200], Train Loss: 0.004574
Validation Loss: 0.00517209
Epoch [196/200], Train Loss: 0.004575
Validation Loss: 0.00517023
Epoch [197/200], Train Loss: 0.004553
Validation Loss: 0.00516787
Epoch [198/200], Train Loss: 0.004539
Validation Loss: 0.00516892
Epoch [199/200], Train Loss: 0.004592
Validation Loss: 0.00517169
Epoch [200/200], Train Loss: 0.004546
Validation Loss: 0.00516532

Evaluating model for: Lamp
Run 21/144 completed in 329.41 seconds with: {'MAE': np.float32(3.1080434), 'MSE': np.float32(194.03319), 'RMSE': np.float32(13.92958), 'SAE': np.float32(0.116862796), 'NDE': np.float32(0.9732738)}

Run 22/144: hidden=64, seq_len=360, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.007443
Validation Loss: 0.00721845
Epoch [2/200], Train Loss: 0.006235
Validation Loss: 0.00631750
Epoch [3/200], Train Loss: 0.005475
Validation Loss: 0.00578589
Epoch [4/200], Train Loss: 0.005054
Validation Loss: 0.00550462
Epoch [5/200], Train Loss: 0.004881
Validation Loss: 0.00540080
Epoch [6/200], Train Loss: 0.004778
Validation Loss: 0.00538739
Epoch [7/200], Train Loss: 0.004762
Validation Loss: 0.00538885
Epoch [8/200], Train Loss: 0.004796
Validation Loss: 0.00538553
Epoch [9/200], Train Loss: 0.004776
Validation Loss: 0.00538164
Epoch [10/200], Train Loss: 0.004787
Validation Loss: 0.00537940
Epoch [11/200], Train Loss: 0.004770
Validation Loss: 0.00537747
Epoch [12/200], Train Loss: 0.004766
Validation Loss: 0.00537576
Epoch [13/200], Train Loss: 0.004762
Validation Loss: 0.00537394
Epoch [14/200], Train Loss: 0.004751
Validation Loss: 0.00537165
Epoch [15/200], Train Loss: 0.004755
Validation Loss: 0.00536877
Epoch [16/200], Train Loss: 0.004735
Validation Loss: 0.00536654
Epoch [17/200], Train Loss: 0.004778
Validation Loss: 0.00536378
Epoch [18/200], Train Loss: 0.004745
Validation Loss: 0.00535978
Epoch [19/200], Train Loss: 0.004772
Validation Loss: 0.00535655
Epoch [20/200], Train Loss: 0.004727
Validation Loss: 0.00535274
Epoch [21/200], Train Loss: 0.004747
Validation Loss: 0.00534871
Epoch [22/200], Train Loss: 0.004742
Validation Loss: 0.00534474
Epoch [23/200], Train Loss: 0.004730
Validation Loss: 0.00533864
Epoch [24/200], Train Loss: 0.004728
Validation Loss: 0.00533135
Epoch [25/200], Train Loss: 0.004717
Validation Loss: 0.00532379
Epoch [26/200], Train Loss: 0.004721
Validation Loss: 0.00531547
Epoch [27/200], Train Loss: 0.004717
Validation Loss: 0.00530763
Epoch [28/200], Train Loss: 0.004688
Validation Loss: 0.00529794
Epoch [29/200], Train Loss: 0.004686
Validation Loss: 0.00529007
Epoch [30/200], Train Loss: 0.004690
Validation Loss: 0.00528332
Epoch [31/200], Train Loss: 0.004691
Validation Loss: 0.00528015
Epoch [32/200], Train Loss: 0.004694
Validation Loss: 0.00527593
Epoch [33/200], Train Loss: 0.004684
Validation Loss: 0.00527410
Epoch [34/200], Train Loss: 0.004679
Validation Loss: 0.00527383
Epoch [35/200], Train Loss: 0.004670
Validation Loss: 0.00527180
Epoch [36/200], Train Loss: 0.004660
Validation Loss: 0.00527302
Epoch [37/200], Train Loss: 0.004666
Validation Loss: 0.00527023
Epoch [38/200], Train Loss: 0.004670
Validation Loss: 0.00527122
Epoch [39/200], Train Loss: 0.004686
Validation Loss: 0.00527006
Epoch [40/200], Train Loss: 0.004663
Validation Loss: 0.00526796
Epoch [41/200], Train Loss: 0.004672
Validation Loss: 0.00526953
Epoch [42/200], Train Loss: 0.004670
Validation Loss: 0.00526686
Epoch [43/200], Train Loss: 0.004678
Validation Loss: 0.00526807
Epoch [44/200], Train Loss: 0.004667
Validation Loss: 0.00526678
Epoch [45/200], Train Loss: 0.004673
Validation Loss: 0.00526582
Epoch [46/200], Train Loss: 0.004665
Validation Loss: 0.00526493
Epoch [47/200], Train Loss: 0.004684
Validation Loss: 0.00526627
Epoch [48/200], Train Loss: 0.004662
Validation Loss: 0.00526347
Epoch [49/200], Train Loss: 0.004645
Validation Loss: 0.00526369
Epoch [50/200], Train Loss: 0.004658
Validation Loss: 0.00526397
Epoch [51/200], Train Loss: 0.004688
Validation Loss: 0.00526357
Epoch [52/200], Train Loss: 0.004662
Validation Loss: 0.00526269
Epoch [53/200], Train Loss: 0.004679
Validation Loss: 0.00526174
Epoch [54/200], Train Loss: 0.004656
Validation Loss: 0.00526111
Epoch [55/200], Train Loss: 0.004655
Validation Loss: 0.00526074
Epoch [56/200], Train Loss: 0.004652
Validation Loss: 0.00526169
Epoch [57/200], Train Loss: 0.004642
Validation Loss: 0.00526208
Epoch [58/200], Train Loss: 0.004640
Validation Loss: 0.00526010
Epoch [59/200], Train Loss: 0.004667
Validation Loss: 0.00526097
Epoch [60/200], Train Loss: 0.004657
Validation Loss: 0.00525963
Epoch [61/200], Train Loss: 0.004650
Validation Loss: 0.00525983
Epoch [62/200], Train Loss: 0.004668
Validation Loss: 0.00525926
Epoch [63/200], Train Loss: 0.004633
Validation Loss: 0.00525807
Epoch [64/200], Train Loss: 0.004659
Validation Loss: 0.00525856
Epoch [65/200], Train Loss: 0.004664
Validation Loss: 0.00525845
Epoch [66/200], Train Loss: 0.004680
Validation Loss: 0.00525843
Epoch [67/200], Train Loss: 0.004636
Validation Loss: 0.00525657
Epoch [68/200], Train Loss: 0.004638
Validation Loss: 0.00525756
Epoch [69/200], Train Loss: 0.004644
Validation Loss: 0.00525716
Epoch [70/200], Train Loss: 0.004639
Validation Loss: 0.00525741
Epoch [71/200], Train Loss: 0.004630
Validation Loss: 0.00525637
Epoch [72/200], Train Loss: 0.004656
Validation Loss: 0.00525618
Epoch [73/200], Train Loss: 0.004663
Validation Loss: 0.00525803
Epoch [74/200], Train Loss: 0.004640
Validation Loss: 0.00525493
Epoch [75/200], Train Loss: 0.004637
Validation Loss: 0.00525710
Epoch [76/200], Train Loss: 0.004630
Validation Loss: 0.00525467
Epoch [77/200], Train Loss: 0.004637
Validation Loss: 0.00525645
Epoch [78/200], Train Loss: 0.004627
Validation Loss: 0.00525550
Epoch [79/200], Train Loss: 0.004643
Validation Loss: 0.00525552
Epoch [80/200], Train Loss: 0.004662
Validation Loss: 0.00525546
Epoch [81/200], Train Loss: 0.004636
Validation Loss: 0.00525373
Epoch [82/200], Train Loss: 0.004654
Validation Loss: 0.00525801
Epoch [83/200], Train Loss: 0.004656
Validation Loss: 0.00525399
Epoch [84/200], Train Loss: 0.004670
Validation Loss: 0.00525448
Epoch [85/200], Train Loss: 0.004654
Validation Loss: 0.00525485
Epoch [86/200], Train Loss: 0.004648
Validation Loss: 0.00525399
Epoch [87/200], Train Loss: 0.004661
Validation Loss: 0.00525349
Epoch [88/200], Train Loss: 0.004628
Validation Loss: 0.00525257
Epoch [89/200], Train Loss: 0.004623
Validation Loss: 0.00525243
Epoch [90/200], Train Loss: 0.004639
Validation Loss: 0.00525344
Epoch [91/200], Train Loss: 0.004633
Validation Loss: 0.00525213
Epoch [92/200], Train Loss: 0.004645
Validation Loss: 0.00525319
Epoch [93/200], Train Loss: 0.004650
Validation Loss: 0.00525385
Epoch [94/200], Train Loss: 0.004641
Validation Loss: 0.00525260
Epoch [95/200], Train Loss: 0.004659
Validation Loss: 0.00525285
Epoch [96/200], Train Loss: 0.004667
Validation Loss: 0.00525238
Epoch [97/200], Train Loss: 0.004635
Validation Loss: 0.00525237
Epoch [98/200], Train Loss: 0.004658
Validation Loss: 0.00525358
Epoch [99/200], Train Loss: 0.004652
Validation Loss: 0.00525086
Epoch [100/200], Train Loss: 0.004640
Validation Loss: 0.00525165
Epoch [101/200], Train Loss: 0.004660
Validation Loss: 0.00525217
Epoch [102/200], Train Loss: 0.004608
Validation Loss: 0.00525080
Epoch [103/200], Train Loss: 0.004644
Validation Loss: 0.00525226
Epoch [104/200], Train Loss: 0.004629
Validation Loss: 0.00525073
Epoch [105/200], Train Loss: 0.004626
Validation Loss: 0.00525095
Epoch [106/200], Train Loss: 0.004645
Validation Loss: 0.00525413
Epoch [107/200], Train Loss: 0.004635
Validation Loss: 0.00525059
Epoch [108/200], Train Loss: 0.004637
Validation Loss: 0.00525005
Epoch [109/200], Train Loss: 0.004630
Validation Loss: 0.00525140
Epoch [110/200], Train Loss: 0.004633
Validation Loss: 0.00525140
Epoch [111/200], Train Loss: 0.004637
Validation Loss: 0.00524992
Epoch [112/200], Train Loss: 0.004644
Validation Loss: 0.00525297
Epoch [113/200], Train Loss: 0.004621
Validation Loss: 0.00525067
Epoch [114/200], Train Loss: 0.004635
Validation Loss: 0.00525122
Epoch [115/200], Train Loss: 0.004640
Validation Loss: 0.00525086
Epoch [116/200], Train Loss: 0.004626
Validation Loss: 0.00524899
Epoch [117/200], Train Loss: 0.004614
Validation Loss: 0.00525006
Epoch [118/200], Train Loss: 0.004641
Validation Loss: 0.00525204
Epoch [119/200], Train Loss: 0.004609
Validation Loss: 0.00524957
Epoch [120/200], Train Loss: 0.004630
Validation Loss: 0.00525048
Epoch [121/200], Train Loss: 0.004632
Validation Loss: 0.00525193
Epoch [122/200], Train Loss: 0.004658
Validation Loss: 0.00525057
Epoch [123/200], Train Loss: 0.004642
Validation Loss: 0.00525032
Epoch [124/200], Train Loss: 0.004621
Validation Loss: 0.00525084
Epoch [125/200], Train Loss: 0.004653
Validation Loss: 0.00525242
Epoch [126/200], Train Loss: 0.004634
Validation Loss: 0.00524927
Early stopping triggered

Evaluating model for: Lamp
Run 22/144 completed in 216.23 seconds with: {'MAE': np.float32(3.1164432), 'MSE': np.float32(196.36562), 'RMSE': np.float32(14.013052), 'SAE': np.float32(0.0993984), 'NDE': np.float32(0.9791062)}

Run 23/144: hidden=64, seq_len=360, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.012407
Validation Loss: 0.01189915
Epoch [2/200], Train Loss: 0.010130
Validation Loss: 0.00980182
Epoch [3/200], Train Loss: 0.008206
Validation Loss: 0.00812197
Epoch [4/200], Train Loss: 0.006812
Validation Loss: 0.00691104
Epoch [5/200], Train Loss: 0.005801
Validation Loss: 0.00604453
Epoch [6/200], Train Loss: 0.005128
Validation Loss: 0.00551934
Epoch [7/200], Train Loss: 0.004816
Validation Loss: 0.00537924
Epoch [8/200], Train Loss: 0.004806
Validation Loss: 0.00538258
Epoch [9/200], Train Loss: 0.004794
Validation Loss: 0.00537802
Epoch [10/200], Train Loss: 0.004763
Validation Loss: 0.00537947
Epoch [11/200], Train Loss: 0.004775
Validation Loss: 0.00537801
Epoch [12/200], Train Loss: 0.004772
Validation Loss: 0.00537435
Epoch [13/200], Train Loss: 0.004769
Validation Loss: 0.00537035
Epoch [14/200], Train Loss: 0.004760
Validation Loss: 0.00536850
Epoch [15/200], Train Loss: 0.004753
Validation Loss: 0.00536609
Epoch [16/200], Train Loss: 0.004767
Validation Loss: 0.00536296
Epoch [17/200], Train Loss: 0.004770
Validation Loss: 0.00536054
Epoch [18/200], Train Loss: 0.004761
Validation Loss: 0.00535629
Epoch [19/200], Train Loss: 0.004782
Validation Loss: 0.00535226
Epoch [20/200], Train Loss: 0.004745
Validation Loss: 0.00534873
Epoch [21/200], Train Loss: 0.004760
Validation Loss: 0.00534447
Epoch [22/200], Train Loss: 0.004738
Validation Loss: 0.00533971
Epoch [23/200], Train Loss: 0.004754
Validation Loss: 0.00533456
Epoch [24/200], Train Loss: 0.004739
Validation Loss: 0.00532768
Epoch [25/200], Train Loss: 0.004739
Validation Loss: 0.00532520
Epoch [26/200], Train Loss: 0.004744
Validation Loss: 0.00532021
Epoch [27/200], Train Loss: 0.004719
Validation Loss: 0.00531354
Epoch [28/200], Train Loss: 0.004752
Validation Loss: 0.00531083
Epoch [29/200], Train Loss: 0.004721
Validation Loss: 0.00530474
Epoch [30/200], Train Loss: 0.004731
Validation Loss: 0.00530261
Epoch [31/200], Train Loss: 0.004713
Validation Loss: 0.00530105
Epoch [32/200], Train Loss: 0.004717
Validation Loss: 0.00529885
Epoch [33/200], Train Loss: 0.004694
Validation Loss: 0.00529590
Epoch [34/200], Train Loss: 0.004726
Validation Loss: 0.00529917
Epoch [35/200], Train Loss: 0.004708
Validation Loss: 0.00529364
Epoch [36/200], Train Loss: 0.004716
Validation Loss: 0.00529449
Epoch [37/200], Train Loss: 0.004711
Validation Loss: 0.00529342
Epoch [38/200], Train Loss: 0.004703
Validation Loss: 0.00529188
Epoch [39/200], Train Loss: 0.004684
Validation Loss: 0.00529130
Epoch [40/200], Train Loss: 0.004698
Validation Loss: 0.00529362
Epoch [41/200], Train Loss: 0.004701
Validation Loss: 0.00528926
Epoch [42/200], Train Loss: 0.004713
Validation Loss: 0.00529213
Epoch [43/200], Train Loss: 0.004699
Validation Loss: 0.00528934
Epoch [44/200], Train Loss: 0.004698
Validation Loss: 0.00528784
Epoch [45/200], Train Loss: 0.004714
Validation Loss: 0.00528903
Epoch [46/200], Train Loss: 0.004695
Validation Loss: 0.00528806
Epoch [47/200], Train Loss: 0.004685
Validation Loss: 0.00528794
Epoch [48/200], Train Loss: 0.004700
Validation Loss: 0.00528696
Epoch [49/200], Train Loss: 0.004718
Validation Loss: 0.00528792
Epoch [50/200], Train Loss: 0.004698
Validation Loss: 0.00528593
Epoch [51/200], Train Loss: 0.004712
Validation Loss: 0.00528690
Epoch [52/200], Train Loss: 0.004701
Validation Loss: 0.00528435
Epoch [53/200], Train Loss: 0.004715
Validation Loss: 0.00528477
Epoch [54/200], Train Loss: 0.004692
Validation Loss: 0.00528508
Epoch [55/200], Train Loss: 0.004725
Validation Loss: 0.00528454
Epoch [56/200], Train Loss: 0.004671
Validation Loss: 0.00528241
Epoch [57/200], Train Loss: 0.004695
Validation Loss: 0.00528445
Epoch [58/200], Train Loss: 0.004709
Validation Loss: 0.00528210
Epoch [59/200], Train Loss: 0.004688
Validation Loss: 0.00528251
Epoch [60/200], Train Loss: 0.004678
Validation Loss: 0.00528372
Epoch [61/200], Train Loss: 0.004715
Validation Loss: 0.00528286
Epoch [62/200], Train Loss: 0.004704
Validation Loss: 0.00528172
Epoch [63/200], Train Loss: 0.004677
Validation Loss: 0.00528380
Epoch [64/200], Train Loss: 0.004668
Validation Loss: 0.00528082
Epoch [65/200], Train Loss: 0.004659
Validation Loss: 0.00528110
Epoch [66/200], Train Loss: 0.004701
Validation Loss: 0.00528325
Epoch [67/200], Train Loss: 0.004704
Validation Loss: 0.00528056
Epoch [68/200], Train Loss: 0.004657
Validation Loss: 0.00528324
Epoch [69/200], Train Loss: 0.004671
Validation Loss: 0.00528258
Epoch [70/200], Train Loss: 0.004662
Validation Loss: 0.00528234
Epoch [71/200], Train Loss: 0.004663
Validation Loss: 0.00528269
Epoch [72/200], Train Loss: 0.004685
Validation Loss: 0.00528316
Epoch [73/200], Train Loss: 0.004665
Validation Loss: 0.00528408
Epoch [74/200], Train Loss: 0.004710
Validation Loss: 0.00528694
Epoch [75/200], Train Loss: 0.004700
Validation Loss: 0.00528396
Epoch [76/200], Train Loss: 0.004685
Validation Loss: 0.00528394
Epoch [77/200], Train Loss: 0.004675
Validation Loss: 0.00528707
Early stopping triggered

Evaluating model for: Lamp
Run 23/144 completed in 131.32 seconds with: {'MAE': np.float32(3.0003636), 'MSE': np.float32(197.69092), 'RMSE': np.float32(14.06026), 'SAE': np.float32(0.28292322), 'NDE': np.float32(0.98240465)}

Run 24/144: hidden=64, seq_len=360, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.005240
Validation Loss: 0.00555482
Epoch [2/200], Train Loss: 0.004849
Validation Loss: 0.00537846
Epoch [3/200], Train Loss: 0.004706
Validation Loss: 0.00538641
Epoch [4/200], Train Loss: 0.004776
Validation Loss: 0.00538523
Epoch [5/200], Train Loss: 0.004739
Validation Loss: 0.00537461
Epoch [6/200], Train Loss: 0.004725
Validation Loss: 0.00537388
Epoch [7/200], Train Loss: 0.004720
Validation Loss: 0.00537370
Epoch [8/200], Train Loss: 0.004722
Validation Loss: 0.00537402
Epoch [9/200], Train Loss: 0.004716
Validation Loss: 0.00537302
Epoch [10/200], Train Loss: 0.004733
Validation Loss: 0.00537264
Epoch [11/200], Train Loss: 0.004722
Validation Loss: 0.00537142
Epoch [12/200], Train Loss: 0.004716
Validation Loss: 0.00536939
Epoch [13/200], Train Loss: 0.004729
Validation Loss: 0.00536785
Epoch [14/200], Train Loss: 0.004720
Validation Loss: 0.00536453
Epoch [15/200], Train Loss: 0.004707
Validation Loss: 0.00535665
Epoch [16/200], Train Loss: 0.004702
Validation Loss: 0.00534828
Epoch [17/200], Train Loss: 0.004681
Validation Loss: 0.00533272
Epoch [18/200], Train Loss: 0.004693
Validation Loss: 0.00530683
Epoch [19/200], Train Loss: 0.004665
Validation Loss: 0.00527894
Epoch [20/200], Train Loss: 0.004641
Validation Loss: 0.00527035
Epoch [21/200], Train Loss: 0.004651
Validation Loss: 0.00526378
Epoch [22/200], Train Loss: 0.004635
Validation Loss: 0.00526522
Epoch [23/200], Train Loss: 0.004639
Validation Loss: 0.00526376
Epoch [24/200], Train Loss: 0.004631
Validation Loss: 0.00526195
Epoch [25/200], Train Loss: 0.004627
Validation Loss: 0.00526275
Epoch [26/200], Train Loss: 0.004635
Validation Loss: 0.00526048
Epoch [27/200], Train Loss: 0.004648
Validation Loss: 0.00526323
Epoch [28/200], Train Loss: 0.004646
Validation Loss: 0.00525992
Epoch [29/200], Train Loss: 0.004636
Validation Loss: 0.00526023
Epoch [30/200], Train Loss: 0.004643
Validation Loss: 0.00525845
Epoch [31/200], Train Loss: 0.004617
Validation Loss: 0.00526013
Epoch [32/200], Train Loss: 0.004631
Validation Loss: 0.00525797
Epoch [33/200], Train Loss: 0.004647
Validation Loss: 0.00525841
Epoch [34/200], Train Loss: 0.004631
Validation Loss: 0.00525644
Epoch [35/200], Train Loss: 0.004624
Validation Loss: 0.00525628
Epoch [36/200], Train Loss: 0.004621
Validation Loss: 0.00525554
Epoch [37/200], Train Loss: 0.004608
Validation Loss: 0.00525604
Epoch [38/200], Train Loss: 0.004622
Validation Loss: 0.00525410
Epoch [39/200], Train Loss: 0.004648
Validation Loss: 0.00525525
Epoch [40/200], Train Loss: 0.004632
Validation Loss: 0.00525465
Epoch [41/200], Train Loss: 0.004637
Validation Loss: 0.00525272
Epoch [42/200], Train Loss: 0.004637
Validation Loss: 0.00525336
Epoch [43/200], Train Loss: 0.004634
Validation Loss: 0.00525302
Epoch [44/200], Train Loss: 0.004620
Validation Loss: 0.00525208
Epoch [45/200], Train Loss: 0.004628
Validation Loss: 0.00525262
Epoch [46/200], Train Loss: 0.004633
Validation Loss: 0.00525145
Epoch [47/200], Train Loss: 0.004615
Validation Loss: 0.00525040
Epoch [48/200], Train Loss: 0.004619
Validation Loss: 0.00525210
Epoch [49/200], Train Loss: 0.004625
Validation Loss: 0.00525005
Epoch [50/200], Train Loss: 0.004625
Validation Loss: 0.00525256
Epoch [51/200], Train Loss: 0.004617
Validation Loss: 0.00524945
Epoch [52/200], Train Loss: 0.004630
Validation Loss: 0.00525062
Epoch [53/200], Train Loss: 0.004613
Validation Loss: 0.00524947
Epoch [54/200], Train Loss: 0.004626
Validation Loss: 0.00525153
Epoch [55/200], Train Loss: 0.004616
Validation Loss: 0.00524982
Epoch [56/200], Train Loss: 0.004672
Validation Loss: 0.00525214
Epoch [57/200], Train Loss: 0.004651
Validation Loss: 0.00524932
Epoch [58/200], Train Loss: 0.004612
Validation Loss: 0.00524916
Epoch [59/200], Train Loss: 0.004605
Validation Loss: 0.00525007
Epoch [60/200], Train Loss: 0.004610
Validation Loss: 0.00525029
Epoch [61/200], Train Loss: 0.004603
Validation Loss: 0.00525086
Epoch [62/200], Train Loss: 0.004611
Validation Loss: 0.00524934
Epoch [63/200], Train Loss: 0.004621
Validation Loss: 0.00524985
Epoch [64/200], Train Loss: 0.004595
Validation Loss: 0.00524886
Epoch [65/200], Train Loss: 0.004605
Validation Loss: 0.00525094
Epoch [66/200], Train Loss: 0.004607
Validation Loss: 0.00524962
Epoch [67/200], Train Loss: 0.004619
Validation Loss: 0.00525300
Epoch [68/200], Train Loss: 0.004625
Validation Loss: 0.00524928
Epoch [69/200], Train Loss: 0.004629
Validation Loss: 0.00525191
Epoch [70/200], Train Loss: 0.004627
Validation Loss: 0.00524893
Epoch [71/200], Train Loss: 0.004617
Validation Loss: 0.00525052
Epoch [72/200], Train Loss: 0.004610
Validation Loss: 0.00525238
Epoch [73/200], Train Loss: 0.004612
Validation Loss: 0.00524969
Epoch [74/200], Train Loss: 0.004622
Validation Loss: 0.00525100
Early stopping triggered

Evaluating model for: Lamp
Run 24/144 completed in 116.67 seconds with: {'MAE': np.float32(2.9407372), 'MSE': np.float32(196.28465), 'RMSE': np.float32(14.010162), 'SAE': np.float32(0.20398071), 'NDE': np.float32(0.97890425)}

Run 25/144: hidden=64, seq_len=720, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.006002
Validation Loss: 0.00543215
Epoch [2/200], Train Loss: 0.005061
Validation Loss: 0.00498237
Epoch [3/200], Train Loss: 0.004884
Validation Loss: 0.00496789
Epoch [4/200], Train Loss: 0.004894
Validation Loss: 0.00496062
Epoch [5/200], Train Loss: 0.004884
Validation Loss: 0.00495165
Epoch [6/200], Train Loss: 0.004857
Validation Loss: 0.00494172
Epoch [7/200], Train Loss: 0.004838
Validation Loss: 0.00493259
Epoch [8/200], Train Loss: 0.004821
Validation Loss: 0.00492161
Epoch [9/200], Train Loss: 0.004833
Validation Loss: 0.00491449
Epoch [10/200], Train Loss: 0.004816
Validation Loss: 0.00490945
Epoch [11/200], Train Loss: 0.004826
Validation Loss: 0.00490761
Epoch [12/200], Train Loss: 0.004806
Validation Loss: 0.00490619
Epoch [13/200], Train Loss: 0.004811
Validation Loss: 0.00490510
Epoch [14/200], Train Loss: 0.004806
Validation Loss: 0.00490413
Epoch [15/200], Train Loss: 0.004812
Validation Loss: 0.00490319
Epoch [16/200], Train Loss: 0.004809
Validation Loss: 0.00490307
Epoch [17/200], Train Loss: 0.004828
Validation Loss: 0.00490146
Epoch [18/200], Train Loss: 0.004805
Validation Loss: 0.00490003
Epoch [19/200], Train Loss: 0.004815
Validation Loss: 0.00489912
Epoch [20/200], Train Loss: 0.004798
Validation Loss: 0.00489876
Epoch [21/200], Train Loss: 0.004794
Validation Loss: 0.00489693
Epoch [22/200], Train Loss: 0.004818
Validation Loss: 0.00489587
Epoch [23/200], Train Loss: 0.004790
Validation Loss: 0.00489457
Epoch [24/200], Train Loss: 0.004789
Validation Loss: 0.00489252
Epoch [25/200], Train Loss: 0.004776
Validation Loss: 0.00489103
Epoch [26/200], Train Loss: 0.004786
Validation Loss: 0.00489304
Epoch [27/200], Train Loss: 0.004789
Validation Loss: 0.00489106
Epoch [28/200], Train Loss: 0.004777
Validation Loss: 0.00488499
Epoch [29/200], Train Loss: 0.004779
Validation Loss: 0.00488172
Epoch [30/200], Train Loss: 0.004769
Validation Loss: 0.00488322
Epoch [31/200], Train Loss: 0.004770
Validation Loss: 0.00487859
Epoch [32/200], Train Loss: 0.004783
Validation Loss: 0.00487940
Epoch [33/200], Train Loss: 0.004782
Validation Loss: 0.00487220
Epoch [34/200], Train Loss: 0.004757
Validation Loss: 0.00487036
Epoch [35/200], Train Loss: 0.004771
Validation Loss: 0.00486933
Epoch [36/200], Train Loss: 0.004775
Validation Loss: 0.00486547
Epoch [37/200], Train Loss: 0.004774
Validation Loss: 0.00486338
Epoch [38/200], Train Loss: 0.004766
Validation Loss: 0.00486054
Epoch [39/200], Train Loss: 0.004748
Validation Loss: 0.00485831
Epoch [40/200], Train Loss: 0.004751
Validation Loss: 0.00485712
Epoch [41/200], Train Loss: 0.004754
Validation Loss: 0.00485343
Epoch [42/200], Train Loss: 0.004764
Validation Loss: 0.00484913
Epoch [43/200], Train Loss: 0.004762
Validation Loss: 0.00484525
Epoch [44/200], Train Loss: 0.004760
Validation Loss: 0.00484084
Epoch [45/200], Train Loss: 0.004735
Validation Loss: 0.00484374
Epoch [46/200], Train Loss: 0.004750
Validation Loss: 0.00483199
Epoch [47/200], Train Loss: 0.004733
Validation Loss: 0.00482678
Epoch [48/200], Train Loss: 0.004746
Validation Loss: 0.00482289
Epoch [49/200], Train Loss: 0.004731
Validation Loss: 0.00481721
Epoch [50/200], Train Loss: 0.004713
Validation Loss: 0.00480868
Epoch [51/200], Train Loss: 0.004724
Validation Loss: 0.00480811
Epoch [52/200], Train Loss: 0.004715
Validation Loss: 0.00479299
Epoch [53/200], Train Loss: 0.004710
Validation Loss: 0.00479123
Epoch [54/200], Train Loss: 0.004701
Validation Loss: 0.00477480
Epoch [55/200], Train Loss: 0.004692
Validation Loss: 0.00477816
Epoch [56/200], Train Loss: 0.004697
Validation Loss: 0.00475785
Epoch [57/200], Train Loss: 0.004683
Validation Loss: 0.00474534
Epoch [58/200], Train Loss: 0.004650
Validation Loss: 0.00472879
Epoch [59/200], Train Loss: 0.004655
Validation Loss: 0.00472089
Epoch [60/200], Train Loss: 0.004648
Validation Loss: 0.00470529
Epoch [61/200], Train Loss: 0.004636
Validation Loss: 0.00468786
Epoch [62/200], Train Loss: 0.004631
Validation Loss: 0.00467506
Epoch [63/200], Train Loss: 0.004613
Validation Loss: 0.00467542
Epoch [64/200], Train Loss: 0.004603
Validation Loss: 0.00464025
Epoch [65/200], Train Loss: 0.004577
Validation Loss: 0.00462192
Epoch [66/200], Train Loss: 0.004567
Validation Loss: 0.00460069
Epoch [67/200], Train Loss: 0.004544
Validation Loss: 0.00458736
Epoch [68/200], Train Loss: 0.004518
Validation Loss: 0.00461774
Epoch [69/200], Train Loss: 0.004530
Validation Loss: 0.00456040
Epoch [70/200], Train Loss: 0.004480
Validation Loss: 0.00452072
Epoch [71/200], Train Loss: 0.004481
Validation Loss: 0.00448017
Epoch [72/200], Train Loss: 0.004451
Validation Loss: 0.00445751
Epoch [73/200], Train Loss: 0.004421
Validation Loss: 0.00449070
Epoch [74/200], Train Loss: 0.004412
Validation Loss: 0.00442026
Epoch [75/200], Train Loss: 0.004405
Validation Loss: 0.00438357
Epoch [76/200], Train Loss: 0.004384
Validation Loss: 0.00436717
Epoch [77/200], Train Loss: 0.004347
Validation Loss: 0.00434258
Epoch [78/200], Train Loss: 0.004340
Validation Loss: 0.00431646
Epoch [79/200], Train Loss: 0.004306
Validation Loss: 0.00431479
Epoch [80/200], Train Loss: 0.004292
Validation Loss: 0.00427953
Epoch [81/200], Train Loss: 0.004285
Validation Loss: 0.00426241
Epoch [82/200], Train Loss: 0.004226
Validation Loss: 0.00423550
Epoch [83/200], Train Loss: 0.004235
Validation Loss: 0.00418147
Epoch [84/200], Train Loss: 0.004193
Validation Loss: 0.00414341
Epoch [85/200], Train Loss: 0.004155
Validation Loss: 0.00414510
Epoch [86/200], Train Loss: 0.004124
Validation Loss: 0.00410132
Epoch [87/200], Train Loss: 0.004114
Validation Loss: 0.00404377
Epoch [88/200], Train Loss: 0.004093
Validation Loss: 0.00400978
Epoch [89/200], Train Loss: 0.004056
Validation Loss: 0.00396292
Epoch [90/200], Train Loss: 0.004012
Validation Loss: 0.00391854
Epoch [91/200], Train Loss: 0.003985
Validation Loss: 0.00388470
Epoch [92/200], Train Loss: 0.003939
Validation Loss: 0.00386271
Epoch [93/200], Train Loss: 0.003899
Validation Loss: 0.00381338
Epoch [94/200], Train Loss: 0.003859
Validation Loss: 0.00375239
Epoch [95/200], Train Loss: 0.003815
Validation Loss: 0.00372749
Epoch [96/200], Train Loss: 0.003776
Validation Loss: 0.00365322
Epoch [97/200], Train Loss: 0.003722
Validation Loss: 0.00366275
Epoch [98/200], Train Loss: 0.003688
Validation Loss: 0.00354885
Epoch [99/200], Train Loss: 0.003631
Validation Loss: 0.00350431
Epoch [100/200], Train Loss: 0.003601
Validation Loss: 0.00345278
Epoch [101/200], Train Loss: 0.003539
Validation Loss: 0.00340176
Epoch [102/200], Train Loss: 0.003497
Validation Loss: 0.00332284
Epoch [103/200], Train Loss: 0.003440
Validation Loss: 0.00330364
Epoch [104/200], Train Loss: 0.003387
Validation Loss: 0.00324539
Epoch [105/200], Train Loss: 0.003323
Validation Loss: 0.00319902
Epoch [106/200], Train Loss: 0.003286
Validation Loss: 0.00319419
Epoch [107/200], Train Loss: 0.003239
Validation Loss: 0.00308806
Epoch [108/200], Train Loss: 0.003189
Validation Loss: 0.00304285
Epoch [109/200], Train Loss: 0.003157
Validation Loss: 0.00305281
Epoch [110/200], Train Loss: 0.003137
Validation Loss: 0.00293244
Epoch [111/200], Train Loss: 0.003055
Validation Loss: 0.00287603
Epoch [112/200], Train Loss: 0.003038
Validation Loss: 0.00290584
Epoch [113/200], Train Loss: 0.002975
Validation Loss: 0.00280079
Epoch [114/200], Train Loss: 0.002921
Validation Loss: 0.00277552
Epoch [115/200], Train Loss: 0.002867
Validation Loss: 0.00271680
Epoch [116/200], Train Loss: 0.002844
Validation Loss: 0.00270524
Epoch [117/200], Train Loss: 0.002812
Validation Loss: 0.00261530
Epoch [118/200], Train Loss: 0.002758
Validation Loss: 0.00261722
Epoch [119/200], Train Loss: 0.002717
Validation Loss: 0.00258412
Epoch [120/200], Train Loss: 0.002693
Validation Loss: 0.00251511
Epoch [121/200], Train Loss: 0.002657
Validation Loss: 0.00251295
Epoch [122/200], Train Loss: 0.002620
Validation Loss: 0.00250239
Epoch [123/200], Train Loss: 0.002592
Validation Loss: 0.00248259
Epoch [124/200], Train Loss: 0.002569
Validation Loss: 0.00243615
Epoch [125/200], Train Loss: 0.002532
Validation Loss: 0.00243365
Epoch [126/200], Train Loss: 0.002505
Validation Loss: 0.00240160
Epoch [127/200], Train Loss: 0.002476
Validation Loss: 0.00238626
Epoch [128/200], Train Loss: 0.002457
Validation Loss: 0.00239252
Epoch [129/200], Train Loss: 0.002437
Validation Loss: 0.00230522
Epoch [130/200], Train Loss: 0.002409
Validation Loss: 0.00231699
Epoch [131/200], Train Loss: 0.002384
Validation Loss: 0.00230358
Epoch [132/200], Train Loss: 0.002367
Validation Loss: 0.00228522
Epoch [133/200], Train Loss: 0.002352
Validation Loss: 0.00228083
Epoch [134/200], Train Loss: 0.002330
Validation Loss: 0.00225071
Epoch [135/200], Train Loss: 0.002310
Validation Loss: 0.00219645
Epoch [136/200], Train Loss: 0.002279
Validation Loss: 0.00219263
Epoch [137/200], Train Loss: 0.002269
Validation Loss: 0.00222358
Epoch [138/200], Train Loss: 0.002268
Validation Loss: 0.00217691
Epoch [139/200], Train Loss: 0.002261
Validation Loss: 0.00215392
Epoch [140/200], Train Loss: 0.002234
Validation Loss: 0.00212882
Epoch [141/200], Train Loss: 0.002211
Validation Loss: 0.00215664
Epoch [142/200], Train Loss: 0.002198
Validation Loss: 0.00213155
Epoch [143/200], Train Loss: 0.002190
Validation Loss: 0.00209709
Epoch [144/200], Train Loss: 0.002170
Validation Loss: 0.00212346
Epoch [145/200], Train Loss: 0.002149
Validation Loss: 0.00207503
Epoch [146/200], Train Loss: 0.002142
Validation Loss: 0.00207306
Epoch [147/200], Train Loss: 0.002130
Validation Loss: 0.00207327
Epoch [148/200], Train Loss: 0.002119
Validation Loss: 0.00202524
Epoch [149/200], Train Loss: 0.002120
Validation Loss: 0.00203545
Epoch [150/200], Train Loss: 0.002106
Validation Loss: 0.00204238
Epoch [151/200], Train Loss: 0.002080
Validation Loss: 0.00203876
Epoch [152/200], Train Loss: 0.002076
Validation Loss: 0.00202524
Epoch [153/200], Train Loss: 0.002071
Validation Loss: 0.00198166
Epoch [154/200], Train Loss: 0.002052
Validation Loss: 0.00202630
Epoch [155/200], Train Loss: 0.002046
Validation Loss: 0.00199407
Epoch [156/200], Train Loss: 0.002034
Validation Loss: 0.00197756
Epoch [157/200], Train Loss: 0.002029
Validation Loss: 0.00199920
Epoch [158/200], Train Loss: 0.002022
Validation Loss: 0.00196517
Epoch [159/200], Train Loss: 0.002008
Validation Loss: 0.00196087
Epoch [160/200], Train Loss: 0.002001
Validation Loss: 0.00192989
Epoch [161/200], Train Loss: 0.001987
Validation Loss: 0.00195228
Epoch [162/200], Train Loss: 0.001981
Validation Loss: 0.00193071
Epoch [163/200], Train Loss: 0.001974
Validation Loss: 0.00193525
Epoch [164/200], Train Loss: 0.001957
Validation Loss: 0.00193379
Epoch [165/200], Train Loss: 0.001948
Validation Loss: 0.00190806
Epoch [166/200], Train Loss: 0.001938
Validation Loss: 0.00192802
Epoch [167/200], Train Loss: 0.001937
Validation Loss: 0.00190484
Epoch [168/200], Train Loss: 0.001934
Validation Loss: 0.00188052
Epoch [169/200], Train Loss: 0.001916
Validation Loss: 0.00189714
Epoch [170/200], Train Loss: 0.001918
Validation Loss: 0.00188186
Epoch [171/200], Train Loss: 0.001913
Validation Loss: 0.00188853
Epoch [172/200], Train Loss: 0.001895
Validation Loss: 0.00186214
Epoch [173/200], Train Loss: 0.001904
Validation Loss: 0.00186805
Epoch [174/200], Train Loss: 0.001898
Validation Loss: 0.00185755
Epoch [175/200], Train Loss: 0.001875
Validation Loss: 0.00184743
Epoch [176/200], Train Loss: 0.001878
Validation Loss: 0.00185977
Epoch [177/200], Train Loss: 0.001870
Validation Loss: 0.00183876
Epoch [178/200], Train Loss: 0.001856
Validation Loss: 0.00186531
Epoch [179/200], Train Loss: 0.001850
Validation Loss: 0.00183745
Epoch [180/200], Train Loss: 0.001849
Validation Loss: 0.00180942
Epoch [181/200], Train Loss: 0.001836
Validation Loss: 0.00182041
Epoch [182/200], Train Loss: 0.001829
Validation Loss: 0.00181568
Epoch [183/200], Train Loss: 0.001834
Validation Loss: 0.00182550
Epoch [184/200], Train Loss: 0.001818
Validation Loss: 0.00180534
Epoch [185/200], Train Loss: 0.001817
Validation Loss: 0.00181760
Epoch [186/200], Train Loss: 0.001813
Validation Loss: 0.00179644
Epoch [187/200], Train Loss: 0.001808
Validation Loss: 0.00179553
Epoch [188/200], Train Loss: 0.001804
Validation Loss: 0.00178698
Epoch [189/200], Train Loss: 0.001801
Validation Loss: 0.00178362
Epoch [190/200], Train Loss: 0.001802
Validation Loss: 0.00177170
Epoch [191/200], Train Loss: 0.001784
Validation Loss: 0.00176614
Epoch [192/200], Train Loss: 0.001785
Validation Loss: 0.00176055
Epoch [193/200], Train Loss: 0.001775
Validation Loss: 0.00176403
Epoch [194/200], Train Loss: 0.001771
Validation Loss: 0.00176892
Epoch [195/200], Train Loss: 0.001766
Validation Loss: 0.00173483
Epoch [196/200], Train Loss: 0.001761
Validation Loss: 0.00175931
Epoch [197/200], Train Loss: 0.001756
Validation Loss: 0.00174172
Epoch [198/200], Train Loss: 0.001757
Validation Loss: 0.00173928
Epoch [199/200], Train Loss: 0.001741
Validation Loss: 0.00173615
Epoch [200/200], Train Loss: 0.001752
Validation Loss: 0.00173480

Evaluating model for: Lamp
Run 25/144 completed in 829.30 seconds with: {'MAE': np.float32(1.2984136), 'MSE': np.float32(56.31474), 'RMSE': np.float32(7.504315), 'SAE': np.float32(0.24136975), 'NDE': np.float32(0.5841491)}

Run 26/144: hidden=64, seq_len=720, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.006748
Validation Loss: 0.00569949
Epoch [2/200], Train Loss: 0.005225
Validation Loss: 0.00502372
Epoch [3/200], Train Loss: 0.004940
Validation Loss: 0.00499210
Epoch [4/200], Train Loss: 0.004923
Validation Loss: 0.00498729
Epoch [5/200], Train Loss: 0.004916
Validation Loss: 0.00498294
Epoch [6/200], Train Loss: 0.004893
Validation Loss: 0.00497794
Epoch [7/200], Train Loss: 0.004904
Validation Loss: 0.00497098
Epoch [8/200], Train Loss: 0.004901
Validation Loss: 0.00496260
Epoch [9/200], Train Loss: 0.004900
Validation Loss: 0.00495109
Epoch [10/200], Train Loss: 0.004881
Validation Loss: 0.00493397
Epoch [11/200], Train Loss: 0.004884
Validation Loss: 0.00491701
Epoch [12/200], Train Loss: 0.004836
Validation Loss: 0.00490685
Epoch [13/200], Train Loss: 0.004844
Validation Loss: 0.00490193
Epoch [14/200], Train Loss: 0.004830
Validation Loss: 0.00489895
Epoch [15/200], Train Loss: 0.004834
Validation Loss: 0.00489779
Epoch [16/200], Train Loss: 0.004816
Validation Loss: 0.00489716
Epoch [17/200], Train Loss: 0.004819
Validation Loss: 0.00489566
Epoch [18/200], Train Loss: 0.004816
Validation Loss: 0.00489457
Epoch [19/200], Train Loss: 0.004806
Validation Loss: 0.00489430
Epoch [20/200], Train Loss: 0.004818
Validation Loss: 0.00489367
Epoch [21/200], Train Loss: 0.004811
Validation Loss: 0.00489321
Epoch [22/200], Train Loss: 0.004810
Validation Loss: 0.00489236
Epoch [23/200], Train Loss: 0.004816
Validation Loss: 0.00489213
Epoch [24/200], Train Loss: 0.004815
Validation Loss: 0.00489070
Epoch [25/200], Train Loss: 0.004807
Validation Loss: 0.00489117
Epoch [26/200], Train Loss: 0.004803
Validation Loss: 0.00489026
Epoch [27/200], Train Loss: 0.004807
Validation Loss: 0.00489002
Epoch [28/200], Train Loss: 0.004793
Validation Loss: 0.00489164
Epoch [29/200], Train Loss: 0.004800
Validation Loss: 0.00489126
Epoch [30/200], Train Loss: 0.004796
Validation Loss: 0.00489089
Epoch [31/200], Train Loss: 0.004802
Validation Loss: 0.00489109
Epoch [32/200], Train Loss: 0.004799
Validation Loss: 0.00488721
Epoch [33/200], Train Loss: 0.004798
Validation Loss: 0.00488660
Epoch [34/200], Train Loss: 0.004798
Validation Loss: 0.00488626
Epoch [35/200], Train Loss: 0.004795
Validation Loss: 0.00488627
Epoch [36/200], Train Loss: 0.004793
Validation Loss: 0.00488556
Epoch [37/200], Train Loss: 0.004784
Validation Loss: 0.00488684
Epoch [38/200], Train Loss: 0.004797
Validation Loss: 0.00488500
Epoch [39/200], Train Loss: 0.004794
Validation Loss: 0.00488520
Epoch [40/200], Train Loss: 0.004781
Validation Loss: 0.00488478
Epoch [41/200], Train Loss: 0.004788
Validation Loss: 0.00488437
Epoch [42/200], Train Loss: 0.004787
Validation Loss: 0.00488402
Epoch [43/200], Train Loss: 0.004776
Validation Loss: 0.00488382
Epoch [44/200], Train Loss: 0.004781
Validation Loss: 0.00488467
Epoch [45/200], Train Loss: 0.004803
Validation Loss: 0.00488479
Epoch [46/200], Train Loss: 0.004790
Validation Loss: 0.00488338
Epoch [47/200], Train Loss: 0.004790
Validation Loss: 0.00488395
Epoch [48/200], Train Loss: 0.004791
Validation Loss: 0.00488359
Epoch [49/200], Train Loss: 0.004798
Validation Loss: 0.00488422
Epoch [50/200], Train Loss: 0.004782
Validation Loss: 0.00488349
Epoch [51/200], Train Loss: 0.004791
Validation Loss: 0.00488315
Epoch [52/200], Train Loss: 0.004781
Validation Loss: 0.00488273
Epoch [53/200], Train Loss: 0.004784
Validation Loss: 0.00488256
Epoch [54/200], Train Loss: 0.004783
Validation Loss: 0.00488237
Epoch [55/200], Train Loss: 0.004783
Validation Loss: 0.00488205
Epoch [56/200], Train Loss: 0.004782
Validation Loss: 0.00488329
Epoch [57/200], Train Loss: 0.004790
Validation Loss: 0.00488232
Epoch [58/200], Train Loss: 0.004784
Validation Loss: 0.00488139
Epoch [59/200], Train Loss: 0.004785
Validation Loss: 0.00488155
Epoch [60/200], Train Loss: 0.004791
Validation Loss: 0.00488114
Epoch [61/200], Train Loss: 0.004776
Validation Loss: 0.00488128
Epoch [62/200], Train Loss: 0.004778
Validation Loss: 0.00488128
Epoch [63/200], Train Loss: 0.004757
Validation Loss: 0.00488091
Epoch [64/200], Train Loss: 0.004767
Validation Loss: 0.00488102
Epoch [65/200], Train Loss: 0.004767
Validation Loss: 0.00488141
Epoch [66/200], Train Loss: 0.004794
Validation Loss: 0.00488099
Epoch [67/200], Train Loss: 0.004794
Validation Loss: 0.00488038
Epoch [68/200], Train Loss: 0.004774
Validation Loss: 0.00488038
Epoch [69/200], Train Loss: 0.004804
Validation Loss: 0.00488033
Epoch [70/200], Train Loss: 0.004782
Validation Loss: 0.00488061
Epoch [71/200], Train Loss: 0.004792
Validation Loss: 0.00487986
Epoch [72/200], Train Loss: 0.004780
Validation Loss: 0.00488069
Epoch [73/200], Train Loss: 0.004779
Validation Loss: 0.00488355
Epoch [74/200], Train Loss: 0.004784
Validation Loss: 0.00487961
Epoch [75/200], Train Loss: 0.004777
Validation Loss: 0.00487916
Epoch [76/200], Train Loss: 0.004775
Validation Loss: 0.00487933
Epoch [77/200], Train Loss: 0.004775
Validation Loss: 0.00487954
Epoch [78/200], Train Loss: 0.004769
Validation Loss: 0.00487948
Epoch [79/200], Train Loss: 0.004762
Validation Loss: 0.00487997
Epoch [80/200], Train Loss: 0.004790
Validation Loss: 0.00487942
Epoch [81/200], Train Loss: 0.004786
Validation Loss: 0.00487946
Epoch [82/200], Train Loss: 0.004777
Validation Loss: 0.00487887
Epoch [83/200], Train Loss: 0.004795
Validation Loss: 0.00487892
Epoch [84/200], Train Loss: 0.004772
Validation Loss: 0.00487838
Epoch [85/200], Train Loss: 0.004779
Validation Loss: 0.00487827
Epoch [86/200], Train Loss: 0.004786
Validation Loss: 0.00487947
Epoch [87/200], Train Loss: 0.004770
Validation Loss: 0.00487958
Epoch [88/200], Train Loss: 0.004773
Validation Loss: 0.00487902
Epoch [89/200], Train Loss: 0.004776
Validation Loss: 0.00487804
Epoch [90/200], Train Loss: 0.004793
Validation Loss: 0.00487811
Epoch [91/200], Train Loss: 0.004790
Validation Loss: 0.00487742
Epoch [92/200], Train Loss: 0.004767
Validation Loss: 0.00487768
Epoch [93/200], Train Loss: 0.004766
Validation Loss: 0.00487766
Epoch [94/200], Train Loss: 0.004772
Validation Loss: 0.00487782
Epoch [95/200], Train Loss: 0.004775
Validation Loss: 0.00487865
Epoch [96/200], Train Loss: 0.004773
Validation Loss: 0.00487696
Epoch [97/200], Train Loss: 0.004780
Validation Loss: 0.00487747
Epoch [98/200], Train Loss: 0.004765
Validation Loss: 0.00487670
Epoch [99/200], Train Loss: 0.004771
Validation Loss: 0.00487824
Epoch [100/200], Train Loss: 0.004771
Validation Loss: 0.00487757
Epoch [101/200], Train Loss: 0.004765
Validation Loss: 0.00487616
Epoch [102/200], Train Loss: 0.004792
Validation Loss: 0.00487950
Epoch [103/200], Train Loss: 0.004762
Validation Loss: 0.00487795
Epoch [104/200], Train Loss: 0.004769
Validation Loss: 0.00487603
Epoch [105/200], Train Loss: 0.004772
Validation Loss: 0.00487705
Epoch [106/200], Train Loss: 0.004796
Validation Loss: 0.00487558
Epoch [107/200], Train Loss: 0.004779
Validation Loss: 0.00487522
Epoch [108/200], Train Loss: 0.004777
Validation Loss: 0.00487606
Epoch [109/200], Train Loss: 0.004761
Validation Loss: 0.00487486
Epoch [110/200], Train Loss: 0.004775
Validation Loss: 0.00487592
Epoch [111/200], Train Loss: 0.004758
Validation Loss: 0.00487473
Epoch [112/200], Train Loss: 0.004759
Validation Loss: 0.00487453
Epoch [113/200], Train Loss: 0.004780
Validation Loss: 0.00487432
Epoch [114/200], Train Loss: 0.004769
Validation Loss: 0.00487512
Epoch [115/200], Train Loss: 0.004758
Validation Loss: 0.00487408
Epoch [116/200], Train Loss: 0.004771
Validation Loss: 0.00487541
Epoch [117/200], Train Loss: 0.004762
Validation Loss: 0.00487395
Epoch [118/200], Train Loss: 0.004765
Validation Loss: 0.00487342
Epoch [119/200], Train Loss: 0.004766
Validation Loss: 0.00487352
Epoch [120/200], Train Loss: 0.004778
Validation Loss: 0.00487302
Epoch [121/200], Train Loss: 0.004766
Validation Loss: 0.00487300
Epoch [122/200], Train Loss: 0.004780
Validation Loss: 0.00487318
Epoch [123/200], Train Loss: 0.004757
Validation Loss: 0.00487280
Epoch [124/200], Train Loss: 0.004761
Validation Loss: 0.00487257
Epoch [125/200], Train Loss: 0.004780
Validation Loss: 0.00487399
Epoch [126/200], Train Loss: 0.004778
Validation Loss: 0.00487189
Epoch [127/200], Train Loss: 0.004766
Validation Loss: 0.00487161
Epoch [128/200], Train Loss: 0.004752
Validation Loss: 0.00487137
Epoch [129/200], Train Loss: 0.004753
Validation Loss: 0.00487248
Epoch [130/200], Train Loss: 0.004780
Validation Loss: 0.00487176
Epoch [131/200], Train Loss: 0.004778
Validation Loss: 0.00487063
Epoch [132/200], Train Loss: 0.004753
Validation Loss: 0.00487048
Epoch [133/200], Train Loss: 0.004754
Validation Loss: 0.00486987
Epoch [134/200], Train Loss: 0.004770
Validation Loss: 0.00486967
Epoch [135/200], Train Loss: 0.004756
Validation Loss: 0.00486964
Epoch [136/200], Train Loss: 0.004767
Validation Loss: 0.00486977
Epoch [137/200], Train Loss: 0.004764
Validation Loss: 0.00486943
Epoch [138/200], Train Loss: 0.004773
Validation Loss: 0.00486863
Epoch [139/200], Train Loss: 0.004772
Validation Loss: 0.00486857
Epoch [140/200], Train Loss: 0.004760
Validation Loss: 0.00486721
Epoch [141/200], Train Loss: 0.004757
Validation Loss: 0.00486684
Epoch [142/200], Train Loss: 0.004753
Validation Loss: 0.00486655
Epoch [143/200], Train Loss: 0.004745
Validation Loss: 0.00486644
Epoch [144/200], Train Loss: 0.004753
Validation Loss: 0.00486545
Epoch [145/200], Train Loss: 0.004772
Validation Loss: 0.00486597
Epoch [146/200], Train Loss: 0.004756
Validation Loss: 0.00486516
Epoch [147/200], Train Loss: 0.004765
Validation Loss: 0.00486389
Epoch [148/200], Train Loss: 0.004755
Validation Loss: 0.00486378
Epoch [149/200], Train Loss: 0.004758
Validation Loss: 0.00486257
Epoch [150/200], Train Loss: 0.004766
Validation Loss: 0.00486212
Epoch [151/200], Train Loss: 0.004754
Validation Loss: 0.00486252
Epoch [152/200], Train Loss: 0.004754
Validation Loss: 0.00486068
Epoch [153/200], Train Loss: 0.004749
Validation Loss: 0.00486028
Epoch [154/200], Train Loss: 0.004764
Validation Loss: 0.00485945
Epoch [155/200], Train Loss: 0.004744
Validation Loss: 0.00485848
Epoch [156/200], Train Loss: 0.004750
Validation Loss: 0.00485813
Epoch [157/200], Train Loss: 0.004747
Validation Loss: 0.00485675
Epoch [158/200], Train Loss: 0.004741
Validation Loss: 0.00485642
Epoch [159/200], Train Loss: 0.004739
Validation Loss: 0.00485549
Epoch [160/200], Train Loss: 0.004757
Validation Loss: 0.00485426
Epoch [161/200], Train Loss: 0.004743
Validation Loss: 0.00485261
Epoch [162/200], Train Loss: 0.004753
Validation Loss: 0.00485151
Epoch [163/200], Train Loss: 0.004737
Validation Loss: 0.00485180
Epoch [164/200], Train Loss: 0.004750
Validation Loss: 0.00484816
Epoch [165/200], Train Loss: 0.004734
Validation Loss: 0.00484643
Epoch [166/200], Train Loss: 0.004726
Validation Loss: 0.00484527
Epoch [167/200], Train Loss: 0.004739
Validation Loss: 0.00484325
Epoch [168/200], Train Loss: 0.004731
Validation Loss: 0.00484156
Epoch [169/200], Train Loss: 0.004726
Validation Loss: 0.00484143
Epoch [170/200], Train Loss: 0.004733
Validation Loss: 0.00483846
Epoch [171/200], Train Loss: 0.004741
Validation Loss: 0.00483562
Epoch [172/200], Train Loss: 0.004730
Validation Loss: 0.00483255
Epoch [173/200], Train Loss: 0.004721
Validation Loss: 0.00483007
Epoch [174/200], Train Loss: 0.004726
Validation Loss: 0.00482806
Epoch [175/200], Train Loss: 0.004734
Validation Loss: 0.00482451
Epoch [176/200], Train Loss: 0.004716
Validation Loss: 0.00482322
Epoch [177/200], Train Loss: 0.004723
Validation Loss: 0.00482202
Epoch [178/200], Train Loss: 0.004713
Validation Loss: 0.00481444
Epoch [179/200], Train Loss: 0.004730
Validation Loss: 0.00481102
Epoch [180/200], Train Loss: 0.004712
Validation Loss: 0.00480597
Epoch [181/200], Train Loss: 0.004705
Validation Loss: 0.00480926
Epoch [182/200], Train Loss: 0.004696
Validation Loss: 0.00479777
Epoch [183/200], Train Loss: 0.004699
Validation Loss: 0.00479299
Epoch [184/200], Train Loss: 0.004719
Validation Loss: 0.00478897
Epoch [185/200], Train Loss: 0.004702
Validation Loss: 0.00478365
Epoch [186/200], Train Loss: 0.004702
Validation Loss: 0.00478035
Epoch [187/200], Train Loss: 0.004694
Validation Loss: 0.00477222
Epoch [188/200], Train Loss: 0.004690
Validation Loss: 0.00476715
Epoch [189/200], Train Loss: 0.004679
Validation Loss: 0.00476187
Epoch [190/200], Train Loss: 0.004688
Validation Loss: 0.00475788
Epoch [191/200], Train Loss: 0.004672
Validation Loss: 0.00475293
Epoch [192/200], Train Loss: 0.004666
Validation Loss: 0.00474788
Epoch [193/200], Train Loss: 0.004677
Validation Loss: 0.00474121
Epoch [194/200], Train Loss: 0.004663
Validation Loss: 0.00473393
Epoch [195/200], Train Loss: 0.004645
Validation Loss: 0.00472827
Epoch [196/200], Train Loss: 0.004671
Validation Loss: 0.00472309
Epoch [197/200], Train Loss: 0.004642
Validation Loss: 0.00471778
Epoch [198/200], Train Loss: 0.004633
Validation Loss: 0.00471176
Epoch [199/200], Train Loss: 0.004658
Validation Loss: 0.00471293
Epoch [200/200], Train Loss: 0.004645
Validation Loss: 0.00470051

Evaluating model for: Lamp
Run 26/144 completed in 851.45 seconds with: {'MAE': np.float32(2.4718013), 'MSE': np.float32(153.39848), 'RMSE': np.float32(12.385414), 'SAE': np.float32(0.091139905), 'NDE': np.float32(0.96410143)}

Run 27/144: hidden=64, seq_len=720, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.011026
Validation Loss: 0.00856230
Epoch [2/200], Train Loss: 0.006941
Validation Loss: 0.00572917
Epoch [3/200], Train Loss: 0.005136
Validation Loss: 0.00499141
Epoch [4/200], Train Loss: 0.004930
Validation Loss: 0.00498580
Epoch [5/200], Train Loss: 0.004924
Validation Loss: 0.00498016
Epoch [6/200], Train Loss: 0.004902
Validation Loss: 0.00497458
Epoch [7/200], Train Loss: 0.004906
Validation Loss: 0.00496689
Epoch [8/200], Train Loss: 0.004921
Validation Loss: 0.00495765
Epoch [9/200], Train Loss: 0.004905
Validation Loss: 0.00494504
Epoch [10/200], Train Loss: 0.004879
Validation Loss: 0.00493261
Epoch [11/200], Train Loss: 0.004867
Validation Loss: 0.00492550
Epoch [12/200], Train Loss: 0.004846
Validation Loss: 0.00491977
Epoch [13/200], Train Loss: 0.004860
Validation Loss: 0.00491857
Epoch [14/200], Train Loss: 0.004857
Validation Loss: 0.00491780
Epoch [15/200], Train Loss: 0.004844
Validation Loss: 0.00491726
Epoch [16/200], Train Loss: 0.004852
Validation Loss: 0.00491465
Epoch [17/200], Train Loss: 0.004842
Validation Loss: 0.00491879
Epoch [18/200], Train Loss: 0.004849
Validation Loss: 0.00491273
Epoch [19/200], Train Loss: 0.004853
Validation Loss: 0.00491308
Epoch [20/200], Train Loss: 0.004826
Validation Loss: 0.00491159
Epoch [21/200], Train Loss: 0.004849
Validation Loss: 0.00491350
Epoch [22/200], Train Loss: 0.004848
Validation Loss: 0.00490999
Epoch [23/200], Train Loss: 0.004840
Validation Loss: 0.00490908
Epoch [24/200], Train Loss: 0.004822
Validation Loss: 0.00491053
Epoch [25/200], Train Loss: 0.004839
Validation Loss: 0.00490653
Epoch [26/200], Train Loss: 0.004841
Validation Loss: 0.00491120
Epoch [27/200], Train Loss: 0.004834
Validation Loss: 0.00490554
Epoch [28/200], Train Loss: 0.004826
Validation Loss: 0.00490384
Epoch [29/200], Train Loss: 0.004829
Validation Loss: 0.00490856
Epoch [30/200], Train Loss: 0.004811
Validation Loss: 0.00490291
Epoch [31/200], Train Loss: 0.004825
Validation Loss: 0.00490251
Epoch [32/200], Train Loss: 0.004813
Validation Loss: 0.00490178
Epoch [33/200], Train Loss: 0.004807
Validation Loss: 0.00490092
Epoch [34/200], Train Loss: 0.004814
Validation Loss: 0.00490200
Epoch [35/200], Train Loss: 0.004813
Validation Loss: 0.00490245
Epoch [36/200], Train Loss: 0.004813
Validation Loss: 0.00489943
Epoch [37/200], Train Loss: 0.004801
Validation Loss: 0.00490238
Epoch [38/200], Train Loss: 0.004832
Validation Loss: 0.00490001
Epoch [39/200], Train Loss: 0.004820
Validation Loss: 0.00489855
Epoch [40/200], Train Loss: 0.004794
Validation Loss: 0.00489830
Epoch [41/200], Train Loss: 0.004813
Validation Loss: 0.00490157
Epoch [42/200], Train Loss: 0.004802
Validation Loss: 0.00490380
Epoch [43/200], Train Loss: 0.004804
Validation Loss: 0.00489898
Epoch [44/200], Train Loss: 0.004800
Validation Loss: 0.00489935
Epoch [45/200], Train Loss: 0.004833
Validation Loss: 0.00489695
Epoch [46/200], Train Loss: 0.004808
Validation Loss: 0.00489717
Epoch [47/200], Train Loss: 0.004815
Validation Loss: 0.00490150
Epoch [48/200], Train Loss: 0.004822
Validation Loss: 0.00489702
Epoch [49/200], Train Loss: 0.004804
Validation Loss: 0.00489627
Epoch [50/200], Train Loss: 0.004808
Validation Loss: 0.00489629
Epoch [51/200], Train Loss: 0.004822
Validation Loss: 0.00489792
Epoch [52/200], Train Loss: 0.004802
Validation Loss: 0.00489591
Epoch [53/200], Train Loss: 0.004797
Validation Loss: 0.00489580
Epoch [54/200], Train Loss: 0.004807
Validation Loss: 0.00490095
Epoch [55/200], Train Loss: 0.004805
Validation Loss: 0.00489762
Epoch [56/200], Train Loss: 0.004807
Validation Loss: 0.00489689
Epoch [57/200], Train Loss: 0.004797
Validation Loss: 0.00489509
Epoch [58/200], Train Loss: 0.004799
Validation Loss: 0.00489535
Epoch [59/200], Train Loss: 0.004789
Validation Loss: 0.00489467
Epoch [60/200], Train Loss: 0.004800
Validation Loss: 0.00490694
Epoch [61/200], Train Loss: 0.004794
Validation Loss: 0.00489422
Epoch [62/200], Train Loss: 0.004799
Validation Loss: 0.00489460
Epoch [63/200], Train Loss: 0.004794
Validation Loss: 0.00490454
Epoch [64/200], Train Loss: 0.004816
Validation Loss: 0.00489635
Epoch [65/200], Train Loss: 0.004804
Validation Loss: 0.00489434
Epoch [66/200], Train Loss: 0.004794
Validation Loss: 0.00489353
Epoch [67/200], Train Loss: 0.004785
Validation Loss: 0.00489365
Epoch [68/200], Train Loss: 0.004802
Validation Loss: 0.00489415
Epoch [69/200], Train Loss: 0.004796
Validation Loss: 0.00489515
Epoch [70/200], Train Loss: 0.004803
Validation Loss: 0.00489495
Epoch [71/200], Train Loss: 0.004786
Validation Loss: 0.00489316
Epoch [72/200], Train Loss: 0.004806
Validation Loss: 0.00489339
Epoch [73/200], Train Loss: 0.004805
Validation Loss: 0.00489303
Epoch [74/200], Train Loss: 0.004788
Validation Loss: 0.00489309
Epoch [75/200], Train Loss: 0.004808
Validation Loss: 0.00489265
Epoch [76/200], Train Loss: 0.004798
Validation Loss: 0.00489237
Epoch [77/200], Train Loss: 0.004800
Validation Loss: 0.00489207
Epoch [78/200], Train Loss: 0.004809
Validation Loss: 0.00489256
Epoch [79/200], Train Loss: 0.004793
Validation Loss: 0.00489197
Epoch [80/200], Train Loss: 0.004792
Validation Loss: 0.00489193
Epoch [81/200], Train Loss: 0.004802
Validation Loss: 0.00489440
Epoch [82/200], Train Loss: 0.004797
Validation Loss: 0.00489208
Epoch [83/200], Train Loss: 0.004805
Validation Loss: 0.00489168
Epoch [84/200], Train Loss: 0.004814
Validation Loss: 0.00489134
Epoch [85/200], Train Loss: 0.004781
Validation Loss: 0.00489173
Epoch [86/200], Train Loss: 0.004799
Validation Loss: 0.00489255
Epoch [87/200], Train Loss: 0.004797
Validation Loss: 0.00489154
Epoch [88/200], Train Loss: 0.004801
Validation Loss: 0.00489440
Epoch [89/200], Train Loss: 0.004784
Validation Loss: 0.00489080
Epoch [90/200], Train Loss: 0.004792
Validation Loss: 0.00489106
Epoch [91/200], Train Loss: 0.004790
Validation Loss: 0.00489192
Epoch [92/200], Train Loss: 0.004789
Validation Loss: 0.00489054
Epoch [93/200], Train Loss: 0.004805
Validation Loss: 0.00489054
Epoch [94/200], Train Loss: 0.004783
Validation Loss: 0.00489394
Epoch [95/200], Train Loss: 0.004800
Validation Loss: 0.00489057
Epoch [96/200], Train Loss: 0.004773
Validation Loss: 0.00489282
Epoch [97/200], Train Loss: 0.004786
Validation Loss: 0.00488974
Epoch [98/200], Train Loss: 0.004792
Validation Loss: 0.00489023
Epoch [99/200], Train Loss: 0.004795
Validation Loss: 0.00488959
Epoch [100/200], Train Loss: 0.004784
Validation Loss: 0.00489108
Epoch [101/200], Train Loss: 0.004794
Validation Loss: 0.00488949
Epoch [102/200], Train Loss: 0.004802
Validation Loss: 0.00488922
Epoch [103/200], Train Loss: 0.004797
Validation Loss: 0.00488911
Epoch [104/200], Train Loss: 0.004783
Validation Loss: 0.00488909
Epoch [105/200], Train Loss: 0.004781
Validation Loss: 0.00488904
Epoch [106/200], Train Loss: 0.004793
Validation Loss: 0.00488955
Epoch [107/200], Train Loss: 0.004795
Validation Loss: 0.00488857
Epoch [108/200], Train Loss: 0.004801
Validation Loss: 0.00488881
Epoch [109/200], Train Loss: 0.004793
Validation Loss: 0.00488871
Epoch [110/200], Train Loss: 0.004792
Validation Loss: 0.00488918
Epoch [111/200], Train Loss: 0.004792
Validation Loss: 0.00488852
Epoch [112/200], Train Loss: 0.004797
Validation Loss: 0.00488836
Epoch [113/200], Train Loss: 0.004793
Validation Loss: 0.00488798
Epoch [114/200], Train Loss: 0.004773
Validation Loss: 0.00488813
Epoch [115/200], Train Loss: 0.004781
Validation Loss: 0.00488852
Epoch [116/200], Train Loss: 0.004780
Validation Loss: 0.00488949
Epoch [117/200], Train Loss: 0.004790
Validation Loss: 0.00488860
Epoch [118/200], Train Loss: 0.004799
Validation Loss: 0.00489174
Epoch [119/200], Train Loss: 0.004773
Validation Loss: 0.00489478
Epoch [120/200], Train Loss: 0.004794
Validation Loss: 0.00488750
Epoch [121/200], Train Loss: 0.004789
Validation Loss: 0.00488781
Epoch [122/200], Train Loss: 0.004777
Validation Loss: 0.00488709
Epoch [123/200], Train Loss: 0.004786
Validation Loss: 0.00488755
Epoch [124/200], Train Loss: 0.004815
Validation Loss: 0.00488707
Epoch [125/200], Train Loss: 0.004795
Validation Loss: 0.00488733
Epoch [126/200], Train Loss: 0.004792
Validation Loss: 0.00488719
Epoch [127/200], Train Loss: 0.004795
Validation Loss: 0.00488676
Epoch [128/200], Train Loss: 0.004796
Validation Loss: 0.00488769
Epoch [129/200], Train Loss: 0.004791
Validation Loss: 0.00489039
Epoch [130/200], Train Loss: 0.004779
Validation Loss: 0.00488739
Epoch [131/200], Train Loss: 0.004781
Validation Loss: 0.00488751
Epoch [132/200], Train Loss: 0.004786
Validation Loss: 0.00488647
Epoch [133/200], Train Loss: 0.004775
Validation Loss: 0.00488634
Epoch [134/200], Train Loss: 0.004789
Validation Loss: 0.00488633
Epoch [135/200], Train Loss: 0.004793
Validation Loss: 0.00488604
Epoch [136/200], Train Loss: 0.004790
Validation Loss: 0.00488618
Epoch [137/200], Train Loss: 0.004770
Validation Loss: 0.00488633
Epoch [138/200], Train Loss: 0.004772
Validation Loss: 0.00488597
Epoch [139/200], Train Loss: 0.004802
Validation Loss: 0.00488697
Epoch [140/200], Train Loss: 0.004785
Validation Loss: 0.00488899
Epoch [141/200], Train Loss: 0.004776
Validation Loss: 0.00488551
Epoch [142/200], Train Loss: 0.004776
Validation Loss: 0.00488539
Epoch [143/200], Train Loss: 0.004784
Validation Loss: 0.00488563
Epoch [144/200], Train Loss: 0.004792
Validation Loss: 0.00488723
Epoch [145/200], Train Loss: 0.004798
Validation Loss: 0.00488543
Epoch [146/200], Train Loss: 0.004779
Validation Loss: 0.00488538
Epoch [147/200], Train Loss: 0.004776
Validation Loss: 0.00488500
Epoch [148/200], Train Loss: 0.004767
Validation Loss: 0.00488624
Epoch [149/200], Train Loss: 0.004791
Validation Loss: 0.00488525
Epoch [150/200], Train Loss: 0.004769
Validation Loss: 0.00488476
Epoch [151/200], Train Loss: 0.004777
Validation Loss: 0.00488456
Epoch [152/200], Train Loss: 0.004795
Validation Loss: 0.00488462
Epoch [153/200], Train Loss: 0.004791
Validation Loss: 0.00488542
Epoch [154/200], Train Loss: 0.004777
Validation Loss: 0.00488531
Epoch [155/200], Train Loss: 0.004781
Validation Loss: 0.00488448
Epoch [156/200], Train Loss: 0.004766
Validation Loss: 0.00488552
Epoch [157/200], Train Loss: 0.004772
Validation Loss: 0.00488468
Epoch [158/200], Train Loss: 0.004781
Validation Loss: 0.00488440
Epoch [159/200], Train Loss: 0.004770
Validation Loss: 0.00488668
Epoch [160/200], Train Loss: 0.004782
Validation Loss: 0.00488501
Epoch [161/200], Train Loss: 0.004781
Validation Loss: 0.00488510
Epoch [162/200], Train Loss: 0.004769
Validation Loss: 0.00488399
Epoch [163/200], Train Loss: 0.004779
Validation Loss: 0.00488381
Epoch [164/200], Train Loss: 0.004786
Validation Loss: 0.00488367
Epoch [165/200], Train Loss: 0.004779
Validation Loss: 0.00488354
Epoch [166/200], Train Loss: 0.004774
Validation Loss: 0.00488400
Epoch [167/200], Train Loss: 0.004767
Validation Loss: 0.00488343
Epoch [168/200], Train Loss: 0.004775
Validation Loss: 0.00488380
Epoch [169/200], Train Loss: 0.004780
Validation Loss: 0.00488316
Epoch [170/200], Train Loss: 0.004790
Validation Loss: 0.00488372
Epoch [171/200], Train Loss: 0.004781
Validation Loss: 0.00488459
Epoch [172/200], Train Loss: 0.004796
Validation Loss: 0.00488644
Epoch [173/200], Train Loss: 0.004766
Validation Loss: 0.00488351
Epoch [174/200], Train Loss: 0.004794
Validation Loss: 0.00488289
Epoch [175/200], Train Loss: 0.004776
Validation Loss: 0.00488527
Epoch [176/200], Train Loss: 0.004782
Validation Loss: 0.00488271
Epoch [177/200], Train Loss: 0.004774
Validation Loss: 0.00488278
Epoch [178/200], Train Loss: 0.004782
Validation Loss: 0.00488297
Epoch [179/200], Train Loss: 0.004759
Validation Loss: 0.00488425
Epoch [180/200], Train Loss: 0.004778
Validation Loss: 0.00488367
Epoch [181/200], Train Loss: 0.004777
Validation Loss: 0.00488341
Epoch [182/200], Train Loss: 0.004799
Validation Loss: 0.00488371
Epoch [183/200], Train Loss: 0.004780
Validation Loss: 0.00488376
Epoch [184/200], Train Loss: 0.004798
Validation Loss: 0.00488263
Epoch [185/200], Train Loss: 0.004787
Validation Loss: 0.00488209
Epoch [186/200], Train Loss: 0.004767
Validation Loss: 0.00488487
Epoch [187/200], Train Loss: 0.004797
Validation Loss: 0.00488282
Epoch [188/200], Train Loss: 0.004783
Validation Loss: 0.00488290
Epoch [189/200], Train Loss: 0.004762
Validation Loss: 0.00488393
Epoch [190/200], Train Loss: 0.004782
Validation Loss: 0.00488283
Epoch [191/200], Train Loss: 0.004774
Validation Loss: 0.00488183
Epoch [192/200], Train Loss: 0.004788
Validation Loss: 0.00488242
Epoch [193/200], Train Loss: 0.004792
Validation Loss: 0.00488271
Epoch [194/200], Train Loss: 0.004784
Validation Loss: 0.00488275
Epoch [195/200], Train Loss: 0.004773
Validation Loss: 0.00488342
Epoch [196/200], Train Loss: 0.004787
Validation Loss: 0.00488151
Epoch [197/200], Train Loss: 0.004780
Validation Loss: 0.00488147
Epoch [198/200], Train Loss: 0.004776
Validation Loss: 0.00488157
Epoch [199/200], Train Loss: 0.004773
Validation Loss: 0.00488210
Epoch [200/200], Train Loss: 0.004770
Validation Loss: 0.00488294

Evaluating model for: Lamp
Run 27/144 completed in 873.65 seconds with: {'MAE': np.float32(2.450088), 'MSE': np.float32(158.81284), 'RMSE': np.float32(12.602097), 'SAE': np.float32(0.19119419), 'NDE': np.float32(0.9809687)}

Run 28/144: hidden=64, seq_len=720, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.005129
Validation Loss: 0.00498834
Epoch [2/200], Train Loss: 0.004896
Validation Loss: 0.00498706
Epoch [3/200], Train Loss: 0.004907
Validation Loss: 0.00498636
Epoch [4/200], Train Loss: 0.004910
Validation Loss: 0.00498484
Epoch [5/200], Train Loss: 0.004890
Validation Loss: 0.00498180
Epoch [6/200], Train Loss: 0.004869
Validation Loss: 0.00497323
Epoch [7/200], Train Loss: 0.004869
Validation Loss: 0.00493637
Epoch [8/200], Train Loss: 0.004796
Validation Loss: 0.00490484
Epoch [9/200], Train Loss: 0.004783
Validation Loss: 0.00489879
Epoch [10/200], Train Loss: 0.004792
Validation Loss: 0.00489636
Epoch [11/200], Train Loss: 0.004794
Validation Loss: 0.00489462
Epoch [12/200], Train Loss: 0.004782
Validation Loss: 0.00489276
Epoch [13/200], Train Loss: 0.004782
Validation Loss: 0.00489215
Epoch [14/200], Train Loss: 0.004790
Validation Loss: 0.00489042
Epoch [15/200], Train Loss: 0.004793
Validation Loss: 0.00488895
Epoch [16/200], Train Loss: 0.004781
Validation Loss: 0.00488793
Epoch [17/200], Train Loss: 0.004774
Validation Loss: 0.00488732
Epoch [18/200], Train Loss: 0.004780
Validation Loss: 0.00488940
Epoch [19/200], Train Loss: 0.004766
Validation Loss: 0.00488900
Epoch [20/200], Train Loss: 0.004793
Validation Loss: 0.00488644
Epoch [21/200], Train Loss: 0.004776
Validation Loss: 0.00488633
Epoch [22/200], Train Loss: 0.004774
Validation Loss: 0.00488548
Epoch [23/200], Train Loss: 0.004759
Validation Loss: 0.00488980
Epoch [24/200], Train Loss: 0.004767
Validation Loss: 0.00488518
Epoch [25/200], Train Loss: 0.004758
Validation Loss: 0.00488493
Epoch [26/200], Train Loss: 0.004781
Validation Loss: 0.00488471
Epoch [27/200], Train Loss: 0.004789
Validation Loss: 0.00488584
Epoch [28/200], Train Loss: 0.004754
Validation Loss: 0.00488667
Epoch [29/200], Train Loss: 0.004770
Validation Loss: 0.00488587
Epoch [30/200], Train Loss: 0.004772
Validation Loss: 0.00488511
Epoch [31/200], Train Loss: 0.004769
Validation Loss: 0.00488356
Epoch [32/200], Train Loss: 0.004758
Validation Loss: 0.00488526
Epoch [33/200], Train Loss: 0.004758
Validation Loss: 0.00488488
Epoch [34/200], Train Loss: 0.004758
Validation Loss: 0.00488322
Epoch [35/200], Train Loss: 0.004784
Validation Loss: 0.00488511
Epoch [36/200], Train Loss: 0.004769
Validation Loss: 0.00488382
Epoch [37/200], Train Loss: 0.004770
Validation Loss: 0.00488219
Epoch [38/200], Train Loss: 0.004774
Validation Loss: 0.00488527
Epoch [39/200], Train Loss: 0.004764
Validation Loss: 0.00488338
Epoch [40/200], Train Loss: 0.004756
Validation Loss: 0.00488205
Epoch [41/200], Train Loss: 0.004757
Validation Loss: 0.00488200
Epoch [42/200], Train Loss: 0.004774
Validation Loss: 0.00488222
Epoch [43/200], Train Loss: 0.004763
Validation Loss: 0.00488156
Epoch [44/200], Train Loss: 0.004763
Validation Loss: 0.00488124
Epoch [45/200], Train Loss: 0.004762
Validation Loss: 0.00488152
Epoch [46/200], Train Loss: 0.004753
Validation Loss: 0.00488120
Epoch [47/200], Train Loss: 0.004760
Validation Loss: 0.00488102
Epoch [48/200], Train Loss: 0.004762
Validation Loss: 0.00488179
Epoch [49/200], Train Loss: 0.004752
Validation Loss: 0.00488105
Epoch [50/200], Train Loss: 0.004770
Validation Loss: 0.00488081
Epoch [51/200], Train Loss: 0.004772
Validation Loss: 0.00488062
Epoch [52/200], Train Loss: 0.004763
Validation Loss: 0.00488199
Epoch [53/200], Train Loss: 0.004776
Validation Loss: 0.00488106
Epoch [54/200], Train Loss: 0.004766
Validation Loss: 0.00488113
Epoch [55/200], Train Loss: 0.004755
Validation Loss: 0.00488101
Epoch [56/200], Train Loss: 0.004767
Validation Loss: 0.00488133
Epoch [57/200], Train Loss: 0.004765
Validation Loss: 0.00488025
Epoch [58/200], Train Loss: 0.004785
Validation Loss: 0.00488068
Epoch [59/200], Train Loss: 0.004761
Validation Loss: 0.00488117
Epoch [60/200], Train Loss: 0.004746
Validation Loss: 0.00487979
Epoch [61/200], Train Loss: 0.004761
Validation Loss: 0.00487984
Epoch [62/200], Train Loss: 0.004770
Validation Loss: 0.00487972
Epoch [63/200], Train Loss: 0.004757
Validation Loss: 0.00487958
Epoch [64/200], Train Loss: 0.004754
Validation Loss: 0.00488019
Epoch [65/200], Train Loss: 0.004767
Validation Loss: 0.00488050
Epoch [66/200], Train Loss: 0.004765
Validation Loss: 0.00487973
Epoch [67/200], Train Loss: 0.004770
Validation Loss: 0.00487939
Epoch [68/200], Train Loss: 0.004784
Validation Loss: 0.00488013
Epoch [69/200], Train Loss: 0.004754
Validation Loss: 0.00487912
Epoch [70/200], Train Loss: 0.004761
Validation Loss: 0.00487928
Epoch [71/200], Train Loss: 0.004753
Validation Loss: 0.00487955
Epoch [72/200], Train Loss: 0.004770
Validation Loss: 0.00488087
Epoch [73/200], Train Loss: 0.004746
Validation Loss: 0.00487892
Epoch [74/200], Train Loss: 0.004767
Validation Loss: 0.00487908
Epoch [75/200], Train Loss: 0.004763
Validation Loss: 0.00488017
Epoch [76/200], Train Loss: 0.004764
Validation Loss: 0.00487891
Epoch [77/200], Train Loss: 0.004764
Validation Loss: 0.00487968
Epoch [78/200], Train Loss: 0.004765
Validation Loss: 0.00487876
Epoch [79/200], Train Loss: 0.004757
Validation Loss: 0.00487892
Epoch [80/200], Train Loss: 0.004743
Validation Loss: 0.00487878
Epoch [81/200], Train Loss: 0.004766
Validation Loss: 0.00487958
Epoch [82/200], Train Loss: 0.004779
Validation Loss: 0.00488120
Epoch [83/200], Train Loss: 0.004756
Validation Loss: 0.00487825
Epoch [84/200], Train Loss: 0.004747
Validation Loss: 0.00487866
Epoch [85/200], Train Loss: 0.004758
Validation Loss: 0.00487815
Epoch [86/200], Train Loss: 0.004770
Validation Loss: 0.00487862
Epoch [87/200], Train Loss: 0.004782
Validation Loss: 0.00488050
Epoch [88/200], Train Loss: 0.004765
Validation Loss: 0.00487960
Epoch [89/200], Train Loss: 0.004758
Validation Loss: 0.00487847
Epoch [90/200], Train Loss: 0.004761
Validation Loss: 0.00487784
Epoch [91/200], Train Loss: 0.004761
Validation Loss: 0.00487822
Epoch [92/200], Train Loss: 0.004760
Validation Loss: 0.00487771
Epoch [93/200], Train Loss: 0.004760
Validation Loss: 0.00487922
Epoch [94/200], Train Loss: 0.004756
Validation Loss: 0.00487815
Epoch [95/200], Train Loss: 0.004768
Validation Loss: 0.00487781
Epoch [96/200], Train Loss: 0.004759
Validation Loss: 0.00487848
Epoch [97/200], Train Loss: 0.004747
Validation Loss: 0.00487733
Epoch [98/200], Train Loss: 0.004748
Validation Loss: 0.00487722
Epoch [99/200], Train Loss: 0.004745
Validation Loss: 0.00487734
Epoch [100/200], Train Loss: 0.004759
Validation Loss: 0.00487708
Epoch [101/200], Train Loss: 0.004767
Validation Loss: 0.00487876
Epoch [102/200], Train Loss: 0.004750
Validation Loss: 0.00487839
Epoch [103/200], Train Loss: 0.004761
Validation Loss: 0.00487718
Epoch [104/200], Train Loss: 0.004757
Validation Loss: 0.00487680
Epoch [105/200], Train Loss: 0.004773
Validation Loss: 0.00487675
Epoch [106/200], Train Loss: 0.004755
Validation Loss: 0.00487634
Epoch [107/200], Train Loss: 0.004764
Validation Loss: 0.00487612
Epoch [108/200], Train Loss: 0.004754
Validation Loss: 0.00487663
Epoch [109/200], Train Loss: 0.004752
Validation Loss: 0.00487584
Epoch [110/200], Train Loss: 0.004765
Validation Loss: 0.00487618
Epoch [111/200], Train Loss: 0.004760
Validation Loss: 0.00487568
Epoch [112/200], Train Loss: 0.004756
Validation Loss: 0.00487610
Epoch [113/200], Train Loss: 0.004760
Validation Loss: 0.00487561
Epoch [114/200], Train Loss: 0.004747
Validation Loss: 0.00487569
Epoch [115/200], Train Loss: 0.004747
Validation Loss: 0.00487517
Epoch [116/200], Train Loss: 0.004744
Validation Loss: 0.00487506
Epoch [117/200], Train Loss: 0.004755
Validation Loss: 0.00487834
Epoch [118/200], Train Loss: 0.004748
Validation Loss: 0.00487590
Epoch [119/200], Train Loss: 0.004774
Validation Loss: 0.00487454
Epoch [120/200], Train Loss: 0.004766
Validation Loss: 0.00487509
Epoch [121/200], Train Loss: 0.004759
Validation Loss: 0.00487427
Epoch [122/200], Train Loss: 0.004757
Validation Loss: 0.00487416
Epoch [123/200], Train Loss: 0.004758
Validation Loss: 0.00487429
Epoch [124/200], Train Loss: 0.004767
Validation Loss: 0.00487560
Epoch [125/200], Train Loss: 0.004751
Validation Loss: 0.00487343
Epoch [126/200], Train Loss: 0.004752
Validation Loss: 0.00487322
Epoch [127/200], Train Loss: 0.004755
Validation Loss: 0.00487305
Epoch [128/200], Train Loss: 0.004759
Validation Loss: 0.00487269
Epoch [129/200], Train Loss: 0.004756
Validation Loss: 0.00487642
Epoch [130/200], Train Loss: 0.004749
Validation Loss: 0.00487283
Epoch [131/200], Train Loss: 0.004785
Validation Loss: 0.00487182
Epoch [132/200], Train Loss: 0.004747
Validation Loss: 0.00487183
Epoch [133/200], Train Loss: 0.004743
Validation Loss: 0.00487100
Epoch [134/200], Train Loss: 0.004751
Validation Loss: 0.00487068
Epoch [135/200], Train Loss: 0.004758
Validation Loss: 0.00486982
Epoch [136/200], Train Loss: 0.004761
Validation Loss: 0.00487068
Epoch [137/200], Train Loss: 0.004745
Validation Loss: 0.00486889
Epoch [138/200], Train Loss: 0.004751
Validation Loss: 0.00486886
Epoch [139/200], Train Loss: 0.004764
Validation Loss: 0.00486767
Epoch [140/200], Train Loss: 0.004775
Validation Loss: 0.00486701
Epoch [141/200], Train Loss: 0.004742
Validation Loss: 0.00486869
Epoch [142/200], Train Loss: 0.004756
Validation Loss: 0.00486576
Epoch [143/200], Train Loss: 0.004754
Validation Loss: 0.00486579
Epoch [144/200], Train Loss: 0.004739
Validation Loss: 0.00486370
Epoch [145/200], Train Loss: 0.004745
Validation Loss: 0.00486378
Epoch [146/200], Train Loss: 0.004744
Validation Loss: 0.00486186
Epoch [147/200], Train Loss: 0.004754
Validation Loss: 0.00486671
Epoch [148/200], Train Loss: 0.004743
Validation Loss: 0.00486191
Epoch [149/200], Train Loss: 0.004741
Validation Loss: 0.00485983
Epoch [150/200], Train Loss: 0.004741
Validation Loss: 0.00485865
Epoch [151/200], Train Loss: 0.004751
Validation Loss: 0.00485751
Epoch [152/200], Train Loss: 0.004727
Validation Loss: 0.00485717
Epoch [153/200], Train Loss: 0.004726
Validation Loss: 0.00485829
Epoch [154/200], Train Loss: 0.004749
Validation Loss: 0.00485783
Epoch [155/200], Train Loss: 0.004755
Validation Loss: 0.00486491
Epoch [156/200], Train Loss: 0.004733
Validation Loss: 0.00485458
Epoch [157/200], Train Loss: 0.004742
Validation Loss: 0.00485379
Epoch [158/200], Train Loss: 0.004751
Validation Loss: 0.00485335
Epoch [159/200], Train Loss: 0.004760
Validation Loss: 0.00485459
Epoch [160/200], Train Loss: 0.004752
Validation Loss: 0.00485201
Epoch [161/200], Train Loss: 0.004741
Validation Loss: 0.00485172
Epoch [162/200], Train Loss: 0.004729
Validation Loss: 0.00485065
Epoch [163/200], Train Loss: 0.004749
Validation Loss: 0.00484946
Epoch [164/200], Train Loss: 0.004730
Validation Loss: 0.00484920
Epoch [165/200], Train Loss: 0.004737
Validation Loss: 0.00484802
Epoch [166/200], Train Loss: 0.004720
Validation Loss: 0.00484776
Epoch [167/200], Train Loss: 0.004738
Validation Loss: 0.00484733
Epoch [168/200], Train Loss: 0.004737
Validation Loss: 0.00484615
Epoch [169/200], Train Loss: 0.004730
Validation Loss: 0.00484595
Epoch [170/200], Train Loss: 0.004727
Validation Loss: 0.00484496
Epoch [171/200], Train Loss: 0.004727
Validation Loss: 0.00484399
Epoch [172/200], Train Loss: 0.004729
Validation Loss: 0.00484354
Epoch [173/200], Train Loss: 0.004737
Validation Loss: 0.00484437
Epoch [174/200], Train Loss: 0.004724
Validation Loss: 0.00484410
Epoch [175/200], Train Loss: 0.004730
Validation Loss: 0.00484217
Epoch [176/200], Train Loss: 0.004720
Validation Loss: 0.00484139
Epoch [177/200], Train Loss: 0.004733
Validation Loss: 0.00484097
Epoch [178/200], Train Loss: 0.004731
Validation Loss: 0.00484414
Epoch [179/200], Train Loss: 0.004740
Validation Loss: 0.00484172
Epoch [180/200], Train Loss: 0.004733
Validation Loss: 0.00484032
Epoch [181/200], Train Loss: 0.004734
Validation Loss: 0.00483877
Epoch [182/200], Train Loss: 0.004722
Validation Loss: 0.00483920
Epoch [183/200], Train Loss: 0.004732
Validation Loss: 0.00483918
Epoch [184/200], Train Loss: 0.004731
Validation Loss: 0.00483878
Epoch [185/200], Train Loss: 0.004728
Validation Loss: 0.00484006
Epoch [186/200], Train Loss: 0.004725
Validation Loss: 0.00483829
Epoch [187/200], Train Loss: 0.004723
Validation Loss: 0.00483784
Epoch [188/200], Train Loss: 0.004737
Validation Loss: 0.00483659
Epoch [189/200], Train Loss: 0.004716
Validation Loss: 0.00483714
Epoch [190/200], Train Loss: 0.004725
Validation Loss: 0.00483685
Epoch [191/200], Train Loss: 0.004722
Validation Loss: 0.00483860
Epoch [192/200], Train Loss: 0.004725
Validation Loss: 0.00483601
Epoch [193/200], Train Loss: 0.004717
Validation Loss: 0.00483819
Epoch [194/200], Train Loss: 0.004724
Validation Loss: 0.00483509
Epoch [195/200], Train Loss: 0.004734
Validation Loss: 0.00483483
Epoch [196/200], Train Loss: 0.004718
Validation Loss: 0.00483472
Epoch [197/200], Train Loss: 0.004735
Validation Loss: 0.00483464
Epoch [198/200], Train Loss: 0.004700
Validation Loss: 0.00483441
Epoch [199/200], Train Loss: 0.004730
Validation Loss: 0.00483335
Epoch [200/200], Train Loss: 0.004714
Validation Loss: 0.00483715

Evaluating model for: Lamp
Run 28/144 completed in 879.69 seconds with: {'MAE': np.float32(2.6483135), 'MSE': np.float32(157.78967), 'RMSE': np.float32(12.561436), 'SAE': np.float32(0.008130843), 'NDE': np.float32(0.9778023)}

Run 29/144: hidden=64, seq_len=720, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.006566
Validation Loss: 0.00632545
Epoch [2/200], Train Loss: 0.005725
Validation Loss: 0.00575055
Epoch [3/200], Train Loss: 0.005339
Validation Loss: 0.00537587
Epoch [4/200], Train Loss: 0.005090
Validation Loss: 0.00517554
Epoch [5/200], Train Loss: 0.004922
Validation Loss: 0.00510975
Epoch [6/200], Train Loss: 0.004881
Validation Loss: 0.00510300
Epoch [7/200], Train Loss: 0.004911
Validation Loss: 0.00509969
Epoch [8/200], Train Loss: 0.004945
Validation Loss: 0.00509710
Epoch [9/200], Train Loss: 0.004897
Validation Loss: 0.00509379
Epoch [10/200], Train Loss: 0.004846
Validation Loss: 0.00508978
Epoch [11/200], Train Loss: 0.004903
Validation Loss: 0.00508655
Epoch [12/200], Train Loss: 0.004852
Validation Loss: 0.00508228
Epoch [13/200], Train Loss: 0.004912
Validation Loss: 0.00507842
Epoch [14/200], Train Loss: 0.004958
Validation Loss: 0.00507382
Epoch [15/200], Train Loss: 0.004767
Validation Loss: 0.00506904
Epoch [16/200], Train Loss: 0.004879
Validation Loss: 0.00506591
Epoch [17/200], Train Loss: 0.004820
Validation Loss: 0.00506230
Epoch [18/200], Train Loss: 0.004824
Validation Loss: 0.00505704
Epoch [19/200], Train Loss: 0.004792
Validation Loss: 0.00505384
Epoch [20/200], Train Loss: 0.004839
Validation Loss: 0.00505108
Epoch [21/200], Train Loss: 0.004859
Validation Loss: 0.00504635
Epoch [22/200], Train Loss: 0.004857
Validation Loss: 0.00504415
Epoch [23/200], Train Loss: 0.004819
Validation Loss: 0.00504100
Epoch [24/200], Train Loss: 0.004863
Validation Loss: 0.00503851
Epoch [25/200], Train Loss: 0.004867
Validation Loss: 0.00503672
Epoch [26/200], Train Loss: 0.004827
Validation Loss: 0.00503593
Epoch [27/200], Train Loss: 0.004915
Validation Loss: 0.00503391
Epoch [28/200], Train Loss: 0.004903
Validation Loss: 0.00503304
Epoch [29/200], Train Loss: 0.004829
Validation Loss: 0.00503385
Epoch [30/200], Train Loss: 0.004823
Validation Loss: 0.00503118
Epoch [31/200], Train Loss: 0.004822
Validation Loss: 0.00503270
Epoch [32/200], Train Loss: 0.004754
Validation Loss: 0.00503109
Epoch [33/200], Train Loss: 0.004843
Validation Loss: 0.00503075
Epoch [34/200], Train Loss: 0.004775
Validation Loss: 0.00503109
Epoch [35/200], Train Loss: 0.004816
Validation Loss: 0.00502998
Epoch [36/200], Train Loss: 0.004903
Validation Loss: 0.00503019
Epoch [37/200], Train Loss: 0.004836
Validation Loss: 0.00503003
Epoch [38/200], Train Loss: 0.004826
Validation Loss: 0.00502855
Epoch [39/200], Train Loss: 0.004823
Validation Loss: 0.00502874
Epoch [40/200], Train Loss: 0.004795
Validation Loss: 0.00502942
Epoch [41/200], Train Loss: 0.004791
Validation Loss: 0.00502768
Epoch [42/200], Train Loss: 0.004964
Validation Loss: 0.00502986
Epoch [43/200], Train Loss: 0.004883
Validation Loss: 0.00502665
Epoch [44/200], Train Loss: 0.004854
Validation Loss: 0.00502805
Epoch [45/200], Train Loss: 0.004775
Validation Loss: 0.00502599
Epoch [46/200], Train Loss: 0.004869
Validation Loss: 0.00502725
Epoch [47/200], Train Loss: 0.004848
Validation Loss: 0.00502607
Epoch [48/200], Train Loss: 0.004835
Validation Loss: 0.00502527
Epoch [49/200], Train Loss: 0.004804
Validation Loss: 0.00502675
Epoch [50/200], Train Loss: 0.004785
Validation Loss: 0.00502515
Epoch [51/200], Train Loss: 0.004803
Validation Loss: 0.00502479
Epoch [52/200], Train Loss: 0.004892
Validation Loss: 0.00502562
Epoch [53/200], Train Loss: 0.004866
Validation Loss: 0.00502342
Epoch [54/200], Train Loss: 0.004906
Validation Loss: 0.00502469
Epoch [55/200], Train Loss: 0.004868
Validation Loss: 0.00502324
Epoch [56/200], Train Loss: 0.004982
Validation Loss: 0.00502294
Epoch [57/200], Train Loss: 0.004882
Validation Loss: 0.00502259
Epoch [58/200], Train Loss: 0.004854
Validation Loss: 0.00502320
Epoch [59/200], Train Loss: 0.004784
Validation Loss: 0.00502100
Epoch [60/200], Train Loss: 0.004800
Validation Loss: 0.00502269
Epoch [61/200], Train Loss: 0.004829
Validation Loss: 0.00502148
Epoch [62/200], Train Loss: 0.004875
Validation Loss: 0.00502175
Epoch [63/200], Train Loss: 0.004772
Validation Loss: 0.00501986
Epoch [64/200], Train Loss: 0.004822
Validation Loss: 0.00502163
Epoch [65/200], Train Loss: 0.004755
Validation Loss: 0.00502027
Epoch [66/200], Train Loss: 0.004755
Validation Loss: 0.00501857
Epoch [67/200], Train Loss: 0.004832
Validation Loss: 0.00501993
Epoch [68/200], Train Loss: 0.004858
Validation Loss: 0.00501804
Epoch [69/200], Train Loss: 0.004795
Validation Loss: 0.00501718
Epoch [70/200], Train Loss: 0.004833
Validation Loss: 0.00501789
Epoch [71/200], Train Loss: 0.004817
Validation Loss: 0.00501697
Epoch [72/200], Train Loss: 0.004919
Validation Loss: 0.00501618
Epoch [73/200], Train Loss: 0.004774
Validation Loss: 0.00501500
Epoch [74/200], Train Loss: 0.004907
Validation Loss: 0.00501526
Epoch [75/200], Train Loss: 0.004740
Validation Loss: 0.00501315
Epoch [76/200], Train Loss: 0.004845
Validation Loss: 0.00501561
Epoch [77/200], Train Loss: 0.004790
Validation Loss: 0.00501267
Epoch [78/200], Train Loss: 0.004815
Validation Loss: 0.00501309
Epoch [79/200], Train Loss: 0.004841
Validation Loss: 0.00501242
Epoch [80/200], Train Loss: 0.004907
Validation Loss: 0.00501087
Epoch [81/200], Train Loss: 0.004774
Validation Loss: 0.00500974
Epoch [82/200], Train Loss: 0.004828
Validation Loss: 0.00501001
Epoch [83/200], Train Loss: 0.004847
Validation Loss: 0.00501002
Epoch [84/200], Train Loss: 0.004789
Validation Loss: 0.00500680
Epoch [85/200], Train Loss: 0.004711
Validation Loss: 0.00500839
Epoch [86/200], Train Loss: 0.004821
Validation Loss: 0.00500855
Epoch [87/200], Train Loss: 0.004922
Validation Loss: 0.00500457
Epoch [88/200], Train Loss: 0.004817
Validation Loss: 0.00500577
Epoch [89/200], Train Loss: 0.004823
Validation Loss: 0.00500348
Epoch [90/200], Train Loss: 0.004853
Validation Loss: 0.00500287
Epoch [91/200], Train Loss: 0.004812
Validation Loss: 0.00500326
Epoch [92/200], Train Loss: 0.004759
Validation Loss: 0.00500108
Epoch [93/200], Train Loss: 0.004913
Validation Loss: 0.00500290
Epoch [94/200], Train Loss: 0.004778
Validation Loss: 0.00499889
Epoch [95/200], Train Loss: 0.004806
Validation Loss: 0.00500178
Epoch [96/200], Train Loss: 0.004806
Validation Loss: 0.00499834
Epoch [97/200], Train Loss: 0.004812
Validation Loss: 0.00499924
Epoch [98/200], Train Loss: 0.004772
Validation Loss: 0.00499695
Epoch [99/200], Train Loss: 0.004791
Validation Loss: 0.00499671
Epoch [100/200], Train Loss: 0.004799
Validation Loss: 0.00499503
Epoch [101/200], Train Loss: 0.004702
Validation Loss: 0.00499488
Epoch [102/200], Train Loss: 0.004766
Validation Loss: 0.00499575
Epoch [103/200], Train Loss: 0.004843
Validation Loss: 0.00499190
Epoch [104/200], Train Loss: 0.004763
Validation Loss: 0.00499484
Epoch [105/200], Train Loss: 0.004959
Validation Loss: 0.00499201
Epoch [106/200], Train Loss: 0.004818
Validation Loss: 0.00499020
Epoch [107/200], Train Loss: 0.004811
Validation Loss: 0.00499285
Epoch [108/200], Train Loss: 0.004787
Validation Loss: 0.00498923
Epoch [109/200], Train Loss: 0.004774
Validation Loss: 0.00498956
Epoch [110/200], Train Loss: 0.004904
Validation Loss: 0.00498828
Epoch [111/200], Train Loss: 0.004806
Validation Loss: 0.00498808
Epoch [112/200], Train Loss: 0.004802
Validation Loss: 0.00498764
Epoch [113/200], Train Loss: 0.004789
Validation Loss: 0.00498619
Epoch [114/200], Train Loss: 0.004809
Validation Loss: 0.00498668
Epoch [115/200], Train Loss: 0.004795
Validation Loss: 0.00498453
Epoch [116/200], Train Loss: 0.004763
Validation Loss: 0.00498393
Epoch [117/200], Train Loss: 0.004757
Validation Loss: 0.00498527
Epoch [118/200], Train Loss: 0.004766
Validation Loss: 0.00498273
Epoch [119/200], Train Loss: 0.004751
Validation Loss: 0.00498347
Epoch [120/200], Train Loss: 0.004794
Validation Loss: 0.00498181
Epoch [121/200], Train Loss: 0.004773
Validation Loss: 0.00498143
Epoch [122/200], Train Loss: 0.004788
Validation Loss: 0.00497964
Epoch [123/200], Train Loss: 0.004742
Validation Loss: 0.00498003
Epoch [124/200], Train Loss: 0.004797
Validation Loss: 0.00497992
Epoch [125/200], Train Loss: 0.004799
Validation Loss: 0.00497691
Epoch [126/200], Train Loss: 0.004790
Validation Loss: 0.00497934
Epoch [127/200], Train Loss: 0.004698
Validation Loss: 0.00497670
Epoch [128/200], Train Loss: 0.004813
Validation Loss: 0.00497734
Epoch [129/200], Train Loss: 0.004815
Validation Loss: 0.00497538
Epoch [130/200], Train Loss: 0.004796
Validation Loss: 0.00497575
Epoch [131/200], Train Loss: 0.004776
Validation Loss: 0.00497384
Epoch [132/200], Train Loss: 0.004842
Validation Loss: 0.00497474
Epoch [133/200], Train Loss: 0.004754
Validation Loss: 0.00497205
Epoch [134/200], Train Loss: 0.004747
Validation Loss: 0.00497409
Epoch [135/200], Train Loss: 0.004766
Validation Loss: 0.00497013
Epoch [136/200], Train Loss: 0.004694
Validation Loss: 0.00497078
Epoch [137/200], Train Loss: 0.004747
Validation Loss: 0.00496891
Epoch [138/200], Train Loss: 0.004769
Validation Loss: 0.00496872
Epoch [139/200], Train Loss: 0.004717
Validation Loss: 0.00496805
Epoch [140/200], Train Loss: 0.004749
Validation Loss: 0.00496656
Epoch [141/200], Train Loss: 0.004814
Validation Loss: 0.00496751
Epoch [142/200], Train Loss: 0.004847
Validation Loss: 0.00496566
Epoch [143/200], Train Loss: 0.004832
Validation Loss: 0.00496388
Epoch [144/200], Train Loss: 0.004787
Validation Loss: 0.00496469
Epoch [145/200], Train Loss: 0.004792
Validation Loss: 0.00496185
Epoch [146/200], Train Loss: 0.004738
Validation Loss: 0.00496506
Epoch [147/200], Train Loss: 0.004794
Validation Loss: 0.00496168
Epoch [148/200], Train Loss: 0.004755
Validation Loss: 0.00496141
Epoch [149/200], Train Loss: 0.004811
Validation Loss: 0.00496011
Epoch [150/200], Train Loss: 0.004729
Validation Loss: 0.00495983
Epoch [151/200], Train Loss: 0.004753
Validation Loss: 0.00495840
Epoch [152/200], Train Loss: 0.004748
Validation Loss: 0.00495687
Epoch [153/200], Train Loss: 0.004737
Validation Loss: 0.00495671
Epoch [154/200], Train Loss: 0.004753
Validation Loss: 0.00495509
Epoch [155/200], Train Loss: 0.004729
Validation Loss: 0.00495606
Epoch [156/200], Train Loss: 0.004701
Validation Loss: 0.00495357
Epoch [157/200], Train Loss: 0.004743
Validation Loss: 0.00495498
Epoch [158/200], Train Loss: 0.004698
Validation Loss: 0.00495176
Epoch [159/200], Train Loss: 0.004798
Validation Loss: 0.00495301
Epoch [160/200], Train Loss: 0.004778
Validation Loss: 0.00495108
Epoch [161/200], Train Loss: 0.004762
Validation Loss: 0.00494828
Epoch [162/200], Train Loss: 0.004747
Validation Loss: 0.00494868
Epoch [163/200], Train Loss: 0.004742
Validation Loss: 0.00494907
Epoch [164/200], Train Loss: 0.004703
Validation Loss: 0.00494647
Epoch [165/200], Train Loss: 0.004713
Validation Loss: 0.00494707
Epoch [166/200], Train Loss: 0.004777
Validation Loss: 0.00494483
Epoch [167/200], Train Loss: 0.004736
Validation Loss: 0.00494735
Epoch [168/200], Train Loss: 0.004790
Validation Loss: 0.00494259
Epoch [169/200], Train Loss: 0.004822
Validation Loss: 0.00494586
Epoch [170/200], Train Loss: 0.004774
Validation Loss: 0.00494166
Epoch [171/200], Train Loss: 0.004789
Validation Loss: 0.00494164
Epoch [172/200], Train Loss: 0.004708
Validation Loss: 0.00493938
Epoch [173/200], Train Loss: 0.004720
Validation Loss: 0.00493825
Epoch [174/200], Train Loss: 0.004762
Validation Loss: 0.00493917
Epoch [175/200], Train Loss: 0.004784
Validation Loss: 0.00493716
Epoch [176/200], Train Loss: 0.004703
Validation Loss: 0.00493616
Epoch [177/200], Train Loss: 0.004733
Validation Loss: 0.00493532
Epoch [178/200], Train Loss: 0.004715
Validation Loss: 0.00493537
Epoch [179/200], Train Loss: 0.004695
Validation Loss: 0.00493208
Epoch [180/200], Train Loss: 0.004741
Validation Loss: 0.00493250
Epoch [181/200], Train Loss: 0.004696
Validation Loss: 0.00493144
Epoch [182/200], Train Loss: 0.004713
Validation Loss: 0.00493013
Epoch [183/200], Train Loss: 0.004801
Validation Loss: 0.00492928
Epoch [184/200], Train Loss: 0.004765
Validation Loss: 0.00492829
Epoch [185/200], Train Loss: 0.004753
Validation Loss: 0.00492696
Epoch [186/200], Train Loss: 0.004721
Validation Loss: 0.00492535
Epoch [187/200], Train Loss: 0.004755
Validation Loss: 0.00492438
Epoch [188/200], Train Loss: 0.004748
Validation Loss: 0.00492429
Epoch [189/200], Train Loss: 0.004742
Validation Loss: 0.00492313
Epoch [190/200], Train Loss: 0.004749
Validation Loss: 0.00492040
Epoch [191/200], Train Loss: 0.004713
Validation Loss: 0.00492032
Epoch [192/200], Train Loss: 0.004667
Validation Loss: 0.00492030
Epoch [193/200], Train Loss: 0.004717
Validation Loss: 0.00491875
Epoch [194/200], Train Loss: 0.004713
Validation Loss: 0.00491865
Epoch [195/200], Train Loss: 0.004694
Validation Loss: 0.00491934
Epoch [196/200], Train Loss: 0.004707
Validation Loss: 0.00491509
Epoch [197/200], Train Loss: 0.004750
Validation Loss: 0.00491460
Epoch [198/200], Train Loss: 0.004797
Validation Loss: 0.00491257
Epoch [199/200], Train Loss: 0.004693
Validation Loss: 0.00491080
Epoch [200/200], Train Loss: 0.004688
Validation Loss: 0.00491013

Evaluating model for: Lamp
Run 29/144 completed in 327.65 seconds with: {'MAE': np.float32(2.5342932), 'MSE': np.float32(147.41444), 'RMSE': np.float32(12.141435), 'SAE': np.float32(0.04678323), 'NDE': np.float32(0.9700905)}

Run 30/144: hidden=64, seq_len=720, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.007561
Validation Loss: 0.00699888
Epoch [2/200], Train Loss: 0.006316
Validation Loss: 0.00608422
Epoch [3/200], Train Loss: 0.005639
Validation Loss: 0.00554266
Epoch [4/200], Train Loss: 0.005206
Validation Loss: 0.00525623
Epoch [5/200], Train Loss: 0.005032
Validation Loss: 0.00514338
Epoch [6/200], Train Loss: 0.004999
Validation Loss: 0.00512434
Epoch [7/200], Train Loss: 0.004898
Validation Loss: 0.00512366
Epoch [8/200], Train Loss: 0.004921
Validation Loss: 0.00512125
Epoch [9/200], Train Loss: 0.004988
Validation Loss: 0.00511904
Epoch [10/200], Train Loss: 0.004954
Validation Loss: 0.00511750
Epoch [11/200], Train Loss: 0.004927
Validation Loss: 0.00511586
Epoch [12/200], Train Loss: 0.004866
Validation Loss: 0.00511399
Epoch [13/200], Train Loss: 0.005051
Validation Loss: 0.00511200
Epoch [14/200], Train Loss: 0.004924
Validation Loss: 0.00510983
Epoch [15/200], Train Loss: 0.004912
Validation Loss: 0.00510732
Epoch [16/200], Train Loss: 0.004938
Validation Loss: 0.00510472
Epoch [17/200], Train Loss: 0.004901
Validation Loss: 0.00510177
Epoch [18/200], Train Loss: 0.004891
Validation Loss: 0.00509869
Epoch [19/200], Train Loss: 0.004972
Validation Loss: 0.00509546
Epoch [20/200], Train Loss: 0.004933
Validation Loss: 0.00509168
Epoch [21/200], Train Loss: 0.004976
Validation Loss: 0.00508748
Epoch [22/200], Train Loss: 0.004899
Validation Loss: 0.00508225
Epoch [23/200], Train Loss: 0.004894
Validation Loss: 0.00507641
Epoch [24/200], Train Loss: 0.004838
Validation Loss: 0.00506977
Epoch [25/200], Train Loss: 0.004825
Validation Loss: 0.00506304
Epoch [26/200], Train Loss: 0.004907
Validation Loss: 0.00505573
Epoch [27/200], Train Loss: 0.004909
Validation Loss: 0.00504863
Epoch [28/200], Train Loss: 0.004809
Validation Loss: 0.00504056
Epoch [29/200], Train Loss: 0.004892
Validation Loss: 0.00503485
Epoch [30/200], Train Loss: 0.004842
Validation Loss: 0.00502929
Epoch [31/200], Train Loss: 0.004871
Validation Loss: 0.00502700
Epoch [32/200], Train Loss: 0.004790
Validation Loss: 0.00502465
Epoch [33/200], Train Loss: 0.004823
Validation Loss: 0.00502490
Epoch [34/200], Train Loss: 0.004885
Validation Loss: 0.00502311
Epoch [35/200], Train Loss: 0.004865
Validation Loss: 0.00502396
Epoch [36/200], Train Loss: 0.004803
Validation Loss: 0.00502216
Epoch [37/200], Train Loss: 0.004902
Validation Loss: 0.00502153
Epoch [38/200], Train Loss: 0.004832
Validation Loss: 0.00502137
Epoch [39/200], Train Loss: 0.004826
Validation Loss: 0.00502056
Epoch [40/200], Train Loss: 0.004888
Validation Loss: 0.00502052
Epoch [41/200], Train Loss: 0.004785
Validation Loss: 0.00501953
Epoch [42/200], Train Loss: 0.004896
Validation Loss: 0.00501971
Epoch [43/200], Train Loss: 0.004784
Validation Loss: 0.00501778
Epoch [44/200], Train Loss: 0.004889
Validation Loss: 0.00502049
Epoch [45/200], Train Loss: 0.004841
Validation Loss: 0.00501694
Epoch [46/200], Train Loss: 0.004809
Validation Loss: 0.00501789
Epoch [47/200], Train Loss: 0.004798
Validation Loss: 0.00501799
Epoch [48/200], Train Loss: 0.004863
Validation Loss: 0.00501650
Epoch [49/200], Train Loss: 0.004873
Validation Loss: 0.00501656
Epoch [50/200], Train Loss: 0.004767
Validation Loss: 0.00501694
Epoch [51/200], Train Loss: 0.004905
Validation Loss: 0.00501604
Epoch [52/200], Train Loss: 0.004903
Validation Loss: 0.00501479
Epoch [53/200], Train Loss: 0.004917
Validation Loss: 0.00501506
Epoch [54/200], Train Loss: 0.004872
Validation Loss: 0.00501353
Epoch [55/200], Train Loss: 0.004778
Validation Loss: 0.00501513
Epoch [56/200], Train Loss: 0.004780
Validation Loss: 0.00501366
Epoch [57/200], Train Loss: 0.004775
Validation Loss: 0.00501331
Epoch [58/200], Train Loss: 0.004855
Validation Loss: 0.00501344
Epoch [59/200], Train Loss: 0.004744
Validation Loss: 0.00501165
Epoch [60/200], Train Loss: 0.004859
Validation Loss: 0.00501414
Epoch [61/200], Train Loss: 0.004782
Validation Loss: 0.00501142
Epoch [62/200], Train Loss: 0.004765
Validation Loss: 0.00501167
Epoch [63/200], Train Loss: 0.004836
Validation Loss: 0.00501208
Epoch [64/200], Train Loss: 0.004795
Validation Loss: 0.00501125
Epoch [65/200], Train Loss: 0.004824
Validation Loss: 0.00501077
Epoch [66/200], Train Loss: 0.004854
Validation Loss: 0.00501211
Epoch [67/200], Train Loss: 0.004886
Validation Loss: 0.00501060
Epoch [68/200], Train Loss: 0.004831
Validation Loss: 0.00500903
Epoch [69/200], Train Loss: 0.004781
Validation Loss: 0.00501008
Epoch [70/200], Train Loss: 0.004835
Validation Loss: 0.00500974
Epoch [71/200], Train Loss: 0.004844
Validation Loss: 0.00500910
Epoch [72/200], Train Loss: 0.004827
Validation Loss: 0.00500934
Epoch [73/200], Train Loss: 0.004832
Validation Loss: 0.00500810
Epoch [74/200], Train Loss: 0.004851
Validation Loss: 0.00500788
Epoch [75/200], Train Loss: 0.004807
Validation Loss: 0.00500781
Epoch [76/200], Train Loss: 0.004848
Validation Loss: 0.00500743
Epoch [77/200], Train Loss: 0.004969
Validation Loss: 0.00500823
Epoch [78/200], Train Loss: 0.004822
Validation Loss: 0.00500553
Epoch [79/200], Train Loss: 0.004858
Validation Loss: 0.00500774
Epoch [80/200], Train Loss: 0.004857
Validation Loss: 0.00500614
Epoch [81/200], Train Loss: 0.004824
Validation Loss: 0.00500626
Epoch [82/200], Train Loss: 0.004814
Validation Loss: 0.00500539
Epoch [83/200], Train Loss: 0.004892
Validation Loss: 0.00500568
Epoch [84/200], Train Loss: 0.004840
Validation Loss: 0.00500449
Epoch [85/200], Train Loss: 0.004815
Validation Loss: 0.00500413
Epoch [86/200], Train Loss: 0.004818
Validation Loss: 0.00500477
Epoch [87/200], Train Loss: 0.004844
Validation Loss: 0.00500345
Epoch [88/200], Train Loss: 0.004864
Validation Loss: 0.00500454
Epoch [89/200], Train Loss: 0.004867
Validation Loss: 0.00500314
Epoch [90/200], Train Loss: 0.004838
Validation Loss: 0.00500362
Epoch [91/200], Train Loss: 0.004864
Validation Loss: 0.00500239
Epoch [92/200], Train Loss: 0.004832
Validation Loss: 0.00500268
Epoch [93/200], Train Loss: 0.004943
Validation Loss: 0.00500259
Epoch [94/200], Train Loss: 0.004810
Validation Loss: 0.00500126
Epoch [95/200], Train Loss: 0.004796
Validation Loss: 0.00500257
Epoch [96/200], Train Loss: 0.004779
Validation Loss: 0.00500179
Epoch [97/200], Train Loss: 0.004883
Validation Loss: 0.00500073
Epoch [98/200], Train Loss: 0.004805
Validation Loss: 0.00500078
Epoch [99/200], Train Loss: 0.004765
Validation Loss: 0.00500119
Epoch [100/200], Train Loss: 0.004755
Validation Loss: 0.00500083
Epoch [101/200], Train Loss: 0.004837
Validation Loss: 0.00500135
Epoch [102/200], Train Loss: 0.004794
Validation Loss: 0.00499947
Epoch [103/200], Train Loss: 0.004771
Validation Loss: 0.00500062
Epoch [104/200], Train Loss: 0.004825
Validation Loss: 0.00499928
Epoch [105/200], Train Loss: 0.004762
Validation Loss: 0.00499946
Epoch [106/200], Train Loss: 0.004854
Validation Loss: 0.00500073
Epoch [107/200], Train Loss: 0.004874
Validation Loss: 0.00499840
Epoch [108/200], Train Loss: 0.004794
Validation Loss: 0.00499945
Epoch [109/200], Train Loss: 0.004872
Validation Loss: 0.00499840
Epoch [110/200], Train Loss: 0.004782
Validation Loss: 0.00499816
Epoch [111/200], Train Loss: 0.004820
Validation Loss: 0.00499853
Epoch [112/200], Train Loss: 0.004859
Validation Loss: 0.00499810
Epoch [113/200], Train Loss: 0.004798
Validation Loss: 0.00499755
Epoch [114/200], Train Loss: 0.004829
Validation Loss: 0.00499774
Epoch [115/200], Train Loss: 0.004899
Validation Loss: 0.00499795
Epoch [116/200], Train Loss: 0.004828
Validation Loss: 0.00499716
Epoch [117/200], Train Loss: 0.004800
Validation Loss: 0.00499693
Epoch [118/200], Train Loss: 0.004830
Validation Loss: 0.00499854
Epoch [119/200], Train Loss: 0.004792
Validation Loss: 0.00499729
Epoch [120/200], Train Loss: 0.004783
Validation Loss: 0.00499659
Epoch [121/200], Train Loss: 0.004845
Validation Loss: 0.00499754
Epoch [122/200], Train Loss: 0.004822
Validation Loss: 0.00499703
Epoch [123/200], Train Loss: 0.004818
Validation Loss: 0.00499627
Epoch [124/200], Train Loss: 0.004899
Validation Loss: 0.00499730
Epoch [125/200], Train Loss: 0.004839
Validation Loss: 0.00499594
Epoch [126/200], Train Loss: 0.004777
Validation Loss: 0.00499569
Epoch [127/200], Train Loss: 0.004747
Validation Loss: 0.00499726
Epoch [128/200], Train Loss: 0.004767
Validation Loss: 0.00499637
Epoch [129/200], Train Loss: 0.004884
Validation Loss: 0.00499640
Epoch [130/200], Train Loss: 0.004802
Validation Loss: 0.00499594
Epoch [131/200], Train Loss: 0.004808
Validation Loss: 0.00499673
Epoch [132/200], Train Loss: 0.004807
Validation Loss: 0.00499589
Epoch [133/200], Train Loss: 0.004835
Validation Loss: 0.00499604
Epoch [134/200], Train Loss: 0.004812
Validation Loss: 0.00499609
Epoch [135/200], Train Loss: 0.004784
Validation Loss: 0.00499530
Epoch [136/200], Train Loss: 0.004818
Validation Loss: 0.00499630
Epoch [137/200], Train Loss: 0.004781
Validation Loss: 0.00499551
Epoch [138/200], Train Loss: 0.004789
Validation Loss: 0.00499534
Epoch [139/200], Train Loss: 0.004933
Validation Loss: 0.00499729
Epoch [140/200], Train Loss: 0.004821
Validation Loss: 0.00499503
Epoch [141/200], Train Loss: 0.004894
Validation Loss: 0.00499665
Epoch [142/200], Train Loss: 0.004778
Validation Loss: 0.00499523
Epoch [143/200], Train Loss: 0.004812
Validation Loss: 0.00499652
Epoch [144/200], Train Loss: 0.004784
Validation Loss: 0.00499637
Epoch [145/200], Train Loss: 0.004767
Validation Loss: 0.00499517
Epoch [146/200], Train Loss: 0.004803
Validation Loss: 0.00499557
Epoch [147/200], Train Loss: 0.004890
Validation Loss: 0.00499535
Epoch [148/200], Train Loss: 0.004855
Validation Loss: 0.00499508
Epoch [149/200], Train Loss: 0.004756
Validation Loss: 0.00499511
Epoch [150/200], Train Loss: 0.004874
Validation Loss: 0.00499578
Early stopping triggered

Evaluating model for: Lamp
Run 30/144 completed in 256.92 seconds with: {'MAE': np.float32(2.5030363), 'MSE': np.float32(150.97336), 'RMSE': np.float32(12.287122), 'SAE': np.float32(0.043400086), 'NDE': np.float32(0.9817308)}

Run 31/144: hidden=64, seq_len=720, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.012685
Validation Loss: 0.01156486
Epoch [2/200], Train Loss: 0.010333
Validation Loss: 0.00947503
Epoch [3/200], Train Loss: 0.008498
Validation Loss: 0.00780551
Epoch [4/200], Train Loss: 0.006998
Validation Loss: 0.00660175
Epoch [5/200], Train Loss: 0.005924
Validation Loss: 0.00574933
Epoch [6/200], Train Loss: 0.005259
Validation Loss: 0.00524213
Epoch [7/200], Train Loss: 0.004997
Validation Loss: 0.00511923
Epoch [8/200], Train Loss: 0.004962
Validation Loss: 0.00512757
Epoch [9/200], Train Loss: 0.004953
Validation Loss: 0.00511707
Epoch [10/200], Train Loss: 0.004881
Validation Loss: 0.00511781
Epoch [11/200], Train Loss: 0.004957
Validation Loss: 0.00511502
Epoch [12/200], Train Loss: 0.005023
Validation Loss: 0.00511222
Epoch [13/200], Train Loss: 0.004908
Validation Loss: 0.00510983
Epoch [14/200], Train Loss: 0.004920
Validation Loss: 0.00510764
Epoch [15/200], Train Loss: 0.004905
Validation Loss: 0.00510448
Epoch [16/200], Train Loss: 0.004911
Validation Loss: 0.00510184
Epoch [17/200], Train Loss: 0.004965
Validation Loss: 0.00509836
Epoch [18/200], Train Loss: 0.004906
Validation Loss: 0.00509478
Epoch [19/200], Train Loss: 0.004951
Validation Loss: 0.00509105
Epoch [20/200], Train Loss: 0.004897
Validation Loss: 0.00508698
Epoch [21/200], Train Loss: 0.004883
Validation Loss: 0.00508214
Epoch [22/200], Train Loss: 0.004921
Validation Loss: 0.00507658
Epoch [23/200], Train Loss: 0.004940
Validation Loss: 0.00507141
Epoch [24/200], Train Loss: 0.004867
Validation Loss: 0.00506505
Epoch [25/200], Train Loss: 0.004856
Validation Loss: 0.00506133
Epoch [26/200], Train Loss: 0.004968
Validation Loss: 0.00505645
Epoch [27/200], Train Loss: 0.004923
Validation Loss: 0.00505036
Epoch [28/200], Train Loss: 0.004876
Validation Loss: 0.00504858
Epoch [29/200], Train Loss: 0.004891
Validation Loss: 0.00504482
Epoch [30/200], Train Loss: 0.004810
Validation Loss: 0.00504454
Epoch [31/200], Train Loss: 0.004888
Validation Loss: 0.00504205
Epoch [32/200], Train Loss: 0.004847
Validation Loss: 0.00504352
Epoch [33/200], Train Loss: 0.004807
Validation Loss: 0.00503993
Epoch [34/200], Train Loss: 0.004871
Validation Loss: 0.00504068
Epoch [35/200], Train Loss: 0.004921
Validation Loss: 0.00503927
Epoch [36/200], Train Loss: 0.004879
Validation Loss: 0.00503889
Epoch [37/200], Train Loss: 0.004829
Validation Loss: 0.00504099
Epoch [38/200], Train Loss: 0.004877
Validation Loss: 0.00503653
Epoch [39/200], Train Loss: 0.004829
Validation Loss: 0.00503908
Epoch [40/200], Train Loss: 0.004856
Validation Loss: 0.00503648
Epoch [41/200], Train Loss: 0.004823
Validation Loss: 0.00503817
Epoch [42/200], Train Loss: 0.004880
Validation Loss: 0.00503473
Epoch [43/200], Train Loss: 0.004831
Validation Loss: 0.00503616
Epoch [44/200], Train Loss: 0.004808
Validation Loss: 0.00503333
Epoch [45/200], Train Loss: 0.004882
Validation Loss: 0.00503408
Epoch [46/200], Train Loss: 0.004886
Validation Loss: 0.00503313
Epoch [47/200], Train Loss: 0.004882
Validation Loss: 0.00503510
Epoch [48/200], Train Loss: 0.004819
Validation Loss: 0.00503129
Epoch [49/200], Train Loss: 0.004831
Validation Loss: 0.00503181
Epoch [50/200], Train Loss: 0.004827
Validation Loss: 0.00502981
Epoch [51/200], Train Loss: 0.004758
Validation Loss: 0.00503125
Epoch [52/200], Train Loss: 0.004768
Validation Loss: 0.00503017
Epoch [53/200], Train Loss: 0.004872
Validation Loss: 0.00502851
Epoch [54/200], Train Loss: 0.004821
Validation Loss: 0.00503000
Epoch [55/200], Train Loss: 0.004957
Validation Loss: 0.00502947
Epoch [56/200], Train Loss: 0.004809
Validation Loss: 0.00502817
Epoch [57/200], Train Loss: 0.004791
Validation Loss: 0.00502794
Epoch [58/200], Train Loss: 0.004918
Validation Loss: 0.00502536
Epoch [59/200], Train Loss: 0.004861
Validation Loss: 0.00502704
Epoch [60/200], Train Loss: 0.004910
Validation Loss: 0.00502394
Epoch [61/200], Train Loss: 0.004844
Validation Loss: 0.00502422
Epoch [62/200], Train Loss: 0.004821
Validation Loss: 0.00502260
Epoch [63/200], Train Loss: 0.004832
Validation Loss: 0.00502316
Epoch [64/200], Train Loss: 0.004844
Validation Loss: 0.00502085
Epoch [65/200], Train Loss: 0.004851
Validation Loss: 0.00502062
Epoch [66/200], Train Loss: 0.004847
Validation Loss: 0.00501903
Epoch [67/200], Train Loss: 0.004808
Validation Loss: 0.00502003
Epoch [68/200], Train Loss: 0.004876
Validation Loss: 0.00502003
Epoch [69/200], Train Loss: 0.004808
Validation Loss: 0.00501799
Epoch [70/200], Train Loss: 0.004829
Validation Loss: 0.00501920
Epoch [71/200], Train Loss: 0.004849
Validation Loss: 0.00501588
Epoch [72/200], Train Loss: 0.004815
Validation Loss: 0.00501693
Epoch [73/200], Train Loss: 0.004899
Validation Loss: 0.00501474
Epoch [74/200], Train Loss: 0.004803
Validation Loss: 0.00501283
Epoch [75/200], Train Loss: 0.004841
Validation Loss: 0.00501408
Epoch [76/200], Train Loss: 0.004862
Validation Loss: 0.00501293
Epoch [77/200], Train Loss: 0.004884
Validation Loss: 0.00501355
Epoch [78/200], Train Loss: 0.004795
Validation Loss: 0.00501237
Epoch [79/200], Train Loss: 0.004859
Validation Loss: 0.00501128
Epoch [80/200], Train Loss: 0.004809
Validation Loss: 0.00501071
Epoch [81/200], Train Loss: 0.004854
Validation Loss: 0.00501153
Epoch [82/200], Train Loss: 0.004834
Validation Loss: 0.00500910
Epoch [83/200], Train Loss: 0.004829
Validation Loss: 0.00501273
Epoch [84/200], Train Loss: 0.004877
Validation Loss: 0.00501182
Epoch [85/200], Train Loss: 0.004780
Validation Loss: 0.00500753
Epoch [86/200], Train Loss: 0.004809
Validation Loss: 0.00501107
Epoch [87/200], Train Loss: 0.004798
Validation Loss: 0.00500741
Epoch [88/200], Train Loss: 0.004870
Validation Loss: 0.00500968
Epoch [89/200], Train Loss: 0.004781
Validation Loss: 0.00500689
Epoch [90/200], Train Loss: 0.004863
Validation Loss: 0.00501010
Epoch [91/200], Train Loss: 0.004859
Validation Loss: 0.00500620
Epoch [92/200], Train Loss: 0.004798
Validation Loss: 0.00501067
Epoch [93/200], Train Loss: 0.004805
Validation Loss: 0.00500698
Epoch [94/200], Train Loss: 0.004843
Validation Loss: 0.00500756
Epoch [95/200], Train Loss: 0.004860
Validation Loss: 0.00500796
Epoch [96/200], Train Loss: 0.004823
Validation Loss: 0.00500591
Epoch [97/200], Train Loss: 0.004893
Validation Loss: 0.00500814
Epoch [98/200], Train Loss: 0.004785
Validation Loss: 0.00500667
Epoch [99/200], Train Loss: 0.004822
Validation Loss: 0.00500900
Epoch [100/200], Train Loss: 0.004857
Validation Loss: 0.00500732
Epoch [101/200], Train Loss: 0.004786
Validation Loss: 0.00500801
Epoch [102/200], Train Loss: 0.004818
Validation Loss: 0.00500762
Epoch [103/200], Train Loss: 0.004839
Validation Loss: 0.00500674
Epoch [104/200], Train Loss: 0.004826
Validation Loss: 0.00500646
Epoch [105/200], Train Loss: 0.004876
Validation Loss: 0.00500954
Epoch [106/200], Train Loss: 0.004843
Validation Loss: 0.00500752
Early stopping triggered

Evaluating model for: Lamp
Run 31/144 completed in 186.96 seconds with: {'MAE': np.float32(2.5281258), 'MSE': np.float32(151.19374), 'RMSE': np.float32(12.296086), 'SAE': np.float32(0.105563), 'NDE': np.float32(0.9824472)}

Run 32/144: hidden=64, seq_len=720, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.005377
Validation Loss: 0.00531323
Epoch [2/200], Train Loss: 0.005030
Validation Loss: 0.00512486
Epoch [3/200], Train Loss: 0.004910
Validation Loss: 0.00512790
Epoch [4/200], Train Loss: 0.004889
Validation Loss: 0.00512179
Epoch [5/200], Train Loss: 0.004932
Validation Loss: 0.00511864
Epoch [6/200], Train Loss: 0.004857
Validation Loss: 0.00511867
Epoch [7/200], Train Loss: 0.004816
Validation Loss: 0.00511800
Epoch [8/200], Train Loss: 0.004960
Validation Loss: 0.00511766
Epoch [9/200], Train Loss: 0.004926
Validation Loss: 0.00511690
Epoch [10/200], Train Loss: 0.004941
Validation Loss: 0.00511610
Epoch [11/200], Train Loss: 0.004868
Validation Loss: 0.00511499
Epoch [12/200], Train Loss: 0.004918
Validation Loss: 0.00511355
Epoch [13/200], Train Loss: 0.004857
Validation Loss: 0.00511117
Epoch [14/200], Train Loss: 0.004949
Validation Loss: 0.00510752
Epoch [15/200], Train Loss: 0.004891
Validation Loss: 0.00510017
Epoch [16/200], Train Loss: 0.004911
Validation Loss: 0.00508949
Epoch [17/200], Train Loss: 0.004954
Validation Loss: 0.00507022
Epoch [18/200], Train Loss: 0.004904
Validation Loss: 0.00504225
Epoch [19/200], Train Loss: 0.004823
Validation Loss: 0.00502355
Epoch [20/200], Train Loss: 0.004781
Validation Loss: 0.00502184
Epoch [21/200], Train Loss: 0.004883
Validation Loss: 0.00502329
Epoch [22/200], Train Loss: 0.004802
Validation Loss: 0.00502007
Epoch [23/200], Train Loss: 0.004824
Validation Loss: 0.00501949
Epoch [24/200], Train Loss: 0.004845
Validation Loss: 0.00501854
Epoch [25/200], Train Loss: 0.004811
Validation Loss: 0.00501942
Epoch [26/200], Train Loss: 0.004862
Validation Loss: 0.00501679
Epoch [27/200], Train Loss: 0.004776
Validation Loss: 0.00501950
Epoch [28/200], Train Loss: 0.004853
Validation Loss: 0.00501514
Epoch [29/200], Train Loss: 0.004754
Validation Loss: 0.00501836
Epoch [30/200], Train Loss: 0.004807
Validation Loss: 0.00501423
Epoch [31/200], Train Loss: 0.004831
Validation Loss: 0.00501508
Epoch [32/200], Train Loss: 0.004822
Validation Loss: 0.00501401
Epoch [33/200], Train Loss: 0.004808
Validation Loss: 0.00501196
Epoch [34/200], Train Loss: 0.004836
Validation Loss: 0.00501415
Epoch [35/200], Train Loss: 0.004817
Validation Loss: 0.00501197
Epoch [36/200], Train Loss: 0.004773
Validation Loss: 0.00501299
Epoch [37/200], Train Loss: 0.004805
Validation Loss: 0.00501116
Epoch [38/200], Train Loss: 0.004760
Validation Loss: 0.00501312
Epoch [39/200], Train Loss: 0.004741
Validation Loss: 0.00501210
Epoch [40/200], Train Loss: 0.004853
Validation Loss: 0.00501138
Epoch [41/200], Train Loss: 0.004827
Validation Loss: 0.00501081
Epoch [42/200], Train Loss: 0.004813
Validation Loss: 0.00501090
Epoch [43/200], Train Loss: 0.004788
Validation Loss: 0.00501165
Epoch [44/200], Train Loss: 0.004827
Validation Loss: 0.00500832
Epoch [45/200], Train Loss: 0.004958
Validation Loss: 0.00501281
Epoch [46/200], Train Loss: 0.004708
Validation Loss: 0.00500848
Epoch [47/200], Train Loss: 0.004815
Validation Loss: 0.00500846
Epoch [48/200], Train Loss: 0.004773
Validation Loss: 0.00500836
Epoch [49/200], Train Loss: 0.004785
Validation Loss: 0.00500924
Epoch [50/200], Train Loss: 0.004794
Validation Loss: 0.00500827
Epoch [51/200], Train Loss: 0.004765
Validation Loss: 0.00500760
Epoch [52/200], Train Loss: 0.004746
Validation Loss: 0.00500791
Epoch [53/200], Train Loss: 0.004824
Validation Loss: 0.00500801
Epoch [54/200], Train Loss: 0.004802
Validation Loss: 0.00500770
Epoch [55/200], Train Loss: 0.004801
Validation Loss: 0.00500855
Epoch [56/200], Train Loss: 0.004804
Validation Loss: 0.00500740
Epoch [57/200], Train Loss: 0.004749
Validation Loss: 0.00500695
Epoch [58/200], Train Loss: 0.004820
Validation Loss: 0.00500633
Epoch [59/200], Train Loss: 0.004807
Validation Loss: 0.00500650
Epoch [60/200], Train Loss: 0.004782
Validation Loss: 0.00500786
Epoch [61/200], Train Loss: 0.004702
Validation Loss: 0.00500664
Epoch [62/200], Train Loss: 0.004815
Validation Loss: 0.00500611
Epoch [63/200], Train Loss: 0.004782
Validation Loss: 0.00500691
Epoch [64/200], Train Loss: 0.004754
Validation Loss: 0.00500557
Epoch [65/200], Train Loss: 0.004856
Validation Loss: 0.00500684
Epoch [66/200], Train Loss: 0.004882
Validation Loss: 0.00500481
Epoch [67/200], Train Loss: 0.004801
Validation Loss: 0.00500570
Epoch [68/200], Train Loss: 0.004765
Validation Loss: 0.00500350
Epoch [69/200], Train Loss: 0.004791
Validation Loss: 0.00500611
Epoch [70/200], Train Loss: 0.004808
Validation Loss: 0.00500513
Epoch [71/200], Train Loss: 0.004789
Validation Loss: 0.00500462
Epoch [72/200], Train Loss: 0.004809
Validation Loss: 0.00500342
Epoch [73/200], Train Loss: 0.004820
Validation Loss: 0.00500596
Epoch [74/200], Train Loss: 0.004814
Validation Loss: 0.00500235
Epoch [75/200], Train Loss: 0.004814
Validation Loss: 0.00500519
Epoch [76/200], Train Loss: 0.004765
Validation Loss: 0.00500240
Epoch [77/200], Train Loss: 0.004807
Validation Loss: 0.00500229
Epoch [78/200], Train Loss: 0.004787
Validation Loss: 0.00500235
Epoch [79/200], Train Loss: 0.004745
Validation Loss: 0.00500320
Epoch [80/200], Train Loss: 0.004800
Validation Loss: 0.00500371
Epoch [81/200], Train Loss: 0.004796
Validation Loss: 0.00500197
Epoch [82/200], Train Loss: 0.004860
Validation Loss: 0.00500255
Epoch [83/200], Train Loss: 0.004779
Validation Loss: 0.00500052
Epoch [84/200], Train Loss: 0.004838
Validation Loss: 0.00500306
Epoch [85/200], Train Loss: 0.004844
Validation Loss: 0.00500120
Epoch [86/200], Train Loss: 0.004770
Validation Loss: 0.00500119
Epoch [87/200], Train Loss: 0.004751
Validation Loss: 0.00500100
Epoch [88/200], Train Loss: 0.004735
Validation Loss: 0.00500041
Epoch [89/200], Train Loss: 0.004723
Validation Loss: 0.00500123
Epoch [90/200], Train Loss: 0.004746
Validation Loss: 0.00500056
Epoch [91/200], Train Loss: 0.004849
Validation Loss: 0.00499984
Epoch [92/200], Train Loss: 0.004853
Validation Loss: 0.00499981
Epoch [93/200], Train Loss: 0.004791
Validation Loss: 0.00499859
Epoch [94/200], Train Loss: 0.004780
Validation Loss: 0.00500015
Epoch [95/200], Train Loss: 0.004799
Validation Loss: 0.00499839
Epoch [96/200], Train Loss: 0.004730
Validation Loss: 0.00499809
Epoch [97/200], Train Loss: 0.004749
Validation Loss: 0.00499879
Epoch [98/200], Train Loss: 0.004766
Validation Loss: 0.00499926
Epoch [99/200], Train Loss: 0.004761
Validation Loss: 0.00499870
Epoch [100/200], Train Loss: 0.004822
Validation Loss: 0.00499789
Epoch [101/200], Train Loss: 0.004852
Validation Loss: 0.00499710
Epoch [102/200], Train Loss: 0.004805
Validation Loss: 0.00499836
Epoch [103/200], Train Loss: 0.004748
Validation Loss: 0.00499644
Epoch [104/200], Train Loss: 0.004782
Validation Loss: 0.00499830
Epoch [105/200], Train Loss: 0.004754
Validation Loss: 0.00499643
Epoch [106/200], Train Loss: 0.004814
Validation Loss: 0.00499717
Epoch [107/200], Train Loss: 0.004777
Validation Loss: 0.00499592
Epoch [108/200], Train Loss: 0.004765
Validation Loss: 0.00499717
Epoch [109/200], Train Loss: 0.004800
Validation Loss: 0.00499606
Epoch [110/200], Train Loss: 0.004855
Validation Loss: 0.00499672
Epoch [111/200], Train Loss: 0.004776
Validation Loss: 0.00499591
Epoch [112/200], Train Loss: 0.004836
Validation Loss: 0.00499631
Epoch [113/200], Train Loss: 0.004747
Validation Loss: 0.00499589
Epoch [114/200], Train Loss: 0.004848
Validation Loss: 0.00499630
Epoch [115/200], Train Loss: 0.004771
Validation Loss: 0.00499531
Epoch [116/200], Train Loss: 0.004806
Validation Loss: 0.00499534
Epoch [117/200], Train Loss: 0.004841
Validation Loss: 0.00499578
Epoch [118/200], Train Loss: 0.004874
Validation Loss: 0.00499578
Epoch [119/200], Train Loss: 0.004767
Validation Loss: 0.00499440
Epoch [120/200], Train Loss: 0.004741
Validation Loss: 0.00499556
Epoch [121/200], Train Loss: 0.004804
Validation Loss: 0.00499465
Epoch [122/200], Train Loss: 0.004863
Validation Loss: 0.00499457
Epoch [123/200], Train Loss: 0.004899
Validation Loss: 0.00499452
Epoch [124/200], Train Loss: 0.004792
Validation Loss: 0.00499415
Epoch [125/200], Train Loss: 0.004791
Validation Loss: 0.00499409
Epoch [126/200], Train Loss: 0.004810
Validation Loss: 0.00499455
Epoch [127/200], Train Loss: 0.004858
Validation Loss: 0.00499487
Epoch [128/200], Train Loss: 0.004746
Validation Loss: 0.00499339
Epoch [129/200], Train Loss: 0.004758
Validation Loss: 0.00499718
Epoch [130/200], Train Loss: 0.004806
Validation Loss: 0.00499415
Epoch [131/200], Train Loss: 0.004771
Validation Loss: 0.00499360
Epoch [132/200], Train Loss: 0.004782
Validation Loss: 0.00499414
Epoch [133/200], Train Loss: 0.004763
Validation Loss: 0.00499284
Epoch [134/200], Train Loss: 0.004776
Validation Loss: 0.00499417
Epoch [135/200], Train Loss: 0.004822
Validation Loss: 0.00499352
Epoch [136/200], Train Loss: 0.004848
Validation Loss: 0.00499452
Epoch [137/200], Train Loss: 0.004724
Validation Loss: 0.00499281
Epoch [138/200], Train Loss: 0.004716
Validation Loss: 0.00499278
Epoch [139/200], Train Loss: 0.004753
Validation Loss: 0.00499405
Epoch [140/200], Train Loss: 0.004806
Validation Loss: 0.00499336
Epoch [141/200], Train Loss: 0.004749
Validation Loss: 0.00499356
Epoch [142/200], Train Loss: 0.004795
Validation Loss: 0.00499403
Epoch [143/200], Train Loss: 0.004736
Validation Loss: 0.00499327
Epoch [144/200], Train Loss: 0.004847
Validation Loss: 0.00499299
Epoch [145/200], Train Loss: 0.004737
Validation Loss: 0.00499287
Epoch [146/200], Train Loss: 0.004730
Validation Loss: 0.00499426
Epoch [147/200], Train Loss: 0.004795
Validation Loss: 0.00499360
Epoch [148/200], Train Loss: 0.004777
Validation Loss: 0.00499278
Early stopping triggered

Evaluating model for: Lamp
Run 32/144 completed in 259.11 seconds with: {'MAE': np.float32(2.485678), 'MSE': np.float32(150.7191), 'RMSE': np.float32(12.276771), 'SAE': np.float32(0.038213365), 'NDE': np.float32(0.9809036)}

Run 33/144: hidden=64, seq_len=720, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.006724
Validation Loss: 0.00495629
Epoch [2/200], Train Loss: 0.006369
Validation Loss: 0.00462474
Epoch [3/200], Train Loss: 0.005963
Validation Loss: 0.00433266
Epoch [4/200], Train Loss: 0.005699
Validation Loss: 0.00409729
Epoch [5/200], Train Loss: 0.005505
Validation Loss: 0.00390867
Epoch [6/200], Train Loss: 0.005285
Validation Loss: 0.00376278
Epoch [7/200], Train Loss: 0.005156
Validation Loss: 0.00366147
Epoch [8/200], Train Loss: 0.005081
Validation Loss: 0.00360200
Epoch [9/200], Train Loss: 0.004961
Validation Loss: 0.00357256
Epoch [10/200], Train Loss: 0.004985
Validation Loss: 0.00356516
Epoch [11/200], Train Loss: 0.004938
Validation Loss: 0.00356818
Epoch [12/200], Train Loss: 0.004958
Validation Loss: 0.00357134
Epoch [13/200], Train Loss: 0.004961
Validation Loss: 0.00357138
Epoch [14/200], Train Loss: 0.004953
Validation Loss: 0.00356848
Epoch [15/200], Train Loss: 0.004959
Validation Loss: 0.00356478
Epoch [16/200], Train Loss: 0.004960
Validation Loss: 0.00356170
Epoch [17/200], Train Loss: 0.004951
Validation Loss: 0.00355922
Epoch [18/200], Train Loss: 0.004977
Validation Loss: 0.00355718
Epoch [19/200], Train Loss: 0.004948
Validation Loss: 0.00355573
Epoch [20/200], Train Loss: 0.004942
Validation Loss: 0.00355450
Epoch [21/200], Train Loss: 0.004926
Validation Loss: 0.00355358
Epoch [22/200], Train Loss: 0.004942
Validation Loss: 0.00355212
Epoch [23/200], Train Loss: 0.004933
Validation Loss: 0.00355063
Epoch [24/200], Train Loss: 0.004954
Validation Loss: 0.00354893
Epoch [25/200], Train Loss: 0.004915
Validation Loss: 0.00354787
Epoch [26/200], Train Loss: 0.004905
Validation Loss: 0.00354642
Epoch [27/200], Train Loss: 0.004945
Validation Loss: 0.00354427
Epoch [28/200], Train Loss: 0.004896
Validation Loss: 0.00354288
Epoch [29/200], Train Loss: 0.004948
Validation Loss: 0.00354073
Epoch [30/200], Train Loss: 0.004917
Validation Loss: 0.00353892
Epoch [31/200], Train Loss: 0.004932
Validation Loss: 0.00353709
Epoch [32/200], Train Loss: 0.004937
Validation Loss: 0.00353563
Epoch [33/200], Train Loss: 0.004909
Validation Loss: 0.00353472
Epoch [34/200], Train Loss: 0.004938
Validation Loss: 0.00353302
Epoch [35/200], Train Loss: 0.004920
Validation Loss: 0.00353143
Epoch [36/200], Train Loss: 0.004884
Validation Loss: 0.00352964
Epoch [37/200], Train Loss: 0.004924
Validation Loss: 0.00352759
Epoch [38/200], Train Loss: 0.004910
Validation Loss: 0.00352604
Epoch [39/200], Train Loss: 0.004888
Validation Loss: 0.00352501
Epoch [40/200], Train Loss: 0.004903
Validation Loss: 0.00352374
Epoch [41/200], Train Loss: 0.004887
Validation Loss: 0.00352186
Epoch [42/200], Train Loss: 0.004879
Validation Loss: 0.00352008
Epoch [43/200], Train Loss: 0.004897
Validation Loss: 0.00351878
Epoch [44/200], Train Loss: 0.004899
Validation Loss: 0.00351827
Epoch [45/200], Train Loss: 0.004909
Validation Loss: 0.00351745
Epoch [46/200], Train Loss: 0.004919
Validation Loss: 0.00351582
Epoch [47/200], Train Loss: 0.004919
Validation Loss: 0.00351543
Epoch [48/200], Train Loss: 0.004865
Validation Loss: 0.00351496
Epoch [49/200], Train Loss: 0.004905
Validation Loss: 0.00351396
Epoch [50/200], Train Loss: 0.004874
Validation Loss: 0.00351328
Epoch [51/200], Train Loss: 0.004902
Validation Loss: 0.00351181
Epoch [52/200], Train Loss: 0.004872
Validation Loss: 0.00351177
Epoch [53/200], Train Loss: 0.004914
Validation Loss: 0.00351124
Epoch [54/200], Train Loss: 0.004851
Validation Loss: 0.00351164
Epoch [55/200], Train Loss: 0.004862
Validation Loss: 0.00351084
Epoch [56/200], Train Loss: 0.004916
Validation Loss: 0.00350982
Epoch [57/200], Train Loss: 0.004893
Validation Loss: 0.00350989
Epoch [58/200], Train Loss: 0.004883
Validation Loss: 0.00350957
Epoch [59/200], Train Loss: 0.004874
Validation Loss: 0.00351027
Epoch [60/200], Train Loss: 0.004894
Validation Loss: 0.00350955
Epoch [61/200], Train Loss: 0.004910
Validation Loss: 0.00350873
Epoch [62/200], Train Loss: 0.004880
Validation Loss: 0.00350938
Epoch [63/200], Train Loss: 0.004901
Validation Loss: 0.00350931
Epoch [64/200], Train Loss: 0.004895
Validation Loss: 0.00350873
Epoch [65/200], Train Loss: 0.004882
Validation Loss: 0.00350923
Epoch [66/200], Train Loss: 0.004863
Validation Loss: 0.00350944
Epoch [67/200], Train Loss: 0.004861
Validation Loss: 0.00350926
Epoch [68/200], Train Loss: 0.004854
Validation Loss: 0.00350822
Epoch [69/200], Train Loss: 0.004880
Validation Loss: 0.00350729
Epoch [70/200], Train Loss: 0.004900
Validation Loss: 0.00350751
Epoch [71/200], Train Loss: 0.004871
Validation Loss: 0.00350934
Epoch [72/200], Train Loss: 0.004862
Validation Loss: 0.00350955
Epoch [73/200], Train Loss: 0.004873
Validation Loss: 0.00350865
Epoch [74/200], Train Loss: 0.004897
Validation Loss: 0.00350762
Epoch [75/200], Train Loss: 0.004882
Validation Loss: 0.00350829
Epoch [76/200], Train Loss: 0.004895
Validation Loss: 0.00350831
Epoch [77/200], Train Loss: 0.004886
Validation Loss: 0.00350833
Epoch [78/200], Train Loss: 0.004882
Validation Loss: 0.00350777
Epoch [79/200], Train Loss: 0.004915
Validation Loss: 0.00350790
Early stopping triggered

Evaluating model for: Lamp
Run 33/144 completed in 70.14 seconds with: {'MAE': np.float32(3.212706), 'MSE': np.float32(179.74687), 'RMSE': np.float32(13.406971), 'SAE': np.float32(0.020522414), 'NDE': np.float32(0.98469967)}

Run 34/144: hidden=64, seq_len=720, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.007974
Validation Loss: 0.00636406
Epoch [2/200], Train Loss: 0.007260
Validation Loss: 0.00567056
Epoch [3/200], Train Loss: 0.006654
Validation Loss: 0.00512157
Epoch [4/200], Train Loss: 0.006208
Validation Loss: 0.00469683
Epoch [5/200], Train Loss: 0.005770
Validation Loss: 0.00436562
Epoch [6/200], Train Loss: 0.005548
Validation Loss: 0.00410702
Epoch [7/200], Train Loss: 0.005339
Validation Loss: 0.00391474
Epoch [8/200], Train Loss: 0.005212
Validation Loss: 0.00377508
Epoch [9/200], Train Loss: 0.005106
Validation Loss: 0.00368344
Epoch [10/200], Train Loss: 0.005021
Validation Loss: 0.00362962
Epoch [11/200], Train Loss: 0.005016
Validation Loss: 0.00360116
Epoch [12/200], Train Loss: 0.005022
Validation Loss: 0.00358907
Epoch [13/200], Train Loss: 0.005019
Validation Loss: 0.00358481
Epoch [14/200], Train Loss: 0.005002
Validation Loss: 0.00358314
Epoch [15/200], Train Loss: 0.004988
Validation Loss: 0.00358303
Epoch [16/200], Train Loss: 0.005016
Validation Loss: 0.00358335
Epoch [17/200], Train Loss: 0.005005
Validation Loss: 0.00358410
Epoch [18/200], Train Loss: 0.004967
Validation Loss: 0.00358479
Epoch [19/200], Train Loss: 0.005031
Validation Loss: 0.00358468
Epoch [20/200], Train Loss: 0.005018
Validation Loss: 0.00358453
Epoch [21/200], Train Loss: 0.004984
Validation Loss: 0.00358409
Epoch [22/200], Train Loss: 0.004979
Validation Loss: 0.00358334
Epoch [23/200], Train Loss: 0.005000
Validation Loss: 0.00358215
Epoch [24/200], Train Loss: 0.004988
Validation Loss: 0.00358119
Epoch [25/200], Train Loss: 0.004993
Validation Loss: 0.00358012
Epoch [26/200], Train Loss: 0.004981
Validation Loss: 0.00357905
Epoch [27/200], Train Loss: 0.004975
Validation Loss: 0.00357831
Epoch [28/200], Train Loss: 0.004999
Validation Loss: 0.00357692
Epoch [29/200], Train Loss: 0.004973
Validation Loss: 0.00357591
Epoch [30/200], Train Loss: 0.004973
Validation Loss: 0.00357487
Epoch [31/200], Train Loss: 0.004962
Validation Loss: 0.00357406
Epoch [32/200], Train Loss: 0.004960
Validation Loss: 0.00357278
Epoch [33/200], Train Loss: 0.004981
Validation Loss: 0.00357159
Epoch [34/200], Train Loss: 0.005027
Validation Loss: 0.00357021
Epoch [35/200], Train Loss: 0.004956
Validation Loss: 0.00356960
Epoch [36/200], Train Loss: 0.004966
Validation Loss: 0.00356846
Epoch [37/200], Train Loss: 0.004969
Validation Loss: 0.00356734
Epoch [38/200], Train Loss: 0.004946
Validation Loss: 0.00356611
Epoch [39/200], Train Loss: 0.004968
Validation Loss: 0.00356439
Epoch [40/200], Train Loss: 0.004959
Validation Loss: 0.00356279
Epoch [41/200], Train Loss: 0.004981
Validation Loss: 0.00356092
Epoch [42/200], Train Loss: 0.004996
Validation Loss: 0.00355942
Epoch [43/200], Train Loss: 0.004959
Validation Loss: 0.00355823
Epoch [44/200], Train Loss: 0.004999
Validation Loss: 0.00355645
Epoch [45/200], Train Loss: 0.004944
Validation Loss: 0.00355522
Epoch [46/200], Train Loss: 0.004941
Validation Loss: 0.00355310
Epoch [47/200], Train Loss: 0.004942
Validation Loss: 0.00355028
Epoch [48/200], Train Loss: 0.004952
Validation Loss: 0.00354803
Epoch [49/200], Train Loss: 0.004981
Validation Loss: 0.00354552
Epoch [50/200], Train Loss: 0.004894
Validation Loss: 0.00354402
Epoch [51/200], Train Loss: 0.004961
Validation Loss: 0.00354151
Epoch [52/200], Train Loss: 0.004937
Validation Loss: 0.00353902
Epoch [53/200], Train Loss: 0.004917
Validation Loss: 0.00353653
Epoch [54/200], Train Loss: 0.004928
Validation Loss: 0.00353426
Epoch [55/200], Train Loss: 0.004910
Validation Loss: 0.00353190
Epoch [56/200], Train Loss: 0.004921
Validation Loss: 0.00352905
Epoch [57/200], Train Loss: 0.004899
Validation Loss: 0.00352641
Epoch [58/200], Train Loss: 0.004908
Validation Loss: 0.00352436
Epoch [59/200], Train Loss: 0.004936
Validation Loss: 0.00352168
Epoch [60/200], Train Loss: 0.004903
Validation Loss: 0.00351971
Epoch [61/200], Train Loss: 0.004975
Validation Loss: 0.00351695
Epoch [62/200], Train Loss: 0.004905
Validation Loss: 0.00351538
Epoch [63/200], Train Loss: 0.004895
Validation Loss: 0.00351401
Epoch [64/200], Train Loss: 0.004958
Validation Loss: 0.00351170
Epoch [65/200], Train Loss: 0.004892
Validation Loss: 0.00351210
Epoch [66/200], Train Loss: 0.004906
Validation Loss: 0.00351097
Epoch [67/200], Train Loss: 0.004896
Validation Loss: 0.00351068
Epoch [68/200], Train Loss: 0.004910
Validation Loss: 0.00350896
Epoch [69/200], Train Loss: 0.004911
Validation Loss: 0.00350835
Epoch [70/200], Train Loss: 0.004892
Validation Loss: 0.00350900
Epoch [71/200], Train Loss: 0.004930
Validation Loss: 0.00350945
Epoch [72/200], Train Loss: 0.004864
Validation Loss: 0.00351098
Epoch [73/200], Train Loss: 0.004892
Validation Loss: 0.00350862
Epoch [74/200], Train Loss: 0.004898
Validation Loss: 0.00350632
Epoch [75/200], Train Loss: 0.004929
Validation Loss: 0.00350662
Epoch [76/200], Train Loss: 0.004937
Validation Loss: 0.00350686
Epoch [77/200], Train Loss: 0.004891
Validation Loss: 0.00350878
Epoch [78/200], Train Loss: 0.004892
Validation Loss: 0.00350777
Epoch [79/200], Train Loss: 0.004914
Validation Loss: 0.00350568
Epoch [80/200], Train Loss: 0.004893
Validation Loss: 0.00350661
Epoch [81/200], Train Loss: 0.004867
Validation Loss: 0.00350681
Epoch [82/200], Train Loss: 0.004904
Validation Loss: 0.00350524
Epoch [83/200], Train Loss: 0.004879
Validation Loss: 0.00350561
Epoch [84/200], Train Loss: 0.004898
Validation Loss: 0.00350592
Epoch [85/200], Train Loss: 0.004890
Validation Loss: 0.00350582
Epoch [86/200], Train Loss: 0.004858
Validation Loss: 0.00350566
Epoch [87/200], Train Loss: 0.004940
Validation Loss: 0.00350469
Epoch [88/200], Train Loss: 0.004854
Validation Loss: 0.00350470
Epoch [89/200], Train Loss: 0.004850
Validation Loss: 0.00350528
Epoch [90/200], Train Loss: 0.004907
Validation Loss: 0.00350485
Epoch [91/200], Train Loss: 0.004918
Validation Loss: 0.00350480
Epoch [92/200], Train Loss: 0.004923
Validation Loss: 0.00350467
Epoch [93/200], Train Loss: 0.004856
Validation Loss: 0.00350476
Epoch [94/200], Train Loss: 0.004919
Validation Loss: 0.00350396
Epoch [95/200], Train Loss: 0.004871
Validation Loss: 0.00350447
Epoch [96/200], Train Loss: 0.004879
Validation Loss: 0.00350487
Epoch [97/200], Train Loss: 0.004932
Validation Loss: 0.00350367
Epoch [98/200], Train Loss: 0.004913
Validation Loss: 0.00350402
Epoch [99/200], Train Loss: 0.004898
Validation Loss: 0.00350434
Epoch [100/200], Train Loss: 0.004872
Validation Loss: 0.00350490
Epoch [101/200], Train Loss: 0.004904
Validation Loss: 0.00350337
Epoch [102/200], Train Loss: 0.004914
Validation Loss: 0.00350269
Epoch [103/200], Train Loss: 0.004889
Validation Loss: 0.00350398
Epoch [104/200], Train Loss: 0.004874
Validation Loss: 0.00350346
Epoch [105/200], Train Loss: 0.004896
Validation Loss: 0.00350346
Epoch [106/200], Train Loss: 0.004881
Validation Loss: 0.00350205
Epoch [107/200], Train Loss: 0.004883
Validation Loss: 0.00350200
Epoch [108/200], Train Loss: 0.004853
Validation Loss: 0.00350400
Epoch [109/200], Train Loss: 0.004891
Validation Loss: 0.00350269
Epoch [110/200], Train Loss: 0.004890
Validation Loss: 0.00350232
Epoch [111/200], Train Loss: 0.004915
Validation Loss: 0.00350259
Epoch [112/200], Train Loss: 0.004863
Validation Loss: 0.00350285
Epoch [113/200], Train Loss: 0.004889
Validation Loss: 0.00350241
Epoch [114/200], Train Loss: 0.004867
Validation Loss: 0.00350258
Epoch [115/200], Train Loss: 0.004934
Validation Loss: 0.00350204
Epoch [116/200], Train Loss: 0.004917
Validation Loss: 0.00350130
Epoch [117/200], Train Loss: 0.004901
Validation Loss: 0.00350322
Epoch [118/200], Train Loss: 0.004878
Validation Loss: 0.00350320
Epoch [119/200], Train Loss: 0.004859
Validation Loss: 0.00350247
Epoch [120/200], Train Loss: 0.004858
Validation Loss: 0.00350144
Epoch [121/200], Train Loss: 0.004880
Validation Loss: 0.00350138
Epoch [122/200], Train Loss: 0.004868
Validation Loss: 0.00350195
Epoch [123/200], Train Loss: 0.004911
Validation Loss: 0.00350087
Epoch [124/200], Train Loss: 0.004885
Validation Loss: 0.00350192
Epoch [125/200], Train Loss: 0.004909
Validation Loss: 0.00350124
Epoch [126/200], Train Loss: 0.004893
Validation Loss: 0.00350273
Epoch [127/200], Train Loss: 0.004856
Validation Loss: 0.00350248
Epoch [128/200], Train Loss: 0.004847
Validation Loss: 0.00350281
Epoch [129/200], Train Loss: 0.004918
Validation Loss: 0.00350081
Epoch [130/200], Train Loss: 0.004880
Validation Loss: 0.00350040
Epoch [131/200], Train Loss: 0.004899
Validation Loss: 0.00350033
Epoch [132/200], Train Loss: 0.004875
Validation Loss: 0.00350133
Epoch [133/200], Train Loss: 0.004886
Validation Loss: 0.00350182
Epoch [134/200], Train Loss: 0.004870
Validation Loss: 0.00350229
Epoch [135/200], Train Loss: 0.004893
Validation Loss: 0.00350126
Epoch [136/200], Train Loss: 0.004892
Validation Loss: 0.00350079
Epoch [137/200], Train Loss: 0.004896
Validation Loss: 0.00350039
Epoch [138/200], Train Loss: 0.004833
Validation Loss: 0.00350102
Epoch [139/200], Train Loss: 0.004857
Validation Loss: 0.00350097
Epoch [140/200], Train Loss: 0.004842
Validation Loss: 0.00350136
Epoch [141/200], Train Loss: 0.004862
Validation Loss: 0.00350151
Early stopping triggered

Evaluating model for: Lamp
Run 34/144 completed in 119.91 seconds with: {'MAE': np.float32(3.2695546), 'MSE': np.float32(179.62976), 'RMSE': np.float32(13.402603), 'SAE': np.float32(0.1270045), 'NDE': np.float32(0.98437876)}

Run 35/144: hidden=64, seq_len=720, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.013376
Validation Loss: 0.01082551
Epoch [2/200], Train Loss: 0.012106
Validation Loss: 0.00964750
Epoch [3/200], Train Loss: 0.010916
Validation Loss: 0.00858074
Epoch [4/200], Train Loss: 0.009866
Validation Loss: 0.00762039
Epoch [5/200], Train Loss: 0.008902
Validation Loss: 0.00675687
Epoch [6/200], Train Loss: 0.008104
Validation Loss: 0.00601882
Epoch [7/200], Train Loss: 0.007415
Validation Loss: 0.00541236
Epoch [8/200], Train Loss: 0.006788
Validation Loss: 0.00488250
Epoch [9/200], Train Loss: 0.006263
Validation Loss: 0.00444680
Epoch [10/200], Train Loss: 0.005787
Validation Loss: 0.00409117
Epoch [11/200], Train Loss: 0.005459
Validation Loss: 0.00381749
Epoch [12/200], Train Loss: 0.005220
Validation Loss: 0.00365265
Epoch [13/200], Train Loss: 0.005060
Validation Loss: 0.00358320
Epoch [14/200], Train Loss: 0.005011
Validation Loss: 0.00358532
Epoch [15/200], Train Loss: 0.004995
Validation Loss: 0.00360692
Epoch [16/200], Train Loss: 0.005019
Validation Loss: 0.00360609
Epoch [17/200], Train Loss: 0.004993
Validation Loss: 0.00359314
Epoch [18/200], Train Loss: 0.004958
Validation Loss: 0.00358312
Epoch [19/200], Train Loss: 0.005026
Validation Loss: 0.00357799
Epoch [20/200], Train Loss: 0.005009
Validation Loss: 0.00357616
Epoch [21/200], Train Loss: 0.004985
Validation Loss: 0.00357541
Epoch [22/200], Train Loss: 0.004996
Validation Loss: 0.00357517
Epoch [23/200], Train Loss: 0.005030
Validation Loss: 0.00357529
Epoch [24/200], Train Loss: 0.004959
Validation Loss: 0.00357602
Epoch [25/200], Train Loss: 0.005027
Validation Loss: 0.00357528
Epoch [26/200], Train Loss: 0.004988
Validation Loss: 0.00357495
Epoch [27/200], Train Loss: 0.004974
Validation Loss: 0.00357351
Epoch [28/200], Train Loss: 0.004952
Validation Loss: 0.00357215
Epoch [29/200], Train Loss: 0.004963
Validation Loss: 0.00357116
Epoch [30/200], Train Loss: 0.004984
Validation Loss: 0.00356968
Epoch [31/200], Train Loss: 0.005047
Validation Loss: 0.00356829
Epoch [32/200], Train Loss: 0.005004
Validation Loss: 0.00356763
Epoch [33/200], Train Loss: 0.004983
Validation Loss: 0.00356711
Epoch [34/200], Train Loss: 0.004981
Validation Loss: 0.00356643
Epoch [35/200], Train Loss: 0.005003
Validation Loss: 0.00356460
Epoch [36/200], Train Loss: 0.005010
Validation Loss: 0.00356323
Epoch [37/200], Train Loss: 0.004979
Validation Loss: 0.00356153
Epoch [38/200], Train Loss: 0.004962
Validation Loss: 0.00356042
Epoch [39/200], Train Loss: 0.004958
Validation Loss: 0.00355890
Epoch [40/200], Train Loss: 0.004968
Validation Loss: 0.00355779
Epoch [41/200], Train Loss: 0.004936
Validation Loss: 0.00355628
Epoch [42/200], Train Loss: 0.004960
Validation Loss: 0.00355356
Epoch [43/200], Train Loss: 0.004949
Validation Loss: 0.00355231
Epoch [44/200], Train Loss: 0.004952
Validation Loss: 0.00355071
Epoch [45/200], Train Loss: 0.004975
Validation Loss: 0.00354873
Epoch [46/200], Train Loss: 0.004941
Validation Loss: 0.00354755
Epoch [47/200], Train Loss: 0.004990
Validation Loss: 0.00354532
Epoch [48/200], Train Loss: 0.004941
Validation Loss: 0.00354439
Epoch [49/200], Train Loss: 0.004970
Validation Loss: 0.00354132
Epoch [50/200], Train Loss: 0.004906
Validation Loss: 0.00353989
Epoch [51/200], Train Loss: 0.004958
Validation Loss: 0.00353744
Epoch [52/200], Train Loss: 0.004955
Validation Loss: 0.00353578
Epoch [53/200], Train Loss: 0.004928
Validation Loss: 0.00353418
Epoch [54/200], Train Loss: 0.004977
Validation Loss: 0.00353176
Epoch [55/200], Train Loss: 0.004945
Validation Loss: 0.00353047
Epoch [56/200], Train Loss: 0.004928
Validation Loss: 0.00352907
Epoch [57/200], Train Loss: 0.004934
Validation Loss: 0.00352811
Epoch [58/200], Train Loss: 0.004900
Validation Loss: 0.00352620
Epoch [59/200], Train Loss: 0.004951
Validation Loss: 0.00352306
Epoch [60/200], Train Loss: 0.004938
Validation Loss: 0.00352395
Epoch [61/200], Train Loss: 0.004929
Validation Loss: 0.00352350
Epoch [62/200], Train Loss: 0.004933
Validation Loss: 0.00352160
Epoch [63/200], Train Loss: 0.004876
Validation Loss: 0.00352123
Epoch [64/200], Train Loss: 0.004903
Validation Loss: 0.00352054
Epoch [65/200], Train Loss: 0.004935
Validation Loss: 0.00352017
Epoch [66/200], Train Loss: 0.004898
Validation Loss: 0.00352168
Epoch [67/200], Train Loss: 0.004976
Validation Loss: 0.00351833
Epoch [68/200], Train Loss: 0.004908
Validation Loss: 0.00351958
Epoch [69/200], Train Loss: 0.004924
Validation Loss: 0.00351934
Epoch [70/200], Train Loss: 0.004917
Validation Loss: 0.00351873
Epoch [71/200], Train Loss: 0.004904
Validation Loss: 0.00351915
Epoch [72/200], Train Loss: 0.004928
Validation Loss: 0.00351922
Epoch [73/200], Train Loss: 0.004981
Validation Loss: 0.00351758
Epoch [74/200], Train Loss: 0.004972
Validation Loss: 0.00351789
Epoch [75/200], Train Loss: 0.004921
Validation Loss: 0.00351953
Epoch [76/200], Train Loss: 0.004880
Validation Loss: 0.00351983
Epoch [77/200], Train Loss: 0.004953
Validation Loss: 0.00351727
Epoch [78/200], Train Loss: 0.004906
Validation Loss: 0.00351815
Epoch [79/200], Train Loss: 0.004920
Validation Loss: 0.00351779
Epoch [80/200], Train Loss: 0.004911
Validation Loss: 0.00351665
Epoch [81/200], Train Loss: 0.004901
Validation Loss: 0.00351806
Epoch [82/200], Train Loss: 0.004940
Validation Loss: 0.00351728
Epoch [83/200], Train Loss: 0.004892
Validation Loss: 0.00351854
Epoch [84/200], Train Loss: 0.004950
Validation Loss: 0.00351737
Epoch [85/200], Train Loss: 0.004911
Validation Loss: 0.00351673
Epoch [86/200], Train Loss: 0.004899
Validation Loss: 0.00351810
Epoch [87/200], Train Loss: 0.004922
Validation Loss: 0.00351619
Epoch [88/200], Train Loss: 0.004918
Validation Loss: 0.00351631
Epoch [89/200], Train Loss: 0.004881
Validation Loss: 0.00351765
Epoch [90/200], Train Loss: 0.004917
Validation Loss: 0.00351779
Epoch [91/200], Train Loss: 0.004904
Validation Loss: 0.00351797
Epoch [92/200], Train Loss: 0.004945
Validation Loss: 0.00351578
Epoch [93/200], Train Loss: 0.004899
Validation Loss: 0.00351598
Epoch [94/200], Train Loss: 0.004939
Validation Loss: 0.00351679
Epoch [95/200], Train Loss: 0.004946
Validation Loss: 0.00351582
Epoch [96/200], Train Loss: 0.004929
Validation Loss: 0.00351594
Epoch [97/200], Train Loss: 0.004956
Validation Loss: 0.00351826
Epoch [98/200], Train Loss: 0.004917
Validation Loss: 0.00351735
Epoch [99/200], Train Loss: 0.004897
Validation Loss: 0.00351651
Epoch [100/200], Train Loss: 0.004932
Validation Loss: 0.00351560
Epoch [101/200], Train Loss: 0.004935
Validation Loss: 0.00351557
Epoch [102/200], Train Loss: 0.004893
Validation Loss: 0.00351710
Epoch [103/200], Train Loss: 0.004925
Validation Loss: 0.00351591
Epoch [104/200], Train Loss: 0.004915
Validation Loss: 0.00351648
Epoch [105/200], Train Loss: 0.004933
Validation Loss: 0.00351588
Epoch [106/200], Train Loss: 0.004914
Validation Loss: 0.00351535
Epoch [107/200], Train Loss: 0.004932
Validation Loss: 0.00351545
Epoch [108/200], Train Loss: 0.004909
Validation Loss: 0.00351524
Epoch [109/200], Train Loss: 0.004937
Validation Loss: 0.00351647
Epoch [110/200], Train Loss: 0.004887
Validation Loss: 0.00351683
Epoch [111/200], Train Loss: 0.004903
Validation Loss: 0.00351591
Epoch [112/200], Train Loss: 0.004916
Validation Loss: 0.00351483
Epoch [113/200], Train Loss: 0.004923
Validation Loss: 0.00351436
Epoch [114/200], Train Loss: 0.004906
Validation Loss: 0.00351447
Epoch [115/200], Train Loss: 0.004877
Validation Loss: 0.00351598
Epoch [116/200], Train Loss: 0.004883
Validation Loss: 0.00351608
Epoch [117/200], Train Loss: 0.004872
Validation Loss: 0.00351421
Epoch [118/200], Train Loss: 0.004956
Validation Loss: 0.00351359
Epoch [119/200], Train Loss: 0.004906
Validation Loss: 0.00351554
Epoch [120/200], Train Loss: 0.004915
Validation Loss: 0.00351575
Epoch [121/200], Train Loss: 0.004933
Validation Loss: 0.00351524
Epoch [122/200], Train Loss: 0.004901
Validation Loss: 0.00351499
Epoch [123/200], Train Loss: 0.004935
Validation Loss: 0.00351382
Epoch [124/200], Train Loss: 0.004886
Validation Loss: 0.00351434
Epoch [125/200], Train Loss: 0.004920
Validation Loss: 0.00351457
Epoch [126/200], Train Loss: 0.004906
Validation Loss: 0.00351419
Epoch [127/200], Train Loss: 0.004906
Validation Loss: 0.00351434
Epoch [128/200], Train Loss: 0.004931
Validation Loss: 0.00351425
Early stopping triggered

Evaluating model for: Lamp
Run 35/144 completed in 112.47 seconds with: {'MAE': np.float32(3.223243), 'MSE': np.float32(179.80849), 'RMSE': np.float32(13.409268), 'SAE': np.float32(0.019951528), 'NDE': np.float32(0.9848683)}

Run 36/144: hidden=64, seq_len=720, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.005601
Validation Loss: 0.00411675
Epoch [2/200], Train Loss: 0.005321
Validation Loss: 0.00384388
Epoch [3/200], Train Loss: 0.005076
Validation Loss: 0.00367945
Epoch [4/200], Train Loss: 0.005022
Validation Loss: 0.00360083
Epoch [5/200], Train Loss: 0.004964
Validation Loss: 0.00357874
Epoch [6/200], Train Loss: 0.004967
Validation Loss: 0.00357769
Epoch [7/200], Train Loss: 0.004934
Validation Loss: 0.00357743
Epoch [8/200], Train Loss: 0.004963
Validation Loss: 0.00357750
Epoch [9/200], Train Loss: 0.005010
Validation Loss: 0.00358007
Epoch [10/200], Train Loss: 0.004961
Validation Loss: 0.00358400
Epoch [11/200], Train Loss: 0.005005
Validation Loss: 0.00358568
Epoch [12/200], Train Loss: 0.004944
Validation Loss: 0.00358541
Epoch [13/200], Train Loss: 0.004932
Validation Loss: 0.00358395
Epoch [14/200], Train Loss: 0.004967
Validation Loss: 0.00358202
Epoch [15/200], Train Loss: 0.004976
Validation Loss: 0.00358093
Epoch [16/200], Train Loss: 0.005003
Validation Loss: 0.00358026
Epoch [17/200], Train Loss: 0.004940
Validation Loss: 0.00358088
Early stopping triggered

Evaluating model for: Lamp
Run 36/144 completed in 16.89 seconds with: {'MAE': np.float32(2.900712), 'MSE': np.float32(182.15784), 'RMSE': np.float32(13.496586), 'SAE': np.float32(0.05957468), 'NDE': np.float32(0.9912815)}

Run 37/144: hidden=64, seq_len=1080, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.006362
Validation Loss: 0.00559836
Epoch [2/200], Train Loss: 0.005491
Validation Loss: 0.00497214
Epoch [3/200], Train Loss: 0.005084
Validation Loss: 0.00477138
Epoch [4/200], Train Loss: 0.004992
Validation Loss: 0.00475939
Epoch [5/200], Train Loss: 0.004992
Validation Loss: 0.00475322
Epoch [6/200], Train Loss: 0.005003
Validation Loss: 0.00474859
Epoch [7/200], Train Loss: 0.004993
Validation Loss: 0.00474321
Epoch [8/200], Train Loss: 0.005015
Validation Loss: 0.00473765
Epoch [9/200], Train Loss: 0.004954
Validation Loss: 0.00473173
Epoch [10/200], Train Loss: 0.004957
Validation Loss: 0.00472565
Epoch [11/200], Train Loss: 0.004947
Validation Loss: 0.00472019
Epoch [12/200], Train Loss: 0.004941
Validation Loss: 0.00471550
Epoch [13/200], Train Loss: 0.004938
Validation Loss: 0.00471120
Epoch [14/200], Train Loss: 0.004934
Validation Loss: 0.00470859
Epoch [15/200], Train Loss: 0.004921
Validation Loss: 0.00470693
Epoch [16/200], Train Loss: 0.004941
Validation Loss: 0.00470606
Epoch [17/200], Train Loss: 0.004933
Validation Loss: 0.00470572
Epoch [18/200], Train Loss: 0.004911
Validation Loss: 0.00470548
Epoch [19/200], Train Loss: 0.004930
Validation Loss: 0.00470571
Epoch [20/200], Train Loss: 0.004923
Validation Loss: 0.00470507
Epoch [21/200], Train Loss: 0.004904
Validation Loss: 0.00470476
Epoch [22/200], Train Loss: 0.004919
Validation Loss: 0.00470398
Epoch [23/200], Train Loss: 0.004940
Validation Loss: 0.00470386
Epoch [24/200], Train Loss: 0.004924
Validation Loss: 0.00470416
Epoch [25/200], Train Loss: 0.004905
Validation Loss: 0.00470218
Epoch [26/200], Train Loss: 0.004907
Validation Loss: 0.00470189
Epoch [27/200], Train Loss: 0.004921
Validation Loss: 0.00470263
Epoch [28/200], Train Loss: 0.004900
Validation Loss: 0.00470086
Epoch [29/200], Train Loss: 0.004906
Validation Loss: 0.00470057
Epoch [30/200], Train Loss: 0.004898
Validation Loss: 0.00469920
Epoch [31/200], Train Loss: 0.004914
Validation Loss: 0.00469857
Epoch [32/200], Train Loss: 0.004906
Validation Loss: 0.00469770
Epoch [33/200], Train Loss: 0.004902
Validation Loss: 0.00469879
Epoch [34/200], Train Loss: 0.004900
Validation Loss: 0.00469598
Epoch [35/200], Train Loss: 0.004898
Validation Loss: 0.00469534
Epoch [36/200], Train Loss: 0.004899
Validation Loss: 0.00469708
Epoch [37/200], Train Loss: 0.004894
Validation Loss: 0.00469284
Epoch [38/200], Train Loss: 0.004873
Validation Loss: 0.00469158
Epoch [39/200], Train Loss: 0.004898
Validation Loss: 0.00469397
Epoch [40/200], Train Loss: 0.004886
Validation Loss: 0.00469010
Epoch [41/200], Train Loss: 0.004886
Validation Loss: 0.00468840
Epoch [42/200], Train Loss: 0.004890
Validation Loss: 0.00468724
Epoch [43/200], Train Loss: 0.004876
Validation Loss: 0.00468553
Epoch [44/200], Train Loss: 0.004886
Validation Loss: 0.00468385
Epoch [45/200], Train Loss: 0.004897
Validation Loss: 0.00468269
Epoch [46/200], Train Loss: 0.004888
Validation Loss: 0.00468245
Epoch [47/200], Train Loss: 0.004892
Validation Loss: 0.00468038
Epoch [48/200], Train Loss: 0.004907
Validation Loss: 0.00467899
Epoch [49/200], Train Loss: 0.004886
Validation Loss: 0.00467822
Epoch [50/200], Train Loss: 0.004901
Validation Loss: 0.00467756
Epoch [51/200], Train Loss: 0.004886
Validation Loss: 0.00467576
Epoch [52/200], Train Loss: 0.004869
Validation Loss: 0.00467421
Epoch [53/200], Train Loss: 0.004884
Validation Loss: 0.00467508
Epoch [54/200], Train Loss: 0.004885
Validation Loss: 0.00467246
Epoch [55/200], Train Loss: 0.004894
Validation Loss: 0.00467075
Epoch [56/200], Train Loss: 0.004858
Validation Loss: 0.00467146
Epoch [57/200], Train Loss: 0.004860
Validation Loss: 0.00466958
Epoch [58/200], Train Loss: 0.004873
Validation Loss: 0.00466790
Epoch [59/200], Train Loss: 0.004869
Validation Loss: 0.00466783
Epoch [60/200], Train Loss: 0.004872
Validation Loss: 0.00466740
Epoch [61/200], Train Loss: 0.004883
Validation Loss: 0.00466394
Epoch [62/200], Train Loss: 0.004862
Validation Loss: 0.00466573
Epoch [63/200], Train Loss: 0.004873
Validation Loss: 0.00466194
Epoch [64/200], Train Loss: 0.004866
Validation Loss: 0.00466092
Epoch [65/200], Train Loss: 0.004861
Validation Loss: 0.00465838
Epoch [66/200], Train Loss: 0.004863
Validation Loss: 0.00465770
Epoch [67/200], Train Loss: 0.004845
Validation Loss: 0.00466081
Epoch [68/200], Train Loss: 0.004835
Validation Loss: 0.00465308
Epoch [69/200], Train Loss: 0.004844
Validation Loss: 0.00465284
Epoch [70/200], Train Loss: 0.004834
Validation Loss: 0.00464894
Epoch [71/200], Train Loss: 0.004845
Validation Loss: 0.00465474
Epoch [72/200], Train Loss: 0.004851
Validation Loss: 0.00464542
Epoch [73/200], Train Loss: 0.004848
Validation Loss: 0.00464129
Epoch [74/200], Train Loss: 0.004851
Validation Loss: 0.00464048
Epoch [75/200], Train Loss: 0.004831
Validation Loss: 0.00463541
Epoch [76/200], Train Loss: 0.004862
Validation Loss: 0.00463222
Epoch [77/200], Train Loss: 0.004839
Validation Loss: 0.00463092
Epoch [78/200], Train Loss: 0.004820
Validation Loss: 0.00462536
Epoch [79/200], Train Loss: 0.004827
Validation Loss: 0.00462249
Epoch [80/200], Train Loss: 0.004831
Validation Loss: 0.00462054
Epoch [81/200], Train Loss: 0.004827
Validation Loss: 0.00461450
Epoch [82/200], Train Loss: 0.004822
Validation Loss: 0.00461079
Epoch [83/200], Train Loss: 0.004798
Validation Loss: 0.00460733
Epoch [84/200], Train Loss: 0.004800
Validation Loss: 0.00460538
Epoch [85/200], Train Loss: 0.004813
Validation Loss: 0.00460082
Epoch [86/200], Train Loss: 0.004799
Validation Loss: 0.00459759
Epoch [87/200], Train Loss: 0.004798
Validation Loss: 0.00458975
Epoch [88/200], Train Loss: 0.004787
Validation Loss: 0.00458259
Epoch [89/200], Train Loss: 0.004774
Validation Loss: 0.00457917
Epoch [90/200], Train Loss: 0.004776
Validation Loss: 0.00456975
Epoch [91/200], Train Loss: 0.004775
Validation Loss: 0.00456616
Epoch [92/200], Train Loss: 0.004794
Validation Loss: 0.00455972
Epoch [93/200], Train Loss: 0.004761
Validation Loss: 0.00455307
Epoch [94/200], Train Loss: 0.004749
Validation Loss: 0.00455346
Epoch [95/200], Train Loss: 0.004745
Validation Loss: 0.00453909
Epoch [96/200], Train Loss: 0.004739
Validation Loss: 0.00453118
Epoch [97/200], Train Loss: 0.004754
Validation Loss: 0.00452399
Epoch [98/200], Train Loss: 0.004733
Validation Loss: 0.00451464
Epoch [99/200], Train Loss: 0.004754
Validation Loss: 0.00453380
Epoch [100/200], Train Loss: 0.004731
Validation Loss: 0.00456206
Epoch [101/200], Train Loss: 0.004734
Validation Loss: 0.00450024
Epoch [102/200], Train Loss: 0.004700
Validation Loss: 0.00449850
Epoch [103/200], Train Loss: 0.004713
Validation Loss: 0.00450895
Epoch [104/200], Train Loss: 0.004715
Validation Loss: 0.00447571
Epoch [105/200], Train Loss: 0.004686
Validation Loss: 0.00446368
Epoch [106/200], Train Loss: 0.004670
Validation Loss: 0.00445591
Epoch [107/200], Train Loss: 0.004677
Validation Loss: 0.00445415
Epoch [108/200], Train Loss: 0.004645
Validation Loss: 0.00444393
Epoch [109/200], Train Loss: 0.004666
Validation Loss: 0.00442381
Epoch [110/200], Train Loss: 0.004632
Validation Loss: 0.00441108
Epoch [111/200], Train Loss: 0.004648
Validation Loss: 0.00440815
Epoch [112/200], Train Loss: 0.004615
Validation Loss: 0.00438763
Epoch [113/200], Train Loss: 0.004619
Validation Loss: 0.00437328
Epoch [114/200], Train Loss: 0.004611
Validation Loss: 0.00436658
Epoch [115/200], Train Loss: 0.004608
Validation Loss: 0.00436049
Epoch [116/200], Train Loss: 0.004562
Validation Loss: 0.00433849
Epoch [117/200], Train Loss: 0.004552
Validation Loss: 0.00432931
Epoch [118/200], Train Loss: 0.004576
Validation Loss: 0.00432464
Epoch [119/200], Train Loss: 0.004580
Validation Loss: 0.00431326
Epoch [120/200], Train Loss: 0.004525
Validation Loss: 0.00430749
Epoch [121/200], Train Loss: 0.004542
Validation Loss: 0.00429787
Epoch [122/200], Train Loss: 0.004514
Validation Loss: 0.00428531
Epoch [123/200], Train Loss: 0.004506
Validation Loss: 0.00428228
Epoch [124/200], Train Loss: 0.004517
Validation Loss: 0.00426866
Epoch [125/200], Train Loss: 0.004514
Validation Loss: 0.00430592
Epoch [126/200], Train Loss: 0.004508
Validation Loss: 0.00426989
Epoch [127/200], Train Loss: 0.004488
Validation Loss: 0.00425438
Epoch [128/200], Train Loss: 0.004484
Validation Loss: 0.00423897
Epoch [129/200], Train Loss: 0.004486
Validation Loss: 0.00423728
Epoch [130/200], Train Loss: 0.004497
Validation Loss: 0.00425599
Epoch [131/200], Train Loss: 0.004439
Validation Loss: 0.00422072
Epoch [132/200], Train Loss: 0.004441
Validation Loss: 0.00420277
Epoch [133/200], Train Loss: 0.004428
Validation Loss: 0.00419963
Epoch [134/200], Train Loss: 0.004420
Validation Loss: 0.00418558
Epoch [135/200], Train Loss: 0.004404
Validation Loss: 0.00418536
Epoch [136/200], Train Loss: 0.004408
Validation Loss: 0.00418749
Epoch [137/200], Train Loss: 0.004398
Validation Loss: 0.00416189
Epoch [138/200], Train Loss: 0.004374
Validation Loss: 0.00414426
Epoch [139/200], Train Loss: 0.004381
Validation Loss: 0.00415275
Epoch [140/200], Train Loss: 0.004352
Validation Loss: 0.00414552
Epoch [141/200], Train Loss: 0.004340
Validation Loss: 0.00411685
Epoch [142/200], Train Loss: 0.004352
Validation Loss: 0.00410647
Epoch [143/200], Train Loss: 0.004332
Validation Loss: 0.00410715
Epoch [144/200], Train Loss: 0.004319
Validation Loss: 0.00410515
Epoch [145/200], Train Loss: 0.004316
Validation Loss: 0.00407983
Epoch [146/200], Train Loss: 0.004287
Validation Loss: 0.00407145
Epoch [147/200], Train Loss: 0.004309
Validation Loss: 0.00404956
Epoch [148/200], Train Loss: 0.004261
Validation Loss: 0.00404409
Epoch [149/200], Train Loss: 0.004273
Validation Loss: 0.00402737
Epoch [150/200], Train Loss: 0.004259
Validation Loss: 0.00401767
Epoch [151/200], Train Loss: 0.004248
Validation Loss: 0.00400315
Epoch [152/200], Train Loss: 0.004247
Validation Loss: 0.00399102
Epoch [153/200], Train Loss: 0.004216
Validation Loss: 0.00397755
Epoch [154/200], Train Loss: 0.004230
Validation Loss: 0.00397345
Epoch [155/200], Train Loss: 0.004230
Validation Loss: 0.00395543
Epoch [156/200], Train Loss: 0.004192
Validation Loss: 0.00395397
Epoch [157/200], Train Loss: 0.004158
Validation Loss: 0.00393241
Epoch [158/200], Train Loss: 0.004152
Validation Loss: 0.00391802
Epoch [159/200], Train Loss: 0.004165
Validation Loss: 0.00390404
Epoch [160/200], Train Loss: 0.004137
Validation Loss: 0.00389160
Epoch [161/200], Train Loss: 0.004117
Validation Loss: 0.00387744
Epoch [162/200], Train Loss: 0.004109
Validation Loss: 0.00387119
Epoch [163/200], Train Loss: 0.004100
Validation Loss: 0.00385848
Epoch [164/200], Train Loss: 0.004084
Validation Loss: 0.00384020
Epoch [165/200], Train Loss: 0.004070
Validation Loss: 0.00381721
Epoch [166/200], Train Loss: 0.004060
Validation Loss: 0.00381169
Epoch [167/200], Train Loss: 0.004061
Validation Loss: 0.00378850
Epoch [168/200], Train Loss: 0.004031
Validation Loss: 0.00378263
Epoch [169/200], Train Loss: 0.004009
Validation Loss: 0.00376361
Epoch [170/200], Train Loss: 0.004001
Validation Loss: 0.00375824
Epoch [171/200], Train Loss: 0.003981
Validation Loss: 0.00372990
Epoch [172/200], Train Loss: 0.003949
Validation Loss: 0.00372222
Epoch [173/200], Train Loss: 0.003965
Validation Loss: 0.00372349
Epoch [174/200], Train Loss: 0.003942
Validation Loss: 0.00367845
Epoch [175/200], Train Loss: 0.003916
Validation Loss: 0.00365832
Epoch [176/200], Train Loss: 0.003933
Validation Loss: 0.00364417
Epoch [177/200], Train Loss: 0.003873
Validation Loss: 0.00362505
Epoch [178/200], Train Loss: 0.003885
Validation Loss: 0.00362131
Epoch [179/200], Train Loss: 0.003833
Validation Loss: 0.00359068
Epoch [180/200], Train Loss: 0.003836
Validation Loss: 0.00357930
Epoch [181/200], Train Loss: 0.003829
Validation Loss: 0.00359707
Epoch [182/200], Train Loss: 0.003805
Validation Loss: 0.00353437
Epoch [183/200], Train Loss: 0.003800
Validation Loss: 0.00351331
Epoch [184/200], Train Loss: 0.003788
Validation Loss: 0.00349238
Epoch [185/200], Train Loss: 0.003765
Validation Loss: 0.00347395
Epoch [186/200], Train Loss: 0.003767
Validation Loss: 0.00345691
Epoch [187/200], Train Loss: 0.003741
Validation Loss: 0.00343928
Epoch [188/200], Train Loss: 0.003710
Validation Loss: 0.00342725
Epoch [189/200], Train Loss: 0.003692
Validation Loss: 0.00340974
Epoch [190/200], Train Loss: 0.003683
Validation Loss: 0.00339438
Epoch [191/200], Train Loss: 0.003667
Validation Loss: 0.00336728
Epoch [192/200], Train Loss: 0.003649
Validation Loss: 0.00337420
Epoch [193/200], Train Loss: 0.003620
Validation Loss: 0.00337847
Epoch [194/200], Train Loss: 0.003592
Validation Loss: 0.00334331
Epoch [195/200], Train Loss: 0.003579
Validation Loss: 0.00332926
Epoch [196/200], Train Loss: 0.003578
Validation Loss: 0.00330600
Epoch [197/200], Train Loss: 0.003563
Validation Loss: 0.00328967
Epoch [198/200], Train Loss: 0.003540
Validation Loss: 0.00325404
Epoch [199/200], Train Loss: 0.003517
Validation Loss: 0.00325179
Epoch [200/200], Train Loss: 0.003506
Validation Loss: 0.00322954

Evaluating model for: Lamp
Run 37/144 completed in 540.30 seconds with: {'MAE': np.float32(1.8558184), 'MSE': np.float32(100.54673), 'RMSE': np.float32(10.027299), 'SAE': np.float32(0.20276323), 'NDE': np.float32(0.8099956)}

Run 38/144: hidden=64, seq_len=1080, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.007290
Validation Loss: 0.00609421
Epoch [2/200], Train Loss: 0.005853
Validation Loss: 0.00515391
Epoch [3/200], Train Loss: 0.005208
Validation Loss: 0.00482348
Epoch [4/200], Train Loss: 0.005054
Validation Loss: 0.00478318
Epoch [5/200], Train Loss: 0.005054
Validation Loss: 0.00478048
Epoch [6/200], Train Loss: 0.005028
Validation Loss: 0.00477837
Epoch [7/200], Train Loss: 0.005021
Validation Loss: 0.00477555
Epoch [8/200], Train Loss: 0.005033
Validation Loss: 0.00477261
Epoch [9/200], Train Loss: 0.005013
Validation Loss: 0.00476879
Epoch [10/200], Train Loss: 0.005029
Validation Loss: 0.00476451
Epoch [11/200], Train Loss: 0.005018
Validation Loss: 0.00476039
Epoch [12/200], Train Loss: 0.004991
Validation Loss: 0.00475507
Epoch [13/200], Train Loss: 0.005019
Validation Loss: 0.00474774
Epoch [14/200], Train Loss: 0.004996
Validation Loss: 0.00473973
Epoch [15/200], Train Loss: 0.004979
Validation Loss: 0.00472938
Epoch [16/200], Train Loss: 0.004956
Validation Loss: 0.00472064
Epoch [17/200], Train Loss: 0.004950
Validation Loss: 0.00471304
Epoch [18/200], Train Loss: 0.004953
Validation Loss: 0.00470819
Epoch [19/200], Train Loss: 0.004943
Validation Loss: 0.00470714
Epoch [20/200], Train Loss: 0.004908
Validation Loss: 0.00470537
Epoch [21/200], Train Loss: 0.004951
Validation Loss: 0.00470767
Epoch [22/200], Train Loss: 0.004911
Validation Loss: 0.00470265
Epoch [23/200], Train Loss: 0.004918
Validation Loss: 0.00470322
Epoch [24/200], Train Loss: 0.004908
Validation Loss: 0.00470152
Epoch [25/200], Train Loss: 0.004945
Validation Loss: 0.00470467
Epoch [26/200], Train Loss: 0.004939
Validation Loss: 0.00469885
Epoch [27/200], Train Loss: 0.004926
Validation Loss: 0.00469856
Epoch [28/200], Train Loss: 0.004906
Validation Loss: 0.00469839
Epoch [29/200], Train Loss: 0.004900
Validation Loss: 0.00469666
Epoch [30/200], Train Loss: 0.004902
Validation Loss: 0.00469779
Epoch [31/200], Train Loss: 0.004925
Validation Loss: 0.00469596
Epoch [32/200], Train Loss: 0.004921
Validation Loss: 0.00469427
Epoch [33/200], Train Loss: 0.004933
Validation Loss: 0.00469451
Epoch [34/200], Train Loss: 0.004911
Validation Loss: 0.00469444
Epoch [35/200], Train Loss: 0.004905
Validation Loss: 0.00469601
Epoch [36/200], Train Loss: 0.004892
Validation Loss: 0.00469284
Epoch [37/200], Train Loss: 0.004909
Validation Loss: 0.00469400
Epoch [38/200], Train Loss: 0.004898
Validation Loss: 0.00469230
Epoch [39/200], Train Loss: 0.004934
Validation Loss: 0.00469497
Epoch [40/200], Train Loss: 0.004911
Validation Loss: 0.00469125
Epoch [41/200], Train Loss: 0.004928
Validation Loss: 0.00469016
Epoch [42/200], Train Loss: 0.004947
Validation Loss: 0.00469029
Epoch [43/200], Train Loss: 0.004908
Validation Loss: 0.00469154
Epoch [44/200], Train Loss: 0.004915
Validation Loss: 0.00469044
Epoch [45/200], Train Loss: 0.004916
Validation Loss: 0.00470347
Epoch [46/200], Train Loss: 0.004916
Validation Loss: 0.00468914
Epoch [47/200], Train Loss: 0.004887
Validation Loss: 0.00468958
Epoch [48/200], Train Loss: 0.004911
Validation Loss: 0.00469078
Epoch [49/200], Train Loss: 0.004903
Validation Loss: 0.00468978
Epoch [50/200], Train Loss: 0.004892
Validation Loss: 0.00469129
Epoch [51/200], Train Loss: 0.004899
Validation Loss: 0.00469354
Epoch [52/200], Train Loss: 0.004897
Validation Loss: 0.00469276
Epoch [53/200], Train Loss: 0.004902
Validation Loss: 0.00468867
Epoch [54/200], Train Loss: 0.004872
Validation Loss: 0.00469028
Epoch [55/200], Train Loss: 0.004897
Validation Loss: 0.00469015
Epoch [56/200], Train Loss: 0.004894
Validation Loss: 0.00469097
Epoch [57/200], Train Loss: 0.004873
Validation Loss: 0.00468881
Epoch [58/200], Train Loss: 0.004909
Validation Loss: 0.00468754
Epoch [59/200], Train Loss: 0.004913
Validation Loss: 0.00468898
Epoch [60/200], Train Loss: 0.004898
Validation Loss: 0.00468770
Epoch [61/200], Train Loss: 0.004885
Validation Loss: 0.00468885
Epoch [62/200], Train Loss: 0.004913
Validation Loss: 0.00468800
Epoch [63/200], Train Loss: 0.004924
Validation Loss: 0.00468759
Epoch [64/200], Train Loss: 0.004874
Validation Loss: 0.00468781
Epoch [65/200], Train Loss: 0.004887
Validation Loss: 0.00469189
Epoch [66/200], Train Loss: 0.004900
Validation Loss: 0.00468840
Epoch [67/200], Train Loss: 0.004889
Validation Loss: 0.00468753
Epoch [68/200], Train Loss: 0.004897
Validation Loss: 0.00469061
Epoch [69/200], Train Loss: 0.004929
Validation Loss: 0.00468737
Epoch [70/200], Train Loss: 0.004907
Validation Loss: 0.00468828
Epoch [71/200], Train Loss: 0.004909
Validation Loss: 0.00468791
Epoch [72/200], Train Loss: 0.004900
Validation Loss: 0.00468837
Epoch [73/200], Train Loss: 0.004874
Validation Loss: 0.00468881
Epoch [74/200], Train Loss: 0.004899
Validation Loss: 0.00468833
Epoch [75/200], Train Loss: 0.004904
Validation Loss: 0.00468941
Epoch [76/200], Train Loss: 0.004885
Validation Loss: 0.00468771
Epoch [77/200], Train Loss: 0.004891
Validation Loss: 0.00468998
Epoch [78/200], Train Loss: 0.004864
Validation Loss: 0.00468895
Epoch [79/200], Train Loss: 0.004900
Validation Loss: 0.00468963
Early stopping triggered

Evaluating model for: Lamp
Run 38/144 completed in 221.49 seconds with: {'MAE': np.float32(2.6200995), 'MSE': np.float32(147.41875), 'RMSE': np.float32(12.141612), 'SAE': np.float32(0.11023662), 'NDE': np.float32(0.9807892)}

Run 39/144: hidden=64, seq_len=1080, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.012043
Validation Loss: 0.00989763
Epoch [2/200], Train Loss: 0.008811
Validation Loss: 0.00719471
Epoch [3/200], Train Loss: 0.006603
Validation Loss: 0.00555906
Epoch [4/200], Train Loss: 0.005392
Validation Loss: 0.00481107
Epoch [5/200], Train Loss: 0.005052
Validation Loss: 0.00478250
Epoch [6/200], Train Loss: 0.005031
Validation Loss: 0.00476762
Epoch [7/200], Train Loss: 0.005002
Validation Loss: 0.00476489
Epoch [8/200], Train Loss: 0.005016
Validation Loss: 0.00476172
Epoch [9/200], Train Loss: 0.005008
Validation Loss: 0.00475790
Epoch [10/200], Train Loss: 0.005019
Validation Loss: 0.00475321
Epoch [11/200], Train Loss: 0.005017
Validation Loss: 0.00474792
Epoch [12/200], Train Loss: 0.004990
Validation Loss: 0.00474153
Epoch [13/200], Train Loss: 0.004992
Validation Loss: 0.00473439
Epoch [14/200], Train Loss: 0.004969
Validation Loss: 0.00472738
Epoch [15/200], Train Loss: 0.004984
Validation Loss: 0.00472044
Epoch [16/200], Train Loss: 0.004956
Validation Loss: 0.00471927
Epoch [17/200], Train Loss: 0.004931
Validation Loss: 0.00471580
Epoch [18/200], Train Loss: 0.004950
Validation Loss: 0.00471473
Epoch [19/200], Train Loss: 0.004942
Validation Loss: 0.00471303
Epoch [20/200], Train Loss: 0.004937
Validation Loss: 0.00471129
Epoch [21/200], Train Loss: 0.004942
Validation Loss: 0.00471613
Epoch [22/200], Train Loss: 0.004947
Validation Loss: 0.00471035
Epoch [23/200], Train Loss: 0.004933
Validation Loss: 0.00470894
Epoch [24/200], Train Loss: 0.004945
Validation Loss: 0.00470880
Epoch [25/200], Train Loss: 0.004947
Validation Loss: 0.00470824
Epoch [26/200], Train Loss: 0.004943
Validation Loss: 0.00470840
Epoch [27/200], Train Loss: 0.004934
Validation Loss: 0.00470627
Epoch [28/200], Train Loss: 0.004940
Validation Loss: 0.00470609
Epoch [29/200], Train Loss: 0.004943
Validation Loss: 0.00471321
Epoch [30/200], Train Loss: 0.004940
Validation Loss: 0.00470491
Epoch [31/200], Train Loss: 0.004937
Validation Loss: 0.00470354
Epoch [32/200], Train Loss: 0.004933
Validation Loss: 0.00470296
Epoch [33/200], Train Loss: 0.004914
Validation Loss: 0.00470252
Epoch [34/200], Train Loss: 0.004944
Validation Loss: 0.00470282
Epoch [35/200], Train Loss: 0.004916
Validation Loss: 0.00470300
Epoch [36/200], Train Loss: 0.004889
Validation Loss: 0.00470117
Epoch [37/200], Train Loss: 0.004925
Validation Loss: 0.00470914
Epoch [38/200], Train Loss: 0.004933
Validation Loss: 0.00470019
Epoch [39/200], Train Loss: 0.004936
Validation Loss: 0.00470167
Epoch [40/200], Train Loss: 0.004933
Validation Loss: 0.00470114
Epoch [41/200], Train Loss: 0.004945
Validation Loss: 0.00469916
Epoch [42/200], Train Loss: 0.004919
Validation Loss: 0.00469900
Epoch [43/200], Train Loss: 0.004917
Validation Loss: 0.00469848
Epoch [44/200], Train Loss: 0.004953
Validation Loss: 0.00469842
Epoch [45/200], Train Loss: 0.004928
Validation Loss: 0.00469827
Epoch [46/200], Train Loss: 0.004920
Validation Loss: 0.00470444
Epoch [47/200], Train Loss: 0.004929
Validation Loss: 0.00470480
Epoch [48/200], Train Loss: 0.004917
Validation Loss: 0.00469737
Epoch [49/200], Train Loss: 0.004907
Validation Loss: 0.00469721
Epoch [50/200], Train Loss: 0.004917
Validation Loss: 0.00469716
Epoch [51/200], Train Loss: 0.004912
Validation Loss: 0.00469747
Epoch [52/200], Train Loss: 0.004923
Validation Loss: 0.00469770
Epoch [53/200], Train Loss: 0.004896
Validation Loss: 0.00469694
Epoch [54/200], Train Loss: 0.004914
Validation Loss: 0.00470140
Epoch [55/200], Train Loss: 0.004917
Validation Loss: 0.00469678
Epoch [56/200], Train Loss: 0.004916
Validation Loss: 0.00469654
Epoch [57/200], Train Loss: 0.004927
Validation Loss: 0.00469651
Epoch [58/200], Train Loss: 0.004892
Validation Loss: 0.00469625
Epoch [59/200], Train Loss: 0.004921
Validation Loss: 0.00469750
Epoch [60/200], Train Loss: 0.004938
Validation Loss: 0.00469634
Epoch [61/200], Train Loss: 0.004902
Validation Loss: 0.00469609
Epoch [62/200], Train Loss: 0.004900
Validation Loss: 0.00470110
Epoch [63/200], Train Loss: 0.004898
Validation Loss: 0.00469608
Epoch [64/200], Train Loss: 0.004912
Validation Loss: 0.00469688
Epoch [65/200], Train Loss: 0.004913
Validation Loss: 0.00469681
Epoch [66/200], Train Loss: 0.004921
Validation Loss: 0.00469644
Epoch [67/200], Train Loss: 0.004928
Validation Loss: 0.00469971
Epoch [68/200], Train Loss: 0.004915
Validation Loss: 0.00469628
Epoch [69/200], Train Loss: 0.004883
Validation Loss: 0.00469524
Epoch [70/200], Train Loss: 0.004903
Validation Loss: 0.00469540
Epoch [71/200], Train Loss: 0.004909
Validation Loss: 0.00469549
Epoch [72/200], Train Loss: 0.004883
Validation Loss: 0.00469662
Epoch [73/200], Train Loss: 0.004917
Validation Loss: 0.00469522
Epoch [74/200], Train Loss: 0.004892
Validation Loss: 0.00469504
Epoch [75/200], Train Loss: 0.004907
Validation Loss: 0.00469538
Epoch [76/200], Train Loss: 0.004931
Validation Loss: 0.00469492
Epoch [77/200], Train Loss: 0.004911
Validation Loss: 0.00469498
Epoch [78/200], Train Loss: 0.004890
Validation Loss: 0.00469482
Epoch [79/200], Train Loss: 0.004903
Validation Loss: 0.00469529
Epoch [80/200], Train Loss: 0.004917
Validation Loss: 0.00469510
Epoch [81/200], Train Loss: 0.004901
Validation Loss: 0.00469920
Epoch [82/200], Train Loss: 0.004914
Validation Loss: 0.00469462
Epoch [83/200], Train Loss: 0.004898
Validation Loss: 0.00469592
Epoch [84/200], Train Loss: 0.004912
Validation Loss: 0.00469447
Epoch [85/200], Train Loss: 0.004915
Validation Loss: 0.00469768
Epoch [86/200], Train Loss: 0.004915
Validation Loss: 0.00469423
Epoch [87/200], Train Loss: 0.004920
Validation Loss: 0.00469477
Epoch [88/200], Train Loss: 0.004926
Validation Loss: 0.00469484
Epoch [89/200], Train Loss: 0.004900
Validation Loss: 0.00469523
Epoch [90/200], Train Loss: 0.004905
Validation Loss: 0.00469390
Epoch [91/200], Train Loss: 0.004916
Validation Loss: 0.00469386
Epoch [92/200], Train Loss: 0.004921
Validation Loss: 0.00469405
Epoch [93/200], Train Loss: 0.004920
Validation Loss: 0.00469455
Epoch [94/200], Train Loss: 0.004922
Validation Loss: 0.00469389
Epoch [95/200], Train Loss: 0.004902
Validation Loss: 0.00469572
Epoch [96/200], Train Loss: 0.004897
Validation Loss: 0.00469410
Epoch [97/200], Train Loss: 0.004893
Validation Loss: 0.00469723
Epoch [98/200], Train Loss: 0.004903
Validation Loss: 0.00469580
Epoch [99/200], Train Loss: 0.004900
Validation Loss: 0.00469483
Epoch [100/200], Train Loss: 0.004903
Validation Loss: 0.00469433
Epoch [101/200], Train Loss: 0.004901
Validation Loss: 0.00469436
Early stopping triggered

Evaluating model for: Lamp
Run 39/144 completed in 291.82 seconds with: {'MAE': np.float32(2.5556345), 'MSE': np.float32(147.47581), 'RMSE': np.float32(12.143962), 'SAE': np.float32(0.0022417912), 'NDE': np.float32(0.98097885)}

Run 40/144: hidden=64, seq_len=1080, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.005358
Validation Loss: 0.00482841
Epoch [2/200], Train Loss: 0.005029
Validation Loss: 0.00478572
Epoch [3/200], Train Loss: 0.005012
Validation Loss: 0.00477829
Epoch [4/200], Train Loss: 0.004992
Validation Loss: 0.00477855
Epoch [5/200], Train Loss: 0.004993
Validation Loss: 0.00477696
Epoch [6/200], Train Loss: 0.004991
Validation Loss: 0.00477612
Epoch [7/200], Train Loss: 0.004979
Validation Loss: 0.00477467
Epoch [8/200], Train Loss: 0.005007
Validation Loss: 0.00477191
Epoch [9/200], Train Loss: 0.004977
Validation Loss: 0.00476592
Epoch [10/200], Train Loss: 0.004980
Validation Loss: 0.00474838
Epoch [11/200], Train Loss: 0.004940
Validation Loss: 0.00471558
Epoch [12/200], Train Loss: 0.004910
Validation Loss: 0.00471070
Epoch [13/200], Train Loss: 0.004920
Validation Loss: 0.00470439
Epoch [14/200], Train Loss: 0.004918
Validation Loss: 0.00470360
Epoch [15/200], Train Loss: 0.004897
Validation Loss: 0.00470270
Epoch [16/200], Train Loss: 0.004903
Validation Loss: 0.00470152
Epoch [17/200], Train Loss: 0.004871
Validation Loss: 0.00470013
Epoch [18/200], Train Loss: 0.004904
Validation Loss: 0.00469848
Epoch [19/200], Train Loss: 0.004887
Validation Loss: 0.00470116
Epoch [20/200], Train Loss: 0.004894
Validation Loss: 0.00470190
Epoch [21/200], Train Loss: 0.004905
Validation Loss: 0.00469631
Epoch [22/200], Train Loss: 0.004892
Validation Loss: 0.00469588
Epoch [23/200], Train Loss: 0.004887
Validation Loss: 0.00469715
Epoch [24/200], Train Loss: 0.004878
Validation Loss: 0.00469301
Epoch [25/200], Train Loss: 0.004864
Validation Loss: 0.00469305
Epoch [26/200], Train Loss: 0.004888
Validation Loss: 0.00469166
Epoch [27/200], Train Loss: 0.004892
Validation Loss: 0.00469342
Epoch [28/200], Train Loss: 0.004863
Validation Loss: 0.00469308
Epoch [29/200], Train Loss: 0.004871
Validation Loss: 0.00469241
Epoch [30/200], Train Loss: 0.004889
Validation Loss: 0.00468931
Epoch [31/200], Train Loss: 0.004879
Validation Loss: 0.00468930
Epoch [32/200], Train Loss: 0.004865
Validation Loss: 0.00469325
Epoch [33/200], Train Loss: 0.004886
Validation Loss: 0.00469144
Epoch [34/200], Train Loss: 0.004867
Validation Loss: 0.00468802
Epoch [35/200], Train Loss: 0.004875
Validation Loss: 0.00469247
Epoch [36/200], Train Loss: 0.004865
Validation Loss: 0.00468956
Epoch [37/200], Train Loss: 0.004874
Validation Loss: 0.00468809
Epoch [38/200], Train Loss: 0.004876
Validation Loss: 0.00468741
Epoch [39/200], Train Loss: 0.004876
Validation Loss: 0.00468853
Epoch [40/200], Train Loss: 0.004851
Validation Loss: 0.00468852
Epoch [41/200], Train Loss: 0.004883
Validation Loss: 0.00468722
Epoch [42/200], Train Loss: 0.004848
Validation Loss: 0.00469039
Epoch [43/200], Train Loss: 0.004881
Validation Loss: 0.00468588
Epoch [44/200], Train Loss: 0.004894
Validation Loss: 0.00468976
Epoch [45/200], Train Loss: 0.004862
Validation Loss: 0.00468724
Epoch [46/200], Train Loss: 0.004860
Validation Loss: 0.00468709
Epoch [47/200], Train Loss: 0.004873
Validation Loss: 0.00468632
Epoch [48/200], Train Loss: 0.004888
Validation Loss: 0.00468548
Epoch [49/200], Train Loss: 0.004872
Validation Loss: 0.00468577
Epoch [50/200], Train Loss: 0.004860
Validation Loss: 0.00468633
Epoch [51/200], Train Loss: 0.004869
Validation Loss: 0.00468552
Epoch [52/200], Train Loss: 0.004873
Validation Loss: 0.00468559
Epoch [53/200], Train Loss: 0.004860
Validation Loss: 0.00468798
Epoch [54/200], Train Loss: 0.004880
Validation Loss: 0.00468628
Epoch [55/200], Train Loss: 0.004908
Validation Loss: 0.00468789
Epoch [56/200], Train Loss: 0.004873
Validation Loss: 0.00468450
Epoch [57/200], Train Loss: 0.004885
Validation Loss: 0.00468562
Epoch [58/200], Train Loss: 0.004882
Validation Loss: 0.00469063
Epoch [59/200], Train Loss: 0.004887
Validation Loss: 0.00468538
Epoch [60/200], Train Loss: 0.004867
Validation Loss: 0.00468559
Epoch [61/200], Train Loss: 0.004856
Validation Loss: 0.00468513
Epoch [62/200], Train Loss: 0.004861
Validation Loss: 0.00468696
Epoch [63/200], Train Loss: 0.004872
Validation Loss: 0.00468403
Epoch [64/200], Train Loss: 0.004865
Validation Loss: 0.00468719
Epoch [65/200], Train Loss: 0.004879
Validation Loss: 0.00468487
Epoch [66/200], Train Loss: 0.004885
Validation Loss: 0.00468631
Epoch [67/200], Train Loss: 0.004853
Validation Loss: 0.00468480
Epoch [68/200], Train Loss: 0.004854
Validation Loss: 0.00468426
Epoch [69/200], Train Loss: 0.004858
Validation Loss: 0.00468974
Epoch [70/200], Train Loss: 0.004863
Validation Loss: 0.00468474
Epoch [71/200], Train Loss: 0.004859
Validation Loss: 0.00468477
Epoch [72/200], Train Loss: 0.004873
Validation Loss: 0.00468724
Epoch [73/200], Train Loss: 0.004855
Validation Loss: 0.00468463
Early stopping triggered

Evaluating model for: Lamp
Run 40/144 completed in 211.95 seconds with: {'MAE': np.float32(2.4464095), 'MSE': np.float32(147.28252), 'RMSE': np.float32(12.136001), 'SAE': np.float32(0.048837427), 'NDE': np.float32(0.9803354)}

Run 41/144: hidden=64, seq_len=1080, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.006613
Validation Loss: 0.00358133
Epoch [2/200], Train Loss: 0.006284
Validation Loss: 0.00319069
Epoch [3/200], Train Loss: 0.005780
Validation Loss: 0.00288612
Epoch [4/200], Train Loss: 0.005499
Validation Loss: 0.00266865
Epoch [5/200], Train Loss: 0.005269
Validation Loss: 0.00252722
Epoch [6/200], Train Loss: 0.005093
Validation Loss: 0.00245895
Epoch [7/200], Train Loss: 0.005158
Validation Loss: 0.00244307
Epoch [8/200], Train Loss: 0.004974
Validation Loss: 0.00245440
Epoch [9/200], Train Loss: 0.005033
Validation Loss: 0.00246573
Epoch [10/200], Train Loss: 0.004918
Validation Loss: 0.00246621
Epoch [11/200], Train Loss: 0.004988
Validation Loss: 0.00245881
Epoch [12/200], Train Loss: 0.005019
Validation Loss: 0.00245266
Epoch [13/200], Train Loss: 0.005012
Validation Loss: 0.00244987
Epoch [14/200], Train Loss: 0.004916
Validation Loss: 0.00244863
Epoch [15/200], Train Loss: 0.004902
Validation Loss: 0.00244819
Epoch [16/200], Train Loss: 0.005032
Validation Loss: 0.00244654
Epoch [17/200], Train Loss: 0.004965
Validation Loss: 0.00244665
Early stopping triggered

Evaluating model for: Lamp
Run 41/144 completed in 18.50 seconds with: {'MAE': np.float32(3.3489783), 'MSE': np.float32(229.02747), 'RMSE': np.float32(15.133654), 'SAE': np.float32(0.20832989), 'NDE': np.float32(0.98585963)}

Run 42/144: hidden=64, seq_len=1080, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.008025
Validation Loss: 0.00518168
Epoch [2/200], Train Loss: 0.006945
Validation Loss: 0.00432137
Epoch [3/200], Train Loss: 0.006261
Validation Loss: 0.00370132
Epoch [4/200], Train Loss: 0.005884
Validation Loss: 0.00324524
Epoch [5/200], Train Loss: 0.005473
Validation Loss: 0.00292512
Epoch [6/200], Train Loss: 0.005278
Validation Loss: 0.00270486
Epoch [7/200], Train Loss: 0.005087
Validation Loss: 0.00257231
Epoch [8/200], Train Loss: 0.005139
Validation Loss: 0.00250315
Epoch [9/200], Train Loss: 0.004964
Validation Loss: 0.00247535
Epoch [10/200], Train Loss: 0.005062
Validation Loss: 0.00246586
Epoch [11/200], Train Loss: 0.005006
Validation Loss: 0.00246572
Epoch [12/200], Train Loss: 0.004956
Validation Loss: 0.00246875
Epoch [13/200], Train Loss: 0.004952
Validation Loss: 0.00247201
Epoch [14/200], Train Loss: 0.005028
Validation Loss: 0.00247276
Epoch [15/200], Train Loss: 0.004994
Validation Loss: 0.00247316
Epoch [16/200], Train Loss: 0.005036
Validation Loss: 0.00247247
Epoch [17/200], Train Loss: 0.005063
Validation Loss: 0.00247180
Epoch [18/200], Train Loss: 0.005011
Validation Loss: 0.00247035
Epoch [19/200], Train Loss: 0.005088
Validation Loss: 0.00246977
Epoch [20/200], Train Loss: 0.005016
Validation Loss: 0.00246938
Epoch [21/200], Train Loss: 0.004935
Validation Loss: 0.00246860
Early stopping triggered

Evaluating model for: Lamp
Run 42/144 completed in 24.24 seconds with: {'MAE': np.float32(3.3709624), 'MSE': np.float32(230.37299), 'RMSE': np.float32(15.178042), 'SAE': np.float32(0.19273272), 'NDE': np.float32(0.98875135)}

Run 43/144: hidden=64, seq_len=1080, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.013160
Validation Loss: 0.00899193
Epoch [2/200], Train Loss: 0.011409
Validation Loss: 0.00752791
Epoch [3/200], Train Loss: 0.010039
Validation Loss: 0.00626047
Epoch [4/200], Train Loss: 0.008804
Validation Loss: 0.00517029
Epoch [5/200], Train Loss: 0.007784
Validation Loss: 0.00430757
Epoch [6/200], Train Loss: 0.006909
Validation Loss: 0.00361880
Epoch [7/200], Train Loss: 0.006216
Validation Loss: 0.00308936
Epoch [8/200], Train Loss: 0.005662
Validation Loss: 0.00270338
Epoch [9/200], Train Loss: 0.005313
Validation Loss: 0.00249384
Epoch [10/200], Train Loss: 0.005017
Validation Loss: 0.00245242
Epoch [11/200], Train Loss: 0.005129
Validation Loss: 0.00249386
Epoch [12/200], Train Loss: 0.005003
Validation Loss: 0.00250379
Epoch [13/200], Train Loss: 0.005006
Validation Loss: 0.00247970
Epoch [14/200], Train Loss: 0.004945
Validation Loss: 0.00246211
Epoch [15/200], Train Loss: 0.004900
Validation Loss: 0.00245594
Epoch [16/200], Train Loss: 0.005053
Validation Loss: 0.00245525
Epoch [17/200], Train Loss: 0.004925
Validation Loss: 0.00245803
Epoch [18/200], Train Loss: 0.005016
Validation Loss: 0.00246018
Epoch [19/200], Train Loss: 0.004992
Validation Loss: 0.00246097
Epoch [20/200], Train Loss: 0.005009
Validation Loss: 0.00246167
Early stopping triggered

Evaluating model for: Lamp
Run 43/144 completed in 23.39 seconds with: {'MAE': np.float32(3.4047441), 'MSE': np.float32(229.78642), 'RMSE': np.float32(15.158708), 'SAE': np.float32(0.20571741), 'NDE': np.float32(0.9874916)}

Run 44/144: hidden=64, seq_len=1080, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.005628
Validation Loss: 0.00296890
Epoch [2/200], Train Loss: 0.005221
Validation Loss: 0.00264336
Epoch [3/200], Train Loss: 0.004987
Validation Loss: 0.00249580
Epoch [4/200], Train Loss: 0.004938
Validation Loss: 0.00245547
Epoch [5/200], Train Loss: 0.004945
Validation Loss: 0.00245191
Epoch [6/200], Train Loss: 0.005075
Validation Loss: 0.00245600
Epoch [7/200], Train Loss: 0.005118
Validation Loss: 0.00246615
Epoch [8/200], Train Loss: 0.004898
Validation Loss: 0.00247613
Epoch [9/200], Train Loss: 0.004993
Validation Loss: 0.00247553
Epoch [10/200], Train Loss: 0.005053
Validation Loss: 0.00246915
Epoch [11/200], Train Loss: 0.004980
Validation Loss: 0.00246687
Epoch [12/200], Train Loss: 0.004914
Validation Loss: 0.00246746
Epoch [13/200], Train Loss: 0.004988
Validation Loss: 0.00246506
Epoch [14/200], Train Loss: 0.004984
Validation Loss: 0.00246586
Epoch [15/200], Train Loss: 0.004999
Validation Loss: 0.00246722
Early stopping triggered

Evaluating model for: Lamp
Run 44/144 completed in 18.41 seconds with: {'MAE': np.float32(3.2845855), 'MSE': np.float32(230.77658), 'RMSE': np.float32(15.191333), 'SAE': np.float32(0.23446278), 'NDE': np.float32(0.98961735)}

Run 45/144: hidden=64, seq_len=1080, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.006434
Validation Loss: 0.00692612
Epoch [2/200], Train Loss: 0.006224
Validation Loss: 0.00667260
Epoch [3/200], Train Loss: 0.005946
Validation Loss: 0.00643863
Epoch [4/200], Train Loss: 0.005712
Validation Loss: 0.00622155
Epoch [5/200], Train Loss: 0.005541
Validation Loss: 0.00602785
Epoch [6/200], Train Loss: 0.005329
Validation Loss: 0.00585992
Epoch [7/200], Train Loss: 0.005209
Validation Loss: 0.00571350
Epoch [8/200], Train Loss: 0.005120
Validation Loss: 0.00558627
Epoch [9/200], Train Loss: 0.004996
Validation Loss: 0.00547813
Epoch [10/200], Train Loss: 0.004908
Validation Loss: 0.00538976
Epoch [11/200], Train Loss: 0.004788
Validation Loss: 0.00532185
Epoch [12/200], Train Loss: 0.004754
Validation Loss: 0.00527226
Epoch [13/200], Train Loss: 0.004668
Validation Loss: 0.00523737
Epoch [14/200], Train Loss: 0.004714
Validation Loss: 0.00521525
Epoch [15/200], Train Loss: 0.004668
Validation Loss: 0.00520224
Epoch [16/200], Train Loss: 0.004707
Validation Loss: 0.00519626
Epoch [17/200], Train Loss: 0.004697
Validation Loss: 0.00519321
Epoch [18/200], Train Loss: 0.004655
Validation Loss: 0.00519150
Epoch [19/200], Train Loss: 0.004693
Validation Loss: 0.00519011
Epoch [20/200], Train Loss: 0.004646
Validation Loss: 0.00518896
Epoch [21/200], Train Loss: 0.004702
Validation Loss: 0.00518806
Epoch [22/200], Train Loss: 0.004710
Validation Loss: 0.00518728
Epoch [23/200], Train Loss: 0.004633
Validation Loss: 0.00518649
Epoch [24/200], Train Loss: 0.004655
Validation Loss: 0.00518587
Epoch [25/200], Train Loss: 0.004659
Validation Loss: 0.00518532
Epoch [26/200], Train Loss: 0.004657
Validation Loss: 0.00518471
Epoch [27/200], Train Loss: 0.004632
Validation Loss: 0.00518385
Epoch [28/200], Train Loss: 0.004648
Validation Loss: 0.00518280
Epoch [29/200], Train Loss: 0.004607
Validation Loss: 0.00518154
Epoch [30/200], Train Loss: 0.004636
Validation Loss: 0.00518031
Epoch [31/200], Train Loss: 0.004668
Validation Loss: 0.00517928
Epoch [32/200], Train Loss: 0.004668
Validation Loss: 0.00517791
Epoch [33/200], Train Loss: 0.004676
Validation Loss: 0.00517641
Epoch [34/200], Train Loss: 0.004678
Validation Loss: 0.00517510
Epoch [35/200], Train Loss: 0.004623
Validation Loss: 0.00517360
Epoch [36/200], Train Loss: 0.004665
Validation Loss: 0.00517230
Epoch [37/200], Train Loss: 0.004613
Validation Loss: 0.00517101
Epoch [38/200], Train Loss: 0.004693
Validation Loss: 0.00516993
Epoch [39/200], Train Loss: 0.004622
Validation Loss: 0.00516860
Epoch [40/200], Train Loss: 0.004655
Validation Loss: 0.00516728
Epoch [41/200], Train Loss: 0.004650
Validation Loss: 0.00516590
Epoch [42/200], Train Loss: 0.004628
Validation Loss: 0.00516457
Epoch [43/200], Train Loss: 0.004649
Validation Loss: 0.00516322
Epoch [44/200], Train Loss: 0.004671
Validation Loss: 0.00516199
Epoch [45/200], Train Loss: 0.004635
Validation Loss: 0.00516051
Epoch [46/200], Train Loss: 0.004624
Validation Loss: 0.00515907
Epoch [47/200], Train Loss: 0.004621
Validation Loss: 0.00515771
Epoch [48/200], Train Loss: 0.004607
Validation Loss: 0.00515633
Epoch [49/200], Train Loss: 0.004632
Validation Loss: 0.00515523
Epoch [50/200], Train Loss: 0.004643
Validation Loss: 0.00515395
Epoch [51/200], Train Loss: 0.004589
Validation Loss: 0.00515264
Epoch [52/200], Train Loss: 0.004610
Validation Loss: 0.00515136
Epoch [53/200], Train Loss: 0.004662
Validation Loss: 0.00515043
Epoch [54/200], Train Loss: 0.004659
Validation Loss: 0.00514895
Epoch [55/200], Train Loss: 0.004591
Validation Loss: 0.00514760
Epoch [56/200], Train Loss: 0.004603
Validation Loss: 0.00514598
Epoch [57/200], Train Loss: 0.004628
Validation Loss: 0.00514469
Epoch [58/200], Train Loss: 0.004695
Validation Loss: 0.00514360
Epoch [59/200], Train Loss: 0.004625
Validation Loss: 0.00514224
Epoch [60/200], Train Loss: 0.004663
Validation Loss: 0.00514098
Epoch [61/200], Train Loss: 0.004600
Validation Loss: 0.00513962
Epoch [62/200], Train Loss: 0.004629
Validation Loss: 0.00513849
Epoch [63/200], Train Loss: 0.004617
Validation Loss: 0.00513736
Epoch [64/200], Train Loss: 0.004634
Validation Loss: 0.00513623
Epoch [65/200], Train Loss: 0.004674
Validation Loss: 0.00513535
Epoch [66/200], Train Loss: 0.004578
Validation Loss: 0.00513421
Epoch [67/200], Train Loss: 0.004607
Validation Loss: 0.00513309
Epoch [68/200], Train Loss: 0.004614
Validation Loss: 0.00513209
Epoch [69/200], Train Loss: 0.004558
Validation Loss: 0.00513103
Epoch [70/200], Train Loss: 0.004547
Validation Loss: 0.00512979
Epoch [71/200], Train Loss: 0.004577
Validation Loss: 0.00512915
Epoch [72/200], Train Loss: 0.004612
Validation Loss: 0.00512889
Epoch [73/200], Train Loss: 0.004595
Validation Loss: 0.00512838
Epoch [74/200], Train Loss: 0.004618
Validation Loss: 0.00512799
Epoch [75/200], Train Loss: 0.004567
Validation Loss: 0.00512661
Epoch [76/200], Train Loss: 0.004631
Validation Loss: 0.00512590
Epoch [77/200], Train Loss: 0.004568
Validation Loss: 0.00512485
Epoch [78/200], Train Loss: 0.004600
Validation Loss: 0.00512407
Epoch [79/200], Train Loss: 0.004591
Validation Loss: 0.00512325
Epoch [80/200], Train Loss: 0.004639
Validation Loss: 0.00512311
Epoch [81/200], Train Loss: 0.004612
Validation Loss: 0.00512259
Epoch [82/200], Train Loss: 0.004562
Validation Loss: 0.00512183
Epoch [83/200], Train Loss: 0.004634
Validation Loss: 0.00512149
Epoch [84/200], Train Loss: 0.004644
Validation Loss: 0.00512114
Epoch [85/200], Train Loss: 0.004617
Validation Loss: 0.00512031
Epoch [86/200], Train Loss: 0.004570
Validation Loss: 0.00511986
Epoch [87/200], Train Loss: 0.004596
Validation Loss: 0.00511918
Epoch [88/200], Train Loss: 0.004561
Validation Loss: 0.00511889
Epoch [89/200], Train Loss: 0.004549
Validation Loss: 0.00511883
Epoch [90/200], Train Loss: 0.004650
Validation Loss: 0.00511908
Epoch [91/200], Train Loss: 0.004570
Validation Loss: 0.00511844
Epoch [92/200], Train Loss: 0.004613
Validation Loss: 0.00511816
Epoch [93/200], Train Loss: 0.004567
Validation Loss: 0.00511759
Epoch [94/200], Train Loss: 0.004608
Validation Loss: 0.00511742
Epoch [95/200], Train Loss: 0.004623
Validation Loss: 0.00511702
Epoch [96/200], Train Loss: 0.004609
Validation Loss: 0.00511670
Epoch [97/200], Train Loss: 0.004562
Validation Loss: 0.00511607
Epoch [98/200], Train Loss: 0.004612
Validation Loss: 0.00511630
Epoch [99/200], Train Loss: 0.004573
Validation Loss: 0.00511611
Epoch [100/200], Train Loss: 0.004638
Validation Loss: 0.00511626
Epoch [101/200], Train Loss: 0.004586
Validation Loss: 0.00511584
Epoch [102/200], Train Loss: 0.004582
Validation Loss: 0.00511552
Epoch [103/200], Train Loss: 0.004582
Validation Loss: 0.00511519
Epoch [104/200], Train Loss: 0.004596
Validation Loss: 0.00511509
Epoch [105/200], Train Loss: 0.004583
Validation Loss: 0.00511500
Epoch [106/200], Train Loss: 0.004601
Validation Loss: 0.00511484
Epoch [107/200], Train Loss: 0.004568
Validation Loss: 0.00511460
Epoch [108/200], Train Loss: 0.004614
Validation Loss: 0.00511465
Epoch [109/200], Train Loss: 0.004605
Validation Loss: 0.00511479
Epoch [110/200], Train Loss: 0.004592
Validation Loss: 0.00511459
Epoch [111/200], Train Loss: 0.004570
Validation Loss: 0.00511411
Epoch [112/200], Train Loss: 0.004616
Validation Loss: 0.00511378
Epoch [113/200], Train Loss: 0.004575
Validation Loss: 0.00511358
Epoch [114/200], Train Loss: 0.004576
Validation Loss: 0.00511361
Epoch [115/200], Train Loss: 0.004629
Validation Loss: 0.00511388
Epoch [116/200], Train Loss: 0.004591
Validation Loss: 0.00511385
Epoch [117/200], Train Loss: 0.004607
Validation Loss: 0.00511381
Epoch [118/200], Train Loss: 0.004586
Validation Loss: 0.00511362
Epoch [119/200], Train Loss: 0.004595
Validation Loss: 0.00511357
Epoch [120/200], Train Loss: 0.004552
Validation Loss: 0.00511321
Epoch [121/200], Train Loss: 0.004564
Validation Loss: 0.00511297
Epoch [122/200], Train Loss: 0.004545
Validation Loss: 0.00511290
Epoch [123/200], Train Loss: 0.004543
Validation Loss: 0.00511311
Epoch [124/200], Train Loss: 0.004579
Validation Loss: 0.00511329
Epoch [125/200], Train Loss: 0.004572
Validation Loss: 0.00511332
Epoch [126/200], Train Loss: 0.004572
Validation Loss: 0.00511357
Epoch [127/200], Train Loss: 0.004577
Validation Loss: 0.00511350
Epoch [128/200], Train Loss: 0.004560
Validation Loss: 0.00511345
Epoch [129/200], Train Loss: 0.004588
Validation Loss: 0.00511335
Epoch [130/200], Train Loss: 0.004598
Validation Loss: 0.00511289
Epoch [131/200], Train Loss: 0.004570
Validation Loss: 0.00511268
Epoch [132/200], Train Loss: 0.004587
Validation Loss: 0.00511263
Epoch [133/200], Train Loss: 0.004575
Validation Loss: 0.00511244
Epoch [134/200], Train Loss: 0.004617
Validation Loss: 0.00511246
Epoch [135/200], Train Loss: 0.004639
Validation Loss: 0.00511262
Epoch [136/200], Train Loss: 0.004577
Validation Loss: 0.00511226
Epoch [137/200], Train Loss: 0.004671
Validation Loss: 0.00511246
Epoch [138/200], Train Loss: 0.004599
Validation Loss: 0.00511219
Epoch [139/200], Train Loss: 0.004634
Validation Loss: 0.00511227
Epoch [140/200], Train Loss: 0.004567
Validation Loss: 0.00511178
Epoch [141/200], Train Loss: 0.004623
Validation Loss: 0.00511194
Epoch [142/200], Train Loss: 0.004593
Validation Loss: 0.00511168
Epoch [143/200], Train Loss: 0.004603
Validation Loss: 0.00511164
Epoch [144/200], Train Loss: 0.004622
Validation Loss: 0.00511199
Epoch [145/200], Train Loss: 0.004625
Validation Loss: 0.00511205
Epoch [146/200], Train Loss: 0.004612
Validation Loss: 0.00511179
Epoch [147/200], Train Loss: 0.004557
Validation Loss: 0.00511150
Epoch [148/200], Train Loss: 0.004630
Validation Loss: 0.00511121
Epoch [149/200], Train Loss: 0.004588
Validation Loss: 0.00511117
Epoch [150/200], Train Loss: 0.004579
Validation Loss: 0.00511135
Epoch [151/200], Train Loss: 0.004597
Validation Loss: 0.00511139
Epoch [152/200], Train Loss: 0.004606
Validation Loss: 0.00511155
Epoch [153/200], Train Loss: 0.004577
Validation Loss: 0.00511158
Epoch [154/200], Train Loss: 0.004566
Validation Loss: 0.00511152
Epoch [155/200], Train Loss: 0.004594
Validation Loss: 0.00511131
Epoch [156/200], Train Loss: 0.004656
Validation Loss: 0.00511156
Epoch [157/200], Train Loss: 0.004615
Validation Loss: 0.00511111
Epoch [158/200], Train Loss: 0.004554
Validation Loss: 0.00511083
Epoch [159/200], Train Loss: 0.004630
Validation Loss: 0.00511075
Epoch [160/200], Train Loss: 0.004579
Validation Loss: 0.00511067
Epoch [161/200], Train Loss: 0.004527
Validation Loss: 0.00511037
Epoch [162/200], Train Loss: 0.004656
Validation Loss: 0.00511111
Epoch [163/200], Train Loss: 0.004602
Validation Loss: 0.00511093
Epoch [164/200], Train Loss: 0.004598
Validation Loss: 0.00511090
Epoch [165/200], Train Loss: 0.004597
Validation Loss: 0.00511071
Epoch [166/200], Train Loss: 0.004628
Validation Loss: 0.00511063
Epoch [167/200], Train Loss: 0.004557
Validation Loss: 0.00511039
Epoch [168/200], Train Loss: 0.004610
Validation Loss: 0.00511030
Epoch [169/200], Train Loss: 0.004620
Validation Loss: 0.00511027
Epoch [170/200], Train Loss: 0.004564
Validation Loss: 0.00511012
Epoch [171/200], Train Loss: 0.004567
Validation Loss: 0.00511003
Epoch [172/200], Train Loss: 0.004580
Validation Loss: 0.00510999
Epoch [173/200], Train Loss: 0.004574
Validation Loss: 0.00510994
Epoch [174/200], Train Loss: 0.004619
Validation Loss: 0.00511008
Epoch [175/200], Train Loss: 0.004551
Validation Loss: 0.00511010
Epoch [176/200], Train Loss: 0.004565
Validation Loss: 0.00510970
Epoch [177/200], Train Loss: 0.004595
Validation Loss: 0.00510977
Epoch [178/200], Train Loss: 0.004550
Validation Loss: 0.00510966
Epoch [179/200], Train Loss: 0.004594
Validation Loss: 0.00510982
Epoch [180/200], Train Loss: 0.004614
Validation Loss: 0.00510962
Epoch [181/200], Train Loss: 0.004618
Validation Loss: 0.00510962
Epoch [182/200], Train Loss: 0.004639
Validation Loss: 0.00510957
Epoch [183/200], Train Loss: 0.004611
Validation Loss: 0.00510928
Epoch [184/200], Train Loss: 0.004515
Validation Loss: 0.00510884
Epoch [185/200], Train Loss: 0.004616
Validation Loss: 0.00510899
Epoch [186/200], Train Loss: 0.004575
Validation Loss: 0.00510890
Epoch [187/200], Train Loss: 0.004586
Validation Loss: 0.00510881
Epoch [188/200], Train Loss: 0.004619
Validation Loss: 0.00510905
Epoch [189/200], Train Loss: 0.004603
Validation Loss: 0.00510920
Epoch [190/200], Train Loss: 0.004638
Validation Loss: 0.00510935
Epoch [191/200], Train Loss: 0.004576
Validation Loss: 0.00510891
Epoch [192/200], Train Loss: 0.004571
Validation Loss: 0.00510864
Epoch [193/200], Train Loss: 0.004562
Validation Loss: 0.00510843
Epoch [194/200], Train Loss: 0.004636
Validation Loss: 0.00510878
Epoch [195/200], Train Loss: 0.004620
Validation Loss: 0.00510888
Epoch [196/200], Train Loss: 0.004595
Validation Loss: 0.00510879
Epoch [197/200], Train Loss: 0.004591
Validation Loss: 0.00510857
Epoch [198/200], Train Loss: 0.004573
Validation Loss: 0.00510840
Epoch [199/200], Train Loss: 0.004585
Validation Loss: 0.00510848
Epoch [200/200], Train Loss: 0.004577
Validation Loss: 0.00510855

Evaluating model for: Lamp
Run 45/144 completed in 113.96 seconds with: {'MAE': np.float32(3.1276674), 'MSE': np.float32(209.06459), 'RMSE': np.float32(14.459066), 'SAE': np.float32(0.3232031), 'NDE': np.float32(0.9806757)}

Run 46/144: hidden=64, seq_len=1080, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.007923
Validation Loss: 0.00799910
Epoch [2/200], Train Loss: 0.007370
Validation Loss: 0.00750391
Epoch [3/200], Train Loss: 0.006901
Validation Loss: 0.00708321
Epoch [4/200], Train Loss: 0.006465
Validation Loss: 0.00672238
Epoch [5/200], Train Loss: 0.006215
Validation Loss: 0.00642340
Epoch [6/200], Train Loss: 0.005911
Validation Loss: 0.00617277
Epoch [7/200], Train Loss: 0.005616
Validation Loss: 0.00596228
Epoch [8/200], Train Loss: 0.005415
Validation Loss: 0.00578166
Epoch [9/200], Train Loss: 0.005214
Validation Loss: 0.00563673
Epoch [10/200], Train Loss: 0.005084
Validation Loss: 0.00551879
Epoch [11/200], Train Loss: 0.004980
Validation Loss: 0.00542431
Epoch [12/200], Train Loss: 0.004871
Validation Loss: 0.00535198
Epoch [13/200], Train Loss: 0.004825
Validation Loss: 0.00529984
Epoch [14/200], Train Loss: 0.004779
Validation Loss: 0.00526532
Epoch [15/200], Train Loss: 0.004789
Validation Loss: 0.00524425
Epoch [16/200], Train Loss: 0.004724
Validation Loss: 0.00523411
Epoch [17/200], Train Loss: 0.004726
Validation Loss: 0.00522972
Epoch [18/200], Train Loss: 0.004730
Validation Loss: 0.00522856
Epoch [19/200], Train Loss: 0.004686
Validation Loss: 0.00522872
Epoch [20/200], Train Loss: 0.004700
Validation Loss: 0.00522896
Epoch [21/200], Train Loss: 0.004710
Validation Loss: 0.00522899
Epoch [22/200], Train Loss: 0.004732
Validation Loss: 0.00522843
Epoch [23/200], Train Loss: 0.004640
Validation Loss: 0.00522736
Epoch [24/200], Train Loss: 0.004677
Validation Loss: 0.00522624
Epoch [25/200], Train Loss: 0.004685
Validation Loss: 0.00522511
Epoch [26/200], Train Loss: 0.004681
Validation Loss: 0.00522407
Epoch [27/200], Train Loss: 0.004666
Validation Loss: 0.00522314
Epoch [28/200], Train Loss: 0.004750
Validation Loss: 0.00522241
Epoch [29/200], Train Loss: 0.004719
Validation Loss: 0.00522168
Epoch [30/200], Train Loss: 0.004686
Validation Loss: 0.00522100
Epoch [31/200], Train Loss: 0.004685
Validation Loss: 0.00522038
Epoch [32/200], Train Loss: 0.004691
Validation Loss: 0.00521982
Epoch [33/200], Train Loss: 0.004690
Validation Loss: 0.00521930
Epoch [34/200], Train Loss: 0.004729
Validation Loss: 0.00521878
Epoch [35/200], Train Loss: 0.004693
Validation Loss: 0.00521822
Epoch [36/200], Train Loss: 0.004654
Validation Loss: 0.00521761
Epoch [37/200], Train Loss: 0.004649
Validation Loss: 0.00521706
Epoch [38/200], Train Loss: 0.004661
Validation Loss: 0.00521649
Epoch [39/200], Train Loss: 0.004671
Validation Loss: 0.00521591
Epoch [40/200], Train Loss: 0.004700
Validation Loss: 0.00521536
Epoch [41/200], Train Loss: 0.004727
Validation Loss: 0.00521481
Epoch [42/200], Train Loss: 0.004695
Validation Loss: 0.00521412
Epoch [43/200], Train Loss: 0.004738
Validation Loss: 0.00521343
Epoch [44/200], Train Loss: 0.004740
Validation Loss: 0.00521277
Epoch [45/200], Train Loss: 0.004729
Validation Loss: 0.00521203
Epoch [46/200], Train Loss: 0.004720
Validation Loss: 0.00521134
Epoch [47/200], Train Loss: 0.004662
Validation Loss: 0.00521057
Epoch [48/200], Train Loss: 0.004720
Validation Loss: 0.00520984
Epoch [49/200], Train Loss: 0.004742
Validation Loss: 0.00520901
Epoch [50/200], Train Loss: 0.004692
Validation Loss: 0.00520821
Epoch [51/200], Train Loss: 0.004714
Validation Loss: 0.00520742
Epoch [52/200], Train Loss: 0.004648
Validation Loss: 0.00520650
Epoch [53/200], Train Loss: 0.004699
Validation Loss: 0.00520568
Epoch [54/200], Train Loss: 0.004683
Validation Loss: 0.00520482
Epoch [55/200], Train Loss: 0.004683
Validation Loss: 0.00520394
Epoch [56/200], Train Loss: 0.004650
Validation Loss: 0.00520301
Epoch [57/200], Train Loss: 0.004734
Validation Loss: 0.00520217
Epoch [58/200], Train Loss: 0.004673
Validation Loss: 0.00520120
Epoch [59/200], Train Loss: 0.004709
Validation Loss: 0.00520031
Epoch [60/200], Train Loss: 0.004702
Validation Loss: 0.00519938
Epoch [61/200], Train Loss: 0.004670
Validation Loss: 0.00519848
Epoch [62/200], Train Loss: 0.004664
Validation Loss: 0.00519759
Epoch [63/200], Train Loss: 0.004645
Validation Loss: 0.00519667
Epoch [64/200], Train Loss: 0.004713
Validation Loss: 0.00519581
Epoch [65/200], Train Loss: 0.004673
Validation Loss: 0.00519489
Epoch [66/200], Train Loss: 0.004686
Validation Loss: 0.00519395
Epoch [67/200], Train Loss: 0.004693
Validation Loss: 0.00519297
Epoch [68/200], Train Loss: 0.004675
Validation Loss: 0.00519193
Epoch [69/200], Train Loss: 0.004699
Validation Loss: 0.00519089
Epoch [70/200], Train Loss: 0.004677
Validation Loss: 0.00518981
Epoch [71/200], Train Loss: 0.004688
Validation Loss: 0.00518872
Epoch [72/200], Train Loss: 0.004693
Validation Loss: 0.00518758
Epoch [73/200], Train Loss: 0.004650
Validation Loss: 0.00518629
Epoch [74/200], Train Loss: 0.004673
Validation Loss: 0.00518495
Epoch [75/200], Train Loss: 0.004653
Validation Loss: 0.00518353
Epoch [76/200], Train Loss: 0.004698
Validation Loss: 0.00518234
Epoch [77/200], Train Loss: 0.004647
Validation Loss: 0.00518102
Epoch [78/200], Train Loss: 0.004668
Validation Loss: 0.00517967
Epoch [79/200], Train Loss: 0.004641
Validation Loss: 0.00517823
Epoch [80/200], Train Loss: 0.004671
Validation Loss: 0.00517684
Epoch [81/200], Train Loss: 0.004677
Validation Loss: 0.00517536
Epoch [82/200], Train Loss: 0.004690
Validation Loss: 0.00517386
Epoch [83/200], Train Loss: 0.004671
Validation Loss: 0.00517220
Epoch [84/200], Train Loss: 0.004664
Validation Loss: 0.00517053
Epoch [85/200], Train Loss: 0.004669
Validation Loss: 0.00516877
Epoch [86/200], Train Loss: 0.004598
Validation Loss: 0.00516700
Epoch [87/200], Train Loss: 0.004634
Validation Loss: 0.00516528
Epoch [88/200], Train Loss: 0.004686
Validation Loss: 0.00516371
Epoch [89/200], Train Loss: 0.004688
Validation Loss: 0.00516205
Epoch [90/200], Train Loss: 0.004649
Validation Loss: 0.00516014
Epoch [91/200], Train Loss: 0.004670
Validation Loss: 0.00515856
Epoch [92/200], Train Loss: 0.004668
Validation Loss: 0.00515685
Epoch [93/200], Train Loss: 0.004687
Validation Loss: 0.00515514
Epoch [94/200], Train Loss: 0.004662
Validation Loss: 0.00515338
Epoch [95/200], Train Loss: 0.004618
Validation Loss: 0.00515159
Epoch [96/200], Train Loss: 0.004684
Validation Loss: 0.00514997
Epoch [97/200], Train Loss: 0.004625
Validation Loss: 0.00514818
Epoch [98/200], Train Loss: 0.004628
Validation Loss: 0.00514644
Epoch [99/200], Train Loss: 0.004618
Validation Loss: 0.00514468
Epoch [100/200], Train Loss: 0.004658
Validation Loss: 0.00514303
Epoch [101/200], Train Loss: 0.004698
Validation Loss: 0.00514162
Epoch [102/200], Train Loss: 0.004633
Validation Loss: 0.00514010
Epoch [103/200], Train Loss: 0.004598
Validation Loss: 0.00513867
Epoch [104/200], Train Loss: 0.004613
Validation Loss: 0.00513715
Epoch [105/200], Train Loss: 0.004600
Validation Loss: 0.00513558
Epoch [106/200], Train Loss: 0.004647
Validation Loss: 0.00513434
Epoch [107/200], Train Loss: 0.004583
Validation Loss: 0.00513282
Epoch [108/200], Train Loss: 0.004607
Validation Loss: 0.00513164
Epoch [109/200], Train Loss: 0.004580
Validation Loss: 0.00513039
Epoch [110/200], Train Loss: 0.004577
Validation Loss: 0.00512920
Epoch [111/200], Train Loss: 0.004587
Validation Loss: 0.00512833
Epoch [112/200], Train Loss: 0.004613
Validation Loss: 0.00512719
Epoch [113/200], Train Loss: 0.004653
Validation Loss: 0.00512640
Epoch [114/200], Train Loss: 0.004643
Validation Loss: 0.00512541
Epoch [115/200], Train Loss: 0.004643
Validation Loss: 0.00512439
Epoch [116/200], Train Loss: 0.004628
Validation Loss: 0.00512324
Epoch [117/200], Train Loss: 0.004620
Validation Loss: 0.00512234
Epoch [118/200], Train Loss: 0.004658
Validation Loss: 0.00512176
Epoch [119/200], Train Loss: 0.004594
Validation Loss: 0.00512070
Epoch [120/200], Train Loss: 0.004596
Validation Loss: 0.00512029
Epoch [121/200], Train Loss: 0.004575
Validation Loss: 0.00511990
Epoch [122/200], Train Loss: 0.004609
Validation Loss: 0.00511964
Epoch [123/200], Train Loss: 0.004659
Validation Loss: 0.00511962
Epoch [124/200], Train Loss: 0.004595
Validation Loss: 0.00511882
Epoch [125/200], Train Loss: 0.004614
Validation Loss: 0.00511842
Epoch [126/200], Train Loss: 0.004657
Validation Loss: 0.00511803
Epoch [127/200], Train Loss: 0.004624
Validation Loss: 0.00511764
Epoch [128/200], Train Loss: 0.004578
Validation Loss: 0.00511701
Epoch [129/200], Train Loss: 0.004647
Validation Loss: 0.00511710
Epoch [130/200], Train Loss: 0.004596
Validation Loss: 0.00511670
Epoch [131/200], Train Loss: 0.004598
Validation Loss: 0.00511661
Epoch [132/200], Train Loss: 0.004660
Validation Loss: 0.00511659
Epoch [133/200], Train Loss: 0.004640
Validation Loss: 0.00511646
Epoch [134/200], Train Loss: 0.004595
Validation Loss: 0.00511589
Epoch [135/200], Train Loss: 0.004526
Validation Loss: 0.00511531
Epoch [136/200], Train Loss: 0.004672
Validation Loss: 0.00511554
Epoch [137/200], Train Loss: 0.004579
Validation Loss: 0.00511513
Epoch [138/200], Train Loss: 0.004622
Validation Loss: 0.00511525
Epoch [139/200], Train Loss: 0.004602
Validation Loss: 0.00511482
Epoch [140/200], Train Loss: 0.004567
Validation Loss: 0.00511455
Epoch [141/200], Train Loss: 0.004594
Validation Loss: 0.00511448
Epoch [142/200], Train Loss: 0.004606
Validation Loss: 0.00511438
Epoch [143/200], Train Loss: 0.004580
Validation Loss: 0.00511424
Epoch [144/200], Train Loss: 0.004581
Validation Loss: 0.00511431
Epoch [145/200], Train Loss: 0.004617
Validation Loss: 0.00511444
Epoch [146/200], Train Loss: 0.004630
Validation Loss: 0.00511402
Epoch [147/200], Train Loss: 0.004565
Validation Loss: 0.00511343
Epoch [148/200], Train Loss: 0.004667
Validation Loss: 0.00511356
Epoch [149/200], Train Loss: 0.004587
Validation Loss: 0.00511314
Epoch [150/200], Train Loss: 0.004610
Validation Loss: 0.00511306
Epoch [151/200], Train Loss: 0.004587
Validation Loss: 0.00511270
Epoch [152/200], Train Loss: 0.004561
Validation Loss: 0.00511273
Epoch [153/200], Train Loss: 0.004620
Validation Loss: 0.00511286
Epoch [154/200], Train Loss: 0.004619
Validation Loss: 0.00511293
Epoch [155/200], Train Loss: 0.004568
Validation Loss: 0.00511239
Epoch [156/200], Train Loss: 0.004632
Validation Loss: 0.00511227
Epoch [157/200], Train Loss: 0.004553
Validation Loss: 0.00511214
Epoch [158/200], Train Loss: 0.004596
Validation Loss: 0.00511254
Epoch [159/200], Train Loss: 0.004618
Validation Loss: 0.00511219
Epoch [160/200], Train Loss: 0.004548
Validation Loss: 0.00511187
Epoch [161/200], Train Loss: 0.004595
Validation Loss: 0.00511194
Epoch [162/200], Train Loss: 0.004565
Validation Loss: 0.00511143
Epoch [163/200], Train Loss: 0.004599
Validation Loss: 0.00511142
Epoch [164/200], Train Loss: 0.004591
Validation Loss: 0.00511128
Epoch [165/200], Train Loss: 0.004629
Validation Loss: 0.00511144
Epoch [166/200], Train Loss: 0.004628
Validation Loss: 0.00511163
Epoch [167/200], Train Loss: 0.004606
Validation Loss: 0.00511136
Epoch [168/200], Train Loss: 0.004570
Validation Loss: 0.00511093
Epoch [169/200], Train Loss: 0.004582
Validation Loss: 0.00511062
Epoch [170/200], Train Loss: 0.004617
Validation Loss: 0.00511090
Epoch [171/200], Train Loss: 0.004574
Validation Loss: 0.00511062
Epoch [172/200], Train Loss: 0.004552
Validation Loss: 0.00511066
Epoch [173/200], Train Loss: 0.004592
Validation Loss: 0.00511066
Epoch [174/200], Train Loss: 0.004588
Validation Loss: 0.00511074
Epoch [175/200], Train Loss: 0.004616
Validation Loss: 0.00511058
Epoch [176/200], Train Loss: 0.004570
Validation Loss: 0.00511039
Epoch [177/200], Train Loss: 0.004646
Validation Loss: 0.00511033
Epoch [178/200], Train Loss: 0.004601
Validation Loss: 0.00511017
Epoch [179/200], Train Loss: 0.004613
Validation Loss: 0.00510973
Epoch [180/200], Train Loss: 0.004624
Validation Loss: 0.00510972
Epoch [181/200], Train Loss: 0.004591
Validation Loss: 0.00510954
Epoch [182/200], Train Loss: 0.004590
Validation Loss: 0.00510953
Epoch [183/200], Train Loss: 0.004608
Validation Loss: 0.00510951
Epoch [184/200], Train Loss: 0.004673
Validation Loss: 0.00510981
Epoch [185/200], Train Loss: 0.004644
Validation Loss: 0.00510976
Epoch [186/200], Train Loss: 0.004594
Validation Loss: 0.00510949
Epoch [187/200], Train Loss: 0.004648
Validation Loss: 0.00510921
Epoch [188/200], Train Loss: 0.004628
Validation Loss: 0.00510917
Epoch [189/200], Train Loss: 0.004599
Validation Loss: 0.00510909
Epoch [190/200], Train Loss: 0.004617
Validation Loss: 0.00510892
Epoch [191/200], Train Loss: 0.004566
Validation Loss: 0.00510857
Epoch [192/200], Train Loss: 0.004636
Validation Loss: 0.00510865
Epoch [193/200], Train Loss: 0.004614
Validation Loss: 0.00510860
Epoch [194/200], Train Loss: 0.004586
Validation Loss: 0.00510831
Epoch [195/200], Train Loss: 0.004632
Validation Loss: 0.00510841
Epoch [196/200], Train Loss: 0.004613
Validation Loss: 0.00510850
Epoch [197/200], Train Loss: 0.004599
Validation Loss: 0.00510861
Epoch [198/200], Train Loss: 0.004569
Validation Loss: 0.00510836
Epoch [199/200], Train Loss: 0.004601
Validation Loss: 0.00510829
Epoch [200/200], Train Loss: 0.004587
Validation Loss: 0.00510833

Evaluating model for: Lamp
Run 46/144 completed in 118.51 seconds with: {'MAE': np.float32(3.1120489), 'MSE': np.float32(208.65005), 'RMSE': np.float32(14.444724), 'SAE': np.float32(0.2639548), 'NDE': np.float32(0.9797029)}

Run 47/144: hidden=64, seq_len=1080, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.013213
Validation Loss: 0.01334456
Epoch [2/200], Train Loss: 0.012336
Validation Loss: 0.01249126
Epoch [3/200], Train Loss: 0.011521
Validation Loss: 0.01169418
Epoch [4/200], Train Loss: 0.010759
Validation Loss: 0.01094698
Epoch [5/200], Train Loss: 0.009972
Validation Loss: 0.01024762
Epoch [6/200], Train Loss: 0.009325
Validation Loss: 0.00959461
Epoch [7/200], Train Loss: 0.008715
Validation Loss: 0.00898401
Epoch [8/200], Train Loss: 0.008173
Validation Loss: 0.00842145
Epoch [9/200], Train Loss: 0.007602
Validation Loss: 0.00791914
Epoch [10/200], Train Loss: 0.007088
Validation Loss: 0.00747922
Epoch [11/200], Train Loss: 0.006739
Validation Loss: 0.00708250
Epoch [12/200], Train Loss: 0.006331
Validation Loss: 0.00671283
Epoch [13/200], Train Loss: 0.005996
Validation Loss: 0.00639103
Epoch [14/200], Train Loss: 0.005730
Validation Loss: 0.00610515
Epoch [15/200], Train Loss: 0.005384
Validation Loss: 0.00585157
Epoch [16/200], Train Loss: 0.005201
Validation Loss: 0.00565347
Epoch [17/200], Train Loss: 0.005033
Validation Loss: 0.00549096
Epoch [18/200], Train Loss: 0.004887
Validation Loss: 0.00537135
Epoch [19/200], Train Loss: 0.004714
Validation Loss: 0.00528632
Epoch [20/200], Train Loss: 0.004722
Validation Loss: 0.00523463
Epoch [21/200], Train Loss: 0.004665
Validation Loss: 0.00521008
Epoch [22/200], Train Loss: 0.004704
Validation Loss: 0.00520366
Epoch [23/200], Train Loss: 0.004736
Validation Loss: 0.00520475
Epoch [24/200], Train Loss: 0.004687
Validation Loss: 0.00520588
Epoch [25/200], Train Loss: 0.004697
Validation Loss: 0.00520481
Epoch [26/200], Train Loss: 0.004690
Validation Loss: 0.00520289
Epoch [27/200], Train Loss: 0.004665
Validation Loss: 0.00520174
Epoch [28/200], Train Loss: 0.004698
Validation Loss: 0.00520187
Epoch [29/200], Train Loss: 0.004688
Validation Loss: 0.00520272
Epoch [30/200], Train Loss: 0.004674
Validation Loss: 0.00520357
Epoch [31/200], Train Loss: 0.004704
Validation Loss: 0.00520379
Epoch [32/200], Train Loss: 0.004687
Validation Loss: 0.00520343
Epoch [33/200], Train Loss: 0.004674
Validation Loss: 0.00520265
Epoch [34/200], Train Loss: 0.004706
Validation Loss: 0.00520153
Epoch [35/200], Train Loss: 0.004676
Validation Loss: 0.00520032
Epoch [36/200], Train Loss: 0.004673
Validation Loss: 0.00519925
Epoch [37/200], Train Loss: 0.004708
Validation Loss: 0.00519821
Epoch [38/200], Train Loss: 0.004717
Validation Loss: 0.00519739
Epoch [39/200], Train Loss: 0.004680
Validation Loss: 0.00519645
Epoch [40/200], Train Loss: 0.004646
Validation Loss: 0.00519561
Epoch [41/200], Train Loss: 0.004705
Validation Loss: 0.00519495
Epoch [42/200], Train Loss: 0.004725
Validation Loss: 0.00519437
Epoch [43/200], Train Loss: 0.004671
Validation Loss: 0.00519341
Epoch [44/200], Train Loss: 0.004682
Validation Loss: 0.00519248
Epoch [45/200], Train Loss: 0.004664
Validation Loss: 0.00519167
Epoch [46/200], Train Loss: 0.004700
Validation Loss: 0.00519097
Epoch [47/200], Train Loss: 0.004598
Validation Loss: 0.00518988
Epoch [48/200], Train Loss: 0.004762
Validation Loss: 0.00518915
Epoch [49/200], Train Loss: 0.004703
Validation Loss: 0.00518825
Epoch [50/200], Train Loss: 0.004710
Validation Loss: 0.00518720
Epoch [51/200], Train Loss: 0.004693
Validation Loss: 0.00518624
Epoch [52/200], Train Loss: 0.004705
Validation Loss: 0.00518523
Epoch [53/200], Train Loss: 0.004705
Validation Loss: 0.00518403
Epoch [54/200], Train Loss: 0.004712
Validation Loss: 0.00518300
Epoch [55/200], Train Loss: 0.004696
Validation Loss: 0.00518193
Epoch [56/200], Train Loss: 0.004690
Validation Loss: 0.00518085
Epoch [57/200], Train Loss: 0.004690
Validation Loss: 0.00517986
Epoch [58/200], Train Loss: 0.004719
Validation Loss: 0.00517889
Epoch [59/200], Train Loss: 0.004706
Validation Loss: 0.00517772
Epoch [60/200], Train Loss: 0.004672
Validation Loss: 0.00517628
Epoch [61/200], Train Loss: 0.004638
Validation Loss: 0.00517501
Epoch [62/200], Train Loss: 0.004646
Validation Loss: 0.00517413
Epoch [63/200], Train Loss: 0.004660
Validation Loss: 0.00517295
Epoch [64/200], Train Loss: 0.004689
Validation Loss: 0.00517198
Epoch [65/200], Train Loss: 0.004640
Validation Loss: 0.00517073
Epoch [66/200], Train Loss: 0.004649
Validation Loss: 0.00516948
Epoch [67/200], Train Loss: 0.004658
Validation Loss: 0.00516814
Epoch [68/200], Train Loss: 0.004618
Validation Loss: 0.00516658
Epoch [69/200], Train Loss: 0.004702
Validation Loss: 0.00516595
Epoch [70/200], Train Loss: 0.004655
Validation Loss: 0.00516444
Epoch [71/200], Train Loss: 0.004636
Validation Loss: 0.00516295
Epoch [72/200], Train Loss: 0.004702
Validation Loss: 0.00516169
Epoch [73/200], Train Loss: 0.004709
Validation Loss: 0.00516020
Epoch [74/200], Train Loss: 0.004647
Validation Loss: 0.00515835
Epoch [75/200], Train Loss: 0.004666
Validation Loss: 0.00515662
Epoch [76/200], Train Loss: 0.004681
Validation Loss: 0.00515521
Epoch [77/200], Train Loss: 0.004644
Validation Loss: 0.00515377
Epoch [78/200], Train Loss: 0.004704
Validation Loss: 0.00515272
Epoch [79/200], Train Loss: 0.004634
Validation Loss: 0.00515093
Epoch [80/200], Train Loss: 0.004630
Validation Loss: 0.00514920
Epoch [81/200], Train Loss: 0.004611
Validation Loss: 0.00514768
Epoch [82/200], Train Loss: 0.004673
Validation Loss: 0.00514685
Epoch [83/200], Train Loss: 0.004646
Validation Loss: 0.00514554
Epoch [84/200], Train Loss: 0.004663
Validation Loss: 0.00514413
Epoch [85/200], Train Loss: 0.004610
Validation Loss: 0.00514278
Epoch [86/200], Train Loss: 0.004635
Validation Loss: 0.00514131
Epoch [87/200], Train Loss: 0.004655
Validation Loss: 0.00513953
Epoch [88/200], Train Loss: 0.004679
Validation Loss: 0.00513849
Epoch [89/200], Train Loss: 0.004642
Validation Loss: 0.00513689
Epoch [90/200], Train Loss: 0.004650
Validation Loss: 0.00513581
Epoch [91/200], Train Loss: 0.004607
Validation Loss: 0.00513430
Epoch [92/200], Train Loss: 0.004633
Validation Loss: 0.00513374
Epoch [93/200], Train Loss: 0.004651
Validation Loss: 0.00513325
Epoch [94/200], Train Loss: 0.004635
Validation Loss: 0.00513279
Epoch [95/200], Train Loss: 0.004654
Validation Loss: 0.00513231
Epoch [96/200], Train Loss: 0.004638
Validation Loss: 0.00513041
Epoch [97/200], Train Loss: 0.004634
Validation Loss: 0.00512916
Epoch [98/200], Train Loss: 0.004685
Validation Loss: 0.00512853
Epoch [99/200], Train Loss: 0.004680
Validation Loss: 0.00512768
Epoch [100/200], Train Loss: 0.004708
Validation Loss: 0.00512700
Epoch [101/200], Train Loss: 0.004640
Validation Loss: 0.00512517
Epoch [102/200], Train Loss: 0.004603
Validation Loss: 0.00512426
Epoch [103/200], Train Loss: 0.004574
Validation Loss: 0.00512399
Epoch [104/200], Train Loss: 0.004638
Validation Loss: 0.00512514
Epoch [105/200], Train Loss: 0.004593
Validation Loss: 0.00512488
Epoch [106/200], Train Loss: 0.004636
Validation Loss: 0.00512549
Epoch [107/200], Train Loss: 0.004689
Validation Loss: 0.00512457
Epoch [108/200], Train Loss: 0.004621
Validation Loss: 0.00512267
Epoch [109/200], Train Loss: 0.004616
Validation Loss: 0.00512103
Epoch [110/200], Train Loss: 0.004593
Validation Loss: 0.00512052
Epoch [111/200], Train Loss: 0.004626
Validation Loss: 0.00512085
Epoch [112/200], Train Loss: 0.004654
Validation Loss: 0.00512154
Epoch [113/200], Train Loss: 0.004602
Validation Loss: 0.00512205
Epoch [114/200], Train Loss: 0.004609
Validation Loss: 0.00512086
Epoch [115/200], Train Loss: 0.004637
Validation Loss: 0.00512039
Epoch [116/200], Train Loss: 0.004683
Validation Loss: 0.00512100
Epoch [117/200], Train Loss: 0.004646
Validation Loss: 0.00511984
Epoch [118/200], Train Loss: 0.004587
Validation Loss: 0.00511883
Epoch [119/200], Train Loss: 0.004639
Validation Loss: 0.00511836
Epoch [120/200], Train Loss: 0.004595
Validation Loss: 0.00511801
Epoch [121/200], Train Loss: 0.004661
Validation Loss: 0.00511918
Epoch [122/200], Train Loss: 0.004598
Validation Loss: 0.00511970
Epoch [123/200], Train Loss: 0.004637
Validation Loss: 0.00511894
Epoch [124/200], Train Loss: 0.004591
Validation Loss: 0.00511847
Epoch [125/200], Train Loss: 0.004565
Validation Loss: 0.00511736
Epoch [126/200], Train Loss: 0.004628
Validation Loss: 0.00511713
Epoch [127/200], Train Loss: 0.004623
Validation Loss: 0.00511765
Epoch [128/200], Train Loss: 0.004622
Validation Loss: 0.00511814
Epoch [129/200], Train Loss: 0.004673
Validation Loss: 0.00511851
Epoch [130/200], Train Loss: 0.004585
Validation Loss: 0.00511760
Epoch [131/200], Train Loss: 0.004609
Validation Loss: 0.00511689
Epoch [132/200], Train Loss: 0.004612
Validation Loss: 0.00511648
Epoch [133/200], Train Loss: 0.004563
Validation Loss: 0.00511605
Epoch [134/200], Train Loss: 0.004617
Validation Loss: 0.00511630
Epoch [135/200], Train Loss: 0.004676
Validation Loss: 0.00511782
Epoch [136/200], Train Loss: 0.004687
Validation Loss: 0.00511811
Epoch [137/200], Train Loss: 0.004611
Validation Loss: 0.00511596
Epoch [138/200], Train Loss: 0.004670
Validation Loss: 0.00511546
Epoch [139/200], Train Loss: 0.004645
Validation Loss: 0.00511580
Epoch [140/200], Train Loss: 0.004597
Validation Loss: 0.00511531
Epoch [141/200], Train Loss: 0.004610
Validation Loss: 0.00511518
Epoch [142/200], Train Loss: 0.004627
Validation Loss: 0.00511566
Epoch [143/200], Train Loss: 0.004653
Validation Loss: 0.00511530
Epoch [144/200], Train Loss: 0.004615
Validation Loss: 0.00511597
Epoch [145/200], Train Loss: 0.004644
Validation Loss: 0.00511599
Epoch [146/200], Train Loss: 0.004632
Validation Loss: 0.00511564
Epoch [147/200], Train Loss: 0.004689
Validation Loss: 0.00511535
Epoch [148/200], Train Loss: 0.004632
Validation Loss: 0.00511412
Epoch [149/200], Train Loss: 0.004599
Validation Loss: 0.00511413
Epoch [150/200], Train Loss: 0.004604
Validation Loss: 0.00511451
Epoch [151/200], Train Loss: 0.004609
Validation Loss: 0.00511487
Epoch [152/200], Train Loss: 0.004659
Validation Loss: 0.00511545
Epoch [153/200], Train Loss: 0.004611
Validation Loss: 0.00511480
Epoch [154/200], Train Loss: 0.004577
Validation Loss: 0.00511422
Epoch [155/200], Train Loss: 0.004590
Validation Loss: 0.00511451
Epoch [156/200], Train Loss: 0.004636
Validation Loss: 0.00511491
Epoch [157/200], Train Loss: 0.004599
Validation Loss: 0.00511465
Epoch [158/200], Train Loss: 0.004600
Validation Loss: 0.00511452
Early stopping triggered

Evaluating model for: Lamp
Run 47/144 completed in 94.63 seconds with: {'MAE': np.float32(3.100085), 'MSE': np.float32(209.44331), 'RMSE': np.float32(14.472157), 'SAE': np.float32(0.32705975), 'NDE': np.float32(0.98156345)}

Run 48/144: hidden=64, seq_len=1080, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.005319
Validation Loss: 0.00574978
Epoch [2/200], Train Loss: 0.005168
Validation Loss: 0.00555562
Epoch [3/200], Train Loss: 0.004979
Validation Loss: 0.00540935
Epoch [4/200], Train Loss: 0.004826
Validation Loss: 0.00530991
Epoch [5/200], Train Loss: 0.004796
Validation Loss: 0.00525209
Epoch [6/200], Train Loss: 0.004710
Validation Loss: 0.00522629
Epoch [7/200], Train Loss: 0.004668
Validation Loss: 0.00522221
Epoch [8/200], Train Loss: 0.004762
Validation Loss: 0.00522853
Epoch [9/200], Train Loss: 0.004763
Validation Loss: 0.00523457
Epoch [10/200], Train Loss: 0.004698
Validation Loss: 0.00523520
Epoch [11/200], Train Loss: 0.004667
Validation Loss: 0.00523144
Epoch [12/200], Train Loss: 0.004723
Validation Loss: 0.00522758
Epoch [13/200], Train Loss: 0.004705
Validation Loss: 0.00522407
Epoch [14/200], Train Loss: 0.004640
Validation Loss: 0.00522197
Epoch [15/200], Train Loss: 0.004674
Validation Loss: 0.00522119
Epoch [16/200], Train Loss: 0.004643
Validation Loss: 0.00522097
Epoch [17/200], Train Loss: 0.004694
Validation Loss: 0.00522086
Epoch [18/200], Train Loss: 0.004702
Validation Loss: 0.00522075
Epoch [19/200], Train Loss: 0.004689
Validation Loss: 0.00522067
Epoch [20/200], Train Loss: 0.004648
Validation Loss: 0.00522064
Epoch [21/200], Train Loss: 0.004699
Validation Loss: 0.00522062
Epoch [22/200], Train Loss: 0.004691
Validation Loss: 0.00522064
Epoch [23/200], Train Loss: 0.004649
Validation Loss: 0.00522060
Epoch [24/200], Train Loss: 0.004632
Validation Loss: 0.00522047
Epoch [25/200], Train Loss: 0.004640
Validation Loss: 0.00522020
Epoch [26/200], Train Loss: 0.004720
Validation Loss: 0.00522008
Epoch [27/200], Train Loss: 0.004645
Validation Loss: 0.00521975
Epoch [28/200], Train Loss: 0.004698
Validation Loss: 0.00521952
Epoch [29/200], Train Loss: 0.004681
Validation Loss: 0.00521926
Epoch [30/200], Train Loss: 0.004682
Validation Loss: 0.00521896
Epoch [31/200], Train Loss: 0.004687
Validation Loss: 0.00521868
Epoch [32/200], Train Loss: 0.004684
Validation Loss: 0.00521847
Epoch [33/200], Train Loss: 0.004634
Validation Loss: 0.00521806
Epoch [34/200], Train Loss: 0.004691
Validation Loss: 0.00521779
Epoch [35/200], Train Loss: 0.004675
Validation Loss: 0.00521736
Epoch [36/200], Train Loss: 0.004695
Validation Loss: 0.00521707
Epoch [37/200], Train Loss: 0.004640
Validation Loss: 0.00521658
Epoch [38/200], Train Loss: 0.004699
Validation Loss: 0.00521621
Epoch [39/200], Train Loss: 0.004641
Validation Loss: 0.00521557
Epoch [40/200], Train Loss: 0.004656
Validation Loss: 0.00521494
Epoch [41/200], Train Loss: 0.004650
Validation Loss: 0.00521430
Epoch [42/200], Train Loss: 0.004622
Validation Loss: 0.00521346
Epoch [43/200], Train Loss: 0.004718
Validation Loss: 0.00521276
Epoch [44/200], Train Loss: 0.004666
Validation Loss: 0.00521172
Epoch [45/200], Train Loss: 0.004631
Validation Loss: 0.00521048
Epoch [46/200], Train Loss: 0.004635
Validation Loss: 0.00520919
Epoch [47/200], Train Loss: 0.004682
Validation Loss: 0.00520795
Epoch [48/200], Train Loss: 0.004650
Validation Loss: 0.00520623
Epoch [49/200], Train Loss: 0.004604
Validation Loss: 0.00520336
Epoch [50/200], Train Loss: 0.004647
Validation Loss: 0.00520103
Epoch [51/200], Train Loss: 0.004643
Validation Loss: 0.00519836
Epoch [52/200], Train Loss: 0.004627
Validation Loss: 0.00519548
Epoch [53/200], Train Loss: 0.004645
Validation Loss: 0.00519222
Epoch [54/200], Train Loss: 0.004631
Validation Loss: 0.00518819
Epoch [55/200], Train Loss: 0.004648
Validation Loss: 0.00518317
Epoch [56/200], Train Loss: 0.004624
Validation Loss: 0.00517718
Epoch [57/200], Train Loss: 0.004617
Validation Loss: 0.00517050
Epoch [58/200], Train Loss: 0.004587
Validation Loss: 0.00516306
Epoch [59/200], Train Loss: 0.004622
Validation Loss: 0.00515502
Epoch [60/200], Train Loss: 0.004604
Validation Loss: 0.00514590
Epoch [61/200], Train Loss: 0.004605
Validation Loss: 0.00513781
Epoch [62/200], Train Loss: 0.004627
Validation Loss: 0.00513022
Epoch [63/200], Train Loss: 0.004660
Validation Loss: 0.00512487
Epoch [64/200], Train Loss: 0.004639
Validation Loss: 0.00511857
Epoch [65/200], Train Loss: 0.004597
Validation Loss: 0.00511486
Epoch [66/200], Train Loss: 0.004601
Validation Loss: 0.00511324
Epoch [67/200], Train Loss: 0.004554
Validation Loss: 0.00511269
Epoch [68/200], Train Loss: 0.004632
Validation Loss: 0.00511485
Epoch [69/200], Train Loss: 0.004596
Validation Loss: 0.00511211
Epoch [70/200], Train Loss: 0.004607
Validation Loss: 0.00510974
Epoch [71/200], Train Loss: 0.004599
Validation Loss: 0.00511061
Epoch [72/200], Train Loss: 0.004519
Validation Loss: 0.00511085
Epoch [73/200], Train Loss: 0.004590
Validation Loss: 0.00511199
Epoch [74/200], Train Loss: 0.004588
Validation Loss: 0.00511230
Epoch [75/200], Train Loss: 0.004578
Validation Loss: 0.00511086
Epoch [76/200], Train Loss: 0.004602
Validation Loss: 0.00510996
Epoch [77/200], Train Loss: 0.004574
Validation Loss: 0.00510929
Epoch [78/200], Train Loss: 0.004591
Validation Loss: 0.00511007
Epoch [79/200], Train Loss: 0.004599
Validation Loss: 0.00510994
Epoch [80/200], Train Loss: 0.004524
Validation Loss: 0.00510921
Epoch [81/200], Train Loss: 0.004580
Validation Loss: 0.00510899
Epoch [82/200], Train Loss: 0.004545
Validation Loss: 0.00510877
Epoch [83/200], Train Loss: 0.004553
Validation Loss: 0.00510859
Epoch [84/200], Train Loss: 0.004590
Validation Loss: 0.00510883
Epoch [85/200], Train Loss: 0.004531
Validation Loss: 0.00510839
Epoch [86/200], Train Loss: 0.004579
Validation Loss: 0.00510895
Epoch [87/200], Train Loss: 0.004582
Validation Loss: 0.00510863
Epoch [88/200], Train Loss: 0.004577
Validation Loss: 0.00510779
Epoch [89/200], Train Loss: 0.004548
Validation Loss: 0.00510699
Epoch [90/200], Train Loss: 0.004540
Validation Loss: 0.00510712
Epoch [91/200], Train Loss: 0.004573
Validation Loss: 0.00510687
Epoch [92/200], Train Loss: 0.004575
Validation Loss: 0.00510765
Epoch [93/200], Train Loss: 0.004614
Validation Loss: 0.00510730
Epoch [94/200], Train Loss: 0.004534
Validation Loss: 0.00510626
Epoch [95/200], Train Loss: 0.004574
Validation Loss: 0.00510562
Epoch [96/200], Train Loss: 0.004541
Validation Loss: 0.00510555
Epoch [97/200], Train Loss: 0.004574
Validation Loss: 0.00510650
Epoch [98/200], Train Loss: 0.004613
Validation Loss: 0.00510759
Epoch [99/200], Train Loss: 0.004543
Validation Loss: 0.00510570
Epoch [100/200], Train Loss: 0.004590
Validation Loss: 0.00510567
Epoch [101/200], Train Loss: 0.004558
Validation Loss: 0.00510417
Epoch [102/200], Train Loss: 0.004568
Validation Loss: 0.00510442
Epoch [103/200], Train Loss: 0.004552
Validation Loss: 0.00510465
Epoch [104/200], Train Loss: 0.004638
Validation Loss: 0.00510553
Epoch [105/200], Train Loss: 0.004601
Validation Loss: 0.00510559
Epoch [106/200], Train Loss: 0.004548
Validation Loss: 0.00510430
Epoch [107/200], Train Loss: 0.004536
Validation Loss: 0.00510345
Epoch [108/200], Train Loss: 0.004579
Validation Loss: 0.00510343
Epoch [109/200], Train Loss: 0.004570
Validation Loss: 0.00510436
Epoch [110/200], Train Loss: 0.004559
Validation Loss: 0.00510466
Epoch [111/200], Train Loss: 0.004571
Validation Loss: 0.00510406
Epoch [112/200], Train Loss: 0.004551
Validation Loss: 0.00510369
Epoch [113/200], Train Loss: 0.004551
Validation Loss: 0.00510325
Epoch [114/200], Train Loss: 0.004569
Validation Loss: 0.00510246
Epoch [115/200], Train Loss: 0.004549
Validation Loss: 0.00510195
Epoch [116/200], Train Loss: 0.004551
Validation Loss: 0.00510230
Epoch [117/200], Train Loss: 0.004615
Validation Loss: 0.00510289
Epoch [118/200], Train Loss: 0.004550
Validation Loss: 0.00510223
Epoch [119/200], Train Loss: 0.004542
Validation Loss: 0.00510129
Epoch [120/200], Train Loss: 0.004564
Validation Loss: 0.00510167
Epoch [121/200], Train Loss: 0.004542
Validation Loss: 0.00510141
Epoch [122/200], Train Loss: 0.004525
Validation Loss: 0.00510082
Epoch [123/200], Train Loss: 0.004554
Validation Loss: 0.00510139
Epoch [124/200], Train Loss: 0.004532
Validation Loss: 0.00510100
Epoch [125/200], Train Loss: 0.004544
Validation Loss: 0.00510119
Epoch [126/200], Train Loss: 0.004585
Validation Loss: 0.00510149
Epoch [127/200], Train Loss: 0.004561
Validation Loss: 0.00510068
Epoch [128/200], Train Loss: 0.004604
Validation Loss: 0.00510031
Epoch [129/200], Train Loss: 0.004573
Validation Loss: 0.00509956
Epoch [130/200], Train Loss: 0.004535
Validation Loss: 0.00509935
Epoch [131/200], Train Loss: 0.004532
Validation Loss: 0.00509925
Epoch [132/200], Train Loss: 0.004600
Validation Loss: 0.00510052
Epoch [133/200], Train Loss: 0.004570
Validation Loss: 0.00510031
Epoch [134/200], Train Loss: 0.004569
Validation Loss: 0.00509938
Epoch [135/200], Train Loss: 0.004582
Validation Loss: 0.00509892
Epoch [136/200], Train Loss: 0.004619
Validation Loss: 0.00509892
Epoch [137/200], Train Loss: 0.004607
Validation Loss: 0.00509876
Epoch [138/200], Train Loss: 0.004614
Validation Loss: 0.00509828
Epoch [139/200], Train Loss: 0.004582
Validation Loss: 0.00509772
Epoch [140/200], Train Loss: 0.004559
Validation Loss: 0.00509775
Epoch [141/200], Train Loss: 0.004568
Validation Loss: 0.00509775
Epoch [142/200], Train Loss: 0.004577
Validation Loss: 0.00509825
Epoch [143/200], Train Loss: 0.004565
Validation Loss: 0.00509800
Epoch [144/200], Train Loss: 0.004585
Validation Loss: 0.00509816
Epoch [145/200], Train Loss: 0.004571
Validation Loss: 0.00509758
Epoch [146/200], Train Loss: 0.004552
Validation Loss: 0.00509655
Epoch [147/200], Train Loss: 0.004599
Validation Loss: 0.00509672
Epoch [148/200], Train Loss: 0.004574
Validation Loss: 0.00509702
Epoch [149/200], Train Loss: 0.004601
Validation Loss: 0.00509722
Epoch [150/200], Train Loss: 0.004621
Validation Loss: 0.00509768
Epoch [151/200], Train Loss: 0.004524
Validation Loss: 0.00509659
Epoch [152/200], Train Loss: 0.004528
Validation Loss: 0.00509602
Epoch [153/200], Train Loss: 0.004582
Validation Loss: 0.00509623
Epoch [154/200], Train Loss: 0.004522
Validation Loss: 0.00509606
Epoch [155/200], Train Loss: 0.004549
Validation Loss: 0.00509636
Epoch [156/200], Train Loss: 0.004559
Validation Loss: 0.00509648
Epoch [157/200], Train Loss: 0.004554
Validation Loss: 0.00509647
Epoch [158/200], Train Loss: 0.004574
Validation Loss: 0.00509694
Epoch [159/200], Train Loss: 0.004553
Validation Loss: 0.00509606
Epoch [160/200], Train Loss: 0.004582
Validation Loss: 0.00509589
Epoch [161/200], Train Loss: 0.004570
Validation Loss: 0.00509570
Epoch [162/200], Train Loss: 0.004574
Validation Loss: 0.00509534
Epoch [163/200], Train Loss: 0.004586
Validation Loss: 0.00509530
Epoch [164/200], Train Loss: 0.004618
Validation Loss: 0.00509582
Epoch [165/200], Train Loss: 0.004578
Validation Loss: 0.00509498
Epoch [166/200], Train Loss: 0.004593
Validation Loss: 0.00509505
Epoch [167/200], Train Loss: 0.004581
Validation Loss: 0.00509556
Epoch [168/200], Train Loss: 0.004575
Validation Loss: 0.00509548
Epoch [169/200], Train Loss: 0.004637
Validation Loss: 0.00509591
Epoch [170/200], Train Loss: 0.004481
Validation Loss: 0.00509510
Epoch [171/200], Train Loss: 0.004527
Validation Loss: 0.00509485
Epoch [172/200], Train Loss: 0.004547
Validation Loss: 0.00509461
Epoch [173/200], Train Loss: 0.004491
Validation Loss: 0.00509444
Epoch [174/200], Train Loss: 0.004590
Validation Loss: 0.00509524
Epoch [175/200], Train Loss: 0.004563
Validation Loss: 0.00509503
Epoch [176/200], Train Loss: 0.004551
Validation Loss: 0.00509434
Epoch [177/200], Train Loss: 0.004593
Validation Loss: 0.00509503
Epoch [178/200], Train Loss: 0.004584
Validation Loss: 0.00509482
Epoch [179/200], Train Loss: 0.004590
Validation Loss: 0.00509482
Epoch [180/200], Train Loss: 0.004590
Validation Loss: 0.00509451
Epoch [181/200], Train Loss: 0.004569
Validation Loss: 0.00509426
Epoch [182/200], Train Loss: 0.004537
Validation Loss: 0.00509375
Epoch [183/200], Train Loss: 0.004541
Validation Loss: 0.00509345
Epoch [184/200], Train Loss: 0.004580
Validation Loss: 0.00509409
Epoch [185/200], Train Loss: 0.004551
Validation Loss: 0.00509420
Epoch [186/200], Train Loss: 0.004578
Validation Loss: 0.00509422
Epoch [187/200], Train Loss: 0.004527
Validation Loss: 0.00509398
Epoch [188/200], Train Loss: 0.004602
Validation Loss: 0.00509392
Epoch [189/200], Train Loss: 0.004519
Validation Loss: 0.00509364
Epoch [190/200], Train Loss: 0.004509
Validation Loss: 0.00509358
Epoch [191/200], Train Loss: 0.004539
Validation Loss: 0.00509347
Epoch [192/200], Train Loss: 0.004580
Validation Loss: 0.00509407
Epoch [193/200], Train Loss: 0.004583
Validation Loss: 0.00509418
Early stopping triggered

Evaluating model for: Lamp
Run 48/144 completed in 121.52 seconds with: {'MAE': np.float32(2.9882367), 'MSE': np.float32(208.53479), 'RMSE': np.float32(14.440734), 'SAE': np.float32(0.29341435), 'NDE': np.float32(0.9794321)}

Run 49/144: hidden=128, seq_len=120, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004811
Validation Loss: 0.00474906
Epoch [2/200], Train Loss: 0.004731
Validation Loss: 0.00473304
Epoch [3/200], Train Loss: 0.004722
Validation Loss: 0.00472860
Epoch [4/200], Train Loss: 0.004717
Validation Loss: 0.00471824
Epoch [5/200], Train Loss: 0.004705
Validation Loss: 0.00469513
Epoch [6/200], Train Loss: 0.004679
Validation Loss: 0.00467800
Epoch [7/200], Train Loss: 0.004655
Validation Loss: 0.00464434
Epoch [8/200], Train Loss: 0.004610
Validation Loss: 0.00458402
Epoch [9/200], Train Loss: 0.004497
Validation Loss: 0.00435000
Epoch [10/200], Train Loss: 0.004221
Validation Loss: 0.00394277
Epoch [11/200], Train Loss: 0.003845
Validation Loss: 0.00343850
Epoch [12/200], Train Loss: 0.003253
Validation Loss: 0.00275984
Epoch [13/200], Train Loss: 0.002674
Validation Loss: 0.00231326
Epoch [14/200], Train Loss: 0.002335
Validation Loss: 0.00208076
Epoch [15/200], Train Loss: 0.002117
Validation Loss: 0.00193387
Epoch [16/200], Train Loss: 0.001966
Validation Loss: 0.00184784
Epoch [17/200], Train Loss: 0.001846
Validation Loss: 0.00174311
Epoch [18/200], Train Loss: 0.001746
Validation Loss: 0.00169003
Epoch [19/200], Train Loss: 0.001671
Validation Loss: 0.00160090
Epoch [20/200], Train Loss: 0.001597
Validation Loss: 0.00153479
Epoch [21/200], Train Loss: 0.001520
Validation Loss: 0.00148761
Epoch [22/200], Train Loss: 0.001461
Validation Loss: 0.00142852
Epoch [23/200], Train Loss: 0.001407
Validation Loss: 0.00140911
Epoch [24/200], Train Loss: 0.001353
Validation Loss: 0.00135011
Epoch [25/200], Train Loss: 0.001302
Validation Loss: 0.00132386
Epoch [26/200], Train Loss: 0.001268
Validation Loss: 0.00125492
Epoch [27/200], Train Loss: 0.001227
Validation Loss: 0.00124964
Epoch [28/200], Train Loss: 0.001187
Validation Loss: 0.00119555
Epoch [29/200], Train Loss: 0.001149
Validation Loss: 0.00116177
Epoch [30/200], Train Loss: 0.001118
Validation Loss: 0.00112609
Epoch [31/200], Train Loss: 0.001116
Validation Loss: 0.00116716
Epoch [32/200], Train Loss: 0.001077
Validation Loss: 0.00108832
Epoch [33/200], Train Loss: 0.001043
Validation Loss: 0.00108249
Epoch [34/200], Train Loss: 0.001022
Validation Loss: 0.00105574
Epoch [35/200], Train Loss: 0.001021
Validation Loss: 0.00103530
Epoch [36/200], Train Loss: 0.000988
Validation Loss: 0.00099525
Epoch [37/200], Train Loss: 0.000956
Validation Loss: 0.00098992
Epoch [38/200], Train Loss: 0.000952
Validation Loss: 0.00097812
Epoch [39/200], Train Loss: 0.000923
Validation Loss: 0.00094527
Epoch [40/200], Train Loss: 0.000898
Validation Loss: 0.00094918
Epoch [41/200], Train Loss: 0.000883
Validation Loss: 0.00093183
Epoch [42/200], Train Loss: 0.000855
Validation Loss: 0.00087469
Epoch [43/200], Train Loss: 0.000835
Validation Loss: 0.00084467
Epoch [44/200], Train Loss: 0.000814
Validation Loss: 0.00082746
Epoch [45/200], Train Loss: 0.000802
Validation Loss: 0.00082529
Epoch [46/200], Train Loss: 0.000777
Validation Loss: 0.00079302
Epoch [47/200], Train Loss: 0.000761
Validation Loss: 0.00078547
Epoch [48/200], Train Loss: 0.000745
Validation Loss: 0.00076221
Epoch [49/200], Train Loss: 0.000733
Validation Loss: 0.00080019
Epoch [50/200], Train Loss: 0.000739
Validation Loss: 0.00075002
Epoch [51/200], Train Loss: 0.000709
Validation Loss: 0.00071695
Epoch [52/200], Train Loss: 0.000697
Validation Loss: 0.00073432
Epoch [53/200], Train Loss: 0.000686
Validation Loss: 0.00070009
Epoch [54/200], Train Loss: 0.000686
Validation Loss: 0.00070571
Epoch [55/200], Train Loss: 0.000671
Validation Loss: 0.00067739
Epoch [56/200], Train Loss: 0.000650
Validation Loss: 0.00067642
Epoch [57/200], Train Loss: 0.000648
Validation Loss: 0.00070470
Epoch [58/200], Train Loss: 0.000638
Validation Loss: 0.00066807
Epoch [59/200], Train Loss: 0.000634
Validation Loss: 0.00064464
Epoch [60/200], Train Loss: 0.000633
Validation Loss: 0.00063947
Epoch [61/200], Train Loss: 0.000610
Validation Loss: 0.00062460
Epoch [62/200], Train Loss: 0.000608
Validation Loss: 0.00061842
Epoch [63/200], Train Loss: 0.000599
Validation Loss: 0.00062091
Epoch [64/200], Train Loss: 0.000609
Validation Loss: 0.00061049
Epoch [65/200], Train Loss: 0.000590
Validation Loss: 0.00063450
Epoch [66/200], Train Loss: 0.000589
Validation Loss: 0.00061290
Epoch [67/200], Train Loss: 0.000589
Validation Loss: 0.00061363
Epoch [68/200], Train Loss: 0.000579
Validation Loss: 0.00059331
Epoch [69/200], Train Loss: 0.000567
Validation Loss: 0.00057492
Epoch [70/200], Train Loss: 0.000565
Validation Loss: 0.00057810
Epoch [71/200], Train Loss: 0.000561
Validation Loss: 0.00060172
Epoch [72/200], Train Loss: 0.000555
Validation Loss: 0.00057290
Epoch [73/200], Train Loss: 0.000569
Validation Loss: 0.00057254
Epoch [74/200], Train Loss: 0.000553
Validation Loss: 0.00057455
Epoch [75/200], Train Loss: 0.000544
Validation Loss: 0.00056004
Epoch [76/200], Train Loss: 0.000540
Validation Loss: 0.00054433
Epoch [77/200], Train Loss: 0.000536
Validation Loss: 0.00055261
Epoch [78/200], Train Loss: 0.000551
Validation Loss: 0.00054142
Epoch [79/200], Train Loss: 0.000529
Validation Loss: 0.00056174
Epoch [80/200], Train Loss: 0.000534
Validation Loss: 0.00052913
Epoch [81/200], Train Loss: 0.000522
Validation Loss: 0.00052575
Epoch [82/200], Train Loss: 0.000527
Validation Loss: 0.00053232
Epoch [83/200], Train Loss: 0.000516
Validation Loss: 0.00054359
Epoch [84/200], Train Loss: 0.000536
Validation Loss: 0.00052639
Epoch [85/200], Train Loss: 0.000508
Validation Loss: 0.00051437
Epoch [86/200], Train Loss: 0.000511
Validation Loss: 0.00053409
Epoch [87/200], Train Loss: 0.000501
Validation Loss: 0.00051074
Epoch [88/200], Train Loss: 0.000502
Validation Loss: 0.00050508
Epoch [89/200], Train Loss: 0.000497
Validation Loss: 0.00053239
Epoch [90/200], Train Loss: 0.000501
Validation Loss: 0.00050057
Epoch [91/200], Train Loss: 0.000493
Validation Loss: 0.00050050
Epoch [92/200], Train Loss: 0.000485
Validation Loss: 0.00049136
Epoch [93/200], Train Loss: 0.000479
Validation Loss: 0.00049204
Epoch [94/200], Train Loss: 0.000501
Validation Loss: 0.00069937
Epoch [95/200], Train Loss: 0.000508
Validation Loss: 0.00049234
Epoch [96/200], Train Loss: 0.000474
Validation Loss: 0.00049030
Epoch [97/200], Train Loss: 0.000473
Validation Loss: 0.00049052
Epoch [98/200], Train Loss: 0.000514
Validation Loss: 0.00051368
Epoch [99/200], Train Loss: 0.000480
Validation Loss: 0.00048452
Epoch [100/200], Train Loss: 0.000470
Validation Loss: 0.00048300
Epoch [101/200], Train Loss: 0.000463
Validation Loss: 0.00047602
Epoch [102/200], Train Loss: 0.000462
Validation Loss: 0.00047290
Epoch [103/200], Train Loss: 0.000461
Validation Loss: 0.00047699
Epoch [104/200], Train Loss: 0.000460
Validation Loss: 0.00046846
Epoch [105/200], Train Loss: 0.000460
Validation Loss: 0.00046138
Epoch [106/200], Train Loss: 0.000457
Validation Loss: 0.00046836
Epoch [107/200], Train Loss: 0.000450
Validation Loss: 0.00046175
Epoch [108/200], Train Loss: 0.000448
Validation Loss: 0.00045870
Epoch [109/200], Train Loss: 0.000449
Validation Loss: 0.00045958
Epoch [110/200], Train Loss: 0.000524
Validation Loss: 0.00056200
Epoch [111/200], Train Loss: 0.000497
Validation Loss: 0.00047634
Epoch [112/200], Train Loss: 0.000454
Validation Loss: 0.00044764
Epoch [113/200], Train Loss: 0.000443
Validation Loss: 0.00045038
Epoch [114/200], Train Loss: 0.000442
Validation Loss: 0.00044800
Epoch [115/200], Train Loss: 0.000440
Validation Loss: 0.00045385
Epoch [116/200], Train Loss: 0.000435
Validation Loss: 0.00044611
Epoch [117/200], Train Loss: 0.000435
Validation Loss: 0.00047111
Epoch [118/200], Train Loss: 0.000437
Validation Loss: 0.00043725
Epoch [119/200], Train Loss: 0.000429
Validation Loss: 0.00043560
Epoch [120/200], Train Loss: 0.000430
Validation Loss: 0.00043236
Epoch [121/200], Train Loss: 0.000435
Validation Loss: 0.00043525
Epoch [122/200], Train Loss: 0.000423
Validation Loss: 0.00042573
Epoch [123/200], Train Loss: 0.000428
Validation Loss: 0.00044406
Epoch [124/200], Train Loss: 0.000426
Validation Loss: 0.00042561
Epoch [125/200], Train Loss: 0.000421
Validation Loss: 0.00042191
Epoch [126/200], Train Loss: 0.000420
Validation Loss: 0.00042324
Epoch [127/200], Train Loss: 0.000413
Validation Loss: 0.00042874
Epoch [128/200], Train Loss: 0.000415
Validation Loss: 0.00042361
Epoch [129/200], Train Loss: 0.000417
Validation Loss: 0.00040920
Epoch [130/200], Train Loss: 0.000422
Validation Loss: 0.00042116
Epoch [131/200], Train Loss: 0.000409
Validation Loss: 0.00040587
Epoch [132/200], Train Loss: 0.000409
Validation Loss: 0.00041491
Epoch [133/200], Train Loss: 0.000409
Validation Loss: 0.00043889
Epoch [134/200], Train Loss: 0.000410
Validation Loss: 0.00040401
Epoch [135/200], Train Loss: 0.000402
Validation Loss: 0.00040727
Epoch [136/200], Train Loss: 0.000405
Validation Loss: 0.00040220
Epoch [137/200], Train Loss: 0.000397
Validation Loss: 0.00039669
Epoch [138/200], Train Loss: 0.000400
Validation Loss: 0.00040065
Epoch [139/200], Train Loss: 0.000402
Validation Loss: 0.00039267
Epoch [140/200], Train Loss: 0.000395
Validation Loss: 0.00039912
Epoch [141/200], Train Loss: 0.000408
Validation Loss: 0.00039876
Epoch [142/200], Train Loss: 0.000398
Validation Loss: 0.00039167
Epoch [143/200], Train Loss: 0.000390
Validation Loss: 0.00039109
Epoch [144/200], Train Loss: 0.000393
Validation Loss: 0.00039182
Epoch [145/200], Train Loss: 0.000388
Validation Loss: 0.00038218
Epoch [146/200], Train Loss: 0.000386
Validation Loss: 0.00038353
Epoch [147/200], Train Loss: 0.000391
Validation Loss: 0.00047950
Epoch [148/200], Train Loss: 0.000401
Validation Loss: 0.00037868
Epoch [149/200], Train Loss: 0.000382
Validation Loss: 0.00039015
Epoch [150/200], Train Loss: 0.000383
Validation Loss: 0.00037463
Epoch [151/200], Train Loss: 0.000375
Validation Loss: 0.00038283
Epoch [152/200], Train Loss: 0.000389
Validation Loss: 0.00037633
Epoch [153/200], Train Loss: 0.000375
Validation Loss: 0.00037089
Epoch [154/200], Train Loss: 0.000372
Validation Loss: 0.00040504
Epoch [155/200], Train Loss: 0.000375
Validation Loss: 0.00042498
Epoch [156/200], Train Loss: 0.000382
Validation Loss: 0.00038334
Epoch [157/200], Train Loss: 0.000378
Validation Loss: 0.00036612
Epoch [158/200], Train Loss: 0.000370
Validation Loss: 0.00041514
Epoch [159/200], Train Loss: 0.000375
Validation Loss: 0.00037090
Epoch [160/200], Train Loss: 0.000371
Validation Loss: 0.00036302
Epoch [161/200], Train Loss: 0.000371
Validation Loss: 0.00036850
Epoch [162/200], Train Loss: 0.000369
Validation Loss: 0.00036250
Epoch [163/200], Train Loss: 0.000365
Validation Loss: 0.00035741
Epoch [164/200], Train Loss: 0.000364
Validation Loss: 0.00035835
Epoch [165/200], Train Loss: 0.000365
Validation Loss: 0.00036024
Epoch [166/200], Train Loss: 0.000360
Validation Loss: 0.00036691
Epoch [167/200], Train Loss: 0.000364
Validation Loss: 0.00035602
Epoch [168/200], Train Loss: 0.000360
Validation Loss: 0.00035273
Epoch [169/200], Train Loss: 0.000360
Validation Loss: 0.00035293
Epoch [170/200], Train Loss: 0.000358
Validation Loss: 0.00035837
Epoch [171/200], Train Loss: 0.000359
Validation Loss: 0.00035292
Epoch [172/200], Train Loss: 0.000360
Validation Loss: 0.00034982
Epoch [173/200], Train Loss: 0.000353
Validation Loss: 0.00035405
Epoch [174/200], Train Loss: 0.000359
Validation Loss: 0.00034824
Epoch [175/200], Train Loss: 0.000354
Validation Loss: 0.00035003
Epoch [176/200], Train Loss: 0.000352
Validation Loss: 0.00034610
Epoch [177/200], Train Loss: 0.000353
Validation Loss: 0.00035271
Epoch [178/200], Train Loss: 0.000360
Validation Loss: 0.00035083
Epoch [179/200], Train Loss: 0.000352
Validation Loss: 0.00034159
Epoch [180/200], Train Loss: 0.000347
Validation Loss: 0.00034255
Epoch [181/200], Train Loss: 0.000348
Validation Loss: 0.00033933
Epoch [182/200], Train Loss: 0.000345
Validation Loss: 0.00037629
Epoch [183/200], Train Loss: 0.000356
Validation Loss: 0.00034598
Epoch [184/200], Train Loss: 0.000347
Validation Loss: 0.00033750
Epoch [185/200], Train Loss: 0.000342
Validation Loss: 0.00034699
Epoch [186/200], Train Loss: 0.000345
Validation Loss: 0.00033463
Epoch [187/200], Train Loss: 0.000342
Validation Loss: 0.00033764
Epoch [188/200], Train Loss: 0.000340
Validation Loss: 0.00033193
Epoch [189/200], Train Loss: 0.000339
Validation Loss: 0.00033626
Epoch [190/200], Train Loss: 0.000339
Validation Loss: 0.00033409
Epoch [191/200], Train Loss: 0.000336
Validation Loss: 0.00032604
Epoch [192/200], Train Loss: 0.000343
Validation Loss: 0.00033490
Epoch [193/200], Train Loss: 0.000339
Validation Loss: 0.00033671
Epoch [194/200], Train Loss: 0.000336
Validation Loss: 0.00033278
Epoch [195/200], Train Loss: 0.000337
Validation Loss: 0.00032377
Epoch [196/200], Train Loss: 0.000337
Validation Loss: 0.00032821
Epoch [197/200], Train Loss: 0.000333
Validation Loss: 0.00032615
Epoch [198/200], Train Loss: 0.000335
Validation Loss: 0.00032296
Epoch [199/200], Train Loss: 0.000332
Validation Loss: 0.00033185
Epoch [200/200], Train Loss: 0.000332
Validation Loss: 0.00032981

Evaluating model for: Lamp
Run 49/144 completed in 4943.59 seconds with: {'MAE': np.float32(0.3873716), 'MSE': np.float32(12.044583), 'RMSE': np.float32(3.4705307), 'SAE': np.float32(0.0127796745), 'NDE': np.float32(0.26654842)}

Run 50/144: hidden=128, seq_len=120, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004932
Validation Loss: 0.00478831
Epoch [2/200], Train Loss: 0.004750
Validation Loss: 0.00474664
Epoch [3/200], Train Loss: 0.004735
Validation Loss: 0.00474555
Epoch [4/200], Train Loss: 0.004731
Validation Loss: 0.00473551
Epoch [5/200], Train Loss: 0.004723
Validation Loss: 0.00472885
Epoch [6/200], Train Loss: 0.004719
Validation Loss: 0.00472530
Epoch [7/200], Train Loss: 0.004717
Validation Loss: 0.00472189
Epoch [8/200], Train Loss: 0.004715
Validation Loss: 0.00471680
Epoch [9/200], Train Loss: 0.004709
Validation Loss: 0.00472164
Epoch [10/200], Train Loss: 0.004707
Validation Loss: 0.00471117
Epoch [11/200], Train Loss: 0.004701
Validation Loss: 0.00471973
Epoch [12/200], Train Loss: 0.004706
Validation Loss: 0.00471441
Epoch [13/200], Train Loss: 0.004701
Validation Loss: 0.00471454
Epoch [14/200], Train Loss: 0.004703
Validation Loss: 0.00471226
Epoch [15/200], Train Loss: 0.004702
Validation Loss: 0.00470225
Epoch [16/200], Train Loss: 0.004689
Validation Loss: 0.00469206
Epoch [17/200], Train Loss: 0.004668
Validation Loss: 0.00466140
Epoch [18/200], Train Loss: 0.004649
Validation Loss: 0.00463900
Epoch [19/200], Train Loss: 0.004617
Validation Loss: 0.00457294
Epoch [20/200], Train Loss: 0.004533
Validation Loss: 0.00440019
Epoch [21/200], Train Loss: 0.004351
Validation Loss: 0.00418403
Epoch [22/200], Train Loss: 0.004075
Validation Loss: 0.00384731
Epoch [23/200], Train Loss: 0.003590
Validation Loss: 0.00305615
Epoch [24/200], Train Loss: 0.002990
Validation Loss: 0.00250278
Epoch [25/200], Train Loss: 0.002539
Validation Loss: 0.00217425
Epoch [26/200], Train Loss: 0.002275
Validation Loss: 0.00201144
Epoch [27/200], Train Loss: 0.002071
Validation Loss: 0.00186445
Epoch [28/200], Train Loss: 0.001907
Validation Loss: 0.00175953
Epoch [29/200], Train Loss: 0.001765
Validation Loss: 0.00163908
Epoch [30/200], Train Loss: 0.001663
Validation Loss: 0.00153166
Epoch [31/200], Train Loss: 0.001557
Validation Loss: 0.00145597
Epoch [32/200], Train Loss: 0.001469
Validation Loss: 0.00138507
Epoch [33/200], Train Loss: 0.001400
Validation Loss: 0.00136813
Epoch [34/200], Train Loss: 0.001331
Validation Loss: 0.00130602
Epoch [35/200], Train Loss: 0.001282
Validation Loss: 0.00124607
Epoch [36/200], Train Loss: 0.001242
Validation Loss: 0.00121980
Epoch [37/200], Train Loss: 0.001198
Validation Loss: 0.00118655
Epoch [38/200], Train Loss: 0.001160
Validation Loss: 0.00116139
Epoch [39/200], Train Loss: 0.001148
Validation Loss: 0.00113662
Epoch [40/200], Train Loss: 0.001122
Validation Loss: 0.00113776
Epoch [41/200], Train Loss: 0.001086
Validation Loss: 0.00109514
Epoch [42/200], Train Loss: 0.001057
Validation Loss: 0.00105485
Epoch [43/200], Train Loss: 0.001051
Validation Loss: 0.00104552
Epoch [44/200], Train Loss: 0.001005
Validation Loss: 0.00101011
Epoch [45/200], Train Loss: 0.000986
Validation Loss: 0.00097721
Epoch [46/200], Train Loss: 0.000955
Validation Loss: 0.00097197
Epoch [47/200], Train Loss: 0.000934
Validation Loss: 0.00092414
Epoch [48/200], Train Loss: 0.000937
Validation Loss: 0.00094491
Epoch [49/200], Train Loss: 0.000886
Validation Loss: 0.00087113
Epoch [50/200], Train Loss: 0.000894
Validation Loss: 0.00084563
Epoch [51/200], Train Loss: 0.000860
Validation Loss: 0.00081371
Epoch [52/200], Train Loss: 0.000805
Validation Loss: 0.00077767
Epoch [53/200], Train Loss: 0.000788
Validation Loss: 0.00081916
Epoch [54/200], Train Loss: 0.000763
Validation Loss: 0.00073617
Epoch [55/200], Train Loss: 0.000744
Validation Loss: 0.00071460
Epoch [56/200], Train Loss: 0.000734
Validation Loss: 0.00069423
Epoch [57/200], Train Loss: 0.000713
Validation Loss: 0.00071377
Epoch [58/200], Train Loss: 0.000701
Validation Loss: 0.00068489
Epoch [59/200], Train Loss: 0.000700
Validation Loss: 0.00068423
Epoch [60/200], Train Loss: 0.000690
Validation Loss: 0.00067581
Epoch [61/200], Train Loss: 0.000665
Validation Loss: 0.00064106
Epoch [62/200], Train Loss: 0.000680
Validation Loss: 0.00066255
Epoch [63/200], Train Loss: 0.000648
Validation Loss: 0.00062540
Epoch [64/200], Train Loss: 0.000639
Validation Loss: 0.00063756
Epoch [65/200], Train Loss: 0.000631
Validation Loss: 0.00061278
Epoch [66/200], Train Loss: 0.000617
Validation Loss: 0.00060413
Epoch [67/200], Train Loss: 0.000617
Validation Loss: 0.00059037
Epoch [68/200], Train Loss: 0.000609
Validation Loss: 0.00061721
Epoch [69/200], Train Loss: 0.000600
Validation Loss: 0.00062410
Epoch [70/200], Train Loss: 0.000602
Validation Loss: 0.00058846
Epoch [71/200], Train Loss: 0.000584
Validation Loss: 0.00056240
Epoch [72/200], Train Loss: 0.000575
Validation Loss: 0.00056407
Epoch [73/200], Train Loss: 0.000570
Validation Loss: 0.00056345
Epoch [74/200], Train Loss: 0.000569
Validation Loss: 0.00075602
Epoch [75/200], Train Loss: 0.000573
Validation Loss: 0.00055520
Epoch [76/200], Train Loss: 0.000580
Validation Loss: 0.00064586
Epoch [77/200], Train Loss: 0.000554
Validation Loss: 0.00055080
Epoch [78/200], Train Loss: 0.000544
Validation Loss: 0.00052400
Epoch [79/200], Train Loss: 0.000538
Validation Loss: 0.00052551
Epoch [80/200], Train Loss: 0.000531
Validation Loss: 0.00053829
Epoch [81/200], Train Loss: 0.000527
Validation Loss: 0.00054240
Epoch [82/200], Train Loss: 0.000528
Validation Loss: 0.00052428
Epoch [83/200], Train Loss: 0.000521
Validation Loss: 0.00050812
Epoch [84/200], Train Loss: 0.000515
Validation Loss: 0.00050237
Epoch [85/200], Train Loss: 0.000514
Validation Loss: 0.00051355
Epoch [86/200], Train Loss: 0.000507
Validation Loss: 0.00050402
Epoch [87/200], Train Loss: 0.000508
Validation Loss: 0.00060262
Epoch [88/200], Train Loss: 0.000506
Validation Loss: 0.00049135
Epoch [89/200], Train Loss: 0.000497
Validation Loss: 0.00047969
Epoch [90/200], Train Loss: 0.000489
Validation Loss: 0.00050328
Epoch [91/200], Train Loss: 0.000489
Validation Loss: 0.00048364
Epoch [92/200], Train Loss: 0.000485
Validation Loss: 0.00047311
Epoch [93/200], Train Loss: 0.000482
Validation Loss: 0.00047296
Epoch [94/200], Train Loss: 0.000480
Validation Loss: 0.00047322
Epoch [95/200], Train Loss: 0.000486
Validation Loss: 0.00049930
Epoch [96/200], Train Loss: 0.000477
Validation Loss: 0.00048243
Epoch [97/200], Train Loss: 0.000467
Validation Loss: 0.00046680
Epoch [98/200], Train Loss: 0.000462
Validation Loss: 0.00047158
Epoch [99/200], Train Loss: 0.000464
Validation Loss: 0.00046078
Epoch [100/200], Train Loss: 0.000460
Validation Loss: 0.00045032
Epoch [101/200], Train Loss: 0.000459
Validation Loss: 0.00047718
Epoch [102/200], Train Loss: 0.000460
Validation Loss: 0.00044430
Epoch [103/200], Train Loss: 0.000451
Validation Loss: 0.00045075
Epoch [104/200], Train Loss: 0.000450
Validation Loss: 0.00046891
Epoch [105/200], Train Loss: 0.000447
Validation Loss: 0.00043573
Epoch [106/200], Train Loss: 0.000444
Validation Loss: 0.00045308
Epoch [107/200], Train Loss: 0.000442
Validation Loss: 0.00043136
Epoch [108/200], Train Loss: 0.000443
Validation Loss: 0.00043397
Epoch [109/200], Train Loss: 0.000441
Validation Loss: 0.00043361
Epoch [110/200], Train Loss: 0.000445
Validation Loss: 0.00042348
Epoch [111/200], Train Loss: 0.000467
Validation Loss: 0.00045424
Epoch [112/200], Train Loss: 0.000428
Validation Loss: 0.00041655
Epoch [113/200], Train Loss: 0.000427
Validation Loss: 0.00041443
Epoch [114/200], Train Loss: 0.000422
Validation Loss: 0.00041809
Epoch [115/200], Train Loss: 0.000420
Validation Loss: 0.00041903
Epoch [116/200], Train Loss: 0.000421
Validation Loss: 0.00040753
Epoch [117/200], Train Loss: 0.000411
Validation Loss: 0.00040710
Epoch [118/200], Train Loss: 0.000420
Validation Loss: 0.00041009
Epoch [119/200], Train Loss: 0.000416
Validation Loss: 0.00039853
Epoch [120/200], Train Loss: 0.000417
Validation Loss: 0.00043230
Epoch [121/200], Train Loss: 0.000409
Validation Loss: 0.00039433
Epoch [122/200], Train Loss: 0.000412
Validation Loss: 0.00039308
Epoch [123/200], Train Loss: 0.000404
Validation Loss: 0.00038987
Epoch [124/200], Train Loss: 0.000414
Validation Loss: 0.00041128
Epoch [125/200], Train Loss: 0.000402
Validation Loss: 0.00039336
Epoch [126/200], Train Loss: 0.000395
Validation Loss: 0.00040394
Epoch [127/200], Train Loss: 0.000403
Validation Loss: 0.00038524
Epoch [128/200], Train Loss: 0.000405
Validation Loss: 0.00039832
Epoch [129/200], Train Loss: 0.000392
Validation Loss: 0.00039178
Epoch [130/200], Train Loss: 0.000386
Validation Loss: 0.00037862
Epoch [131/200], Train Loss: 0.000394
Validation Loss: 0.00037649
Epoch [132/200], Train Loss: 0.000388
Validation Loss: 0.00038108
Epoch [133/200], Train Loss: 0.000385
Validation Loss: 0.00038137
Epoch [134/200], Train Loss: 0.000381
Validation Loss: 0.00037323
Epoch [135/200], Train Loss: 0.000389
Validation Loss: 0.00039839
Epoch [136/200], Train Loss: 0.000384
Validation Loss: 0.00036048
Epoch [137/200], Train Loss: 0.000380
Validation Loss: 0.00036642
Epoch [138/200], Train Loss: 0.000382
Validation Loss: 0.00035848
Epoch [139/200], Train Loss: 0.000388
Validation Loss: 0.00039973
Epoch [140/200], Train Loss: 0.000380
Validation Loss: 0.00035394
Epoch [141/200], Train Loss: 0.000371
Validation Loss: 0.00035363
Epoch [142/200], Train Loss: 0.000369
Validation Loss: 0.00035553
Epoch [143/200], Train Loss: 0.000372
Validation Loss: 0.00036285
Epoch [144/200], Train Loss: 0.000365
Validation Loss: 0.00036171
Epoch [145/200], Train Loss: 0.000362
Validation Loss: 0.00034140
Epoch [146/200], Train Loss: 0.000362
Validation Loss: 0.00035410
Epoch [147/200], Train Loss: 0.000361
Validation Loss: 0.00039385
Epoch [148/200], Train Loss: 0.000364
Validation Loss: 0.00038996
Epoch [149/200], Train Loss: 0.000358
Validation Loss: 0.00034377
Epoch [150/200], Train Loss: 0.000353
Validation Loss: 0.00034627
Epoch [151/200], Train Loss: 0.000361
Validation Loss: 0.00033819
Epoch [152/200], Train Loss: 0.000357
Validation Loss: 0.00034359
Epoch [153/200], Train Loss: 0.000352
Validation Loss: 0.00032859
Epoch [154/200], Train Loss: 0.000348
Validation Loss: 0.00033845
Epoch [155/200], Train Loss: 0.000346
Validation Loss: 0.00034609
Epoch [156/200], Train Loss: 0.000348
Validation Loss: 0.00032862
Epoch [157/200], Train Loss: 0.000346
Validation Loss: 0.00032723
Epoch [158/200], Train Loss: 0.000346
Validation Loss: 0.00032386
Epoch [159/200], Train Loss: 0.000355
Validation Loss: 0.00031831
Epoch [160/200], Train Loss: 0.000348
Validation Loss: 0.00035612
Epoch [161/200], Train Loss: 0.000347
Validation Loss: 0.00031995
Epoch [162/200], Train Loss: 0.000346
Validation Loss: 0.00034201
Epoch [163/200], Train Loss: 0.000339
Validation Loss: 0.00031702
Epoch [164/200], Train Loss: 0.000339
Validation Loss: 0.00032049
Epoch [165/200], Train Loss: 0.000340
Validation Loss: 0.00034648
Epoch [166/200], Train Loss: 0.000343
Validation Loss: 0.00031338
Epoch [167/200], Train Loss: 0.000330
Validation Loss: 0.00031848
Epoch [168/200], Train Loss: 0.000333
Validation Loss: 0.00032671
Epoch [169/200], Train Loss: 0.000332
Validation Loss: 0.00031345
Epoch [170/200], Train Loss: 0.000328
Validation Loss: 0.00032252
Epoch [171/200], Train Loss: 0.000330
Validation Loss: 0.00032642
Epoch [172/200], Train Loss: 0.000329
Validation Loss: 0.00030674
Epoch [173/200], Train Loss: 0.000327
Validation Loss: 0.00030357
Epoch [174/200], Train Loss: 0.000332
Validation Loss: 0.00031610
Epoch [175/200], Train Loss: 0.000326
Validation Loss: 0.00030750
Epoch [176/200], Train Loss: 0.000332
Validation Loss: 0.00030993
Epoch [177/200], Train Loss: 0.000321
Validation Loss: 0.00030643
Epoch [178/200], Train Loss: 0.000321
Validation Loss: 0.00031974
Epoch [179/200], Train Loss: 0.000323
Validation Loss: 0.00029978
Epoch [180/200], Train Loss: 0.000316
Validation Loss: 0.00030802
Epoch [181/200], Train Loss: 0.000319
Validation Loss: 0.00031895
Epoch [182/200], Train Loss: 0.000318
Validation Loss: 0.00030296
Epoch [183/200], Train Loss: 0.000314
Validation Loss: 0.00030026
Epoch [184/200], Train Loss: 0.000314
Validation Loss: 0.00029636
Epoch [185/200], Train Loss: 0.000319
Validation Loss: 0.00032145
Epoch [186/200], Train Loss: 0.000313
Validation Loss: 0.00029660
Epoch [187/200], Train Loss: 0.000320
Validation Loss: 0.00029748
Epoch [188/200], Train Loss: 0.000312
Validation Loss: 0.00029270
Epoch [189/200], Train Loss: 0.000311
Validation Loss: 0.00029156
Epoch [190/200], Train Loss: 0.000311
Validation Loss: 0.00029330
Epoch [191/200], Train Loss: 0.000308
Validation Loss: 0.00030745
Epoch [192/200], Train Loss: 0.000313
Validation Loss: 0.00028680
Epoch [193/200], Train Loss: 0.000306
Validation Loss: 0.00029580
Epoch [194/200], Train Loss: 0.000306
Validation Loss: 0.00028732
Epoch [195/200], Train Loss: 0.000310
Validation Loss: 0.00029412
Epoch [196/200], Train Loss: 0.000302
Validation Loss: 0.00029046
Epoch [197/200], Train Loss: 0.000306
Validation Loss: 0.00028623
Epoch [198/200], Train Loss: 0.000305
Validation Loss: 0.00031145
Epoch [199/200], Train Loss: 0.000304
Validation Loss: 0.00028721
Epoch [200/200], Train Loss: 0.000304
Validation Loss: 0.00028640

Evaluating model for: Lamp
Run 50/144 completed in 4989.79 seconds with: {'MAE': np.float32(0.3397539), 'MSE': np.float32(10.750474), 'RMSE': np.float32(3.2787914), 'SAE': np.float32(0.007181001), 'NDE': np.float32(0.2518224)}

Run 51/144: hidden=128, seq_len=120, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004897
Validation Loss: 0.00480892
Epoch [2/200], Train Loss: 0.004740
Validation Loss: 0.00473748
Epoch [3/200], Train Loss: 0.004732
Validation Loss: 0.00473364
Epoch [4/200], Train Loss: 0.004718
Validation Loss: 0.00472370
Epoch [5/200], Train Loss: 0.004716
Validation Loss: 0.00471779
Epoch [6/200], Train Loss: 0.004711
Validation Loss: 0.00472434
Epoch [7/200], Train Loss: 0.004707
Validation Loss: 0.00471686
Epoch [8/200], Train Loss: 0.004706
Validation Loss: 0.00472087
Epoch [9/200], Train Loss: 0.004703
Validation Loss: 0.00471311
Epoch [10/200], Train Loss: 0.004704
Validation Loss: 0.00471133
Epoch [11/200], Train Loss: 0.004699
Validation Loss: 0.00470928
Epoch [12/200], Train Loss: 0.004703
Validation Loss: 0.00471839
Epoch [13/200], Train Loss: 0.004702
Validation Loss: 0.00471846
Epoch [14/200], Train Loss: 0.004702
Validation Loss: 0.00471150
Epoch [15/200], Train Loss: 0.004697
Validation Loss: 0.00471118
Epoch [16/200], Train Loss: 0.004700
Validation Loss: 0.00470784
Epoch [17/200], Train Loss: 0.004695
Validation Loss: 0.00470343
Epoch [18/200], Train Loss: 0.004694
Validation Loss: 0.00470200
Epoch [19/200], Train Loss: 0.004698
Validation Loss: 0.00469507
Epoch [20/200], Train Loss: 0.004689
Validation Loss: 0.00469280
Epoch [21/200], Train Loss: 0.004683
Validation Loss: 0.00469369
Epoch [22/200], Train Loss: 0.004672
Validation Loss: 0.00466681
Epoch [23/200], Train Loss: 0.004657
Validation Loss: 0.00465236
Epoch [24/200], Train Loss: 0.004611
Validation Loss: 0.00454407
Epoch [25/200], Train Loss: 0.004517
Validation Loss: 0.00432563
Epoch [26/200], Train Loss: 0.003972
Validation Loss: 0.00315998
Epoch [27/200], Train Loss: 0.002773
Validation Loss: 0.00230481
Epoch [28/200], Train Loss: 0.002177
Validation Loss: 0.00189278
Epoch [29/200], Train Loss: 0.001867
Validation Loss: 0.00164424
Epoch [30/200], Train Loss: 0.001677
Validation Loss: 0.00150592
Epoch [31/200], Train Loss: 0.001499
Validation Loss: 0.00145346
Epoch [32/200], Train Loss: 0.001414
Validation Loss: 0.00131979
Epoch [33/200], Train Loss: 0.001317
Validation Loss: 0.00125685
Epoch [34/200], Train Loss: 0.001235
Validation Loss: 0.00117024
Epoch [35/200], Train Loss: 0.001176
Validation Loss: 0.00109006
Epoch [36/200], Train Loss: 0.001102
Validation Loss: 0.00107184
Epoch [37/200], Train Loss: 0.001105
Validation Loss: 0.00097043
Epoch [38/200], Train Loss: 0.000980
Validation Loss: 0.00096694
Epoch [39/200], Train Loss: 0.000943
Validation Loss: 0.00085804
Epoch [40/200], Train Loss: 0.000889
Validation Loss: 0.00083535
Epoch [41/200], Train Loss: 0.000843
Validation Loss: 0.00078720
Epoch [42/200], Train Loss: 0.000815
Validation Loss: 0.00077012
Epoch [43/200], Train Loss: 0.000781
Validation Loss: 0.00073933
Epoch [44/200], Train Loss: 0.000770
Validation Loss: 0.00071091
Epoch [45/200], Train Loss: 0.000735
Validation Loss: 0.00068311
Epoch [46/200], Train Loss: 0.000712
Validation Loss: 0.00067620
Epoch [47/200], Train Loss: 0.000697
Validation Loss: 0.00066105
Epoch [48/200], Train Loss: 0.000693
Validation Loss: 0.00066162
Epoch [49/200], Train Loss: 0.000679
Validation Loss: 0.00064055
Epoch [50/200], Train Loss: 0.000656
Validation Loss: 0.00062213
Epoch [51/200], Train Loss: 0.000641
Validation Loss: 0.00063684
Epoch [52/200], Train Loss: 0.000635
Validation Loss: 0.00059582
Epoch [53/200], Train Loss: 0.000629
Validation Loss: 0.00070192
Epoch [54/200], Train Loss: 0.000635
Validation Loss: 0.00059833
Epoch [55/200], Train Loss: 0.000662
Validation Loss: 0.00061808
Epoch [56/200], Train Loss: 0.000612
Validation Loss: 0.00058814
Epoch [57/200], Train Loss: 0.000607
Validation Loss: 0.00056880
Epoch [58/200], Train Loss: 0.000585
Validation Loss: 0.00056271
Epoch [59/200], Train Loss: 0.000571
Validation Loss: 0.00056635
Epoch [60/200], Train Loss: 0.000567
Validation Loss: 0.00055360
Epoch [61/200], Train Loss: 0.000559
Validation Loss: 0.00054975
Epoch [62/200], Train Loss: 0.000548
Validation Loss: 0.00054981
Epoch [63/200], Train Loss: 0.000544
Validation Loss: 0.00053986
Epoch [64/200], Train Loss: 0.000536
Validation Loss: 0.00052819
Epoch [65/200], Train Loss: 0.000566
Validation Loss: 0.00054498
Epoch [66/200], Train Loss: 0.000529
Validation Loss: 0.00053365
Epoch [67/200], Train Loss: 0.000522
Validation Loss: 0.00052648
Epoch [68/200], Train Loss: 0.000513
Validation Loss: 0.00051035
Epoch [69/200], Train Loss: 0.000518
Validation Loss: 0.00051436
Epoch [70/200], Train Loss: 0.000514
Validation Loss: 0.00052461
Epoch [71/200], Train Loss: 0.000512
Validation Loss: 0.00050383
Epoch [72/200], Train Loss: 0.000507
Validation Loss: 0.00049912
Epoch [73/200], Train Loss: 0.000495
Validation Loss: 0.00051112
Epoch [74/200], Train Loss: 0.000490
Validation Loss: 0.00052296
Epoch [75/200], Train Loss: 0.000490
Validation Loss: 0.00047973
Epoch [76/200], Train Loss: 0.000480
Validation Loss: 0.00047446
Epoch [77/200], Train Loss: 0.000477
Validation Loss: 0.00047609
Epoch [78/200], Train Loss: 0.000477
Validation Loss: 0.00046760
Epoch [79/200], Train Loss: 0.000485
Validation Loss: 0.00048308
Epoch [80/200], Train Loss: 0.000471
Validation Loss: 0.00046444
Epoch [81/200], Train Loss: 0.000466
Validation Loss: 0.00048384
Epoch [82/200], Train Loss: 0.000458
Validation Loss: 0.00046064
Epoch [83/200], Train Loss: 0.000454
Validation Loss: 0.00046816
Epoch [84/200], Train Loss: 0.000450
Validation Loss: 0.00044198
Epoch [85/200], Train Loss: 0.000450
Validation Loss: 0.00045516
Epoch [86/200], Train Loss: 0.000445
Validation Loss: 0.00048444
Epoch [87/200], Train Loss: 0.000459
Validation Loss: 0.00047708
Epoch [88/200], Train Loss: 0.000455
Validation Loss: 0.00046848
Epoch [89/200], Train Loss: 0.000441
Validation Loss: 0.00042678
Epoch [90/200], Train Loss: 0.000437
Validation Loss: 0.00044757
Epoch [91/200], Train Loss: 0.000429
Validation Loss: 0.00045042
Epoch [92/200], Train Loss: 0.000447
Validation Loss: 0.00045941
Epoch [93/200], Train Loss: 0.000432
Validation Loss: 0.00042528
Epoch [94/200], Train Loss: 0.000428
Validation Loss: 0.00043388
Epoch [95/200], Train Loss: 0.000430
Validation Loss: 0.00043344
Epoch [96/200], Train Loss: 0.000430
Validation Loss: 0.00041976
Epoch [97/200], Train Loss: 0.000420
Validation Loss: 0.00042199
Epoch [98/200], Train Loss: 0.000422
Validation Loss: 0.00041358
Epoch [99/200], Train Loss: 0.000410
Validation Loss: 0.00043585
Epoch [100/200], Train Loss: 0.000406
Validation Loss: 0.00043565
Epoch [101/200], Train Loss: 0.000418
Validation Loss: 0.00041552
Epoch [102/200], Train Loss: 0.000406
Validation Loss: 0.00040501
Epoch [103/200], Train Loss: 0.000401
Validation Loss: 0.00040784
Epoch [104/200], Train Loss: 0.000420
Validation Loss: 0.00044880
Epoch [105/200], Train Loss: 0.000418
Validation Loss: 0.00040935
Epoch [106/200], Train Loss: 0.000401
Validation Loss: 0.00039360
Epoch [107/200], Train Loss: 0.000392
Validation Loss: 0.00039007
Epoch [108/200], Train Loss: 0.000393
Validation Loss: 0.00038724
Epoch [109/200], Train Loss: 0.000392
Validation Loss: 0.00039115
Epoch [110/200], Train Loss: 0.000387
Validation Loss: 0.00039058
Epoch [111/200], Train Loss: 0.000385
Validation Loss: 0.00043222
Epoch [112/200], Train Loss: 0.000391
Validation Loss: 0.00037741
Epoch [113/200], Train Loss: 0.000379
Validation Loss: 0.00037619
Epoch [114/200], Train Loss: 0.000375
Validation Loss: 0.00037785
Epoch [115/200], Train Loss: 0.000375
Validation Loss: 0.00037140
Epoch [116/200], Train Loss: 0.000380
Validation Loss: 0.00038243
Epoch [117/200], Train Loss: 0.000378
Validation Loss: 0.00038009
Epoch [118/200], Train Loss: 0.000373
Validation Loss: 0.00038653
Epoch [119/200], Train Loss: 0.000369
Validation Loss: 0.00036542
Epoch [120/200], Train Loss: 0.000367
Validation Loss: 0.00039094
Epoch [121/200], Train Loss: 0.000369
Validation Loss: 0.00035873
Epoch [122/200], Train Loss: 0.000372
Validation Loss: 0.00035663
Epoch [123/200], Train Loss: 0.000361
Validation Loss: 0.00037901
Epoch [124/200], Train Loss: 0.000360
Validation Loss: 0.00035553
Epoch [125/200], Train Loss: 0.000356
Validation Loss: 0.00035786
Epoch [126/200], Train Loss: 0.000355
Validation Loss: 0.00035942
Epoch [127/200], Train Loss: 0.000353
Validation Loss: 0.00035176
Epoch [128/200], Train Loss: 0.000355
Validation Loss: 0.00034805
Epoch [129/200], Train Loss: 0.000363
Validation Loss: 0.00034104
Epoch [130/200], Train Loss: 0.000357
Validation Loss: 0.00033871
Epoch [131/200], Train Loss: 0.000347
Validation Loss: 0.00036315
Epoch [132/200], Train Loss: 0.000355
Validation Loss: 0.00035666
Epoch [133/200], Train Loss: 0.000343
Validation Loss: 0.00033792
Epoch [134/200], Train Loss: 0.000340
Validation Loss: 0.00033238
Epoch [135/200], Train Loss: 0.000344
Validation Loss: 0.00036443
Epoch [136/200], Train Loss: 0.000335
Validation Loss: 0.00033088
Epoch [137/200], Train Loss: 0.000339
Validation Loss: 0.00033051
Epoch [138/200], Train Loss: 0.000332
Validation Loss: 0.00032758
Epoch [139/200], Train Loss: 0.000326
Validation Loss: 0.00033115
Epoch [140/200], Train Loss: 0.000325
Validation Loss: 0.00032943
Epoch [141/200], Train Loss: 0.000326
Validation Loss: 0.00032081
Epoch [142/200], Train Loss: 0.000329
Validation Loss: 0.00032057
Epoch [143/200], Train Loss: 0.000338
Validation Loss: 0.00064158
Epoch [144/200], Train Loss: 0.000333
Validation Loss: 0.00033676
Epoch [145/200], Train Loss: 0.000317
Validation Loss: 0.00030799
Epoch [146/200], Train Loss: 0.000316
Validation Loss: 0.00030036
Epoch [147/200], Train Loss: 0.000311
Validation Loss: 0.00030239
Epoch [148/200], Train Loss: 0.000314
Validation Loss: 0.00029556
Epoch [149/200], Train Loss: 0.000312
Validation Loss: 0.00030573
Epoch [150/200], Train Loss: 0.000314
Validation Loss: 0.00035410
Epoch [151/200], Train Loss: 0.000314
Validation Loss: 0.00029090
Epoch [152/200], Train Loss: 0.000302
Validation Loss: 0.00030337
Epoch [153/200], Train Loss: 0.000300
Validation Loss: 0.00030731
Epoch [154/200], Train Loss: 0.000298
Validation Loss: 0.00028248
Epoch [155/200], Train Loss: 0.000298
Validation Loss: 0.00027960
Epoch [156/200], Train Loss: 0.000295
Validation Loss: 0.00028913
Epoch [157/200], Train Loss: 0.000292
Validation Loss: 0.00026573
Epoch [158/200], Train Loss: 0.000298
Validation Loss: 0.00027773
Epoch [159/200], Train Loss: 0.000292
Validation Loss: 0.00026178
Epoch [160/200], Train Loss: 0.000286
Validation Loss: 0.00027089
Epoch [161/200], Train Loss: 0.000286
Validation Loss: 0.00026243
Epoch [162/200], Train Loss: 0.000283
Validation Loss: 0.00025362
Epoch [163/200], Train Loss: 0.000283
Validation Loss: 0.00025927
Epoch [164/200], Train Loss: 0.000285
Validation Loss: 0.00025456
Epoch [165/200], Train Loss: 0.000280
Validation Loss: 0.00027378
Epoch [166/200], Train Loss: 0.000277
Validation Loss: 0.00024988
Epoch [167/200], Train Loss: 0.000291
Validation Loss: 0.00041124
Epoch [168/200], Train Loss: 0.000384
Validation Loss: 0.00027981
Epoch [169/200], Train Loss: 0.000304
Validation Loss: 0.00025561
Epoch [170/200], Train Loss: 0.000284
Validation Loss: 0.00024612
Epoch [171/200], Train Loss: 0.000276
Validation Loss: 0.00024507
Epoch [172/200], Train Loss: 0.000274
Validation Loss: 0.00023853
Epoch [173/200], Train Loss: 0.000272
Validation Loss: 0.00023516
Epoch [174/200], Train Loss: 0.000272
Validation Loss: 0.00023758
Epoch [175/200], Train Loss: 0.000271
Validation Loss: 0.00023798
Epoch [176/200], Train Loss: 0.000266
Validation Loss: 0.00023485
Epoch [177/200], Train Loss: 0.000266
Validation Loss: 0.00024677
Epoch [178/200], Train Loss: 0.000264
Validation Loss: 0.00023558
Epoch [179/200], Train Loss: 0.000263
Validation Loss: 0.00023339
Epoch [180/200], Train Loss: 0.000266
Validation Loss: 0.00023254
Epoch [181/200], Train Loss: 0.000263
Validation Loss: 0.00022433
Epoch [182/200], Train Loss: 0.000261
Validation Loss: 0.00025450
Epoch [183/200], Train Loss: 0.000263
Validation Loss: 0.00022777
Epoch [184/200], Train Loss: 0.000263
Validation Loss: 0.00023129
Epoch [185/200], Train Loss: 0.000258
Validation Loss: 0.00023405
Epoch [186/200], Train Loss: 0.000261
Validation Loss: 0.00022215
Epoch [187/200], Train Loss: 0.000256
Validation Loss: 0.00024368
Epoch [188/200], Train Loss: 0.000260
Validation Loss: 0.00022299
Epoch [189/200], Train Loss: 0.000259
Validation Loss: 0.00022187
Epoch [190/200], Train Loss: 0.000258
Validation Loss: 0.00022921
Epoch [191/200], Train Loss: 0.000257
Validation Loss: 0.00022489
Epoch [192/200], Train Loss: 0.000264
Validation Loss: 0.00022461
Epoch [193/200], Train Loss: 0.000255
Validation Loss: 0.00022820
Epoch [194/200], Train Loss: 0.000254
Validation Loss: 0.00021861
Epoch [195/200], Train Loss: 0.000251
Validation Loss: 0.00021765
Epoch [196/200], Train Loss: 0.000254
Validation Loss: 0.00022398
Epoch [197/200], Train Loss: 0.000251
Validation Loss: 0.00021680
Epoch [198/200], Train Loss: 0.000253
Validation Loss: 0.00022522
Epoch [199/200], Train Loss: 0.000252
Validation Loss: 0.00021321
Epoch [200/200], Train Loss: 0.000249
Validation Loss: 0.00021402

Evaluating model for: Lamp
Run 51/144 completed in 5066.38 seconds with: {'MAE': np.float32(0.27862546), 'MSE': np.float32(9.202048), 'RMSE': np.float32(3.0334878), 'SAE': np.float32(0.007671089), 'NDE': np.float32(0.23298226)}

Run 52/144: hidden=128, seq_len=120, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004952
Validation Loss: 0.00484392
Epoch [2/200], Train Loss: 0.004781
Validation Loss: 0.00475549
Epoch [3/200], Train Loss: 0.004748
Validation Loss: 0.00473329
Epoch [4/200], Train Loss: 0.004735
Validation Loss: 0.00472492
Epoch [5/200], Train Loss: 0.004725
Validation Loss: 0.00472432
Epoch [6/200], Train Loss: 0.004717
Validation Loss: 0.00472337
Epoch [7/200], Train Loss: 0.004724
Validation Loss: 0.00471942
Epoch [8/200], Train Loss: 0.004716
Validation Loss: 0.00472255
Epoch [9/200], Train Loss: 0.004713
Validation Loss: 0.00471514
Epoch [10/200], Train Loss: 0.004719
Validation Loss: 0.00471526
Epoch [11/200], Train Loss: 0.004708
Validation Loss: 0.00471263
Epoch [12/200], Train Loss: 0.004704
Validation Loss: 0.00471427
Epoch [13/200], Train Loss: 0.004710
Validation Loss: 0.00470978
Epoch [14/200], Train Loss: 0.004710
Validation Loss: 0.00471233
Epoch [15/200], Train Loss: 0.004707
Validation Loss: 0.00471650
Epoch [16/200], Train Loss: 0.004708
Validation Loss: 0.00471095
Epoch [17/200], Train Loss: 0.004703
Validation Loss: 0.00471099
Epoch [18/200], Train Loss: 0.004700
Validation Loss: 0.00470702
Epoch [19/200], Train Loss: 0.004702
Validation Loss: 0.00470786
Epoch [20/200], Train Loss: 0.004699
Validation Loss: 0.00471157
Epoch [21/200], Train Loss: 0.004700
Validation Loss: 0.00470616
Epoch [22/200], Train Loss: 0.004695
Validation Loss: 0.00470493
Epoch [23/200], Train Loss: 0.004694
Validation Loss: 0.00470460
Epoch [24/200], Train Loss: 0.004694
Validation Loss: 0.00470214
Epoch [25/200], Train Loss: 0.004693
Validation Loss: 0.00469747
Epoch [26/200], Train Loss: 0.004690
Validation Loss: 0.00468839
Epoch [27/200], Train Loss: 0.004678
Validation Loss: 0.00467777
Epoch [28/200], Train Loss: 0.004666
Validation Loss: 0.00465780
Epoch [29/200], Train Loss: 0.004630
Validation Loss: 0.00459142
Epoch [30/200], Train Loss: 0.004496
Validation Loss: 0.00428470
Epoch [31/200], Train Loss: 0.003363
Validation Loss: 0.00235828
Epoch [32/200], Train Loss: 0.002277
Validation Loss: 0.00200343
Epoch [33/200], Train Loss: 0.001873
Validation Loss: 0.00169826
Epoch [34/200], Train Loss: 0.001638
Validation Loss: 0.00152128
Epoch [35/200], Train Loss: 0.001494
Validation Loss: 0.00139902
Epoch [36/200], Train Loss: 0.001373
Validation Loss: 0.00131359
Epoch [37/200], Train Loss: 0.001289
Validation Loss: 0.00122906
Epoch [38/200], Train Loss: 0.001188
Validation Loss: 0.00117577
Epoch [39/200], Train Loss: 0.001096
Validation Loss: 0.00109203
Epoch [40/200], Train Loss: 0.001036
Validation Loss: 0.00101980
Epoch [41/200], Train Loss: 0.000964
Validation Loss: 0.00094313
Epoch [42/200], Train Loss: 0.000921
Validation Loss: 0.00089791
Epoch [43/200], Train Loss: 0.000844
Validation Loss: 0.00083310
Epoch [44/200], Train Loss: 0.000823
Validation Loss: 0.00080224
Epoch [45/200], Train Loss: 0.000781
Validation Loss: 0.00076638
Epoch [46/200], Train Loss: 0.000752
Validation Loss: 0.00083274
Epoch [47/200], Train Loss: 0.000778
Validation Loss: 0.00073788
Epoch [48/200], Train Loss: 0.000711
Validation Loss: 0.00071835
Epoch [49/200], Train Loss: 0.000697
Validation Loss: 0.00072208
Epoch [50/200], Train Loss: 0.000680
Validation Loss: 0.00068257
Epoch [51/200], Train Loss: 0.000677
Validation Loss: 0.00068127
Epoch [52/200], Train Loss: 0.000658
Validation Loss: 0.00063772
Epoch [53/200], Train Loss: 0.000635
Validation Loss: 0.00062404
Epoch [54/200], Train Loss: 0.000629
Validation Loss: 0.00061279
Epoch [55/200], Train Loss: 0.000610
Validation Loss: 0.00059717
Epoch [56/200], Train Loss: 0.000608
Validation Loss: 0.00059588
Epoch [57/200], Train Loss: 0.000591
Validation Loss: 0.00059354
Epoch [58/200], Train Loss: 0.000587
Validation Loss: 0.00057913
Epoch [59/200], Train Loss: 0.000565
Validation Loss: 0.00056654
Epoch [60/200], Train Loss: 0.000586
Validation Loss: 0.00054745
Epoch [61/200], Train Loss: 0.000552
Validation Loss: 0.00054779
Epoch [62/200], Train Loss: 0.000548
Validation Loss: 0.00053572
Epoch [63/200], Train Loss: 0.000532
Validation Loss: 0.00051893
Epoch [64/200], Train Loss: 0.000534
Validation Loss: 0.00051836
Epoch [65/200], Train Loss: 0.000534
Validation Loss: 0.00052555
Epoch [66/200], Train Loss: 0.000508
Validation Loss: 0.00051284
Epoch [67/200], Train Loss: 0.000515
Validation Loss: 0.00052088
Epoch [68/200], Train Loss: 0.000505
Validation Loss: 0.00049510
Epoch [69/200], Train Loss: 0.000497
Validation Loss: 0.00050191
Epoch [70/200], Train Loss: 0.000489
Validation Loss: 0.00050335
Epoch [71/200], Train Loss: 0.000489
Validation Loss: 0.00047466
Epoch [72/200], Train Loss: 0.000478
Validation Loss: 0.00047991
Epoch [73/200], Train Loss: 0.000480
Validation Loss: 0.00048258
Epoch [74/200], Train Loss: 0.000469
Validation Loss: 0.00046409
Epoch [75/200], Train Loss: 0.000459
Validation Loss: 0.00048795
Epoch [76/200], Train Loss: 0.000458
Validation Loss: 0.00045173
Epoch [77/200], Train Loss: 0.000461
Validation Loss: 0.00045005
Epoch [78/200], Train Loss: 0.000456
Validation Loss: 0.00045596
Epoch [79/200], Train Loss: 0.000458
Validation Loss: 0.00046276
Epoch [80/200], Train Loss: 0.000445
Validation Loss: 0.00043708
Epoch [81/200], Train Loss: 0.000437
Validation Loss: 0.00044202
Epoch [82/200], Train Loss: 0.000432
Validation Loss: 0.00043561
Epoch [83/200], Train Loss: 0.000423
Validation Loss: 0.00042998
Epoch [84/200], Train Loss: 0.000428
Validation Loss: 0.00043493
Epoch [85/200], Train Loss: 0.000424
Validation Loss: 0.00042217
Epoch [86/200], Train Loss: 0.000419
Validation Loss: 0.00041152
Epoch [87/200], Train Loss: 0.000420
Validation Loss: 0.00040594
Epoch [88/200], Train Loss: 0.000411
Validation Loss: 0.00041975
Epoch [89/200], Train Loss: 0.000411
Validation Loss: 0.00041019
Epoch [90/200], Train Loss: 0.000403
Validation Loss: 0.00040462
Epoch [91/200], Train Loss: 0.000399
Validation Loss: 0.00040231
Epoch [92/200], Train Loss: 0.000412
Validation Loss: 0.00038984
Epoch [93/200], Train Loss: 0.000396
Validation Loss: 0.00038394
Epoch [94/200], Train Loss: 0.000392
Validation Loss: 0.00038262
Epoch [95/200], Train Loss: 0.000388
Validation Loss: 0.00038733
Epoch [96/200], Train Loss: 0.000386
Validation Loss: 0.00038981
Epoch [97/200], Train Loss: 0.000385
Validation Loss: 0.00037402
Epoch [98/200], Train Loss: 0.000382
Validation Loss: 0.00037766
Epoch [99/200], Train Loss: 0.000386
Validation Loss: 0.00045447
Epoch [100/200], Train Loss: 0.000394
Validation Loss: 0.00039444
Epoch [101/200], Train Loss: 0.000380
Validation Loss: 0.00037453
Epoch [102/200], Train Loss: 0.000376
Validation Loss: 0.00037145
Epoch [103/200], Train Loss: 0.000374
Validation Loss: 0.00038164
Epoch [104/200], Train Loss: 0.000376
Validation Loss: 0.00036053
Epoch [105/200], Train Loss: 0.000374
Validation Loss: 0.00036242
Epoch [106/200], Train Loss: 0.000367
Validation Loss: 0.00038061
Epoch [107/200], Train Loss: 0.000362
Validation Loss: 0.00035686
Epoch [108/200], Train Loss: 0.000360
Validation Loss: 0.00035946
Epoch [109/200], Train Loss: 0.000369
Validation Loss: 0.00036464
Epoch [110/200], Train Loss: 0.000362
Validation Loss: 0.00034798
Epoch [111/200], Train Loss: 0.000357
Validation Loss: 0.00035666
Epoch [112/200], Train Loss: 0.000352
Validation Loss: 0.00036013
Epoch [113/200], Train Loss: 0.000377
Validation Loss: 0.00037861
Epoch [114/200], Train Loss: 0.000354
Validation Loss: 0.00035323
Epoch [115/200], Train Loss: 0.000361
Validation Loss: 0.00035679
Epoch [116/200], Train Loss: 0.000349
Validation Loss: 0.00034584
Epoch [117/200], Train Loss: 0.000351
Validation Loss: 0.00033818
Epoch [118/200], Train Loss: 0.000349
Validation Loss: 0.00033910
Epoch [119/200], Train Loss: 0.000345
Validation Loss: 0.00033529
Epoch [120/200], Train Loss: 0.000339
Validation Loss: 0.00033667
Epoch [121/200], Train Loss: 0.000339
Validation Loss: 0.00033412
Epoch [122/200], Train Loss: 0.000336
Validation Loss: 0.00033154
Epoch [123/200], Train Loss: 0.000335
Validation Loss: 0.00033290
Epoch [124/200], Train Loss: 0.000335
Validation Loss: 0.00033341
Epoch [125/200], Train Loss: 0.000341
Validation Loss: 0.00035075
Epoch [126/200], Train Loss: 0.000339
Validation Loss: 0.00033707
Epoch [127/200], Train Loss: 0.000339
Validation Loss: 0.00032741
Epoch [128/200], Train Loss: 0.000331
Validation Loss: 0.00032388
Epoch [129/200], Train Loss: 0.000330
Validation Loss: 0.00032344
Epoch [130/200], Train Loss: 0.000328
Validation Loss: 0.00032145
Epoch [131/200], Train Loss: 0.000327
Validation Loss: 0.00031147
Epoch [132/200], Train Loss: 0.000327
Validation Loss: 0.00033674
Epoch [133/200], Train Loss: 0.000343
Validation Loss: 0.00031873
Epoch [134/200], Train Loss: 0.000328
Validation Loss: 0.00032076
Epoch [135/200], Train Loss: 0.000321
Validation Loss: 0.00031681
Epoch [136/200], Train Loss: 0.000321
Validation Loss: 0.00032515
Epoch [137/200], Train Loss: 0.000320
Validation Loss: 0.00030637
Epoch [138/200], Train Loss: 0.000323
Validation Loss: 0.00030669
Epoch [139/200], Train Loss: 0.000319
Validation Loss: 0.00035036
Epoch [140/200], Train Loss: 0.000323
Validation Loss: 0.00031328
Epoch [141/200], Train Loss: 0.000323
Validation Loss: 0.00030771
Epoch [142/200], Train Loss: 0.000316
Validation Loss: 0.00031459
Epoch [143/200], Train Loss: 0.000318
Validation Loss: 0.00029910
Epoch [144/200], Train Loss: 0.000308
Validation Loss: 0.00029629
Epoch [145/200], Train Loss: 0.000314
Validation Loss: 0.00030744
Epoch [146/200], Train Loss: 0.000311
Validation Loss: 0.00030286
Epoch [147/200], Train Loss: 0.000310
Validation Loss: 0.00029660
Epoch [148/200], Train Loss: 0.000307
Validation Loss: 0.00031132
Epoch [149/200], Train Loss: 0.000313
Validation Loss: 0.00029975
Epoch [150/200], Train Loss: 0.000317
Validation Loss: 0.00030503
Epoch [151/200], Train Loss: 0.000309
Validation Loss: 0.00032541
Epoch [152/200], Train Loss: 0.000308
Validation Loss: 0.00029883
Epoch [153/200], Train Loss: 0.000306
Validation Loss: 0.00029795
Epoch [154/200], Train Loss: 0.000303
Validation Loss: 0.00029164
Epoch [155/200], Train Loss: 0.000302
Validation Loss: 0.00028881
Epoch [156/200], Train Loss: 0.000302
Validation Loss: 0.00028523
Epoch [157/200], Train Loss: 0.000303
Validation Loss: 0.00028424
Epoch [158/200], Train Loss: 0.000297
Validation Loss: 0.00029305
Epoch [159/200], Train Loss: 0.000299
Validation Loss: 0.00028449
Epoch [160/200], Train Loss: 0.000305
Validation Loss: 0.00031615
Epoch [161/200], Train Loss: 0.000306
Validation Loss: 0.00028783
Epoch [162/200], Train Loss: 0.000302
Validation Loss: 0.00027783
Epoch [163/200], Train Loss: 0.000299
Validation Loss: 0.00027914
Epoch [164/200], Train Loss: 0.000297
Validation Loss: 0.00027865
Epoch [165/200], Train Loss: 0.000296
Validation Loss: 0.00028904
Epoch [166/200], Train Loss: 0.000298
Validation Loss: 0.00027482
Epoch [167/200], Train Loss: 0.000295
Validation Loss: 0.00027106
Epoch [168/200], Train Loss: 0.000295
Validation Loss: 0.00027501
Epoch [169/200], Train Loss: 0.000304
Validation Loss: 0.00028243
Epoch [170/200], Train Loss: 0.000291
Validation Loss: 0.00026931
Epoch [171/200], Train Loss: 0.000291
Validation Loss: 0.00028039
Epoch [172/200], Train Loss: 0.000289
Validation Loss: 0.00026799
Epoch [173/200], Train Loss: 0.000292
Validation Loss: 0.00026584
Epoch [174/200], Train Loss: 0.000292
Validation Loss: 0.00027175
Epoch [175/200], Train Loss: 0.000285
Validation Loss: 0.00026318
Epoch [176/200], Train Loss: 0.000284
Validation Loss: 0.00026120
Epoch [177/200], Train Loss: 0.000282
Validation Loss: 0.00026876
Epoch [178/200], Train Loss: 0.000288
Validation Loss: 0.00027748
Epoch [179/200], Train Loss: 0.000284
Validation Loss: 0.00026016
Epoch [180/200], Train Loss: 0.000282
Validation Loss: 0.00026092
Epoch [181/200], Train Loss: 0.000281
Validation Loss: 0.00025790
Epoch [182/200], Train Loss: 0.000282
Validation Loss: 0.00025805
Epoch [183/200], Train Loss: 0.000288
Validation Loss: 0.00027804
Epoch [184/200], Train Loss: 0.000284
Validation Loss: 0.00025312
Epoch [185/200], Train Loss: 0.000277
Validation Loss: 0.00025485
Epoch [186/200], Train Loss: 0.000276
Validation Loss: 0.00026689
Epoch [187/200], Train Loss: 0.000276
Validation Loss: 0.00025233
Epoch [188/200], Train Loss: 0.000275
Validation Loss: 0.00025204
Epoch [189/200], Train Loss: 0.000277
Validation Loss: 0.00029500
Epoch [190/200], Train Loss: 0.000272
Validation Loss: 0.00027061
Epoch [191/200], Train Loss: 0.000272
Validation Loss: 0.00024777
Epoch [192/200], Train Loss: 0.000268
Validation Loss: 0.00023810
Epoch [193/200], Train Loss: 0.000264
Validation Loss: 0.00023699
Epoch [194/200], Train Loss: 0.000275
Validation Loss: 0.00023625
Epoch [195/200], Train Loss: 0.000265
Validation Loss: 0.00023090
Epoch [196/200], Train Loss: 0.000262
Validation Loss: 0.00023166
Epoch [197/200], Train Loss: 0.000259
Validation Loss: 0.00023206
Epoch [198/200], Train Loss: 0.000258
Validation Loss: 0.00022404
Epoch [199/200], Train Loss: 0.000258
Validation Loss: 0.00022706
Epoch [200/200], Train Loss: 0.000255
Validation Loss: 0.00022686

Evaluating model for: Lamp
Run 52/144 completed in 5185.10 seconds with: {'MAE': np.float32(0.28248757), 'MSE': np.float32(9.281304), 'RMSE': np.float32(3.0465233), 'SAE': np.float32(0.007611617), 'NDE': np.float32(0.23398393)}

Run 53/144: hidden=128, seq_len=120, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.004923
Validation Loss: 0.00453843
Epoch [2/200], Train Loss: 0.004801
Validation Loss: 0.00451246
Epoch [3/200], Train Loss: 0.004749
Validation Loss: 0.00450125
Epoch [4/200], Train Loss: 0.004751
Validation Loss: 0.00449335
Epoch [5/200], Train Loss: 0.004734
Validation Loss: 0.00449698
Epoch [6/200], Train Loss: 0.004745
Validation Loss: 0.00448852
Epoch [7/200], Train Loss: 0.004742
Validation Loss: 0.00448646
Epoch [8/200], Train Loss: 0.004734
Validation Loss: 0.00448495
Epoch [9/200], Train Loss: 0.004753
Validation Loss: 0.00448295
Epoch [10/200], Train Loss: 0.004713
Validation Loss: 0.00447409
Epoch [11/200], Train Loss: 0.004733
Validation Loss: 0.00447321
Epoch [12/200], Train Loss: 0.004707
Validation Loss: 0.00446119
Epoch [13/200], Train Loss: 0.004699
Validation Loss: 0.00444635
Epoch [14/200], Train Loss: 0.004691
Validation Loss: 0.00443737
Epoch [15/200], Train Loss: 0.004682
Validation Loss: 0.00442938
Epoch [16/200], Train Loss: 0.004704
Validation Loss: 0.00442097
Epoch [17/200], Train Loss: 0.004671
Validation Loss: 0.00440906
Epoch [18/200], Train Loss: 0.004659
Validation Loss: 0.00438888
Epoch [19/200], Train Loss: 0.004638
Validation Loss: 0.00436962
Epoch [20/200], Train Loss: 0.004621
Validation Loss: 0.00438106
Epoch [21/200], Train Loss: 0.004607
Validation Loss: 0.00431588
Epoch [22/200], Train Loss: 0.004541
Validation Loss: 0.00423079
Epoch [23/200], Train Loss: 0.004483
Validation Loss: 0.00414270
Epoch [24/200], Train Loss: 0.004388
Validation Loss: 0.00404710
Epoch [25/200], Train Loss: 0.004263
Validation Loss: 0.00383210
Epoch [26/200], Train Loss: 0.004114
Validation Loss: 0.00364674
Epoch [27/200], Train Loss: 0.003926
Validation Loss: 0.00345056
Epoch [28/200], Train Loss: 0.003774
Validation Loss: 0.00331823
Epoch [29/200], Train Loss: 0.003522
Validation Loss: 0.00296888
Epoch [30/200], Train Loss: 0.003242
Validation Loss: 0.00267727
Epoch [31/200], Train Loss: 0.003008
Validation Loss: 0.00244064
Epoch [32/200], Train Loss: 0.002810
Validation Loss: 0.00232390
Epoch [33/200], Train Loss: 0.002660
Validation Loss: 0.00223832
Epoch [34/200], Train Loss: 0.002510
Validation Loss: 0.00214509
Epoch [35/200], Train Loss: 0.002407
Validation Loss: 0.00200183
Epoch [36/200], Train Loss: 0.002332
Validation Loss: 0.00195656
Epoch [37/200], Train Loss: 0.002246
Validation Loss: 0.00189252
Epoch [38/200], Train Loss: 0.002195
Validation Loss: 0.00185933
Epoch [39/200], Train Loss: 0.002128
Validation Loss: 0.00181244
Epoch [40/200], Train Loss: 0.002091
Validation Loss: 0.00174874
Epoch [41/200], Train Loss: 0.002029
Validation Loss: 0.00170630
Epoch [42/200], Train Loss: 0.001978
Validation Loss: 0.00165074
Epoch [43/200], Train Loss: 0.001938
Validation Loss: 0.00163973
Epoch [44/200], Train Loss: 0.001908
Validation Loss: 0.00160872
Epoch [45/200], Train Loss: 0.001856
Validation Loss: 0.00156198
Epoch [46/200], Train Loss: 0.001818
Validation Loss: 0.00153737
Epoch [47/200], Train Loss: 0.001783
Validation Loss: 0.00150945
Epoch [48/200], Train Loss: 0.001750
Validation Loss: 0.00149854
Epoch [49/200], Train Loss: 0.001727
Validation Loss: 0.00149314
Epoch [50/200], Train Loss: 0.001696
Validation Loss: 0.00142449
Epoch [51/200], Train Loss: 0.001673
Validation Loss: 0.00142293
Epoch [52/200], Train Loss: 0.001640
Validation Loss: 0.00141125
Epoch [53/200], Train Loss: 0.001623
Validation Loss: 0.00136493
Epoch [54/200], Train Loss: 0.001598
Validation Loss: 0.00135973
Epoch [55/200], Train Loss: 0.001568
Validation Loss: 0.00132124
Epoch [56/200], Train Loss: 0.001541
Validation Loss: 0.00131069
Epoch [57/200], Train Loss: 0.001519
Validation Loss: 0.00128071
Epoch [58/200], Train Loss: 0.001498
Validation Loss: 0.00127508
Epoch [59/200], Train Loss: 0.001474
Validation Loss: 0.00126577
Epoch [60/200], Train Loss: 0.001467
Validation Loss: 0.00124057
Epoch [61/200], Train Loss: 0.001433
Validation Loss: 0.00123244
Epoch [62/200], Train Loss: 0.001414
Validation Loss: 0.00119797
Epoch [63/200], Train Loss: 0.001420
Validation Loss: 0.00118147
Epoch [64/200], Train Loss: 0.001387
Validation Loss: 0.00117203
Epoch [65/200], Train Loss: 0.001376
Validation Loss: 0.00118105
Epoch [66/200], Train Loss: 0.001368
Validation Loss: 0.00115992
Epoch [67/200], Train Loss: 0.001335
Validation Loss: 0.00113143
Epoch [68/200], Train Loss: 0.001327
Validation Loss: 0.00113033
Epoch [69/200], Train Loss: 0.001305
Validation Loss: 0.00113480
Epoch [70/200], Train Loss: 0.001301
Validation Loss: 0.00110368
Epoch [71/200], Train Loss: 0.001286
Validation Loss: 0.00109336
Epoch [72/200], Train Loss: 0.001273
Validation Loss: 0.00109554
Epoch [73/200], Train Loss: 0.001267
Validation Loss: 0.00107009
Epoch [74/200], Train Loss: 0.001234
Validation Loss: 0.00105670
Epoch [75/200], Train Loss: 0.001213
Validation Loss: 0.00104467
Epoch [76/200], Train Loss: 0.001208
Validation Loss: 0.00103189
Epoch [77/200], Train Loss: 0.001199
Validation Loss: 0.00103523
Epoch [78/200], Train Loss: 0.001193
Validation Loss: 0.00101651
Epoch [79/200], Train Loss: 0.001177
Validation Loss: 0.00101576
Epoch [80/200], Train Loss: 0.001165
Validation Loss: 0.00104413
Epoch [81/200], Train Loss: 0.001164
Validation Loss: 0.00101187
Epoch [82/200], Train Loss: 0.001150
Validation Loss: 0.00098202
Epoch [83/200], Train Loss: 0.001131
Validation Loss: 0.00098152
Epoch [84/200], Train Loss: 0.001120
Validation Loss: 0.00096787
Epoch [85/200], Train Loss: 0.001111
Validation Loss: 0.00096407
Epoch [86/200], Train Loss: 0.001119
Validation Loss: 0.00098133
Epoch [87/200], Train Loss: 0.001097
Validation Loss: 0.00095762
Epoch [88/200], Train Loss: 0.001133
Validation Loss: 0.00095412
Epoch [89/200], Train Loss: 0.001089
Validation Loss: 0.00094839
Epoch [90/200], Train Loss: 0.001091
Validation Loss: 0.00095268
Epoch [91/200], Train Loss: 0.001070
Validation Loss: 0.00093030
Epoch [92/200], Train Loss: 0.001057
Validation Loss: 0.00096613
Epoch [93/200], Train Loss: 0.001059
Validation Loss: 0.00092191
Epoch [94/200], Train Loss: 0.001049
Validation Loss: 0.00091577
Epoch [95/200], Train Loss: 0.001038
Validation Loss: 0.00090595
Epoch [96/200], Train Loss: 0.001042
Validation Loss: 0.00090386
Epoch [97/200], Train Loss: 0.001054
Validation Loss: 0.00091719
Epoch [98/200], Train Loss: 0.001022
Validation Loss: 0.00090034
Epoch [99/200], Train Loss: 0.001027
Validation Loss: 0.00093186
Epoch [100/200], Train Loss: 0.001014
Validation Loss: 0.00090850
Epoch [101/200], Train Loss: 0.001015
Validation Loss: 0.00089015
Epoch [102/200], Train Loss: 0.000996
Validation Loss: 0.00089460
Epoch [103/200], Train Loss: 0.001005
Validation Loss: 0.00091304
Epoch [104/200], Train Loss: 0.001004
Validation Loss: 0.00087853
Epoch [105/200], Train Loss: 0.000990
Validation Loss: 0.00087681
Epoch [106/200], Train Loss: 0.000983
Validation Loss: 0.00087265
Epoch [107/200], Train Loss: 0.000969
Validation Loss: 0.00086465
Epoch [108/200], Train Loss: 0.000966
Validation Loss: 0.00086263
Epoch [109/200], Train Loss: 0.000978
Validation Loss: 0.00086967
Epoch [110/200], Train Loss: 0.000965
Validation Loss: 0.00085358
Epoch [111/200], Train Loss: 0.000963
Validation Loss: 0.00085493
Epoch [112/200], Train Loss: 0.000953
Validation Loss: 0.00085161
Epoch [113/200], Train Loss: 0.000942
Validation Loss: 0.00083752
Epoch [114/200], Train Loss: 0.000957
Validation Loss: 0.00084300
Epoch [115/200], Train Loss: 0.000941
Validation Loss: 0.00084401
Epoch [116/200], Train Loss: 0.000938
Validation Loss: 0.00083430
Epoch [117/200], Train Loss: 0.000926
Validation Loss: 0.00084008
Epoch [118/200], Train Loss: 0.000926
Validation Loss: 0.00085405
Epoch [119/200], Train Loss: 0.000927
Validation Loss: 0.00082638
Epoch [120/200], Train Loss: 0.000925
Validation Loss: 0.00081889
Epoch [121/200], Train Loss: 0.000915
Validation Loss: 0.00083005
Epoch [122/200], Train Loss: 0.000931
Validation Loss: 0.00082624
Epoch [123/200], Train Loss: 0.000901
Validation Loss: 0.00081155
Epoch [124/200], Train Loss: 0.000955
Validation Loss: 0.00083669
Epoch [125/200], Train Loss: 0.000903
Validation Loss: 0.00080589
Epoch [126/200], Train Loss: 0.000894
Validation Loss: 0.00080007
Epoch [127/200], Train Loss: 0.000890
Validation Loss: 0.00080149
Epoch [128/200], Train Loss: 0.000887
Validation Loss: 0.00080600
Epoch [129/200], Train Loss: 0.000885
Validation Loss: 0.00079549
Epoch [130/200], Train Loss: 0.000874
Validation Loss: 0.00078741
Epoch [131/200], Train Loss: 0.000876
Validation Loss: 0.00078215
Epoch [132/200], Train Loss: 0.000863
Validation Loss: 0.00079476
Epoch [133/200], Train Loss: 0.000869
Validation Loss: 0.00077308
Epoch [134/200], Train Loss: 0.000859
Validation Loss: 0.00076876
Epoch [135/200], Train Loss: 0.000854
Validation Loss: 0.00076126
Epoch [136/200], Train Loss: 0.000843
Validation Loss: 0.00076175
Epoch [137/200], Train Loss: 0.000853
Validation Loss: 0.00077727
Epoch [138/200], Train Loss: 0.000837
Validation Loss: 0.00075214
Epoch [139/200], Train Loss: 0.000837
Validation Loss: 0.00076291
Epoch [140/200], Train Loss: 0.000843
Validation Loss: 0.00075985
Epoch [141/200], Train Loss: 0.000840
Validation Loss: 0.00074299
Epoch [142/200], Train Loss: 0.000830
Validation Loss: 0.00073875
Epoch [143/200], Train Loss: 0.000821
Validation Loss: 0.00072914
Epoch [144/200], Train Loss: 0.000825
Validation Loss: 0.00073207
Epoch [145/200], Train Loss: 0.000816
Validation Loss: 0.00074275
Epoch [146/200], Train Loss: 0.000817
Validation Loss: 0.00072283
Epoch [147/200], Train Loss: 0.000803
Validation Loss: 0.00071789
Epoch [148/200], Train Loss: 0.000808
Validation Loss: 0.00072049
Epoch [149/200], Train Loss: 0.000804
Validation Loss: 0.00071080
Epoch [150/200], Train Loss: 0.000811
Validation Loss: 0.00071825
Epoch [151/200], Train Loss: 0.000788
Validation Loss: 0.00070075
Epoch [152/200], Train Loss: 0.000788
Validation Loss: 0.00070450
Epoch [153/200], Train Loss: 0.000784
Validation Loss: 0.00069747
Epoch [154/200], Train Loss: 0.000795
Validation Loss: 0.00071355
Epoch [155/200], Train Loss: 0.000781
Validation Loss: 0.00069009
Epoch [156/200], Train Loss: 0.000780
Validation Loss: 0.00069063
Epoch [157/200], Train Loss: 0.000772
Validation Loss: 0.00068592
Epoch [158/200], Train Loss: 0.000774
Validation Loss: 0.00068294
Epoch [159/200], Train Loss: 0.000768
Validation Loss: 0.00067443
Epoch [160/200], Train Loss: 0.000765
Validation Loss: 0.00068845
Epoch [161/200], Train Loss: 0.000756
Validation Loss: 0.00068815
Epoch [162/200], Train Loss: 0.000761
Validation Loss: 0.00066396
Epoch [163/200], Train Loss: 0.000757
Validation Loss: 0.00066649
Epoch [164/200], Train Loss: 0.000754
Validation Loss: 0.00068216
Epoch [165/200], Train Loss: 0.000750
Validation Loss: 0.00065509
Epoch [166/200], Train Loss: 0.000748
Validation Loss: 0.00066746
Epoch [167/200], Train Loss: 0.000747
Validation Loss: 0.00065844
Epoch [168/200], Train Loss: 0.000734
Validation Loss: 0.00065342
Epoch [169/200], Train Loss: 0.000735
Validation Loss: 0.00066497
Epoch [170/200], Train Loss: 0.000732
Validation Loss: 0.00066029
Epoch [171/200], Train Loss: 0.000727
Validation Loss: 0.00068023
Epoch [172/200], Train Loss: 0.000731
Validation Loss: 0.00064344
Epoch [173/200], Train Loss: 0.000733
Validation Loss: 0.00064251
Epoch [174/200], Train Loss: 0.000728
Validation Loss: 0.00064090
Epoch [175/200], Train Loss: 0.000732
Validation Loss: 0.00066105
Epoch [176/200], Train Loss: 0.000724
Validation Loss: 0.00063195
Epoch [177/200], Train Loss: 0.000724
Validation Loss: 0.00062993
Epoch [178/200], Train Loss: 0.000716
Validation Loss: 0.00062559
Epoch [179/200], Train Loss: 0.000711
Validation Loss: 0.00062671
Epoch [180/200], Train Loss: 0.000715
Validation Loss: 0.00062895
Epoch [181/200], Train Loss: 0.000710
Validation Loss: 0.00063071
Epoch [182/200], Train Loss: 0.000714
Validation Loss: 0.00062526
Epoch [183/200], Train Loss: 0.000707
Validation Loss: 0.00062152
Epoch [184/200], Train Loss: 0.000708
Validation Loss: 0.00062588
Epoch [185/200], Train Loss: 0.000700
Validation Loss: 0.00061679
Epoch [186/200], Train Loss: 0.000703
Validation Loss: 0.00062479
Epoch [187/200], Train Loss: 0.000699
Validation Loss: 0.00063037
Epoch [188/200], Train Loss: 0.000693
Validation Loss: 0.00060596
Epoch [189/200], Train Loss: 0.000703
Validation Loss: 0.00061122
Epoch [190/200], Train Loss: 0.000699
Validation Loss: 0.00061980
Epoch [191/200], Train Loss: 0.000690
Validation Loss: 0.00060223
Epoch [192/200], Train Loss: 0.000692
Validation Loss: 0.00060387
Epoch [193/200], Train Loss: 0.000685
Validation Loss: 0.00060194
Epoch [194/200], Train Loss: 0.000682
Validation Loss: 0.00060432
Epoch [195/200], Train Loss: 0.000682
Validation Loss: 0.00060222
Epoch [196/200], Train Loss: 0.000683
Validation Loss: 0.00060601
Epoch [197/200], Train Loss: 0.000675
Validation Loss: 0.00059606
Epoch [198/200], Train Loss: 0.000684
Validation Loss: 0.00060450
Epoch [199/200], Train Loss: 0.000678
Validation Loss: 0.00059528
Epoch [200/200], Train Loss: 0.000672
Validation Loss: 0.00058929

Evaluating model for: Lamp
Run 53/144 completed in 1926.01 seconds with: {'MAE': np.float32(0.62407047), 'MSE': np.float32(17.760185), 'RMSE': np.float32(4.2142835), 'SAE': np.float32(0.071886696), 'NDE': np.float32(0.3221278)}

Run 54/144: hidden=128, seq_len=120, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.005118
Validation Loss: 0.00460581
Epoch [2/200], Train Loss: 0.004868
Validation Loss: 0.00457552
Epoch [3/200], Train Loss: 0.004809
Validation Loss: 0.00451484
Epoch [4/200], Train Loss: 0.004760
Validation Loss: 0.00450919
Epoch [5/200], Train Loss: 0.004773
Validation Loss: 0.00451039
Epoch [6/200], Train Loss: 0.004766
Validation Loss: 0.00450682
Epoch [7/200], Train Loss: 0.004764
Validation Loss: 0.00449698
Epoch [8/200], Train Loss: 0.004743
Validation Loss: 0.00449357
Epoch [9/200], Train Loss: 0.004742
Validation Loss: 0.00449024
Epoch [10/200], Train Loss: 0.004750
Validation Loss: 0.00448807
Epoch [11/200], Train Loss: 0.004762
Validation Loss: 0.00448710
Epoch [12/200], Train Loss: 0.004759
Validation Loss: 0.00448663
Epoch [13/200], Train Loss: 0.004754
Validation Loss: 0.00448078
Epoch [14/200], Train Loss: 0.004752
Validation Loss: 0.00448009
Epoch [15/200], Train Loss: 0.004741
Validation Loss: 0.00447910
Epoch [16/200], Train Loss: 0.004729
Validation Loss: 0.00447724
Epoch [17/200], Train Loss: 0.004721
Validation Loss: 0.00447596
Epoch [18/200], Train Loss: 0.004759
Validation Loss: 0.00447439
Epoch [19/200], Train Loss: 0.004735
Validation Loss: 0.00447284
Epoch [20/200], Train Loss: 0.004739
Validation Loss: 0.00447122
Epoch [21/200], Train Loss: 0.004732
Validation Loss: 0.00447239
Epoch [22/200], Train Loss: 0.004730
Validation Loss: 0.00447358
Epoch [23/200], Train Loss: 0.004721
Validation Loss: 0.00446934
Epoch [24/200], Train Loss: 0.004721
Validation Loss: 0.00446914
Epoch [25/200], Train Loss: 0.004746
Validation Loss: 0.00446658
Epoch [26/200], Train Loss: 0.004746
Validation Loss: 0.00446840
Epoch [27/200], Train Loss: 0.004741
Validation Loss: 0.00446101
Epoch [28/200], Train Loss: 0.004723
Validation Loss: 0.00446014
Epoch [29/200], Train Loss: 0.004719
Validation Loss: 0.00446758
Epoch [30/200], Train Loss: 0.004721
Validation Loss: 0.00446100
Epoch [31/200], Train Loss: 0.004714
Validation Loss: 0.00445798
Epoch [32/200], Train Loss: 0.004725
Validation Loss: 0.00446114
Epoch [33/200], Train Loss: 0.004725
Validation Loss: 0.00445755
Epoch [34/200], Train Loss: 0.004737
Validation Loss: 0.00445634
Epoch [35/200], Train Loss: 0.004709
Validation Loss: 0.00445250
Epoch [36/200], Train Loss: 0.004702
Validation Loss: 0.00445229
Epoch [37/200], Train Loss: 0.004744
Validation Loss: 0.00444876
Epoch [38/200], Train Loss: 0.004732
Validation Loss: 0.00444266
Epoch [39/200], Train Loss: 0.004697
Validation Loss: 0.00443866
Epoch [40/200], Train Loss: 0.004682
Validation Loss: 0.00443578
Epoch [41/200], Train Loss: 0.004711
Validation Loss: 0.00442564
Epoch [42/200], Train Loss: 0.004681
Validation Loss: 0.00442599
Epoch [43/200], Train Loss: 0.004666
Validation Loss: 0.00440842
Epoch [44/200], Train Loss: 0.004668
Validation Loss: 0.00440497
Epoch [45/200], Train Loss: 0.004666
Validation Loss: 0.00438895
Epoch [46/200], Train Loss: 0.004641
Validation Loss: 0.00436968
Epoch [47/200], Train Loss: 0.004631
Validation Loss: 0.00437304
Epoch [48/200], Train Loss: 0.004614
Validation Loss: 0.00433623
Epoch [49/200], Train Loss: 0.004589
Validation Loss: 0.00427767
Epoch [50/200], Train Loss: 0.004546
Validation Loss: 0.00423056
Epoch [51/200], Train Loss: 0.004506
Validation Loss: 0.00416209
Epoch [52/200], Train Loss: 0.004457
Validation Loss: 0.00409678
Epoch [53/200], Train Loss: 0.004367
Validation Loss: 0.00398642
Epoch [54/200], Train Loss: 0.004257
Validation Loss: 0.00383603
Epoch [55/200], Train Loss: 0.004134
Validation Loss: 0.00368681
Epoch [56/200], Train Loss: 0.003948
Validation Loss: 0.00341736
Epoch [57/200], Train Loss: 0.003714
Validation Loss: 0.00308054
Epoch [58/200], Train Loss: 0.003490
Validation Loss: 0.00285126
Epoch [59/200], Train Loss: 0.003263
Validation Loss: 0.00274198
Epoch [60/200], Train Loss: 0.003078
Validation Loss: 0.00245232
Epoch [61/200], Train Loss: 0.002829
Validation Loss: 0.00228199
Epoch [62/200], Train Loss: 0.002680
Validation Loss: 0.00213171
Epoch [63/200], Train Loss: 0.002579
Validation Loss: 0.00220110
Epoch [64/200], Train Loss: 0.002473
Validation Loss: 0.00200966
Epoch [65/200], Train Loss: 0.002360
Validation Loss: 0.00192760
Epoch [66/200], Train Loss: 0.002277
Validation Loss: 0.00186336
Epoch [67/200], Train Loss: 0.002214
Validation Loss: 0.00182867
Epoch [68/200], Train Loss: 0.002150
Validation Loss: 0.00175909
Epoch [69/200], Train Loss: 0.002074
Validation Loss: 0.00173088
Epoch [70/200], Train Loss: 0.002046
Validation Loss: 0.00166089
Epoch [71/200], Train Loss: 0.001980
Validation Loss: 0.00164537
Epoch [72/200], Train Loss: 0.001932
Validation Loss: 0.00161262
Epoch [73/200], Train Loss: 0.001902
Validation Loss: 0.00159047
Epoch [74/200], Train Loss: 0.001854
Validation Loss: 0.00155251
Epoch [75/200], Train Loss: 0.001805
Validation Loss: 0.00153184
Epoch [76/200], Train Loss: 0.001774
Validation Loss: 0.00147597
Epoch [77/200], Train Loss: 0.001744
Validation Loss: 0.00149207
Epoch [78/200], Train Loss: 0.001697
Validation Loss: 0.00142550
Epoch [79/200], Train Loss: 0.001667
Validation Loss: 0.00141026
Epoch [80/200], Train Loss: 0.001645
Validation Loss: 0.00140785
Epoch [81/200], Train Loss: 0.001608
Validation Loss: 0.00137503
Epoch [82/200], Train Loss: 0.001590
Validation Loss: 0.00136602
Epoch [83/200], Train Loss: 0.001557
Validation Loss: 0.00131501
Epoch [84/200], Train Loss: 0.001538
Validation Loss: 0.00131209
Epoch [85/200], Train Loss: 0.001503
Validation Loss: 0.00127860
Epoch [86/200], Train Loss: 0.001487
Validation Loss: 0.00127273
Epoch [87/200], Train Loss: 0.001471
Validation Loss: 0.00126232
Epoch [88/200], Train Loss: 0.001445
Validation Loss: 0.00122898
Epoch [89/200], Train Loss: 0.001415
Validation Loss: 0.00120736
Epoch [90/200], Train Loss: 0.001398
Validation Loss: 0.00120112
Epoch [91/200], Train Loss: 0.001365
Validation Loss: 0.00118768
Epoch [92/200], Train Loss: 0.001375
Validation Loss: 0.00116516
Epoch [93/200], Train Loss: 0.001356
Validation Loss: 0.00117861
Epoch [94/200], Train Loss: 0.001324
Validation Loss: 0.00113710
Epoch [95/200], Train Loss: 0.001311
Validation Loss: 0.00114813
Epoch [96/200], Train Loss: 0.001313
Validation Loss: 0.00113947
Epoch [97/200], Train Loss: 0.001288
Validation Loss: 0.00111858
Epoch [98/200], Train Loss: 0.001273
Validation Loss: 0.00110233
Epoch [99/200], Train Loss: 0.001259
Validation Loss: 0.00109125
Epoch [100/200], Train Loss: 0.001254
Validation Loss: 0.00111732
Epoch [101/200], Train Loss: 0.001237
Validation Loss: 0.00108181
Epoch [102/200], Train Loss: 0.001227
Validation Loss: 0.00107425
Epoch [103/200], Train Loss: 0.001226
Validation Loss: 0.00106334
Epoch [104/200], Train Loss: 0.001197
Validation Loss: 0.00105820
Epoch [105/200], Train Loss: 0.001202
Validation Loss: 0.00102700
Epoch [106/200], Train Loss: 0.001188
Validation Loss: 0.00102542
Epoch [107/200], Train Loss: 0.001179
Validation Loss: 0.00102155
Epoch [108/200], Train Loss: 0.001198
Validation Loss: 0.00101138
Epoch [109/200], Train Loss: 0.001171
Validation Loss: 0.00101607
Epoch [110/200], Train Loss: 0.001160
Validation Loss: 0.00100281
Epoch [111/200], Train Loss: 0.001142
Validation Loss: 0.00101026
Epoch [112/200], Train Loss: 0.001128
Validation Loss: 0.00104083
Epoch [113/200], Train Loss: 0.001131
Validation Loss: 0.00100048
Epoch [114/200], Train Loss: 0.001114
Validation Loss: 0.00098127
Epoch [115/200], Train Loss: 0.001122
Validation Loss: 0.00100492
Epoch [116/200], Train Loss: 0.001103
Validation Loss: 0.00096019
Epoch [117/200], Train Loss: 0.001095
Validation Loss: 0.00095450
Epoch [118/200], Train Loss: 0.001107
Validation Loss: 0.00095233
Epoch [119/200], Train Loss: 0.001089
Validation Loss: 0.00095102
Epoch [120/200], Train Loss: 0.001076
Validation Loss: 0.00094644
Epoch [121/200], Train Loss: 0.001074
Validation Loss: 0.00093393
Epoch [122/200], Train Loss: 0.001069
Validation Loss: 0.00091880
Epoch [123/200], Train Loss: 0.001059
Validation Loss: 0.00091485
Epoch [124/200], Train Loss: 0.001053
Validation Loss: 0.00091020
Epoch [125/200], Train Loss: 0.001046
Validation Loss: 0.00089467
Epoch [126/200], Train Loss: 0.001040
Validation Loss: 0.00089670
Epoch [127/200], Train Loss: 0.001037
Validation Loss: 0.00090505
Epoch [128/200], Train Loss: 0.001025
Validation Loss: 0.00088644
Epoch [129/200], Train Loss: 0.001033
Validation Loss: 0.00087265
Epoch [130/200], Train Loss: 0.001016
Validation Loss: 0.00086683
Epoch [131/200], Train Loss: 0.001003
Validation Loss: 0.00085529
Epoch [132/200], Train Loss: 0.000998
Validation Loss: 0.00085321
Epoch [133/200], Train Loss: 0.000989
Validation Loss: 0.00085359
Epoch [134/200], Train Loss: 0.000988
Validation Loss: 0.00084570
Epoch [135/200], Train Loss: 0.000981
Validation Loss: 0.00081354
Epoch [136/200], Train Loss: 0.000971
Validation Loss: 0.00086537
Epoch [137/200], Train Loss: 0.000973
Validation Loss: 0.00081120
Epoch [138/200], Train Loss: 0.000955
Validation Loss: 0.00084593
Epoch [139/200], Train Loss: 0.000955
Validation Loss: 0.00080843
Epoch [140/200], Train Loss: 0.000939
Validation Loss: 0.00082629
Epoch [141/200], Train Loss: 0.000935
Validation Loss: 0.00080565
Epoch [142/200], Train Loss: 0.000945
Validation Loss: 0.00077678
Epoch [143/200], Train Loss: 0.000922
Validation Loss: 0.00077407
Epoch [144/200], Train Loss: 0.000911
Validation Loss: 0.00076864
Epoch [145/200], Train Loss: 0.000906
Validation Loss: 0.00078298
Epoch [146/200], Train Loss: 0.000905
Validation Loss: 0.00075751
Epoch [147/200], Train Loss: 0.000894
Validation Loss: 0.00076055
Epoch [148/200], Train Loss: 0.000892
Validation Loss: 0.00074100
Epoch [149/200], Train Loss: 0.000870
Validation Loss: 0.00072504
Epoch [150/200], Train Loss: 0.000893
Validation Loss: 0.00074729
Epoch [151/200], Train Loss: 0.000865
Validation Loss: 0.00071700
Epoch [152/200], Train Loss: 0.000856
Validation Loss: 0.00071056
Epoch [153/200], Train Loss: 0.000866
Validation Loss: 0.00072030
Epoch [154/200], Train Loss: 0.000846
Validation Loss: 0.00069971
Epoch [155/200], Train Loss: 0.000843
Validation Loss: 0.00069461
Epoch [156/200], Train Loss: 0.000842
Validation Loss: 0.00068910
Epoch [157/200], Train Loss: 0.000834
Validation Loss: 0.00068643
Epoch [158/200], Train Loss: 0.000829
Validation Loss: 0.00068244
Epoch [159/200], Train Loss: 0.000822
Validation Loss: 0.00066309
Epoch [160/200], Train Loss: 0.000826
Validation Loss: 0.00067248
Epoch [161/200], Train Loss: 0.000817
Validation Loss: 0.00067839
Epoch [162/200], Train Loss: 0.000810
Validation Loss: 0.00067882
Epoch [163/200], Train Loss: 0.000808
Validation Loss: 0.00066034
Epoch [164/200], Train Loss: 0.000832
Validation Loss: 0.00071893
Epoch [165/200], Train Loss: 0.000805
Validation Loss: 0.00065533
Epoch [166/200], Train Loss: 0.000791
Validation Loss: 0.00064469
Epoch [167/200], Train Loss: 0.000796
Validation Loss: 0.00065862
Epoch [168/200], Train Loss: 0.000788
Validation Loss: 0.00064700
Epoch [169/200], Train Loss: 0.000790
Validation Loss: 0.00063695
Epoch [170/200], Train Loss: 0.000784
Validation Loss: 0.00066325
Epoch [171/200], Train Loss: 0.000777
Validation Loss: 0.00063330
Epoch [172/200], Train Loss: 0.000776
Validation Loss: 0.00063407
Epoch [173/200], Train Loss: 0.000787
Validation Loss: 0.00062585
Epoch [174/200], Train Loss: 0.000779
Validation Loss: 0.00063163
Epoch [175/200], Train Loss: 0.000781
Validation Loss: 0.00063453
Epoch [176/200], Train Loss: 0.000781
Validation Loss: 0.00062784
Epoch [177/200], Train Loss: 0.000763
Validation Loss: 0.00062424
Epoch [178/200], Train Loss: 0.000774
Validation Loss: 0.00063111
Epoch [179/200], Train Loss: 0.000762
Validation Loss: 0.00061151
Epoch [180/200], Train Loss: 0.000752
Validation Loss: 0.00060071
Epoch [181/200], Train Loss: 0.000754
Validation Loss: 0.00062126
Epoch [182/200], Train Loss: 0.000746
Validation Loss: 0.00063222
Epoch [183/200], Train Loss: 0.000750
Validation Loss: 0.00067760
Epoch [184/200], Train Loss: 0.000765
Validation Loss: 0.00060195
Epoch [185/200], Train Loss: 0.000746
Validation Loss: 0.00061849
Epoch [186/200], Train Loss: 0.000749
Validation Loss: 0.00061123
Epoch [187/200], Train Loss: 0.000748
Validation Loss: 0.00061985
Epoch [188/200], Train Loss: 0.000754
Validation Loss: 0.00059172
Epoch [189/200], Train Loss: 0.000744
Validation Loss: 0.00059039
Epoch [190/200], Train Loss: 0.000744
Validation Loss: 0.00059855
Epoch [191/200], Train Loss: 0.000735
Validation Loss: 0.00059287
Epoch [192/200], Train Loss: 0.000730
Validation Loss: 0.00058856
Epoch [193/200], Train Loss: 0.000738
Validation Loss: 0.00058728
Epoch [194/200], Train Loss: 0.000729
Validation Loss: 0.00059288
Epoch [195/200], Train Loss: 0.000722
Validation Loss: 0.00057411
Epoch [196/200], Train Loss: 0.000728
Validation Loss: 0.00058427
Epoch [197/200], Train Loss: 0.000723
Validation Loss: 0.00057895
Epoch [198/200], Train Loss: 0.000722
Validation Loss: 0.00056944
Epoch [199/200], Train Loss: 0.000709
Validation Loss: 0.00057762
Epoch [200/200], Train Loss: 0.000724
Validation Loss: 0.00058272

Evaluating model for: Lamp
Run 54/144 completed in 2013.05 seconds with: {'MAE': np.float32(0.62016505), 'MSE': np.float32(21.54301), 'RMSE': np.float32(4.6414447), 'SAE': np.float32(0.012007847), 'NDE': np.float32(0.35477892)}

Run 55/144: hidden=128, seq_len=120, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.005021
Validation Loss: 0.00458970
Epoch [2/200], Train Loss: 0.004835
Validation Loss: 0.00457561
Epoch [3/200], Train Loss: 0.004791
Validation Loss: 0.00450790
Epoch [4/200], Train Loss: 0.004769
Validation Loss: 0.00449801
Epoch [5/200], Train Loss: 0.004762
Validation Loss: 0.00449508
Epoch [6/200], Train Loss: 0.004752
Validation Loss: 0.00449208
Epoch [7/200], Train Loss: 0.004747
Validation Loss: 0.00449052
Epoch [8/200], Train Loss: 0.004738
Validation Loss: 0.00448681
Epoch [9/200], Train Loss: 0.004733
Validation Loss: 0.00448446
Epoch [10/200], Train Loss: 0.004736
Validation Loss: 0.00448144
Epoch [11/200], Train Loss: 0.004721
Validation Loss: 0.00448496
Epoch [12/200], Train Loss: 0.004725
Validation Loss: 0.00447720
Epoch [13/200], Train Loss: 0.004726
Validation Loss: 0.00448239
Epoch [14/200], Train Loss: 0.004714
Validation Loss: 0.00447948
Epoch [15/200], Train Loss: 0.004733
Validation Loss: 0.00447152
Epoch [16/200], Train Loss: 0.004715
Validation Loss: 0.00447077
Epoch [17/200], Train Loss: 0.004719
Validation Loss: 0.00447472
Epoch [18/200], Train Loss: 0.004722
Validation Loss: 0.00447270
Epoch [19/200], Train Loss: 0.004717
Validation Loss: 0.00446666
Epoch [20/200], Train Loss: 0.004730
Validation Loss: 0.00446799
Epoch [21/200], Train Loss: 0.004710
Validation Loss: 0.00446558
Epoch [22/200], Train Loss: 0.004735
Validation Loss: 0.00446739
Epoch [23/200], Train Loss: 0.004719
Validation Loss: 0.00446190
Epoch [24/200], Train Loss: 0.004716
Validation Loss: 0.00446522
Epoch [25/200], Train Loss: 0.004729
Validation Loss: 0.00446384
Epoch [26/200], Train Loss: 0.004726
Validation Loss: 0.00446622
Epoch [27/200], Train Loss: 0.004718
Validation Loss: 0.00446754
Epoch [28/200], Train Loss: 0.004741
Validation Loss: 0.00446211
Epoch [29/200], Train Loss: 0.004736
Validation Loss: 0.00446519
Epoch [30/200], Train Loss: 0.004722
Validation Loss: 0.00445850
Epoch [31/200], Train Loss: 0.004712
Validation Loss: 0.00445761
Epoch [32/200], Train Loss: 0.004729
Validation Loss: 0.00445970
Epoch [33/200], Train Loss: 0.004714
Validation Loss: 0.00445568
Epoch [34/200], Train Loss: 0.004713
Validation Loss: 0.00445822
Epoch [35/200], Train Loss: 0.004729
Validation Loss: 0.00445953
Epoch [36/200], Train Loss: 0.004711
Validation Loss: 0.00445814
Epoch [37/200], Train Loss: 0.004713
Validation Loss: 0.00445610
Epoch [38/200], Train Loss: 0.004722
Validation Loss: 0.00446102
Epoch [39/200], Train Loss: 0.004738
Validation Loss: 0.00446409
Epoch [40/200], Train Loss: 0.004735
Validation Loss: 0.00445445
Epoch [41/200], Train Loss: 0.004723
Validation Loss: 0.00445581
Epoch [42/200], Train Loss: 0.004721
Validation Loss: 0.00445452
Epoch [43/200], Train Loss: 0.004711
Validation Loss: 0.00445306
Epoch [44/200], Train Loss: 0.004741
Validation Loss: 0.00445292
Epoch [45/200], Train Loss: 0.004742
Validation Loss: 0.00444866
Epoch [46/200], Train Loss: 0.004720
Validation Loss: 0.00444543
Epoch [47/200], Train Loss: 0.004717
Validation Loss: 0.00444477
Epoch [48/200], Train Loss: 0.004724
Validation Loss: 0.00445551
Epoch [49/200], Train Loss: 0.004724
Validation Loss: 0.00444250
Epoch [50/200], Train Loss: 0.004695
Validation Loss: 0.00444372
Epoch [51/200], Train Loss: 0.004701
Validation Loss: 0.00443997
Epoch [52/200], Train Loss: 0.004710
Validation Loss: 0.00443613
Epoch [53/200], Train Loss: 0.004712
Validation Loss: 0.00443898
Epoch [54/200], Train Loss: 0.004687
Validation Loss: 0.00443969
Epoch [55/200], Train Loss: 0.004709
Validation Loss: 0.00443161
Epoch [56/200], Train Loss: 0.004685
Validation Loss: 0.00443276
Epoch [57/200], Train Loss: 0.004687
Validation Loss: 0.00443756
Epoch [58/200], Train Loss: 0.004682
Validation Loss: 0.00442458
Epoch [59/200], Train Loss: 0.004677
Validation Loss: 0.00441161
Epoch [60/200], Train Loss: 0.004697
Validation Loss: 0.00442233
Epoch [61/200], Train Loss: 0.004677
Validation Loss: 0.00439350
Epoch [62/200], Train Loss: 0.004648
Validation Loss: 0.00437080
Epoch [63/200], Train Loss: 0.004658
Validation Loss: 0.00434792
Epoch [64/200], Train Loss: 0.004608
Validation Loss: 0.00434041
Epoch [65/200], Train Loss: 0.004566
Validation Loss: 0.00427609
Epoch [66/200], Train Loss: 0.004546
Validation Loss: 0.00423563
Epoch [67/200], Train Loss: 0.004502
Validation Loss: 0.00416269
Epoch [68/200], Train Loss: 0.004398
Validation Loss: 0.00411012
Epoch [69/200], Train Loss: 0.004272
Validation Loss: 0.00376677
Epoch [70/200], Train Loss: 0.004041
Validation Loss: 0.00339902
Epoch [71/200], Train Loss: 0.003699
Validation Loss: 0.00305304
Epoch [72/200], Train Loss: 0.003343
Validation Loss: 0.00258850
Epoch [73/200], Train Loss: 0.002920
Validation Loss: 0.00232136
Epoch [74/200], Train Loss: 0.002663
Validation Loss: 0.00216682
Epoch [75/200], Train Loss: 0.002499
Validation Loss: 0.00232039
Epoch [76/200], Train Loss: 0.002363
Validation Loss: 0.00197639
Epoch [77/200], Train Loss: 0.002238
Validation Loss: 0.00182841
Epoch [78/200], Train Loss: 0.002140
Validation Loss: 0.00174478
Epoch [79/200], Train Loss: 0.002070
Validation Loss: 0.00171856
Epoch [80/200], Train Loss: 0.001996
Validation Loss: 0.00163179
Epoch [81/200], Train Loss: 0.001908
Validation Loss: 0.00163314
Epoch [82/200], Train Loss: 0.001865
Validation Loss: 0.00152635
Epoch [83/200], Train Loss: 0.001807
Validation Loss: 0.00146636
Epoch [84/200], Train Loss: 0.001760
Validation Loss: 0.00145997
Epoch [85/200], Train Loss: 0.001694
Validation Loss: 0.00141549
Epoch [86/200], Train Loss: 0.001637
Validation Loss: 0.00137610
Epoch [87/200], Train Loss: 0.001611
Validation Loss: 0.00141674
Epoch [88/200], Train Loss: 0.001575
Validation Loss: 0.00138389
Epoch [89/200], Train Loss: 0.001538
Validation Loss: 0.00132145
Epoch [90/200], Train Loss: 0.001499
Validation Loss: 0.00126094
Epoch [91/200], Train Loss: 0.001464
Validation Loss: 0.00124257
Epoch [92/200], Train Loss: 0.001450
Validation Loss: 0.00122940
Epoch [93/200], Train Loss: 0.001412
Validation Loss: 0.00121435
Epoch [94/200], Train Loss: 0.001387
Validation Loss: 0.00120465
Epoch [95/200], Train Loss: 0.001360
Validation Loss: 0.00117598
Epoch [96/200], Train Loss: 0.001345
Validation Loss: 0.00119820
Epoch [97/200], Train Loss: 0.001328
Validation Loss: 0.00114881
Epoch [98/200], Train Loss: 0.001300
Validation Loss: 0.00111612
Epoch [99/200], Train Loss: 0.001269
Validation Loss: 0.00108683
Epoch [100/200], Train Loss: 0.001258
Validation Loss: 0.00109167
Epoch [101/200], Train Loss: 0.001237
Validation Loss: 0.00106081
Epoch [102/200], Train Loss: 0.001209
Validation Loss: 0.00108110
Epoch [103/200], Train Loss: 0.001201
Validation Loss: 0.00103373
Epoch [104/200], Train Loss: 0.001168
Validation Loss: 0.00102202
Epoch [105/200], Train Loss: 0.001172
Validation Loss: 0.00104349
Epoch [106/200], Train Loss: 0.001174
Validation Loss: 0.00105086
Epoch [107/200], Train Loss: 0.001138
Validation Loss: 0.00098789
Epoch [108/200], Train Loss: 0.001103
Validation Loss: 0.00101409
Epoch [109/200], Train Loss: 0.001094
Validation Loss: 0.00097752
Epoch [110/200], Train Loss: 0.001090
Validation Loss: 0.00098398
Epoch [111/200], Train Loss: 0.001073
Validation Loss: 0.00096009
Epoch [112/200], Train Loss: 0.001059
Validation Loss: 0.00095248
Epoch [113/200], Train Loss: 0.001037
Validation Loss: 0.00093583
Epoch [114/200], Train Loss: 0.001019
Validation Loss: 0.00091101
Epoch [115/200], Train Loss: 0.001023
Validation Loss: 0.00095384
Epoch [116/200], Train Loss: 0.001004
Validation Loss: 0.00090106
Epoch [117/200], Train Loss: 0.000968
Validation Loss: 0.00088600
Epoch [118/200], Train Loss: 0.000985
Validation Loss: 0.00085198
Epoch [119/200], Train Loss: 0.000939
Validation Loss: 0.00082514
Epoch [120/200], Train Loss: 0.000939
Validation Loss: 0.00082880
Epoch [121/200], Train Loss: 0.000919
Validation Loss: 0.00080894
Epoch [122/200], Train Loss: 0.000984
Validation Loss: 0.00085441
Epoch [123/200], Train Loss: 0.000903
Validation Loss: 0.00077808
Epoch [124/200], Train Loss: 0.000893
Validation Loss: 0.00078476
Epoch [125/200], Train Loss: 0.000886
Validation Loss: 0.00076688
Epoch [126/200], Train Loss: 0.000857
Validation Loss: 0.00076029
Epoch [127/200], Train Loss: 0.000865
Validation Loss: 0.00076348
Epoch [128/200], Train Loss: 0.000843
Validation Loss: 0.00074081
Epoch [129/200], Train Loss: 0.000843
Validation Loss: 0.00073175
Epoch [130/200], Train Loss: 0.000832
Validation Loss: 0.00073667
Epoch [131/200], Train Loss: 0.000838
Validation Loss: 0.00071816
Epoch [132/200], Train Loss: 0.000821
Validation Loss: 0.00072383
Epoch [133/200], Train Loss: 0.000809
Validation Loss: 0.00072213
Epoch [134/200], Train Loss: 0.000801
Validation Loss: 0.00070631
Epoch [135/200], Train Loss: 0.000804
Validation Loss: 0.00069567
Epoch [136/200], Train Loss: 0.000794
Validation Loss: 0.00068641
Epoch [137/200], Train Loss: 0.000811
Validation Loss: 0.00073286
Epoch [138/200], Train Loss: 0.000802
Validation Loss: 0.00067577
Epoch [139/200], Train Loss: 0.000784
Validation Loss: 0.00068535
Epoch [140/200], Train Loss: 0.000776
Validation Loss: 0.00070415
Epoch [141/200], Train Loss: 0.000765
Validation Loss: 0.00066838
Epoch [142/200], Train Loss: 0.000769
Validation Loss: 0.00065513
Epoch [143/200], Train Loss: 0.000765
Validation Loss: 0.00066388
Epoch [144/200], Train Loss: 0.000758
Validation Loss: 0.00065558
Epoch [145/200], Train Loss: 0.000746
Validation Loss: 0.00066032
Epoch [146/200], Train Loss: 0.000750
Validation Loss: 0.00063942
Epoch [147/200], Train Loss: 0.000746
Validation Loss: 0.00065122
Epoch [148/200], Train Loss: 0.000738
Validation Loss: 0.00063344
Epoch [149/200], Train Loss: 0.000744
Validation Loss: 0.00063979
Epoch [150/200], Train Loss: 0.000741
Validation Loss: 0.00062958
Epoch [151/200], Train Loss: 0.000731
Validation Loss: 0.00062498
Epoch [152/200], Train Loss: 0.000733
Validation Loss: 0.00062871
Epoch [153/200], Train Loss: 0.000733
Validation Loss: 0.00062394
Epoch [154/200], Train Loss: 0.000714
Validation Loss: 0.00061891
Epoch [155/200], Train Loss: 0.000722
Validation Loss: 0.00061532
Epoch [156/200], Train Loss: 0.000718
Validation Loss: 0.00061368
Epoch [157/200], Train Loss: 0.000712
Validation Loss: 0.00062935
Epoch [158/200], Train Loss: 0.000715
Validation Loss: 0.00061584
Epoch [159/200], Train Loss: 0.000710
Validation Loss: 0.00059903
Epoch [160/200], Train Loss: 0.000690
Validation Loss: 0.00058272
Epoch [161/200], Train Loss: 0.000696
Validation Loss: 0.00059275
Epoch [162/200], Train Loss: 0.000717
Validation Loss: 0.00063471
Epoch [163/200], Train Loss: 0.000705
Validation Loss: 0.00065137
Epoch [164/200], Train Loss: 0.000699
Validation Loss: 0.00058650
Epoch [165/200], Train Loss: 0.000689
Validation Loss: 0.00058655
Epoch [166/200], Train Loss: 0.000674
Validation Loss: 0.00057849
Epoch [167/200], Train Loss: 0.000680
Validation Loss: 0.00057874
Epoch [168/200], Train Loss: 0.000673
Validation Loss: 0.00057150
Epoch [169/200], Train Loss: 0.000668
Validation Loss: 0.00058719
Epoch [170/200], Train Loss: 0.000663
Validation Loss: 0.00056706
Epoch [171/200], Train Loss: 0.000664
Validation Loss: 0.00056750
Epoch [172/200], Train Loss: 0.000668
Validation Loss: 0.00058310
Epoch [173/200], Train Loss: 0.000663
Validation Loss: 0.00055959
Epoch [174/200], Train Loss: 0.000666
Validation Loss: 0.00057277
Epoch [175/200], Train Loss: 0.000660
Validation Loss: 0.00057520
Epoch [176/200], Train Loss: 0.000667
Validation Loss: 0.00057221
Epoch [177/200], Train Loss: 0.000646
Validation Loss: 0.00055689
Epoch [178/200], Train Loss: 0.000655
Validation Loss: 0.00055999
Epoch [179/200], Train Loss: 0.000648
Validation Loss: 0.00055318
Epoch [180/200], Train Loss: 0.000655
Validation Loss: 0.00054432
Epoch [181/200], Train Loss: 0.000646
Validation Loss: 0.00055584
Epoch [182/200], Train Loss: 0.000645
Validation Loss: 0.00054253
Epoch [183/200], Train Loss: 0.000645
Validation Loss: 0.00053988
Epoch [184/200], Train Loss: 0.000649
Validation Loss: 0.00055128
Epoch [185/200], Train Loss: 0.000644
Validation Loss: 0.00054158
Epoch [186/200], Train Loss: 0.000640
Validation Loss: 0.00053863
Epoch [187/200], Train Loss: 0.000631
Validation Loss: 0.00053225
Epoch [188/200], Train Loss: 0.000635
Validation Loss: 0.00055448
Epoch [189/200], Train Loss: 0.000646
Validation Loss: 0.00054031
Epoch [190/200], Train Loss: 0.000636
Validation Loss: 0.00053573
Epoch [191/200], Train Loss: 0.000625
Validation Loss: 0.00052691
Epoch [192/200], Train Loss: 0.000625
Validation Loss: 0.00052769
Epoch [193/200], Train Loss: 0.000634
Validation Loss: 0.00056883
Epoch [194/200], Train Loss: 0.000630
Validation Loss: 0.00053151
Epoch [195/200], Train Loss: 0.000636
Validation Loss: 0.00052348
Epoch [196/200], Train Loss: 0.000623
Validation Loss: 0.00052938
Epoch [197/200], Train Loss: 0.000627
Validation Loss: 0.00052670
Epoch [198/200], Train Loss: 0.000621
Validation Loss: 0.00052792
Epoch [199/200], Train Loss: 0.000612
Validation Loss: 0.00054752
Epoch [200/200], Train Loss: 0.000620
Validation Loss: 0.00052606

Evaluating model for: Lamp
Run 55/144 completed in 2019.41 seconds with: {'MAE': np.float32(0.566885), 'MSE': np.float32(19.987236), 'RMSE': np.float32(4.470709), 'SAE': np.float32(0.059512816), 'NDE': np.float32(0.3417282)}

Run 56/144: hidden=128, seq_len=120, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.005118
Validation Loss: 0.00460121
Epoch [2/200], Train Loss: 0.004872
Validation Loss: 0.00459447
Epoch [3/200], Train Loss: 0.004878
Validation Loss: 0.00456072
Epoch [4/200], Train Loss: 0.004791
Validation Loss: 0.00452050
Epoch [5/200], Train Loss: 0.004786
Validation Loss: 0.00450232
Epoch [6/200], Train Loss: 0.004770
Validation Loss: 0.00449722
Epoch [7/200], Train Loss: 0.004768
Validation Loss: 0.00449505
Epoch [8/200], Train Loss: 0.004770
Validation Loss: 0.00449310
Epoch [9/200], Train Loss: 0.004773
Validation Loss: 0.00449248
Epoch [10/200], Train Loss: 0.004795
Validation Loss: 0.00450037
Epoch [11/200], Train Loss: 0.004755
Validation Loss: 0.00449270
Epoch [12/200], Train Loss: 0.004743
Validation Loss: 0.00449975
Epoch [13/200], Train Loss: 0.004753
Validation Loss: 0.00448627
Epoch [14/200], Train Loss: 0.004738
Validation Loss: 0.00448469
Epoch [15/200], Train Loss: 0.004756
Validation Loss: 0.00448555
Epoch [16/200], Train Loss: 0.004759
Validation Loss: 0.00448388
Epoch [17/200], Train Loss: 0.004730
Validation Loss: 0.00448299
Epoch [18/200], Train Loss: 0.004761
Validation Loss: 0.00447933
Epoch [19/200], Train Loss: 0.004740
Validation Loss: 0.00447514
Epoch [20/200], Train Loss: 0.004740
Validation Loss: 0.00447381
Epoch [21/200], Train Loss: 0.004737
Validation Loss: 0.00447341
Epoch [22/200], Train Loss: 0.004737
Validation Loss: 0.00446941
Epoch [23/200], Train Loss: 0.004745
Validation Loss: 0.00447066
Epoch [24/200], Train Loss: 0.004730
Validation Loss: 0.00447093
Epoch [25/200], Train Loss: 0.004750
Validation Loss: 0.00446778
Epoch [26/200], Train Loss: 0.004742
Validation Loss: 0.00446421
Epoch [27/200], Train Loss: 0.004720
Validation Loss: 0.00446384
Epoch [28/200], Train Loss: 0.004726
Validation Loss: 0.00446168
Epoch [29/200], Train Loss: 0.004745
Validation Loss: 0.00446199
Epoch [30/200], Train Loss: 0.004729
Validation Loss: 0.00446394
Epoch [31/200], Train Loss: 0.004728
Validation Loss: 0.00446099
Epoch [32/200], Train Loss: 0.004745
Validation Loss: 0.00446458
Epoch [33/200], Train Loss: 0.004712
Validation Loss: 0.00446446
Epoch [34/200], Train Loss: 0.004720
Validation Loss: 0.00446402
Epoch [35/200], Train Loss: 0.004706
Validation Loss: 0.00446253
Epoch [36/200], Train Loss: 0.004719
Validation Loss: 0.00446355
Epoch [37/200], Train Loss: 0.004729
Validation Loss: 0.00445969
Epoch [38/200], Train Loss: 0.004711
Validation Loss: 0.00445988
Epoch [39/200], Train Loss: 0.004721
Validation Loss: 0.00446150
Epoch [40/200], Train Loss: 0.004734
Validation Loss: 0.00445880
Epoch [41/200], Train Loss: 0.004716
Validation Loss: 0.00445596
Epoch [42/200], Train Loss: 0.004716
Validation Loss: 0.00445571
Epoch [43/200], Train Loss: 0.004711
Validation Loss: 0.00445851
Epoch [44/200], Train Loss: 0.004725
Validation Loss: 0.00445813
Epoch [45/200], Train Loss: 0.004729
Validation Loss: 0.00446133
Epoch [46/200], Train Loss: 0.004739
Validation Loss: 0.00445841
Epoch [47/200], Train Loss: 0.004739
Validation Loss: 0.00445567
Epoch [48/200], Train Loss: 0.004716
Validation Loss: 0.00445685
Epoch [49/200], Train Loss: 0.004727
Validation Loss: 0.00444956
Epoch [50/200], Train Loss: 0.004710
Validation Loss: 0.00445255
Epoch [51/200], Train Loss: 0.004719
Validation Loss: 0.00444862
Epoch [52/200], Train Loss: 0.004711
Validation Loss: 0.00445634
Epoch [53/200], Train Loss: 0.004714
Validation Loss: 0.00444939
Epoch [54/200], Train Loss: 0.004708
Validation Loss: 0.00444963
Epoch [55/200], Train Loss: 0.004718
Validation Loss: 0.00444598
Epoch [56/200], Train Loss: 0.004704
Validation Loss: 0.00444475
Epoch [57/200], Train Loss: 0.004739
Validation Loss: 0.00444971
Epoch [58/200], Train Loss: 0.004715
Validation Loss: 0.00444658
Epoch [59/200], Train Loss: 0.004720
Validation Loss: 0.00444393
Epoch [60/200], Train Loss: 0.004731
Validation Loss: 0.00444272
Epoch [61/200], Train Loss: 0.004694
Validation Loss: 0.00444215
Epoch [62/200], Train Loss: 0.004707
Validation Loss: 0.00444444
Epoch [63/200], Train Loss: 0.004720
Validation Loss: 0.00443648
Epoch [64/200], Train Loss: 0.004690
Validation Loss: 0.00443443
Epoch [65/200], Train Loss: 0.004705
Validation Loss: 0.00443265
Epoch [66/200], Train Loss: 0.004693
Validation Loss: 0.00442192
Epoch [67/200], Train Loss: 0.004694
Validation Loss: 0.00442102
Epoch [68/200], Train Loss: 0.004678
Validation Loss: 0.00442636
Epoch [69/200], Train Loss: 0.004678
Validation Loss: 0.00440350
Epoch [70/200], Train Loss: 0.004647
Validation Loss: 0.00438701
Epoch [71/200], Train Loss: 0.004638
Validation Loss: 0.00436316
Epoch [72/200], Train Loss: 0.004598
Validation Loss: 0.00431313
Epoch [73/200], Train Loss: 0.004573
Validation Loss: 0.00424926
Epoch [74/200], Train Loss: 0.004492
Validation Loss: 0.00412827
Epoch [75/200], Train Loss: 0.004289
Validation Loss: 0.00372483
Epoch [76/200], Train Loss: 0.003944
Validation Loss: 0.00331340
Epoch [77/200], Train Loss: 0.003555
Validation Loss: 0.00299496
Epoch [78/200], Train Loss: 0.003302
Validation Loss: 0.00281233
Epoch [79/200], Train Loss: 0.003066
Validation Loss: 0.00260389
Epoch [80/200], Train Loss: 0.002927
Validation Loss: 0.00244365
Epoch [81/200], Train Loss: 0.002760
Validation Loss: 0.00236603
Epoch [82/200], Train Loss: 0.002675
Validation Loss: 0.00226053
Epoch [83/200], Train Loss: 0.002492
Validation Loss: 0.00227352
Epoch [84/200], Train Loss: 0.002459
Validation Loss: 0.00220856
Epoch [85/200], Train Loss: 0.002305
Validation Loss: 0.00193895
Epoch [86/200], Train Loss: 0.002213
Validation Loss: 0.00188228
Epoch [87/200], Train Loss: 0.002136
Validation Loss: 0.00180620
Epoch [88/200], Train Loss: 0.002022
Validation Loss: 0.00174444
Epoch [89/200], Train Loss: 0.001963
Validation Loss: 0.00165708
Epoch [90/200], Train Loss: 0.001907
Validation Loss: 0.00159679
Epoch [91/200], Train Loss: 0.001818
Validation Loss: 0.00155229
Epoch [92/200], Train Loss: 0.001784
Validation Loss: 0.00152168
Epoch [93/200], Train Loss: 0.001747
Validation Loss: 0.00149140
Epoch [94/200], Train Loss: 0.001694
Validation Loss: 0.00144231
Epoch [95/200], Train Loss: 0.001663
Validation Loss: 0.00139088
Epoch [96/200], Train Loss: 0.001617
Validation Loss: 0.00137485
Epoch [97/200], Train Loss: 0.001594
Validation Loss: 0.00135371
Epoch [98/200], Train Loss: 0.001557
Validation Loss: 0.00130907
Epoch [99/200], Train Loss: 0.001517
Validation Loss: 0.00130571
Epoch [100/200], Train Loss: 0.001500
Validation Loss: 0.00131163
Epoch [101/200], Train Loss: 0.001512
Validation Loss: 0.00125745
Epoch [102/200], Train Loss: 0.001446
Validation Loss: 0.00125367
Epoch [103/200], Train Loss: 0.001426
Validation Loss: 0.00120632
Epoch [104/200], Train Loss: 0.001388
Validation Loss: 0.00118594
Epoch [105/200], Train Loss: 0.001351
Validation Loss: 0.00117173
Epoch [106/200], Train Loss: 0.001328
Validation Loss: 0.00114880
Epoch [107/200], Train Loss: 0.001300
Validation Loss: 0.00122416
Epoch [108/200], Train Loss: 0.001287
Validation Loss: 0.00108099
Epoch [109/200], Train Loss: 0.001238
Validation Loss: 0.00105907
Epoch [110/200], Train Loss: 0.001218
Validation Loss: 0.00104994
Epoch [111/200], Train Loss: 0.001194
Validation Loss: 0.00104043
Epoch [112/200], Train Loss: 0.001173
Validation Loss: 0.00099790
Epoch [113/200], Train Loss: 0.001159
Validation Loss: 0.00098303
Epoch [114/200], Train Loss: 0.001119
Validation Loss: 0.00098578
Epoch [115/200], Train Loss: 0.001121
Validation Loss: 0.00094038
Epoch [116/200], Train Loss: 0.001071
Validation Loss: 0.00090243
Epoch [117/200], Train Loss: 0.001061
Validation Loss: 0.00087978
Epoch [118/200], Train Loss: 0.001030
Validation Loss: 0.00084448
Epoch [119/200], Train Loss: 0.001025
Validation Loss: 0.00084094
Epoch [120/200], Train Loss: 0.000986
Validation Loss: 0.00082047
Epoch [121/200], Train Loss: 0.000957
Validation Loss: 0.00081693
Epoch [122/200], Train Loss: 0.000949
Validation Loss: 0.00077409
Epoch [123/200], Train Loss: 0.000921
Validation Loss: 0.00077023
Epoch [124/200], Train Loss: 0.000913
Validation Loss: 0.00083293
Epoch [125/200], Train Loss: 0.000924
Validation Loss: 0.00074611
Epoch [126/200], Train Loss: 0.000905
Validation Loss: 0.00075782
Epoch [127/200], Train Loss: 0.000877
Validation Loss: 0.00111675
Epoch [128/200], Train Loss: 0.000908
Validation Loss: 0.00072260
Epoch [129/200], Train Loss: 0.000865
Validation Loss: 0.00071268
Epoch [130/200], Train Loss: 0.000840
Validation Loss: 0.00070122
Epoch [131/200], Train Loss: 0.000855
Validation Loss: 0.00070300
Epoch [132/200], Train Loss: 0.000825
Validation Loss: 0.00070806
Epoch [133/200], Train Loss: 0.000836
Validation Loss: 0.00068227
Epoch [134/200], Train Loss: 0.000813
Validation Loss: 0.00067556
Epoch [135/200], Train Loss: 0.000797
Validation Loss: 0.00066530
Epoch [136/200], Train Loss: 0.000801
Validation Loss: 0.00070337
Epoch [137/200], Train Loss: 0.000787
Validation Loss: 0.00066386
Epoch [138/200], Train Loss: 0.000772
Validation Loss: 0.00066064
Epoch [139/200], Train Loss: 0.000767
Validation Loss: 0.00065370
Epoch [140/200], Train Loss: 0.000768
Validation Loss: 0.00064999
Epoch [141/200], Train Loss: 0.000757
Validation Loss: 0.00065200
Epoch [142/200], Train Loss: 0.000761
Validation Loss: 0.00063080
Epoch [143/200], Train Loss: 0.000755
Validation Loss: 0.00062292
Epoch [144/200], Train Loss: 0.000736
Validation Loss: 0.00062131
Epoch [145/200], Train Loss: 0.000750
Validation Loss: 0.00064951
Epoch [146/200], Train Loss: 0.000739
Validation Loss: 0.00062162
Epoch [147/200], Train Loss: 0.000719
Validation Loss: 0.00061001
Epoch [148/200], Train Loss: 0.000715
Validation Loss: 0.00060395
Epoch [149/200], Train Loss: 0.000707
Validation Loss: 0.00060830
Epoch [150/200], Train Loss: 0.000704
Validation Loss: 0.00061307
Epoch [151/200], Train Loss: 0.000707
Validation Loss: 0.00060872
Epoch [152/200], Train Loss: 0.000697
Validation Loss: 0.00059849
Epoch [153/200], Train Loss: 0.000695
Validation Loss: 0.00059088
Epoch [154/200], Train Loss: 0.000675
Validation Loss: 0.00058692
Epoch [155/200], Train Loss: 0.000686
Validation Loss: 0.00058568
Epoch [156/200], Train Loss: 0.000690
Validation Loss: 0.00059319
Epoch [157/200], Train Loss: 0.000672
Validation Loss: 0.00058725
Epoch [158/200], Train Loss: 0.000678
Validation Loss: 0.00058590
Epoch [159/200], Train Loss: 0.000669
Validation Loss: 0.00058523
Epoch [160/200], Train Loss: 0.000668
Validation Loss: 0.00058252
Epoch [161/200], Train Loss: 0.000666
Validation Loss: 0.00058943
Epoch [162/200], Train Loss: 0.000693
Validation Loss: 0.00057417
Epoch [163/200], Train Loss: 0.000655
Validation Loss: 0.00056765
Epoch [164/200], Train Loss: 0.000647
Validation Loss: 0.00056445
Epoch [165/200], Train Loss: 0.000640
Validation Loss: 0.00057763
Epoch [166/200], Train Loss: 0.000640
Validation Loss: 0.00056172
Epoch [167/200], Train Loss: 0.000649
Validation Loss: 0.00063909
Epoch [168/200], Train Loss: 0.000659
Validation Loss: 0.00056000
Epoch [169/200], Train Loss: 0.000629
Validation Loss: 0.00055747
Epoch [170/200], Train Loss: 0.000632
Validation Loss: 0.00055311
Epoch [171/200], Train Loss: 0.000628
Validation Loss: 0.00054935
Epoch [172/200], Train Loss: 0.000628
Validation Loss: 0.00054672
Epoch [173/200], Train Loss: 0.000638
Validation Loss: 0.00056029
Epoch [174/200], Train Loss: 0.000618
Validation Loss: 0.00054532
Epoch [175/200], Train Loss: 0.000611
Validation Loss: 0.00054210
Epoch [176/200], Train Loss: 0.000617
Validation Loss: 0.00054156
Epoch [177/200], Train Loss: 0.000613
Validation Loss: 0.00054300
Epoch [178/200], Train Loss: 0.000611
Validation Loss: 0.00054016
Epoch [179/200], Train Loss: 0.000610
Validation Loss: 0.00053998
Epoch [180/200], Train Loss: 0.000615
Validation Loss: 0.00053146
Epoch [181/200], Train Loss: 0.000604
Validation Loss: 0.00053430
Epoch [182/200], Train Loss: 0.000598
Validation Loss: 0.00052973
Epoch [183/200], Train Loss: 0.000607
Validation Loss: 0.00053738
Epoch [184/200], Train Loss: 0.000609
Validation Loss: 0.00053086
Epoch [185/200], Train Loss: 0.000594
Validation Loss: 0.00052399
Epoch [186/200], Train Loss: 0.000592
Validation Loss: 0.00052081
Epoch [187/200], Train Loss: 0.000591
Validation Loss: 0.00052958
Epoch [188/200], Train Loss: 0.000595
Validation Loss: 0.00052256
Epoch [189/200], Train Loss: 0.000576
Validation Loss: 0.00053501
Epoch [190/200], Train Loss: 0.000588
Validation Loss: 0.00051333
Epoch [191/200], Train Loss: 0.000586
Validation Loss: 0.00052787
Epoch [192/200], Train Loss: 0.000580
Validation Loss: 0.00051548
Epoch [193/200], Train Loss: 0.000574
Validation Loss: 0.00051442
Epoch [194/200], Train Loss: 0.000572
Validation Loss: 0.00050608
Epoch [195/200], Train Loss: 0.000570
Validation Loss: 0.00051032
Epoch [196/200], Train Loss: 0.001367
Validation Loss: 0.00221306
Epoch [197/200], Train Loss: 0.001444
Validation Loss: 0.00104755
Epoch [198/200], Train Loss: 0.001117
Validation Loss: 0.00094030
Epoch [199/200], Train Loss: 0.001018
Validation Loss: 0.00087716
Epoch [200/200], Train Loss: 0.000950
Validation Loss: 0.00081941

Evaluating model for: Lamp
Run 56/144 completed in 2088.38 seconds with: {'MAE': np.float32(0.781332), 'MSE': np.float32(30.779127), 'RMSE': np.float32(5.547894), 'SAE': np.float32(0.05096706), 'NDE': np.float32(0.42406538)}

Run 57/144: hidden=128, seq_len=120, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.004908
Validation Loss: 0.00657465
Epoch [2/200], Train Loss: 0.004724
Validation Loss: 0.00655633
Epoch [3/200], Train Loss: 0.004704
Validation Loss: 0.00654462
Epoch [4/200], Train Loss: 0.004704
Validation Loss: 0.00651793
Epoch [5/200], Train Loss: 0.004682
Validation Loss: 0.00650869
Epoch [6/200], Train Loss: 0.004683
Validation Loss: 0.00651018
Epoch [7/200], Train Loss: 0.004681
Validation Loss: 0.00648599
Epoch [8/200], Train Loss: 0.004665
Validation Loss: 0.00649151
Epoch [9/200], Train Loss: 0.004668
Validation Loss: 0.00647925
Epoch [10/200], Train Loss: 0.004674
Validation Loss: 0.00648644
Epoch [11/200], Train Loss: 0.004662
Validation Loss: 0.00649214
Epoch [12/200], Train Loss: 0.004665
Validation Loss: 0.00648922
Epoch [13/200], Train Loss: 0.004658
Validation Loss: 0.00649060
Epoch [14/200], Train Loss: 0.004658
Validation Loss: 0.00648106
Epoch [15/200], Train Loss: 0.004660
Validation Loss: 0.00648974
Epoch [16/200], Train Loss: 0.004649
Validation Loss: 0.00647788
Epoch [17/200], Train Loss: 0.004664
Validation Loss: 0.00646914
Epoch [18/200], Train Loss: 0.004643
Validation Loss: 0.00647087
Epoch [19/200], Train Loss: 0.004656
Validation Loss: 0.00648610
Epoch [20/200], Train Loss: 0.004654
Validation Loss: 0.00646182
Epoch [21/200], Train Loss: 0.004645
Validation Loss: 0.00646428
Epoch [22/200], Train Loss: 0.004646
Validation Loss: 0.00645930
Epoch [23/200], Train Loss: 0.004648
Validation Loss: 0.00645317
Epoch [24/200], Train Loss: 0.004634
Validation Loss: 0.00645807
Epoch [25/200], Train Loss: 0.004632
Validation Loss: 0.00646046
Epoch [26/200], Train Loss: 0.004632
Validation Loss: 0.00645946
Epoch [27/200], Train Loss: 0.004618
Validation Loss: 0.00641583
Epoch [28/200], Train Loss: 0.004626
Validation Loss: 0.00644474
Epoch [29/200], Train Loss: 0.004624
Validation Loss: 0.00646096
Epoch [30/200], Train Loss: 0.004636
Validation Loss: 0.00641398
Epoch [31/200], Train Loss: 0.004604
Validation Loss: 0.00640789
Epoch [32/200], Train Loss: 0.004598
Validation Loss: 0.00638636
Epoch [33/200], Train Loss: 0.004600
Validation Loss: 0.00638253
Epoch [34/200], Train Loss: 0.004604
Validation Loss: 0.00637867
Epoch [35/200], Train Loss: 0.004612
Validation Loss: 0.00637411
Epoch [36/200], Train Loss: 0.004595
Validation Loss: 0.00635378
Epoch [37/200], Train Loss: 0.004582
Validation Loss: 0.00634373
Epoch [38/200], Train Loss: 0.004577
Validation Loss: 0.00633416
Epoch [39/200], Train Loss: 0.004565
Validation Loss: 0.00632406
Epoch [40/200], Train Loss: 0.004561
Validation Loss: 0.00629987
Epoch [41/200], Train Loss: 0.004546
Validation Loss: 0.00629968
Epoch [42/200], Train Loss: 0.004533
Validation Loss: 0.00631914
Epoch [43/200], Train Loss: 0.004522
Validation Loss: 0.00622726
Epoch [44/200], Train Loss: 0.004512
Validation Loss: 0.00619855
Epoch [45/200], Train Loss: 0.004497
Validation Loss: 0.00618459
Epoch [46/200], Train Loss: 0.004462
Validation Loss: 0.00612741
Epoch [47/200], Train Loss: 0.004461
Validation Loss: 0.00618257
Epoch [48/200], Train Loss: 0.004424
Validation Loss: 0.00606797
Epoch [49/200], Train Loss: 0.004430
Validation Loss: 0.00602538
Epoch [50/200], Train Loss: 0.004372
Validation Loss: 0.00597071
Epoch [51/200], Train Loss: 0.004360
Validation Loss: 0.00592624
Epoch [52/200], Train Loss: 0.004308
Validation Loss: 0.00586925
Epoch [53/200], Train Loss: 0.004275
Validation Loss: 0.00579417
Epoch [54/200], Train Loss: 0.004248
Validation Loss: 0.00576870
Epoch [55/200], Train Loss: 0.004240
Validation Loss: 0.00570432
Epoch [56/200], Train Loss: 0.004147
Validation Loss: 0.00562405
Epoch [57/200], Train Loss: 0.004115
Validation Loss: 0.00565815
Epoch [58/200], Train Loss: 0.004074
Validation Loss: 0.00551024
Epoch [59/200], Train Loss: 0.004034
Validation Loss: 0.00545965
Epoch [60/200], Train Loss: 0.004027
Validation Loss: 0.00532035
Epoch [61/200], Train Loss: 0.003966
Validation Loss: 0.00524416
Epoch [62/200], Train Loss: 0.003922
Validation Loss: 0.00525594
Epoch [63/200], Train Loss: 0.003868
Validation Loss: 0.00511821
Epoch [64/200], Train Loss: 0.003825
Validation Loss: 0.00501637
Epoch [65/200], Train Loss: 0.003762
Validation Loss: 0.00497077
Epoch [66/200], Train Loss: 0.003709
Validation Loss: 0.00486098
Epoch [67/200], Train Loss: 0.003648
Validation Loss: 0.00489119
Epoch [68/200], Train Loss: 0.003608
Validation Loss: 0.00467681
Epoch [69/200], Train Loss: 0.003510
Validation Loss: 0.00453741
Epoch [70/200], Train Loss: 0.003425
Validation Loss: 0.00445791
Epoch [71/200], Train Loss: 0.003339
Validation Loss: 0.00443867
Epoch [72/200], Train Loss: 0.003277
Validation Loss: 0.00433138
Epoch [73/200], Train Loss: 0.003188
Validation Loss: 0.00418995
Epoch [74/200], Train Loss: 0.003138
Validation Loss: 0.00401692
Epoch [75/200], Train Loss: 0.003036
Validation Loss: 0.00398738
Epoch [76/200], Train Loss: 0.002988
Validation Loss: 0.00382350
Epoch [77/200], Train Loss: 0.002930
Validation Loss: 0.00374299
Epoch [78/200], Train Loss: 0.002861
Validation Loss: 0.00372803
Epoch [79/200], Train Loss: 0.002849
Validation Loss: 0.00364351
Epoch [80/200], Train Loss: 0.002772
Validation Loss: 0.00351793
Epoch [81/200], Train Loss: 0.002727
Validation Loss: 0.00357461
Epoch [82/200], Train Loss: 0.002654
Validation Loss: 0.00343211
Epoch [83/200], Train Loss: 0.002630
Validation Loss: 0.00343765
Epoch [84/200], Train Loss: 0.002597
Validation Loss: 0.00341878
Epoch [85/200], Train Loss: 0.002571
Validation Loss: 0.00334232
Epoch [86/200], Train Loss: 0.002509
Validation Loss: 0.00330711
Epoch [87/200], Train Loss: 0.002514
Validation Loss: 0.00331040
Epoch [88/200], Train Loss: 0.002457
Validation Loss: 0.00324987
Epoch [89/200], Train Loss: 0.002429
Validation Loss: 0.00318723
Epoch [90/200], Train Loss: 0.002388
Validation Loss: 0.00321247
Epoch [91/200], Train Loss: 0.002378
Validation Loss: 0.00320053
Epoch [92/200], Train Loss: 0.002354
Validation Loss: 0.00311240
Epoch [93/200], Train Loss: 0.002330
Validation Loss: 0.00312001
Epoch [94/200], Train Loss: 0.002294
Validation Loss: 0.00306247
Epoch [95/200], Train Loss: 0.002274
Validation Loss: 0.00308232
Epoch [96/200], Train Loss: 0.002260
Validation Loss: 0.00299803
Epoch [97/200], Train Loss: 0.002231
Validation Loss: 0.00298798
Epoch [98/200], Train Loss: 0.002216
Validation Loss: 0.00299947
Epoch [99/200], Train Loss: 0.002204
Validation Loss: 0.00299030
Epoch [100/200], Train Loss: 0.002178
Validation Loss: 0.00295663
Epoch [101/200], Train Loss: 0.002157
Validation Loss: 0.00294386
Epoch [102/200], Train Loss: 0.002133
Validation Loss: 0.00293229
Epoch [103/200], Train Loss: 0.002146
Validation Loss: 0.00291271
Epoch [104/200], Train Loss: 0.002122
Validation Loss: 0.00288329
Epoch [105/200], Train Loss: 0.002097
Validation Loss: 0.00289479
Epoch [106/200], Train Loss: 0.002082
Validation Loss: 0.00292820
Epoch [107/200], Train Loss: 0.002084
Validation Loss: 0.00285177
Epoch [108/200], Train Loss: 0.002061
Validation Loss: 0.00280129
Epoch [109/200], Train Loss: 0.002054
Validation Loss: 0.00279835
Epoch [110/200], Train Loss: 0.002025
Validation Loss: 0.00280780
Epoch [111/200], Train Loss: 0.002004
Validation Loss: 0.00276063
Epoch [112/200], Train Loss: 0.002001
Validation Loss: 0.00275413
Epoch [113/200], Train Loss: 0.001997
Validation Loss: 0.00275704
Epoch [114/200], Train Loss: 0.001973
Validation Loss: 0.00270156
Epoch [115/200], Train Loss: 0.001965
Validation Loss: 0.00268099
Epoch [116/200], Train Loss: 0.001948
Validation Loss: 0.00270761
Epoch [117/200], Train Loss: 0.001940
Validation Loss: 0.00264760
Epoch [118/200], Train Loss: 0.001932
Validation Loss: 0.00267173
Epoch [119/200], Train Loss: 0.001917
Validation Loss: 0.00263658
Epoch [120/200], Train Loss: 0.001895
Validation Loss: 0.00266067
Epoch [121/200], Train Loss: 0.001884
Validation Loss: 0.00262222
Epoch [122/200], Train Loss: 0.001886
Validation Loss: 0.00262992
Epoch [123/200], Train Loss: 0.001868
Validation Loss: 0.00261408
Epoch [124/200], Train Loss: 0.001851
Validation Loss: 0.00257350
Epoch [125/200], Train Loss: 0.001861
Validation Loss: 0.00258439
Epoch [126/200], Train Loss: 0.001836
Validation Loss: 0.00255036
Epoch [127/200], Train Loss: 0.001827
Validation Loss: 0.00254623
Epoch [128/200], Train Loss: 0.001822
Validation Loss: 0.00251884
Epoch [129/200], Train Loss: 0.001814
Validation Loss: 0.00253442
Epoch [130/200], Train Loss: 0.001798
Validation Loss: 0.00251155
Epoch [131/200], Train Loss: 0.001788
Validation Loss: 0.00254438
Epoch [132/200], Train Loss: 0.001784
Validation Loss: 0.00249627
Epoch [133/200], Train Loss: 0.001775
Validation Loss: 0.00246760
Epoch [134/200], Train Loss: 0.001766
Validation Loss: 0.00246765
Epoch [135/200], Train Loss: 0.001754
Validation Loss: 0.00245700
Epoch [136/200], Train Loss: 0.001749
Validation Loss: 0.00245067
Epoch [137/200], Train Loss: 0.001739
Validation Loss: 0.00242661
Epoch [138/200], Train Loss: 0.001752
Validation Loss: 0.00241431
Epoch [139/200], Train Loss: 0.001723
Validation Loss: 0.00240734
Epoch [140/200], Train Loss: 0.001708
Validation Loss: 0.00239508
Epoch [141/200], Train Loss: 0.001704
Validation Loss: 0.00239170
Epoch [142/200], Train Loss: 0.001692
Validation Loss: 0.00241250
Epoch [143/200], Train Loss: 0.001702
Validation Loss: 0.00237468
Epoch [144/200], Train Loss: 0.001686
Validation Loss: 0.00236919
Epoch [145/200], Train Loss: 0.001677
Validation Loss: 0.00234797
Epoch [146/200], Train Loss: 0.001670
Validation Loss: 0.00232197
Epoch [147/200], Train Loss: 0.001660
Validation Loss: 0.00233579
Epoch [148/200], Train Loss: 0.001669
Validation Loss: 0.00232142
Epoch [149/200], Train Loss: 0.001660
Validation Loss: 0.00234353
Epoch [150/200], Train Loss: 0.001643
Validation Loss: 0.00228482
Epoch [151/200], Train Loss: 0.001626
Validation Loss: 0.00230269
Epoch [152/200], Train Loss: 0.001638
Validation Loss: 0.00229675
Epoch [153/200], Train Loss: 0.001612
Validation Loss: 0.00229410
Epoch [154/200], Train Loss: 0.001619
Validation Loss: 0.00227875
Epoch [155/200], Train Loss: 0.001621
Validation Loss: 0.00226992
Epoch [156/200], Train Loss: 0.001601
Validation Loss: 0.00225172
Epoch [157/200], Train Loss: 0.001603
Validation Loss: 0.00226698
Epoch [158/200], Train Loss: 0.001596
Validation Loss: 0.00224854
Epoch [159/200], Train Loss: 0.001590
Validation Loss: 0.00225145
Epoch [160/200], Train Loss: 0.001602
Validation Loss: 0.00226339
Epoch [161/200], Train Loss: 0.001576
Validation Loss: 0.00220558
Epoch [162/200], Train Loss: 0.001576
Validation Loss: 0.00220984
Epoch [163/200], Train Loss: 0.001573
Validation Loss: 0.00221758
Epoch [164/200], Train Loss: 0.001558
Validation Loss: 0.00221279
Epoch [165/200], Train Loss: 0.001549
Validation Loss: 0.00218111
Epoch [166/200], Train Loss: 0.001543
Validation Loss: 0.00218328
Epoch [167/200], Train Loss: 0.001538
Validation Loss: 0.00218779
Epoch [168/200], Train Loss: 0.001541
Validation Loss: 0.00219919
Epoch [169/200], Train Loss: 0.001538
Validation Loss: 0.00215045
Epoch [170/200], Train Loss: 0.001533
Validation Loss: 0.00216045
Epoch [171/200], Train Loss: 0.001518
Validation Loss: 0.00215409
Epoch [172/200], Train Loss: 0.001519
Validation Loss: 0.00214958
Epoch [173/200], Train Loss: 0.001510
Validation Loss: 0.00212728
Epoch [174/200], Train Loss: 0.001508
Validation Loss: 0.00216324
Epoch [175/200], Train Loss: 0.001513
Validation Loss: 0.00213486
Epoch [176/200], Train Loss: 0.001502
Validation Loss: 0.00212913
Epoch [177/200], Train Loss: 0.001503
Validation Loss: 0.00211189
Epoch [178/200], Train Loss: 0.001484
Validation Loss: 0.00211417
Epoch [179/200], Train Loss: 0.001498
Validation Loss: 0.00211787
Epoch [180/200], Train Loss: 0.001487
Validation Loss: 0.00209363
Epoch [181/200], Train Loss: 0.001480
Validation Loss: 0.00209256
Epoch [182/200], Train Loss: 0.001486
Validation Loss: 0.00214886
Epoch [183/200], Train Loss: 0.001475
Validation Loss: 0.00207486
Epoch [184/200], Train Loss: 0.001466
Validation Loss: 0.00208504
Epoch [185/200], Train Loss: 0.001462
Validation Loss: 0.00205554
Epoch [186/200], Train Loss: 0.001454
Validation Loss: 0.00206822
Epoch [187/200], Train Loss: 0.001445
Validation Loss: 0.00207379
Epoch [188/200], Train Loss: 0.001439
Validation Loss: 0.00206656
Epoch [189/200], Train Loss: 0.001444
Validation Loss: 0.00205724
Epoch [190/200], Train Loss: 0.001443
Validation Loss: 0.00204238
Epoch [191/200], Train Loss: 0.001440
Validation Loss: 0.00203451
Epoch [192/200], Train Loss: 0.001433
Validation Loss: 0.00201945
Epoch [193/200], Train Loss: 0.001430
Validation Loss: 0.00203632
Epoch [194/200], Train Loss: 0.001419
Validation Loss: 0.00203670
Epoch [195/200], Train Loss: 0.001418
Validation Loss: 0.00201244
Epoch [196/200], Train Loss: 0.001422
Validation Loss: 0.00200520
Epoch [197/200], Train Loss: 0.001413
Validation Loss: 0.00201010
Epoch [198/200], Train Loss: 0.001403
Validation Loss: 0.00199286
Epoch [199/200], Train Loss: 0.001407
Validation Loss: 0.00198764
Epoch [200/200], Train Loss: 0.001404
Validation Loss: 0.00198653

Evaluating model for: Lamp
Run 57/144 completed in 971.67 seconds with: {'MAE': np.float32(0.9613905), 'MSE': np.float32(26.259369), 'RMSE': np.float32(5.1243896), 'SAE': np.float32(0.048305534), 'NDE': np.float32(0.45584807)}

Run 58/144: hidden=128, seq_len=120, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.005287
Validation Loss: 0.00672829
Epoch [2/200], Train Loss: 0.004784
Validation Loss: 0.00663861
Epoch [3/200], Train Loss: 0.004771
Validation Loss: 0.00662758
Epoch [4/200], Train Loss: 0.004766
Validation Loss: 0.00660993
Epoch [5/200], Train Loss: 0.004752
Validation Loss: 0.00655916
Epoch [6/200], Train Loss: 0.004684
Validation Loss: 0.00651494
Epoch [7/200], Train Loss: 0.004693
Validation Loss: 0.00650985
Epoch [8/200], Train Loss: 0.004687
Validation Loss: 0.00649584
Epoch [9/200], Train Loss: 0.004688
Validation Loss: 0.00653001
Epoch [10/200], Train Loss: 0.004681
Validation Loss: 0.00650010
Epoch [11/200], Train Loss: 0.004686
Validation Loss: 0.00649851
Epoch [12/200], Train Loss: 0.004682
Validation Loss: 0.00650305
Epoch [13/200], Train Loss: 0.004667
Validation Loss: 0.00650480
Epoch [14/200], Train Loss: 0.004670
Validation Loss: 0.00649441
Epoch [15/200], Train Loss: 0.004669
Validation Loss: 0.00650181
Epoch [16/200], Train Loss: 0.004668
Validation Loss: 0.00648537
Epoch [17/200], Train Loss: 0.004660
Validation Loss: 0.00648886
Epoch [18/200], Train Loss: 0.004664
Validation Loss: 0.00648314
Epoch [19/200], Train Loss: 0.004650
Validation Loss: 0.00651869
Epoch [20/200], Train Loss: 0.004657
Validation Loss: 0.00648792
Epoch [21/200], Train Loss: 0.004653
Validation Loss: 0.00649896
Epoch [22/200], Train Loss: 0.004663
Validation Loss: 0.00648535
Epoch [23/200], Train Loss: 0.004665
Validation Loss: 0.00648798
Epoch [24/200], Train Loss: 0.004658
Validation Loss: 0.00650154
Epoch [25/200], Train Loss: 0.004673
Validation Loss: 0.00648148
Epoch [26/200], Train Loss: 0.004652
Validation Loss: 0.00648303
Epoch [27/200], Train Loss: 0.004663
Validation Loss: 0.00650518
Epoch [28/200], Train Loss: 0.004651
Validation Loss: 0.00650007
Epoch [29/200], Train Loss: 0.004657
Validation Loss: 0.00647916
Epoch [30/200], Train Loss: 0.004662
Validation Loss: 0.00648288
Epoch [31/200], Train Loss: 0.004655
Validation Loss: 0.00648626
Epoch [32/200], Train Loss: 0.004645
Validation Loss: 0.00648370
Epoch [33/200], Train Loss: 0.004652
Validation Loss: 0.00648728
Epoch [34/200], Train Loss: 0.004646
Validation Loss: 0.00648389
Epoch [35/200], Train Loss: 0.004645
Validation Loss: 0.00648443
Epoch [36/200], Train Loss: 0.004646
Validation Loss: 0.00649574
Epoch [37/200], Train Loss: 0.004645
Validation Loss: 0.00649317
Epoch [38/200], Train Loss: 0.004642
Validation Loss: 0.00649114
Epoch [39/200], Train Loss: 0.004645
Validation Loss: 0.00647861
Epoch [40/200], Train Loss: 0.004637
Validation Loss: 0.00650956
Epoch [41/200], Train Loss: 0.004640
Validation Loss: 0.00648828
Epoch [42/200], Train Loss: 0.004655
Validation Loss: 0.00648063
Epoch [43/200], Train Loss: 0.004651
Validation Loss: 0.00649792
Epoch [44/200], Train Loss: 0.004654
Validation Loss: 0.00652404
Epoch [45/200], Train Loss: 0.004640
Validation Loss: 0.00648359
Epoch [46/200], Train Loss: 0.004642
Validation Loss: 0.00651790
Epoch [47/200], Train Loss: 0.004636
Validation Loss: 0.00648359
Epoch [48/200], Train Loss: 0.004637
Validation Loss: 0.00649716
Epoch [49/200], Train Loss: 0.004644
Validation Loss: 0.00650087
Early stopping triggered

Evaluating model for: Lamp
Run 58/144 completed in 241.58 seconds with: {'MAE': np.float32(2.287923), 'MSE': np.float32(122.9136), 'RMSE': np.float32(11.08664), 'SAE': np.float32(0.21695806), 'NDE': np.float32(0.9862296)}

Run 59/144: hidden=128, seq_len=120, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.005106
Validation Loss: 0.00667091
Epoch [2/200], Train Loss: 0.004775
Validation Loss: 0.00662511
Epoch [3/200], Train Loss: 0.004759
Validation Loss: 0.00661909
Epoch [4/200], Train Loss: 0.004757
Validation Loss: 0.00659942
Epoch [5/200], Train Loss: 0.004728
Validation Loss: 0.00656500
Epoch [6/200], Train Loss: 0.004693
Validation Loss: 0.00649274
Epoch [7/200], Train Loss: 0.004696
Validation Loss: 0.00655862
Epoch [8/200], Train Loss: 0.004685
Validation Loss: 0.00648822
Epoch [9/200], Train Loss: 0.004676
Validation Loss: 0.00650226
Epoch [10/200], Train Loss: 0.004676
Validation Loss: 0.00649957
Epoch [11/200], Train Loss: 0.004672
Validation Loss: 0.00648932
Epoch [12/200], Train Loss: 0.004671
Validation Loss: 0.00648649
Epoch [13/200], Train Loss: 0.004668
Validation Loss: 0.00648209
Epoch [14/200], Train Loss: 0.004667
Validation Loss: 0.00648992
Epoch [15/200], Train Loss: 0.004665
Validation Loss: 0.00651292
Epoch [16/200], Train Loss: 0.004656
Validation Loss: 0.00648327
Epoch [17/200], Train Loss: 0.004653
Validation Loss: 0.00647996
Epoch [18/200], Train Loss: 0.004651
Validation Loss: 0.00647760
Epoch [19/200], Train Loss: 0.004647
Validation Loss: 0.00649409
Epoch [20/200], Train Loss: 0.004652
Validation Loss: 0.00649797
Epoch [21/200], Train Loss: 0.004644
Validation Loss: 0.00649432
Epoch [22/200], Train Loss: 0.004650
Validation Loss: 0.00648169
Epoch [23/200], Train Loss: 0.004655
Validation Loss: 0.00647963
Epoch [24/200], Train Loss: 0.004646
Validation Loss: 0.00650241
Epoch [25/200], Train Loss: 0.004647
Validation Loss: 0.00647900
Epoch [26/200], Train Loss: 0.004648
Validation Loss: 0.00649703
Epoch [27/200], Train Loss: 0.004654
Validation Loss: 0.00647649
Epoch [28/200], Train Loss: 0.004637
Validation Loss: 0.00649315
Epoch [29/200], Train Loss: 0.004640
Validation Loss: 0.00649672
Epoch [30/200], Train Loss: 0.004663
Validation Loss: 0.00648121
Epoch [31/200], Train Loss: 0.004641
Validation Loss: 0.00649344
Epoch [32/200], Train Loss: 0.004634
Validation Loss: 0.00648935
Epoch [33/200], Train Loss: 0.004633
Validation Loss: 0.00650580
Epoch [34/200], Train Loss: 0.004643
Validation Loss: 0.00651663
Epoch [35/200], Train Loss: 0.004636
Validation Loss: 0.00648055
Epoch [36/200], Train Loss: 0.004645
Validation Loss: 0.00648951
Epoch [37/200], Train Loss: 0.004642
Validation Loss: 0.00648296
Early stopping triggered

Evaluating model for: Lamp
Run 59/144 completed in 183.08 seconds with: {'MAE': np.float32(2.3950658), 'MSE': np.float32(123.01213), 'RMSE': np.float32(11.091084), 'SAE': np.float32(0.3051047), 'NDE': np.float32(0.9866245)}

Run 60/144: hidden=128, seq_len=120, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.005266
Validation Loss: 0.00663860
Epoch [2/200], Train Loss: 0.004815
Validation Loss: 0.00663810
Epoch [3/200], Train Loss: 0.004798
Validation Loss: 0.00664928
Epoch [4/200], Train Loss: 0.004781
Validation Loss: 0.00664087
Epoch [5/200], Train Loss: 0.004787
Validation Loss: 0.00662260
Epoch [6/200], Train Loss: 0.004777
Validation Loss: 0.00658346
Epoch [7/200], Train Loss: 0.004731
Validation Loss: 0.00652412
Epoch [8/200], Train Loss: 0.004701
Validation Loss: 0.00652658
Epoch [9/200], Train Loss: 0.004699
Validation Loss: 0.00650622
Epoch [10/200], Train Loss: 0.004696
Validation Loss: 0.00650280
Epoch [11/200], Train Loss: 0.004685
Validation Loss: 0.00652240
Epoch [12/200], Train Loss: 0.004685
Validation Loss: 0.00648237
Epoch [13/200], Train Loss: 0.004700
Validation Loss: 0.00648330
Epoch [14/200], Train Loss: 0.004674
Validation Loss: 0.00650821
Epoch [15/200], Train Loss: 0.004671
Validation Loss: 0.00648105
Epoch [16/200], Train Loss: 0.004666
Validation Loss: 0.00650427
Epoch [17/200], Train Loss: 0.004660
Validation Loss: 0.00648174
Epoch [18/200], Train Loss: 0.004668
Validation Loss: 0.00650250
Epoch [19/200], Train Loss: 0.004663
Validation Loss: 0.00650810
Epoch [20/200], Train Loss: 0.004672
Validation Loss: 0.00649176
Epoch [21/200], Train Loss: 0.004666
Validation Loss: 0.00648493
Epoch [22/200], Train Loss: 0.004660
Validation Loss: 0.00650150
Epoch [23/200], Train Loss: 0.004659
Validation Loss: 0.00648641
Epoch [24/200], Train Loss: 0.004646
Validation Loss: 0.00651662
Epoch [25/200], Train Loss: 0.004654
Validation Loss: 0.00650202
Early stopping triggered

Evaluating model for: Lamp
Run 60/144 completed in 131.51 seconds with: {'MAE': np.float32(2.2920508), 'MSE': np.float32(123.13591), 'RMSE': np.float32(11.0966625), 'SAE': np.float32(0.1654923), 'NDE': np.float32(0.9871208)}

Run 61/144: hidden=128, seq_len=360, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.004889
Validation Loss: 0.00472059
Epoch [2/200], Train Loss: 0.004759
Validation Loss: 0.00469349
Epoch [3/200], Train Loss: 0.004736
Validation Loss: 0.00467878
Epoch [4/200], Train Loss: 0.004717
Validation Loss: 0.00467901
Epoch [5/200], Train Loss: 0.004714
Validation Loss: 0.00467617
Epoch [6/200], Train Loss: 0.004714
Validation Loss: 0.00467247
Epoch [7/200], Train Loss: 0.004706
Validation Loss: 0.00467045
Epoch [8/200], Train Loss: 0.004706
Validation Loss: 0.00466936
Epoch [9/200], Train Loss: 0.004701
Validation Loss: 0.00466506
Epoch [10/200], Train Loss: 0.004690
Validation Loss: 0.00466237
Epoch [11/200], Train Loss: 0.004694
Validation Loss: 0.00466475
Epoch [12/200], Train Loss: 0.004693
Validation Loss: 0.00465532
Epoch [13/200], Train Loss: 0.004687
Validation Loss: 0.00464474
Epoch [14/200], Train Loss: 0.004668
Validation Loss: 0.00463363
Epoch [15/200], Train Loss: 0.004660
Validation Loss: 0.00462308
Epoch [16/200], Train Loss: 0.004652
Validation Loss: 0.00461332
Epoch [17/200], Train Loss: 0.004643
Validation Loss: 0.00460088
Epoch [18/200], Train Loss: 0.004625
Validation Loss: 0.00457778
Epoch [19/200], Train Loss: 0.004600
Validation Loss: 0.00454713
Epoch [20/200], Train Loss: 0.004562
Validation Loss: 0.00451412
Epoch [21/200], Train Loss: 0.004521
Validation Loss: 0.00444650
Epoch [22/200], Train Loss: 0.004448
Validation Loss: 0.00438876
Epoch [23/200], Train Loss: 0.004378
Validation Loss: 0.00426971
Epoch [24/200], Train Loss: 0.004262
Validation Loss: 0.00416881
Epoch [25/200], Train Loss: 0.004152
Validation Loss: 0.00399517
Epoch [26/200], Train Loss: 0.004026
Validation Loss: 0.00407734
Epoch [27/200], Train Loss: 0.003879
Validation Loss: 0.00372133
Epoch [28/200], Train Loss: 0.003730
Validation Loss: 0.00350622
Epoch [29/200], Train Loss: 0.003512
Validation Loss: 0.00329529
Epoch [30/200], Train Loss: 0.003313
Validation Loss: 0.00307765
Epoch [31/200], Train Loss: 0.003115
Validation Loss: 0.00291046
Epoch [32/200], Train Loss: 0.002935
Validation Loss: 0.00274911
Epoch [33/200], Train Loss: 0.002765
Validation Loss: 0.00258078
Epoch [34/200], Train Loss: 0.002621
Validation Loss: 0.00246911
Epoch [35/200], Train Loss: 0.002507
Validation Loss: 0.00236882
Epoch [36/200], Train Loss: 0.002393
Validation Loss: 0.00229645
Epoch [37/200], Train Loss: 0.002308
Validation Loss: 0.00222191
Epoch [38/200], Train Loss: 0.002220
Validation Loss: 0.00216644
Epoch [39/200], Train Loss: 0.002167
Validation Loss: 0.00213435
Epoch [40/200], Train Loss: 0.002099
Validation Loss: 0.00207252
Epoch [41/200], Train Loss: 0.002051
Validation Loss: 0.00203761
Epoch [42/200], Train Loss: 0.002000
Validation Loss: 0.00199025
Epoch [43/200], Train Loss: 0.001951
Validation Loss: 0.00195478
Epoch [44/200], Train Loss: 0.001902
Validation Loss: 0.00189919
Epoch [45/200], Train Loss: 0.001852
Validation Loss: 0.00186294
Epoch [46/200], Train Loss: 0.001822
Validation Loss: 0.00182909
Epoch [47/200], Train Loss: 0.001789
Validation Loss: 0.00179924
Epoch [48/200], Train Loss: 0.001760
Validation Loss: 0.00177322
Epoch [49/200], Train Loss: 0.001712
Validation Loss: 0.00173032
Epoch [50/200], Train Loss: 0.001679
Validation Loss: 0.00170903
Epoch [51/200], Train Loss: 0.001666
Validation Loss: 0.00166066
Epoch [52/200], Train Loss: 0.001620
Validation Loss: 0.00162761
Epoch [53/200], Train Loss: 0.001594
Validation Loss: 0.00159980
Epoch [54/200], Train Loss: 0.001567
Validation Loss: 0.00159997
Epoch [55/200], Train Loss: 0.001541
Validation Loss: 0.00154339
Epoch [56/200], Train Loss: 0.001521
Validation Loss: 0.00153531
Epoch [57/200], Train Loss: 0.001523
Validation Loss: 0.00151395
Epoch [58/200], Train Loss: 0.001470
Validation Loss: 0.00148586
Epoch [59/200], Train Loss: 0.001449
Validation Loss: 0.00145781
Epoch [60/200], Train Loss: 0.001428
Validation Loss: 0.00143376
Epoch [61/200], Train Loss: 0.001403
Validation Loss: 0.00141651
Epoch [62/200], Train Loss: 0.001397
Validation Loss: 0.00142068
Epoch [63/200], Train Loss: 0.001377
Validation Loss: 0.00138951
Epoch [64/200], Train Loss: 0.001353
Validation Loss: 0.00135152
Epoch [65/200], Train Loss: 0.001332
Validation Loss: 0.00134557
Epoch [66/200], Train Loss: 0.001321
Validation Loss: 0.00131873
Epoch [67/200], Train Loss: 0.001302
Validation Loss: 0.00131858
Epoch [68/200], Train Loss: 0.001287
Validation Loss: 0.00129862
Epoch [69/200], Train Loss: 0.001284
Validation Loss: 0.00127790
Epoch [70/200], Train Loss: 0.001259
Validation Loss: 0.00128101
Epoch [71/200], Train Loss: 0.001239
Validation Loss: 0.00125284
Epoch [72/200], Train Loss: 0.001223
Validation Loss: 0.00122420
Epoch [73/200], Train Loss: 0.001216
Validation Loss: 0.00121986
Epoch [74/200], Train Loss: 0.001201
Validation Loss: 0.00120446
Epoch [75/200], Train Loss: 0.001193
Validation Loss: 0.00122120
Epoch [76/200], Train Loss: 0.001180
Validation Loss: 0.00116963
Epoch [77/200], Train Loss: 0.001155
Validation Loss: 0.00117744
Epoch [78/200], Train Loss: 0.001148
Validation Loss: 0.00115583
Epoch [79/200], Train Loss: 0.001132
Validation Loss: 0.00114772
Epoch [80/200], Train Loss: 0.001124
Validation Loss: 0.00115628
Epoch [81/200], Train Loss: 0.001130
Validation Loss: 0.00113455
Epoch [82/200], Train Loss: 0.001104
Validation Loss: 0.00110791
Epoch [83/200], Train Loss: 0.001095
Validation Loss: 0.00110434
Epoch [84/200], Train Loss: 0.001088
Validation Loss: 0.00108843
Epoch [85/200], Train Loss: 0.001076
Validation Loss: 0.00108062
Epoch [86/200], Train Loss: 0.001058
Validation Loss: 0.00106837
Epoch [87/200], Train Loss: 0.001046
Validation Loss: 0.00105891
Epoch [88/200], Train Loss: 0.001038
Validation Loss: 0.00106328
Epoch [89/200], Train Loss: 0.001039
Validation Loss: 0.00104247
Epoch [90/200], Train Loss: 0.001030
Validation Loss: 0.00103375
Epoch [91/200], Train Loss: 0.001008
Validation Loss: 0.00102406
Epoch [92/200], Train Loss: 0.001007
Validation Loss: 0.00102430
Epoch [93/200], Train Loss: 0.001002
Validation Loss: 0.00102128
Epoch [94/200], Train Loss: 0.000999
Validation Loss: 0.00099789
Epoch [95/200], Train Loss: 0.000985
Validation Loss: 0.00098773
Epoch [96/200], Train Loss: 0.000973
Validation Loss: 0.00098887
Epoch [97/200], Train Loss: 0.000968
Validation Loss: 0.00098677
Epoch [98/200], Train Loss: 0.000955
Validation Loss: 0.00096934
Epoch [99/200], Train Loss: 0.000953
Validation Loss: 0.00096878
Epoch [100/200], Train Loss: 0.000941
Validation Loss: 0.00095766
Epoch [101/200], Train Loss: 0.000936
Validation Loss: 0.00094849
Epoch [102/200], Train Loss: 0.000930
Validation Loss: 0.00094633
Epoch [103/200], Train Loss: 0.000923
Validation Loss: 0.00093083
Epoch [104/200], Train Loss: 0.001216
Validation Loss: 0.00115914
Epoch [105/200], Train Loss: 0.001033
Validation Loss: 0.00102614
Epoch [106/200], Train Loss: 0.000957
Validation Loss: 0.00097604
Epoch [107/200], Train Loss: 0.000928
Validation Loss: 0.00094378
Epoch [108/200], Train Loss: 0.000909
Validation Loss: 0.00092283
Epoch [109/200], Train Loss: 0.000892
Validation Loss: 0.00091381
Epoch [110/200], Train Loss: 0.000891
Validation Loss: 0.00090807
Epoch [111/200], Train Loss: 0.000887
Validation Loss: 0.00090189
Epoch [112/200], Train Loss: 0.000876
Validation Loss: 0.00089355
Epoch [113/200], Train Loss: 0.000871
Validation Loss: 0.00088948
Epoch [114/200], Train Loss: 0.000868
Validation Loss: 0.00088088
Epoch [115/200], Train Loss: 0.000864
Validation Loss: 0.00087982
Epoch [116/200], Train Loss: 0.000857
Validation Loss: 0.00087609
Epoch [117/200], Train Loss: 0.000851
Validation Loss: 0.00085678
Epoch [118/200], Train Loss: 0.000845
Validation Loss: 0.00085249
Epoch [119/200], Train Loss: 0.000843
Validation Loss: 0.00084977
Epoch [120/200], Train Loss: 0.000834
Validation Loss: 0.00083937
Epoch [121/200], Train Loss: 0.000826
Validation Loss: 0.00084024
Epoch [122/200], Train Loss: 0.000823
Validation Loss: 0.00082784
Epoch [123/200], Train Loss: 0.000812
Validation Loss: 0.00081586
Epoch [124/200], Train Loss: 0.000810
Validation Loss: 0.00081776
Epoch [125/200], Train Loss: 0.000804
Validation Loss: 0.00081826
Epoch [126/200], Train Loss: 0.000803
Validation Loss: 0.00082010
Epoch [127/200], Train Loss: 0.000797
Validation Loss: 0.00079505
Epoch [128/200], Train Loss: 0.000793
Validation Loss: 0.00079048
Epoch [129/200], Train Loss: 0.000785
Validation Loss: 0.00078765
Epoch [130/200], Train Loss: 0.000779
Validation Loss: 0.00078972
Epoch [131/200], Train Loss: 0.000772
Validation Loss: 0.00077492
Epoch [132/200], Train Loss: 0.000779
Validation Loss: 0.00076453
Epoch [133/200], Train Loss: 0.000764
Validation Loss: 0.00076628
Epoch [134/200], Train Loss: 0.000754
Validation Loss: 0.00075075
Epoch [135/200], Train Loss: 0.000757
Validation Loss: 0.00076334
Epoch [136/200], Train Loss: 0.000747
Validation Loss: 0.00074083
Epoch [137/200], Train Loss: 0.000737
Validation Loss: 0.00073082
Epoch [138/200], Train Loss: 0.000737
Validation Loss: 0.00074467
Epoch [139/200], Train Loss: 0.000739
Validation Loss: 0.00072165
Epoch [140/200], Train Loss: 0.000728
Validation Loss: 0.00071277
Epoch [141/200], Train Loss: 0.000722
Validation Loss: 0.00070339
Epoch [142/200], Train Loss: 0.000716
Validation Loss: 0.00070167
Epoch [143/200], Train Loss: 0.000711
Validation Loss: 0.00069326
Epoch [144/200], Train Loss: 0.000710
Validation Loss: 0.00069153
Epoch [145/200], Train Loss: 0.000702
Validation Loss: 0.00069193
Epoch [146/200], Train Loss: 0.000705
Validation Loss: 0.00068299
Epoch [147/200], Train Loss: 0.000692
Validation Loss: 0.00067396
Epoch [148/200], Train Loss: 0.000689
Validation Loss: 0.00067568
Epoch [149/200], Train Loss: 0.000698
Validation Loss: 0.00068327
Epoch [150/200], Train Loss: 0.000687
Validation Loss: 0.00066715
Epoch [151/200], Train Loss: 0.000679
Validation Loss: 0.00066273
Epoch [152/200], Train Loss: 0.000676
Validation Loss: 0.00065948
Epoch [153/200], Train Loss: 0.000672
Validation Loss: 0.00065473
Epoch [154/200], Train Loss: 0.000673
Validation Loss: 0.00064619
Epoch [155/200], Train Loss: 0.000671
Validation Loss: 0.00065720
Epoch [156/200], Train Loss: 0.000666
Validation Loss: 0.00065354
Epoch [157/200], Train Loss: 0.000658
Validation Loss: 0.00064053
Epoch [158/200], Train Loss: 0.000659
Validation Loss: 0.00065692
Epoch [159/200], Train Loss: 0.000654
Validation Loss: 0.00064430
Epoch [160/200], Train Loss: 0.000650
Validation Loss: 0.00063527
Epoch [161/200], Train Loss: 0.000648
Validation Loss: 0.00063660
Epoch [162/200], Train Loss: 0.000649
Validation Loss: 0.00062983
Epoch [163/200], Train Loss: 0.000645
Validation Loss: 0.00064174
Epoch [164/200], Train Loss: 0.000642
Validation Loss: 0.00062128
Epoch [165/200], Train Loss: 0.000639
Validation Loss: 0.00062550
Epoch [166/200], Train Loss: 0.000630
Validation Loss: 0.00061513
Epoch [167/200], Train Loss: 0.000633
Validation Loss: 0.00061593
Epoch [168/200], Train Loss: 0.000629
Validation Loss: 0.00062686
Epoch [169/200], Train Loss: 0.000627
Validation Loss: 0.00062304
Epoch [170/200], Train Loss: 0.000625
Validation Loss: 0.00061526
Epoch [171/200], Train Loss: 0.000621
Validation Loss: 0.00060921
Epoch [172/200], Train Loss: 0.000614
Validation Loss: 0.00060447
Epoch [173/200], Train Loss: 0.000615
Validation Loss: 0.00060841
Epoch [174/200], Train Loss: 0.000619
Validation Loss: 0.00060528
Epoch [175/200], Train Loss: 0.000620
Validation Loss: 0.00060063
Epoch [176/200], Train Loss: 0.000612
Validation Loss: 0.00061373
Epoch [177/200], Train Loss: 0.000609
Validation Loss: 0.00059634
Epoch [178/200], Train Loss: 0.000607
Validation Loss: 0.00059224
Epoch [179/200], Train Loss: 0.000610
Validation Loss: 0.00058743
Epoch [180/200], Train Loss: 0.000602
Validation Loss: 0.00058866
Epoch [181/200], Train Loss: 0.000606
Validation Loss: 0.00058738
Epoch [182/200], Train Loss: 0.000600
Validation Loss: 0.00058159
Epoch [183/200], Train Loss: 0.000596
Validation Loss: 0.00058350
Epoch [184/200], Train Loss: 0.000593
Validation Loss: 0.00057947
Epoch [185/200], Train Loss: 0.000593
Validation Loss: 0.00060131
Epoch [186/200], Train Loss: 0.000601
Validation Loss: 0.00058216
Epoch [187/200], Train Loss: 0.000592
Validation Loss: 0.00058082
Epoch [188/200], Train Loss: 0.000589
Validation Loss: 0.00057263
Epoch [189/200], Train Loss: 0.000587
Validation Loss: 0.00057575
Epoch [190/200], Train Loss: 0.000582
Validation Loss: 0.00057043
Epoch [191/200], Train Loss: 0.000584
Validation Loss: 0.00057041
Epoch [192/200], Train Loss: 0.000580
Validation Loss: 0.00057637
Epoch [193/200], Train Loss: 0.000596
Validation Loss: 0.00059074
Epoch [194/200], Train Loss: 0.000581
Validation Loss: 0.00057821
Epoch [195/200], Train Loss: 0.000587
Validation Loss: 0.00056747
Epoch [196/200], Train Loss: 0.000572
Validation Loss: 0.00056049
Epoch [197/200], Train Loss: 0.000574
Validation Loss: 0.00056121
Epoch [198/200], Train Loss: 0.000575
Validation Loss: 0.00056932
Epoch [199/200], Train Loss: 0.000574
Validation Loss: 0.00055683
Epoch [200/200], Train Loss: 0.000569
Validation Loss: 0.00055616

Evaluating model for: Lamp
Run 61/144 completed in 1719.47 seconds with: {'MAE': np.float32(0.6261678), 'MSE': np.float32(21.53385), 'RMSE': np.float32(4.640458), 'SAE': np.float32(0.014488906), 'NDE': np.float32(0.34131157)}

Run 62/144: hidden=128, seq_len=360, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.005138
Validation Loss: 0.00476906
Epoch [2/200], Train Loss: 0.004806
Validation Loss: 0.00474840
Epoch [3/200], Train Loss: 0.004770
Validation Loss: 0.00470164
Epoch [4/200], Train Loss: 0.004732
Validation Loss: 0.00468273
Epoch [5/200], Train Loss: 0.004715
Validation Loss: 0.00467932
Epoch [6/200], Train Loss: 0.004731
Validation Loss: 0.00467727
Epoch [7/200], Train Loss: 0.004718
Validation Loss: 0.00467879
Epoch [8/200], Train Loss: 0.004716
Validation Loss: 0.00467246
Epoch [9/200], Train Loss: 0.004710
Validation Loss: 0.00467189
Epoch [10/200], Train Loss: 0.004703
Validation Loss: 0.00467105
Epoch [11/200], Train Loss: 0.004705
Validation Loss: 0.00466991
Epoch [12/200], Train Loss: 0.004700
Validation Loss: 0.00466727
Epoch [13/200], Train Loss: 0.004708
Validation Loss: 0.00466756
Epoch [14/200], Train Loss: 0.004694
Validation Loss: 0.00466807
Epoch [15/200], Train Loss: 0.004692
Validation Loss: 0.00466314
Epoch [16/200], Train Loss: 0.004698
Validation Loss: 0.00466328
Epoch [17/200], Train Loss: 0.004693
Validation Loss: 0.00466170
Epoch [18/200], Train Loss: 0.004695
Validation Loss: 0.00466164
Epoch [19/200], Train Loss: 0.004689
Validation Loss: 0.00466120
Epoch [20/200], Train Loss: 0.004692
Validation Loss: 0.00466761
Epoch [21/200], Train Loss: 0.004687
Validation Loss: 0.00465923
Epoch [22/200], Train Loss: 0.004688
Validation Loss: 0.00465881
Epoch [23/200], Train Loss: 0.004690
Validation Loss: 0.00466151
Epoch [24/200], Train Loss: 0.004682
Validation Loss: 0.00465745
Epoch [25/200], Train Loss: 0.004699
Validation Loss: 0.00465679
Epoch [26/200], Train Loss: 0.004685
Validation Loss: 0.00465582
Epoch [27/200], Train Loss: 0.004690
Validation Loss: 0.00465612
Epoch [28/200], Train Loss: 0.004686
Validation Loss: 0.00465411
Epoch [29/200], Train Loss: 0.004687
Validation Loss: 0.00465074
Epoch [30/200], Train Loss: 0.004673
Validation Loss: 0.00464959
Epoch [31/200], Train Loss: 0.004675
Validation Loss: 0.00464791
Epoch [32/200], Train Loss: 0.004665
Validation Loss: 0.00464919
Epoch [33/200], Train Loss: 0.004673
Validation Loss: 0.00464188
Epoch [34/200], Train Loss: 0.004667
Validation Loss: 0.00463701
Epoch [35/200], Train Loss: 0.004657
Validation Loss: 0.00463286
Epoch [36/200], Train Loss: 0.004663
Validation Loss: 0.00462204
Epoch [37/200], Train Loss: 0.004667
Validation Loss: 0.00461531
Epoch [38/200], Train Loss: 0.004649
Validation Loss: 0.00460779
Epoch [39/200], Train Loss: 0.004632
Validation Loss: 0.00460302
Epoch [40/200], Train Loss: 0.004638
Validation Loss: 0.00460763
Epoch [41/200], Train Loss: 0.004628
Validation Loss: 0.00460252
Epoch [42/200], Train Loss: 0.004617
Validation Loss: 0.00458294
Epoch [43/200], Train Loss: 0.004607
Validation Loss: 0.00456870
Epoch [44/200], Train Loss: 0.004581
Validation Loss: 0.00454165
Epoch [45/200], Train Loss: 0.004552
Validation Loss: 0.00449212
Epoch [46/200], Train Loss: 0.004495
Validation Loss: 0.00442848
Epoch [47/200], Train Loss: 0.004443
Validation Loss: 0.00439460
Epoch [48/200], Train Loss: 0.004382
Validation Loss: 0.00430514
Epoch [49/200], Train Loss: 0.004315
Validation Loss: 0.00422700
Epoch [50/200], Train Loss: 0.004260
Validation Loss: 0.00417124
Epoch [51/200], Train Loss: 0.004204
Validation Loss: 0.00415653
Epoch [52/200], Train Loss: 0.004099
Validation Loss: 0.00396967
Epoch [53/200], Train Loss: 0.004001
Validation Loss: 0.00382621
Epoch [54/200], Train Loss: 0.003876
Validation Loss: 0.00369099
Epoch [55/200], Train Loss: 0.003748
Validation Loss: 0.00355238
Epoch [56/200], Train Loss: 0.003597
Validation Loss: 0.00336124
Epoch [57/200], Train Loss: 0.003391
Validation Loss: 0.00311226
Epoch [58/200], Train Loss: 0.003209
Validation Loss: 0.00293436
Epoch [59/200], Train Loss: 0.002999
Validation Loss: 0.00270116
Epoch [60/200], Train Loss: 0.002823
Validation Loss: 0.00253852
Epoch [61/200], Train Loss: 0.002644
Validation Loss: 0.00241802
Epoch [62/200], Train Loss: 0.002512
Validation Loss: 0.00232060
Epoch [63/200], Train Loss: 0.002413
Validation Loss: 0.00227903
Epoch [64/200], Train Loss: 0.002337
Validation Loss: 0.00219365
Epoch [65/200], Train Loss: 0.002250
Validation Loss: 0.00210560
Epoch [66/200], Train Loss: 0.002187
Validation Loss: 0.00209908
Epoch [67/200], Train Loss: 0.002124
Validation Loss: 0.00204019
Epoch [68/200], Train Loss: 0.002058
Validation Loss: 0.00198071
Epoch [69/200], Train Loss: 0.002008
Validation Loss: 0.00190900
Epoch [70/200], Train Loss: 0.001959
Validation Loss: 0.00188496
Epoch [71/200], Train Loss: 0.001908
Validation Loss: 0.00184637
Epoch [72/200], Train Loss: 0.001869
Validation Loss: 0.00181063
Epoch [73/200], Train Loss: 0.001827
Validation Loss: 0.00177341
Epoch [74/200], Train Loss: 0.001794
Validation Loss: 0.00171863
Epoch [75/200], Train Loss: 0.001756
Validation Loss: 0.00169511
Epoch [76/200], Train Loss: 0.001718
Validation Loss: 0.00165947
Epoch [77/200], Train Loss: 0.001677
Validation Loss: 0.00163394
Epoch [78/200], Train Loss: 0.001649
Validation Loss: 0.00160984
Epoch [79/200], Train Loss: 0.001618
Validation Loss: 0.00158306
Epoch [80/200], Train Loss: 0.001595
Validation Loss: 0.00152829
Epoch [81/200], Train Loss: 0.001563
Validation Loss: 0.00151003
Epoch [82/200], Train Loss: 0.001541
Validation Loss: 0.00151561
Epoch [83/200], Train Loss: 0.001508
Validation Loss: 0.00149331
Epoch [84/200], Train Loss: 0.001481
Validation Loss: 0.00145736
Epoch [85/200], Train Loss: 0.001462
Validation Loss: 0.00144358
Epoch [86/200], Train Loss: 0.001449
Validation Loss: 0.00141499
Epoch [87/200], Train Loss: 0.001410
Validation Loss: 0.00140264
Epoch [88/200], Train Loss: 0.001396
Validation Loss: 0.00135904
Epoch [89/200], Train Loss: 0.001406
Validation Loss: 0.00138416
Epoch [90/200], Train Loss: 0.001379
Validation Loss: 0.00137808
Epoch [91/200], Train Loss: 0.001348
Validation Loss: 0.00132490
Epoch [92/200], Train Loss: 0.001320
Validation Loss: 0.00131860
Epoch [93/200], Train Loss: 0.001301
Validation Loss: 0.00129477
Epoch [94/200], Train Loss: 0.001288
Validation Loss: 0.00128644
Epoch [95/200], Train Loss: 0.001273
Validation Loss: 0.00127239
Epoch [96/200], Train Loss: 0.001263
Validation Loss: 0.00130951
Epoch [97/200], Train Loss: 0.001278
Validation Loss: 0.00127234
Epoch [98/200], Train Loss: 0.001237
Validation Loss: 0.00126643
Epoch [99/200], Train Loss: 0.001224
Validation Loss: 0.00122171
Epoch [100/200], Train Loss: 0.001227
Validation Loss: 0.00122277
Epoch [101/200], Train Loss: 0.001200
Validation Loss: 0.00119850
Epoch [102/200], Train Loss: 0.001183
Validation Loss: 0.00120682
Epoch [103/200], Train Loss: 0.001173
Validation Loss: 0.00120046
Epoch [104/200], Train Loss: 0.001166
Validation Loss: 0.00116854
Epoch [105/200], Train Loss: 0.001156
Validation Loss: 0.00115761
Epoch [106/200], Train Loss: 0.001136
Validation Loss: 0.00116880
Epoch [107/200], Train Loss: 0.001131
Validation Loss: 0.00116583
Epoch [108/200], Train Loss: 0.001131
Validation Loss: 0.00116471
Epoch [109/200], Train Loss: 0.001117
Validation Loss: 0.00115060
Epoch [110/200], Train Loss: 0.001106
Validation Loss: 0.00115683
Epoch [111/200], Train Loss: 0.001236
Validation Loss: 0.00129811
Epoch [112/200], Train Loss: 0.001145
Validation Loss: 0.00113449
Epoch [113/200], Train Loss: 0.001086
Validation Loss: 0.00111635
Epoch [114/200], Train Loss: 0.001070
Validation Loss: 0.00109732
Epoch [115/200], Train Loss: 0.001069
Validation Loss: 0.00109922
Epoch [116/200], Train Loss: 0.001060
Validation Loss: 0.00109113
Epoch [117/200], Train Loss: 0.001073
Validation Loss: 0.00108232
Epoch [118/200], Train Loss: 0.001049
Validation Loss: 0.00107781
Epoch [119/200], Train Loss: 0.001045
Validation Loss: 0.00107598
Epoch [120/200], Train Loss: 0.001038
Validation Loss: 0.00106039
Epoch [121/200], Train Loss: 0.001025
Validation Loss: 0.00105121
Epoch [122/200], Train Loss: 0.001017
Validation Loss: 0.00105031
Epoch [123/200], Train Loss: 0.001018
Validation Loss: 0.00105176
Epoch [124/200], Train Loss: 0.001013
Validation Loss: 0.00105297
Epoch [125/200], Train Loss: 0.001007
Validation Loss: 0.00103709
Epoch [126/200], Train Loss: 0.001016
Validation Loss: 0.00103837
Epoch [127/200], Train Loss: 0.000995
Validation Loss: 0.00102563
Epoch [128/200], Train Loss: 0.000991
Validation Loss: 0.00102717
Epoch [129/200], Train Loss: 0.000980
Validation Loss: 0.00100936
Epoch [130/200], Train Loss: 0.000985
Validation Loss: 0.00103158
Epoch [131/200], Train Loss: 0.000971
Validation Loss: 0.00100959
Epoch [132/200], Train Loss: 0.000971
Validation Loss: 0.00099838
Epoch [133/200], Train Loss: 0.000973
Validation Loss: 0.00099345
Epoch [134/200], Train Loss: 0.000981
Validation Loss: 0.00100209
Epoch [135/200], Train Loss: 0.000949
Validation Loss: 0.00099056
Epoch [136/200], Train Loss: 0.000946
Validation Loss: 0.00098053
Epoch [137/200], Train Loss: 0.000955
Validation Loss: 0.00098704
Epoch [138/200], Train Loss: 0.000946
Validation Loss: 0.00097542
Epoch [139/200], Train Loss: 0.000936
Validation Loss: 0.00096855
Epoch [140/200], Train Loss: 0.000938
Validation Loss: 0.00096903
Epoch [141/200], Train Loss: 0.000936
Validation Loss: 0.00096114
Epoch [142/200], Train Loss: 0.000963
Validation Loss: 0.00099462
Epoch [143/200], Train Loss: 0.000944
Validation Loss: 0.00096327
Epoch [144/200], Train Loss: 0.000923
Validation Loss: 0.00095406
Epoch [145/200], Train Loss: 0.000919
Validation Loss: 0.00094933
Epoch [146/200], Train Loss: 0.000920
Validation Loss: 0.00094875
Epoch [147/200], Train Loss: 0.000906
Validation Loss: 0.00097178
Epoch [148/200], Train Loss: 0.000900
Validation Loss: 0.00094332
Epoch [149/200], Train Loss: 0.000900
Validation Loss: 0.00093256
Epoch [150/200], Train Loss: 0.000891
Validation Loss: 0.00093483
Epoch [151/200], Train Loss: 0.000889
Validation Loss: 0.00092991
Epoch [152/200], Train Loss: 0.000890
Validation Loss: 0.00093723
Epoch [153/200], Train Loss: 0.000896
Validation Loss: 0.00093408
Epoch [154/200], Train Loss: 0.000886
Validation Loss: 0.00092681
Epoch [155/200], Train Loss: 0.000883
Validation Loss: 0.00092316
Epoch [156/200], Train Loss: 0.000875
Validation Loss: 0.00091382
Epoch [157/200], Train Loss: 0.000875
Validation Loss: 0.00091448
Epoch [158/200], Train Loss: 0.000880
Validation Loss: 0.00091159
Epoch [159/200], Train Loss: 0.000869
Validation Loss: 0.00090726
Epoch [160/200], Train Loss: 0.000869
Validation Loss: 0.00091693
Epoch [161/200], Train Loss: 0.000867
Validation Loss: 0.00090050
Epoch [162/200], Train Loss: 0.000859
Validation Loss: 0.00090497
Epoch [163/200], Train Loss: 0.000857
Validation Loss: 0.00090401
Epoch [164/200], Train Loss: 0.000900
Validation Loss: 0.00097687
Epoch [165/200], Train Loss: 0.000867
Validation Loss: 0.00090065
Epoch [166/200], Train Loss: 0.000852
Validation Loss: 0.00090714
Epoch [167/200], Train Loss: 0.000862
Validation Loss: 0.00089344
Epoch [168/200], Train Loss: 0.000847
Validation Loss: 0.00088663
Epoch [169/200], Train Loss: 0.000841
Validation Loss: 0.00088307
Epoch [170/200], Train Loss: 0.000842
Validation Loss: 0.00088437
Epoch [171/200], Train Loss: 0.000836
Validation Loss: 0.00087883
Epoch [172/200], Train Loss: 0.000834
Validation Loss: 0.00088444
Epoch [173/200], Train Loss: 0.000832
Validation Loss: 0.00087718
Epoch [174/200], Train Loss: 0.000832
Validation Loss: 0.00087682
Epoch [175/200], Train Loss: 0.000827
Validation Loss: 0.00086916
Epoch [176/200], Train Loss: 0.000826
Validation Loss: 0.00086833
Epoch [177/200], Train Loss: 0.000825
Validation Loss: 0.00087430
Epoch [178/200], Train Loss: 0.000825
Validation Loss: 0.00086649
Epoch [179/200], Train Loss: 0.000821
Validation Loss: 0.00086320
Epoch [180/200], Train Loss: 0.000821
Validation Loss: 0.00086061
Epoch [181/200], Train Loss: 0.000820
Validation Loss: 0.00085391
Epoch [182/200], Train Loss: 0.000826
Validation Loss: 0.00088242
Epoch [183/200], Train Loss: 0.000818
Validation Loss: 0.00085490
Epoch [184/200], Train Loss: 0.000813
Validation Loss: 0.00084842
Epoch [185/200], Train Loss: 0.000807
Validation Loss: 0.00084425
Epoch [186/200], Train Loss: 0.000804
Validation Loss: 0.00084242
Epoch [187/200], Train Loss: 0.000805
Validation Loss: 0.00083832
Epoch [188/200], Train Loss: 0.000801
Validation Loss: 0.00083831
Epoch [189/200], Train Loss: 0.000797
Validation Loss: 0.00083022
Epoch [190/200], Train Loss: 0.000796
Validation Loss: 0.00083051
Epoch [191/200], Train Loss: 0.000795
Validation Loss: 0.00082708
Epoch [192/200], Train Loss: 0.000788
Validation Loss: 0.00082851
Epoch [193/200], Train Loss: 0.000790
Validation Loss: 0.00082843
Epoch [194/200], Train Loss: 0.000867
Validation Loss: 0.00091827
Epoch [195/200], Train Loss: 0.000801
Validation Loss: 0.00082246
Epoch [196/200], Train Loss: 0.000786
Validation Loss: 0.00081701
Epoch [197/200], Train Loss: 0.000781
Validation Loss: 0.00081518
Epoch [198/200], Train Loss: 0.000782
Validation Loss: 0.00082107
Epoch [199/200], Train Loss: 0.000777
Validation Loss: 0.00081482
Epoch [200/200], Train Loss: 0.000777
Validation Loss: 0.00080826

Evaluating model for: Lamp
Run 62/144 completed in 1816.74 seconds with: {'MAE': np.float32(0.7504617), 'MSE': np.float32(30.720377), 'RMSE': np.float32(5.542597), 'SAE': np.float32(0.03776599), 'NDE': np.float32(0.40766487)}

Run 63/144: hidden=128, seq_len=360, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.005027
Validation Loss: 0.00475911
Epoch [2/200], Train Loss: 0.004808
Validation Loss: 0.00474939
Epoch [3/200], Train Loss: 0.004784
Validation Loss: 0.00470365
Epoch [4/200], Train Loss: 0.004726
Validation Loss: 0.00467910
Epoch [5/200], Train Loss: 0.004715
Validation Loss: 0.00467706
Epoch [6/200], Train Loss: 0.004710
Validation Loss: 0.00467382
Epoch [7/200], Train Loss: 0.004705
Validation Loss: 0.00467551
Epoch [8/200], Train Loss: 0.004706
Validation Loss: 0.00466954
Epoch [9/200], Train Loss: 0.004696
Validation Loss: 0.00467186
Epoch [10/200], Train Loss: 0.004710
Validation Loss: 0.00467201
Epoch [11/200], Train Loss: 0.004690
Validation Loss: 0.00466487
Epoch [12/200], Train Loss: 0.004698
Validation Loss: 0.00466683
Epoch [13/200], Train Loss: 0.004696
Validation Loss: 0.00467220
Epoch [14/200], Train Loss: 0.004702
Validation Loss: 0.00466497
Epoch [15/200], Train Loss: 0.004694
Validation Loss: 0.00466259
Epoch [16/200], Train Loss: 0.004689
Validation Loss: 0.00466219
Epoch [17/200], Train Loss: 0.004693
Validation Loss: 0.00466308
Epoch [18/200], Train Loss: 0.004692
Validation Loss: 0.00466344
Epoch [19/200], Train Loss: 0.004692
Validation Loss: 0.00466666
Epoch [20/200], Train Loss: 0.004690
Validation Loss: 0.00466349
Epoch [21/200], Train Loss: 0.004689
Validation Loss: 0.00466194
Epoch [22/200], Train Loss: 0.004690
Validation Loss: 0.00466103
Epoch [23/200], Train Loss: 0.004690
Validation Loss: 0.00466040
Epoch [24/200], Train Loss: 0.004680
Validation Loss: 0.00465927
Epoch [25/200], Train Loss: 0.004678
Validation Loss: 0.00465902
Epoch [26/200], Train Loss: 0.004700
Validation Loss: 0.00466845
Epoch [27/200], Train Loss: 0.004687
Validation Loss: 0.00465964
Epoch [28/200], Train Loss: 0.004686
Validation Loss: 0.00465824
Epoch [29/200], Train Loss: 0.004682
Validation Loss: 0.00465947
Epoch [30/200], Train Loss: 0.004687
Validation Loss: 0.00465828
Epoch [31/200], Train Loss: 0.004677
Validation Loss: 0.00465782
Epoch [32/200], Train Loss: 0.004686
Validation Loss: 0.00465764
Epoch [33/200], Train Loss: 0.004681
Validation Loss: 0.00466963
Epoch [34/200], Train Loss: 0.004689
Validation Loss: 0.00465975
Epoch [35/200], Train Loss: 0.004684
Validation Loss: 0.00465728
Epoch [36/200], Train Loss: 0.004686
Validation Loss: 0.00465792
Epoch [37/200], Train Loss: 0.004680
Validation Loss: 0.00465929
Epoch [38/200], Train Loss: 0.004675
Validation Loss: 0.00466283
Epoch [39/200], Train Loss: 0.004675
Validation Loss: 0.00466722
Epoch [40/200], Train Loss: 0.004689
Validation Loss: 0.00465589
Epoch [41/200], Train Loss: 0.004683
Validation Loss: 0.00467214
Epoch [42/200], Train Loss: 0.004674
Validation Loss: 0.00465560
Epoch [43/200], Train Loss: 0.004685
Validation Loss: 0.00465598
Epoch [44/200], Train Loss: 0.004674
Validation Loss: 0.00465492
Epoch [45/200], Train Loss: 0.004684
Validation Loss: 0.00465138
Epoch [46/200], Train Loss: 0.004676
Validation Loss: 0.00465038
Epoch [47/200], Train Loss: 0.004666
Validation Loss: 0.00464918
Epoch [48/200], Train Loss: 0.004673
Validation Loss: 0.00464777
Epoch [49/200], Train Loss: 0.004672
Validation Loss: 0.00464879
Epoch [50/200], Train Loss: 0.004674
Validation Loss: 0.00464811
Epoch [51/200], Train Loss: 0.004672
Validation Loss: 0.00465060
Epoch [52/200], Train Loss: 0.004674
Validation Loss: 0.00464851
Epoch [53/200], Train Loss: 0.004660
Validation Loss: 0.00464956
Epoch [54/200], Train Loss: 0.004663
Validation Loss: 0.00464103
Epoch [55/200], Train Loss: 0.004659
Validation Loss: 0.00463708
Epoch [56/200], Train Loss: 0.004654
Validation Loss: 0.00463036
Epoch [57/200], Train Loss: 0.004644
Validation Loss: 0.00461821
Epoch [58/200], Train Loss: 0.004639
Validation Loss: 0.00461707
Epoch [59/200], Train Loss: 0.004635
Validation Loss: 0.00459827
Epoch [60/200], Train Loss: 0.004616
Validation Loss: 0.00459247
Epoch [61/200], Train Loss: 0.004604
Validation Loss: 0.00455638
Epoch [62/200], Train Loss: 0.004572
Validation Loss: 0.00453846
Epoch [63/200], Train Loss: 0.004547
Validation Loss: 0.00451096
Epoch [64/200], Train Loss: 0.004504
Validation Loss: 0.00445998
Epoch [65/200], Train Loss: 0.004466
Validation Loss: 0.00440448
Epoch [66/200], Train Loss: 0.004434
Validation Loss: 0.00436152
Epoch [67/200], Train Loss: 0.004376
Validation Loss: 0.00431705
Epoch [68/200], Train Loss: 0.004299
Validation Loss: 0.00423829
Epoch [69/200], Train Loss: 0.004243
Validation Loss: 0.00415444
Epoch [70/200], Train Loss: 0.004167
Validation Loss: 0.00406432
Epoch [71/200], Train Loss: 0.004099
Validation Loss: 0.00397578
Epoch [72/200], Train Loss: 0.003979
Validation Loss: 0.00385727
Epoch [73/200], Train Loss: 0.003844
Validation Loss: 0.00365640
Epoch [74/200], Train Loss: 0.003642
Validation Loss: 0.00338569
Epoch [75/200], Train Loss: 0.003443
Validation Loss: 0.00329489
Epoch [76/200], Train Loss: 0.003180
Validation Loss: 0.00299628
Epoch [77/200], Train Loss: 0.002965
Validation Loss: 0.00268216
Epoch [78/200], Train Loss: 0.002768
Validation Loss: 0.00259956
Epoch [79/200], Train Loss: 0.002612
Validation Loss: 0.00247298
Epoch [80/200], Train Loss: 0.002496
Validation Loss: 0.00237114
Epoch [81/200], Train Loss: 0.002389
Validation Loss: 0.00226328
Epoch [82/200], Train Loss: 0.002289
Validation Loss: 0.00219538
Epoch [83/200], Train Loss: 0.002204
Validation Loss: 0.00210592
Epoch [84/200], Train Loss: 0.002134
Validation Loss: 0.00207302
Epoch [85/200], Train Loss: 0.002057
Validation Loss: 0.00199727
Epoch [86/200], Train Loss: 0.002005
Validation Loss: 0.00195632
Epoch [87/200], Train Loss: 0.001927
Validation Loss: 0.00188493
Epoch [88/200], Train Loss: 0.001875
Validation Loss: 0.00184275
Epoch [89/200], Train Loss: 0.001822
Validation Loss: 0.00179844
Epoch [90/200], Train Loss: 0.001756
Validation Loss: 0.00175439
Epoch [91/200], Train Loss: 0.001704
Validation Loss: 0.00167673
Epoch [92/200], Train Loss: 0.001665
Validation Loss: 0.00171510
Epoch [93/200], Train Loss: 0.001637
Validation Loss: 0.00165655
Epoch [94/200], Train Loss: 0.001589
Validation Loss: 0.00160721
Epoch [95/200], Train Loss: 0.001554
Validation Loss: 0.00157497
Epoch [96/200], Train Loss: 0.001513
Validation Loss: 0.00156050
Epoch [97/200], Train Loss: 0.001484
Validation Loss: 0.00151349
Epoch [98/200], Train Loss: 0.001466
Validation Loss: 0.00148253
Epoch [99/200], Train Loss: 0.001424
Validation Loss: 0.00142992
Epoch [100/200], Train Loss: 0.001390
Validation Loss: 0.00141980
Epoch [101/200], Train Loss: 0.001370
Validation Loss: 0.00141179
Epoch [102/200], Train Loss: 0.001341
Validation Loss: 0.00138921
Epoch [103/200], Train Loss: 0.001305
Validation Loss: 0.00137384
Epoch [104/200], Train Loss: 0.001290
Validation Loss: 0.00130695
Epoch [105/200], Train Loss: 0.001259
Validation Loss: 0.00128708
Epoch [106/200], Train Loss: 0.001232
Validation Loss: 0.00126514
Epoch [107/200], Train Loss: 0.001215
Validation Loss: 0.00128927
Epoch [108/200], Train Loss: 0.001208
Validation Loss: 0.00123369
Epoch [109/200], Train Loss: 0.001173
Validation Loss: 0.00121346
Epoch [110/200], Train Loss: 0.001163
Validation Loss: 0.00120111
Epoch [111/200], Train Loss: 0.001148
Validation Loss: 0.00118308
Epoch [112/200], Train Loss: 0.001150
Validation Loss: 0.00121898
Epoch [113/200], Train Loss: 0.001114
Validation Loss: 0.00114878
Epoch [114/200], Train Loss: 0.001094
Validation Loss: 0.00111368
Epoch [115/200], Train Loss: 0.001081
Validation Loss: 0.00114647
Epoch [116/200], Train Loss: 0.001071
Validation Loss: 0.00111925
Epoch [117/200], Train Loss: 0.001076
Validation Loss: 0.00108564
Epoch [118/200], Train Loss: 0.001054
Validation Loss: 0.00107307
Epoch [119/200], Train Loss: 0.001051
Validation Loss: 0.00107549
Epoch [120/200], Train Loss: 0.001027
Validation Loss: 0.00103320
Epoch [121/200], Train Loss: 0.000996
Validation Loss: 0.00101345
Epoch [122/200], Train Loss: 0.001002
Validation Loss: 0.00100881
Epoch [123/200], Train Loss: 0.000993
Validation Loss: 0.00101197
Epoch [124/200], Train Loss: 0.000976
Validation Loss: 0.00100313
Epoch [125/200], Train Loss: 0.000961
Validation Loss: 0.00095921
Epoch [126/200], Train Loss: 0.000949
Validation Loss: 0.00094494
Epoch [127/200], Train Loss: 0.000936
Validation Loss: 0.00093934
Epoch [128/200], Train Loss: 0.000920
Validation Loss: 0.00091804
Epoch [129/200], Train Loss: 0.000911
Validation Loss: 0.00089992
Epoch [130/200], Train Loss: 0.000892
Validation Loss: 0.00089791
Epoch [131/200], Train Loss: 0.000879
Validation Loss: 0.00086255
Epoch [132/200], Train Loss: 0.000872
Validation Loss: 0.00083545
Epoch [133/200], Train Loss: 0.000858
Validation Loss: 0.00083679
Epoch [134/200], Train Loss: 0.000842
Validation Loss: 0.00080865
Epoch [135/200], Train Loss: 0.000829
Validation Loss: 0.00079147
Epoch [136/200], Train Loss: 0.000814
Validation Loss: 0.00077366
Epoch [137/200], Train Loss: 0.000814
Validation Loss: 0.00080412
Epoch [138/200], Train Loss: 0.000791
Validation Loss: 0.00075490
Epoch [139/200], Train Loss: 0.000792
Validation Loss: 0.00079510
Epoch [140/200], Train Loss: 0.000799
Validation Loss: 0.00075479
Epoch [141/200], Train Loss: 0.000775
Validation Loss: 0.00073285
Epoch [142/200], Train Loss: 0.000773
Validation Loss: 0.00070853
Epoch [143/200], Train Loss: 0.000754
Validation Loss: 0.00069061
Epoch [144/200], Train Loss: 0.000743
Validation Loss: 0.00068292
Epoch [145/200], Train Loss: 0.000732
Validation Loss: 0.00066993
Epoch [146/200], Train Loss: 0.000721
Validation Loss: 0.00069158
Epoch [147/200], Train Loss: 0.000727
Validation Loss: 0.00066035
Epoch [148/200], Train Loss: 0.000753
Validation Loss: 0.00065104
Epoch [149/200], Train Loss: 0.000700
Validation Loss: 0.00063680
Epoch [150/200], Train Loss: 0.000693
Validation Loss: 0.00063413
Epoch [151/200], Train Loss: 0.000681
Validation Loss: 0.00062689
Epoch [152/200], Train Loss: 0.000679
Validation Loss: 0.00061910
Epoch [153/200], Train Loss: 0.000690
Validation Loss: 0.00064184
Epoch [154/200], Train Loss: 0.000681
Validation Loss: 0.00061356
Epoch [155/200], Train Loss: 0.000655
Validation Loss: 0.00060148
Epoch [156/200], Train Loss: 0.000653
Validation Loss: 0.00059891
Epoch [157/200], Train Loss: 0.000657
Validation Loss: 0.00060373
Epoch [158/200], Train Loss: 0.000669
Validation Loss: 0.00058097
Epoch [159/200], Train Loss: 0.000635
Validation Loss: 0.00057544
Epoch [160/200], Train Loss: 0.000633
Validation Loss: 0.00058382
Epoch [161/200], Train Loss: 0.000630
Validation Loss: 0.00057416
Epoch [162/200], Train Loss: 0.000618
Validation Loss: 0.00056142
Epoch [163/200], Train Loss: 0.000618
Validation Loss: 0.00056373
Epoch [164/200], Train Loss: 0.000613
Validation Loss: 0.00056271
Epoch [165/200], Train Loss: 0.000611
Validation Loss: 0.00055417
Epoch [166/200], Train Loss: 0.000601
Validation Loss: 0.00055098
Epoch [167/200], Train Loss: 0.000618
Validation Loss: 0.00055610
Epoch [168/200], Train Loss: 0.000598
Validation Loss: 0.00054250
Epoch [169/200], Train Loss: 0.000593
Validation Loss: 0.00053844
Epoch [170/200], Train Loss: 0.000588
Validation Loss: 0.00054254
Epoch [171/200], Train Loss: 0.000588
Validation Loss: 0.00053296
Epoch [172/200], Train Loss: 0.000581
Validation Loss: 0.00053150
Epoch [173/200], Train Loss: 0.000585
Validation Loss: 0.00063194
Epoch [174/200], Train Loss: 0.000613
Validation Loss: 0.00055611
Epoch [175/200], Train Loss: 0.000586
Validation Loss: 0.00053584
Epoch [176/200], Train Loss: 0.000593
Validation Loss: 0.00052854
Epoch [177/200], Train Loss: 0.000579
Validation Loss: 0.00052792
Epoch [178/200], Train Loss: 0.000574
Validation Loss: 0.00052433
Epoch [179/200], Train Loss: 0.000574
Validation Loss: 0.00052163
Epoch [180/200], Train Loss: 0.000572
Validation Loss: 0.00054382
Epoch [181/200], Train Loss: 0.000572
Validation Loss: 0.00051379
Epoch [182/200], Train Loss: 0.000561
Validation Loss: 0.00051068
Epoch [183/200], Train Loss: 0.000558
Validation Loss: 0.00050832
Epoch [184/200], Train Loss: 0.000555
Validation Loss: 0.00050164
Epoch [185/200], Train Loss: 0.000551
Validation Loss: 0.00050375
Epoch [186/200], Train Loss: 0.000549
Validation Loss: 0.00049910
Epoch [187/200], Train Loss: 0.000549
Validation Loss: 0.00049493
Epoch [188/200], Train Loss: 0.000556
Validation Loss: 0.00050236
Epoch [189/200], Train Loss: 0.000549
Validation Loss: 0.00049194
Epoch [190/200], Train Loss: 0.000538
Validation Loss: 0.00050046
Epoch [191/200], Train Loss: 0.000539
Validation Loss: 0.00049174
Epoch [192/200], Train Loss: 0.000534
Validation Loss: 0.00048634
Epoch [193/200], Train Loss: 0.000551
Validation Loss: 0.00058095
Epoch [194/200], Train Loss: 0.000582
Validation Loss: 0.00049684
Epoch [195/200], Train Loss: 0.000539
Validation Loss: 0.00048169
Epoch [196/200], Train Loss: 0.000529
Validation Loss: 0.00048235
Epoch [197/200], Train Loss: 0.000525
Validation Loss: 0.00047218
Epoch [198/200], Train Loss: 0.000521
Validation Loss: 0.00047625
Epoch [199/200], Train Loss: 0.000520
Validation Loss: 0.00047279
Epoch [200/200], Train Loss: 0.000518
Validation Loss: 0.00046736

Evaluating model for: Lamp
Run 63/144 completed in 1896.18 seconds with: {'MAE': np.float32(0.5308589), 'MSE': np.float32(18.33718), 'RMSE': np.float32(4.282193), 'SAE': np.float32(0.015719058), 'NDE': np.float32(0.31496036)}

Run 64/144: hidden=128, seq_len=360, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.005132
Validation Loss: 0.00476733
Epoch [2/200], Train Loss: 0.004824
Validation Loss: 0.00476251
Epoch [3/200], Train Loss: 0.004818
Validation Loss: 0.00475101
Epoch [4/200], Train Loss: 0.004772
Validation Loss: 0.00469133
Epoch [5/200], Train Loss: 0.004729
Validation Loss: 0.00468454
Epoch [6/200], Train Loss: 0.004733
Validation Loss: 0.00467936
Epoch [7/200], Train Loss: 0.004727
Validation Loss: 0.00468179
Epoch [8/200], Train Loss: 0.004719
Validation Loss: 0.00467633
Epoch [9/200], Train Loss: 0.004712
Validation Loss: 0.00467909
Epoch [10/200], Train Loss: 0.004711
Validation Loss: 0.00467169
Epoch [11/200], Train Loss: 0.004701
Validation Loss: 0.00466981
Epoch [12/200], Train Loss: 0.004710
Validation Loss: 0.00466864
Epoch [13/200], Train Loss: 0.004701
Validation Loss: 0.00466873
Epoch [14/200], Train Loss: 0.004701
Validation Loss: 0.00466534
Epoch [15/200], Train Loss: 0.004697
Validation Loss: 0.00466711
Epoch [16/200], Train Loss: 0.004708
Validation Loss: 0.00468532
Epoch [17/200], Train Loss: 0.004704
Validation Loss: 0.00466639
Epoch [18/200], Train Loss: 0.004692
Validation Loss: 0.00467010
Epoch [19/200], Train Loss: 0.004706
Validation Loss: 0.00466556
Epoch [20/200], Train Loss: 0.004698
Validation Loss: 0.00467038
Epoch [21/200], Train Loss: 0.004699
Validation Loss: 0.00466413
Epoch [22/200], Train Loss: 0.004691
Validation Loss: 0.00466240
Epoch [23/200], Train Loss: 0.004696
Validation Loss: 0.00466399
Epoch [24/200], Train Loss: 0.004691
Validation Loss: 0.00466227
Epoch [25/200], Train Loss: 0.004693
Validation Loss: 0.00466227
Epoch [26/200], Train Loss: 0.004694
Validation Loss: 0.00466429
Epoch [27/200], Train Loss: 0.004692
Validation Loss: 0.00466353
Epoch [28/200], Train Loss: 0.004698
Validation Loss: 0.00466462
Epoch [29/200], Train Loss: 0.004687
Validation Loss: 0.00466365
Epoch [30/200], Train Loss: 0.004697
Validation Loss: 0.00466164
Epoch [31/200], Train Loss: 0.004684
Validation Loss: 0.00466910
Epoch [32/200], Train Loss: 0.004687
Validation Loss: 0.00466082
Epoch [33/200], Train Loss: 0.004693
Validation Loss: 0.00466301
Epoch [34/200], Train Loss: 0.004699
Validation Loss: 0.00466044
Epoch [35/200], Train Loss: 0.004694
Validation Loss: 0.00466155
Epoch [36/200], Train Loss: 0.004693
Validation Loss: 0.00465921
Epoch [37/200], Train Loss: 0.004696
Validation Loss: 0.00466037
Epoch [38/200], Train Loss: 0.004690
Validation Loss: 0.00466464
Epoch [39/200], Train Loss: 0.004698
Validation Loss: 0.00466597
Epoch [40/200], Train Loss: 0.004688
Validation Loss: 0.00465989
Epoch [41/200], Train Loss: 0.004683
Validation Loss: 0.00465946
Epoch [42/200], Train Loss: 0.004692
Validation Loss: 0.00465921
Epoch [43/200], Train Loss: 0.004698
Validation Loss: 0.00466173
Epoch [44/200], Train Loss: 0.004695
Validation Loss: 0.00466642
Epoch [45/200], Train Loss: 0.004688
Validation Loss: 0.00465799
Epoch [46/200], Train Loss: 0.004686
Validation Loss: 0.00465810
Epoch [47/200], Train Loss: 0.004680
Validation Loss: 0.00465898
Epoch [48/200], Train Loss: 0.004690
Validation Loss: 0.00465809
Epoch [49/200], Train Loss: 0.004681
Validation Loss: 0.00466199
Epoch [50/200], Train Loss: 0.004680
Validation Loss: 0.00466532
Epoch [51/200], Train Loss: 0.004676
Validation Loss: 0.00465708
Epoch [52/200], Train Loss: 0.004681
Validation Loss: 0.00466377
Epoch [53/200], Train Loss: 0.004689
Validation Loss: 0.00465909
Epoch [54/200], Train Loss: 0.004694
Validation Loss: 0.00465713
Epoch [55/200], Train Loss: 0.004685
Validation Loss: 0.00465777
Epoch [56/200], Train Loss: 0.004687
Validation Loss: 0.00465690
Epoch [57/200], Train Loss: 0.004681
Validation Loss: 0.00465797
Epoch [58/200], Train Loss: 0.004684
Validation Loss: 0.00465868
Epoch [59/200], Train Loss: 0.004678
Validation Loss: 0.00465649
Epoch [60/200], Train Loss: 0.004688
Validation Loss: 0.00465743
Epoch [61/200], Train Loss: 0.004688
Validation Loss: 0.00465688
Epoch [62/200], Train Loss: 0.004683
Validation Loss: 0.00465734
Epoch [63/200], Train Loss: 0.004684
Validation Loss: 0.00465570
Epoch [64/200], Train Loss: 0.004682
Validation Loss: 0.00465847
Epoch [65/200], Train Loss: 0.004678
Validation Loss: 0.00466124
Epoch [66/200], Train Loss: 0.004674
Validation Loss: 0.00465151
Epoch [67/200], Train Loss: 0.004676
Validation Loss: 0.00465389
Epoch [68/200], Train Loss: 0.004671
Validation Loss: 0.00465173
Epoch [69/200], Train Loss: 0.004685
Validation Loss: 0.00465020
Epoch [70/200], Train Loss: 0.004676
Validation Loss: 0.00465404
Epoch [71/200], Train Loss: 0.004678
Validation Loss: 0.00465204
Epoch [72/200], Train Loss: 0.004675
Validation Loss: 0.00464813
Epoch [73/200], Train Loss: 0.004673
Validation Loss: 0.00465115
Epoch [74/200], Train Loss: 0.004669
Validation Loss: 0.00465375
Epoch [75/200], Train Loss: 0.004676
Validation Loss: 0.00465220
Epoch [76/200], Train Loss: 0.004664
Validation Loss: 0.00464840
Epoch [77/200], Train Loss: 0.004671
Validation Loss: 0.00464983
Epoch [78/200], Train Loss: 0.004679
Validation Loss: 0.00464657
Epoch [79/200], Train Loss: 0.004674
Validation Loss: 0.00465015
Epoch [80/200], Train Loss: 0.004666
Validation Loss: 0.00464589
Epoch [81/200], Train Loss: 0.004672
Validation Loss: 0.00464523
Epoch [82/200], Train Loss: 0.004678
Validation Loss: 0.00464926
Epoch [83/200], Train Loss: 0.004673
Validation Loss: 0.00464514
Epoch [84/200], Train Loss: 0.004669
Validation Loss: 0.00464962
Epoch [85/200], Train Loss: 0.004674
Validation Loss: 0.00464569
Epoch [86/200], Train Loss: 0.004670
Validation Loss: 0.00464552
Epoch [87/200], Train Loss: 0.004668
Validation Loss: 0.00464312
Epoch [88/200], Train Loss: 0.004667
Validation Loss: 0.00464365
Epoch [89/200], Train Loss: 0.004660
Validation Loss: 0.00466460
Epoch [90/200], Train Loss: 0.004664
Validation Loss: 0.00464175
Epoch [91/200], Train Loss: 0.004667
Validation Loss: 0.00464255
Epoch [92/200], Train Loss: 0.004666
Validation Loss: 0.00463892
Epoch [93/200], Train Loss: 0.004662
Validation Loss: 0.00464104
Epoch [94/200], Train Loss: 0.004669
Validation Loss: 0.00463946
Epoch [95/200], Train Loss: 0.004664
Validation Loss: 0.00463864
Epoch [96/200], Train Loss: 0.004663
Validation Loss: 0.00463744
Epoch [97/200], Train Loss: 0.004662
Validation Loss: 0.00463144
Epoch [98/200], Train Loss: 0.004650
Validation Loss: 0.00463147
Epoch [99/200], Train Loss: 0.004666
Validation Loss: 0.00463950
Epoch [100/200], Train Loss: 0.004650
Validation Loss: 0.00462006
Epoch [101/200], Train Loss: 0.004644
Validation Loss: 0.00462574
Epoch [102/200], Train Loss: 0.004653
Validation Loss: 0.00463390
Epoch [103/200], Train Loss: 0.004640
Validation Loss: 0.00461536
Epoch [104/200], Train Loss: 0.004658
Validation Loss: 0.00461331
Epoch [105/200], Train Loss: 0.004647
Validation Loss: 0.00461212
Epoch [106/200], Train Loss: 0.004627
Validation Loss: 0.00459865
Epoch [107/200], Train Loss: 0.004618
Validation Loss: 0.00460007
Epoch [108/200], Train Loss: 0.004617
Validation Loss: 0.00459967
Epoch [109/200], Train Loss: 0.004611
Validation Loss: 0.00457464
Epoch [110/200], Train Loss: 0.004594
Validation Loss: 0.00457581
Epoch [111/200], Train Loss: 0.004581
Validation Loss: 0.00453813
Epoch [112/200], Train Loss: 0.004572
Validation Loss: 0.00453375
Epoch [113/200], Train Loss: 0.004546
Validation Loss: 0.00450606
Epoch [114/200], Train Loss: 0.004527
Validation Loss: 0.00447432
Epoch [115/200], Train Loss: 0.004486
Validation Loss: 0.00444793
Epoch [116/200], Train Loss: 0.004439
Validation Loss: 0.00438694
Epoch [117/200], Train Loss: 0.004414
Validation Loss: 0.00435569
Epoch [118/200], Train Loss: 0.004339
Validation Loss: 0.00423543
Epoch [119/200], Train Loss: 0.004224
Validation Loss: 0.00416031
Epoch [120/200], Train Loss: 0.004108
Validation Loss: 0.00393202
Epoch [121/200], Train Loss: 0.003916
Validation Loss: 0.00372754
Epoch [122/200], Train Loss: 0.003658
Validation Loss: 0.00334033
Epoch [123/200], Train Loss: 0.003337
Validation Loss: 0.00311159
Epoch [124/200], Train Loss: 0.003110
Validation Loss: 0.00290639
Epoch [125/200], Train Loss: 0.002897
Validation Loss: 0.00272706
Epoch [126/200], Train Loss: 0.002735
Validation Loss: 0.00265507
Epoch [127/200], Train Loss: 0.002583
Validation Loss: 0.00247587
Epoch [128/200], Train Loss: 0.002464
Validation Loss: 0.00236620
Epoch [129/200], Train Loss: 0.002344
Validation Loss: 0.00228196
Epoch [130/200], Train Loss: 0.002263
Validation Loss: 0.00219156
Epoch [131/200], Train Loss: 0.002176
Validation Loss: 0.00214899
Epoch [132/200], Train Loss: 0.002095
Validation Loss: 0.00210349
Epoch [133/200], Train Loss: 0.002018
Validation Loss: 0.00200432
Epoch [134/200], Train Loss: 0.001980
Validation Loss: 0.00196843
Epoch [135/200], Train Loss: 0.001903
Validation Loss: 0.00190228
Epoch [136/200], Train Loss: 0.001860
Validation Loss: 0.00186765
Epoch [137/200], Train Loss: 0.001801
Validation Loss: 0.00182989
Epoch [138/200], Train Loss: 0.001770
Validation Loss: 0.00176156
Epoch [139/200], Train Loss: 0.001718
Validation Loss: 0.00173382
Epoch [140/200], Train Loss: 0.001688
Validation Loss: 0.00171630
Epoch [141/200], Train Loss: 0.001629
Validation Loss: 0.00165465
Epoch [142/200], Train Loss: 0.001583
Validation Loss: 0.00164316
Epoch [143/200], Train Loss: 0.001566
Validation Loss: 0.00157445
Epoch [144/200], Train Loss: 0.001519
Validation Loss: 0.00156257
Epoch [145/200], Train Loss: 0.001485
Validation Loss: 0.00150882
Epoch [146/200], Train Loss: 0.001457
Validation Loss: 0.00146878
Epoch [147/200], Train Loss: 0.001438
Validation Loss: 0.00144733
Epoch [148/200], Train Loss: 0.001389
Validation Loss: 0.00144393
Epoch [149/200], Train Loss: 0.001368
Validation Loss: 0.00143642
Epoch [150/200], Train Loss: 0.001342
Validation Loss: 0.00141407
Epoch [151/200], Train Loss: 0.001313
Validation Loss: 0.00136292
Epoch [152/200], Train Loss: 0.001292
Validation Loss: 0.00135085
Epoch [153/200], Train Loss: 0.001276
Validation Loss: 0.00131789
Epoch [154/200], Train Loss: 0.001253
Validation Loss: 0.00129633
Epoch [155/200], Train Loss: 0.001232
Validation Loss: 0.00126704
Epoch [156/200], Train Loss: 0.001210
Validation Loss: 0.00126055
Epoch [157/200], Train Loss: 0.001190
Validation Loss: 0.00123899
Epoch [158/200], Train Loss: 0.001179
Validation Loss: 0.00123102
Epoch [159/200], Train Loss: 0.001156
Validation Loss: 0.00121255
Epoch [160/200], Train Loss: 0.001136
Validation Loss: 0.00118989
Epoch [161/200], Train Loss: 0.001111
Validation Loss: 0.00115934
Epoch [162/200], Train Loss: 0.001097
Validation Loss: 0.00113019
Epoch [163/200], Train Loss: 0.001083
Validation Loss: 0.00111666
Epoch [164/200], Train Loss: 0.001077
Validation Loss: 0.00108179
Epoch [165/200], Train Loss: 0.001052
Validation Loss: 0.00107661
Epoch [166/200], Train Loss: 0.001029
Validation Loss: 0.00103564
Epoch [167/200], Train Loss: 0.001010
Validation Loss: 0.00102075
Epoch [168/200], Train Loss: 0.000994
Validation Loss: 0.00098221
Epoch [169/200], Train Loss: 0.000983
Validation Loss: 0.00094634
Epoch [170/200], Train Loss: 0.000962
Validation Loss: 0.00094446
Epoch [171/200], Train Loss: 0.000944
Validation Loss: 0.00090980
Epoch [172/200], Train Loss: 0.000923
Validation Loss: 0.00089154
Epoch [173/200], Train Loss: 0.000918
Validation Loss: 0.00087969
Epoch [174/200], Train Loss: 0.000897
Validation Loss: 0.00086289
Epoch [175/200], Train Loss: 0.000878
Validation Loss: 0.00086106
Epoch [176/200], Train Loss: 0.000875
Validation Loss: 0.00083141
Epoch [177/200], Train Loss: 0.000857
Validation Loss: 0.00081680
Epoch [178/200], Train Loss: 0.000845
Validation Loss: 0.00085052
Epoch [179/200], Train Loss: 0.000855
Validation Loss: 0.00080499
Epoch [180/200], Train Loss: 0.000826
Validation Loss: 0.00080323
Epoch [181/200], Train Loss: 0.000818
Validation Loss: 0.00077471
Epoch [182/200], Train Loss: 0.000803
Validation Loss: 0.00078346
Epoch [183/200], Train Loss: 0.000804
Validation Loss: 0.00077393
Epoch [184/200], Train Loss: 0.000794
Validation Loss: 0.00074556
Epoch [185/200], Train Loss: 0.000774
Validation Loss: 0.00073807
Epoch [186/200], Train Loss: 0.000763
Validation Loss: 0.00072640
Epoch [187/200], Train Loss: 0.000753
Validation Loss: 0.00071722
Epoch [188/200], Train Loss: 0.000753
Validation Loss: 0.00070417
Epoch [189/200], Train Loss: 0.000737
Validation Loss: 0.00073062
Epoch [190/200], Train Loss: 0.000746
Validation Loss: 0.00070261
Epoch [191/200], Train Loss: 0.000726
Validation Loss: 0.00069404
Epoch [192/200], Train Loss: 0.000716
Validation Loss: 0.00068847
Epoch [193/200], Train Loss: 0.000713
Validation Loss: 0.00067593
Epoch [194/200], Train Loss: 0.000736
Validation Loss: 0.00068441
Epoch [195/200], Train Loss: 0.000712
Validation Loss: 0.00067362
Epoch [196/200], Train Loss: 0.000700
Validation Loss: 0.00066869
Epoch [197/200], Train Loss: 0.000694
Validation Loss: 0.00066835
Epoch [198/200], Train Loss: 0.000691
Validation Loss: 0.00065488
Epoch [199/200], Train Loss: 0.000688
Validation Loss: 0.00065164
Epoch [200/200], Train Loss: 0.000676
Validation Loss: 0.00065020

Evaluating model for: Lamp
Run 64/144 completed in 1981.04 seconds with: {'MAE': np.float32(0.5719015), 'MSE': np.float32(25.090359), 'RMSE': np.float32(5.009028), 'SAE': np.float32(0.022369312), 'NDE': np.float32(0.36842075)}

Run 65/144: hidden=128, seq_len=360, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.005253
Validation Loss: 0.00384927
Epoch [2/200], Train Loss: 0.004946
Validation Loss: 0.00381160
Epoch [3/200], Train Loss: 0.004983
Validation Loss: 0.00380720
Epoch [4/200], Train Loss: 0.004909
Validation Loss: 0.00379586
Epoch [5/200], Train Loss: 0.004911
Validation Loss: 0.00378676
Epoch [6/200], Train Loss: 0.004920
Validation Loss: 0.00377987
Epoch [7/200], Train Loss: 0.004914
Validation Loss: 0.00378066
Epoch [8/200], Train Loss: 0.004897
Validation Loss: 0.00377629
Epoch [9/200], Train Loss: 0.004891
Validation Loss: 0.00378180
Epoch [10/200], Train Loss: 0.004849
Validation Loss: 0.00377946
Epoch [11/200], Train Loss: 0.004885
Validation Loss: 0.00378001
Epoch [12/200], Train Loss: 0.004920
Validation Loss: 0.00378285
Epoch [13/200], Train Loss: 0.004893
Validation Loss: 0.00377327
Epoch [14/200], Train Loss: 0.004883
Validation Loss: 0.00377206
Epoch [15/200], Train Loss: 0.004895
Validation Loss: 0.00377211
Epoch [16/200], Train Loss: 0.004852
Validation Loss: 0.00377184
Epoch [17/200], Train Loss: 0.004855
Validation Loss: 0.00377146
Epoch [18/200], Train Loss: 0.004871
Validation Loss: 0.00377529
Epoch [19/200], Train Loss: 0.004859
Validation Loss: 0.00377212
Epoch [20/200], Train Loss: 0.004904
Validation Loss: 0.00377138
Epoch [21/200], Train Loss: 0.004855
Validation Loss: 0.00377222
Epoch [22/200], Train Loss: 0.004854
Validation Loss: 0.00376920
Epoch [23/200], Train Loss: 0.004838
Validation Loss: 0.00376504
Epoch [24/200], Train Loss: 0.004859
Validation Loss: 0.00376726
Epoch [25/200], Train Loss: 0.004906
Validation Loss: 0.00376485
Epoch [26/200], Train Loss: 0.004889
Validation Loss: 0.00376647
Epoch [27/200], Train Loss: 0.004880
Validation Loss: 0.00376029
Epoch [28/200], Train Loss: 0.004860
Validation Loss: 0.00375895
Epoch [29/200], Train Loss: 0.004885
Validation Loss: 0.00376457
Epoch [30/200], Train Loss: 0.004871
Validation Loss: 0.00375657
Epoch [31/200], Train Loss: 0.004903
Validation Loss: 0.00375533
Epoch [32/200], Train Loss: 0.004855
Validation Loss: 0.00375061
Epoch [33/200], Train Loss: 0.004825
Validation Loss: 0.00374863
Epoch [34/200], Train Loss: 0.004890
Validation Loss: 0.00374914
Epoch [35/200], Train Loss: 0.004835
Validation Loss: 0.00374892
Epoch [36/200], Train Loss: 0.004839
Validation Loss: 0.00374315
Epoch [37/200], Train Loss: 0.004856
Validation Loss: 0.00374142
Epoch [38/200], Train Loss: 0.004836
Validation Loss: 0.00373664
Epoch [39/200], Train Loss: 0.004876
Validation Loss: 0.00373671
Epoch [40/200], Train Loss: 0.004835
Validation Loss: 0.00373019
Epoch [41/200], Train Loss: 0.004834
Validation Loss: 0.00373963
Epoch [42/200], Train Loss: 0.004803
Validation Loss: 0.00373058
Epoch [43/200], Train Loss: 0.004831
Validation Loss: 0.00372425
Epoch [44/200], Train Loss: 0.004823
Validation Loss: 0.00372181
Epoch [45/200], Train Loss: 0.004803
Validation Loss: 0.00372501
Epoch [46/200], Train Loss: 0.004827
Validation Loss: 0.00371452
Epoch [47/200], Train Loss: 0.004789
Validation Loss: 0.00370941
Epoch [48/200], Train Loss: 0.004842
Validation Loss: 0.00370291
Epoch [49/200], Train Loss: 0.004788
Validation Loss: 0.00371009
Epoch [50/200], Train Loss: 0.004773
Validation Loss: 0.00369746
Epoch [51/200], Train Loss: 0.004752
Validation Loss: 0.00369080
Epoch [52/200], Train Loss: 0.004811
Validation Loss: 0.00368715
Epoch [53/200], Train Loss: 0.004704
Validation Loss: 0.00368114
Epoch [54/200], Train Loss: 0.004767
Validation Loss: 0.00368022
Epoch [55/200], Train Loss: 0.004741
Validation Loss: 0.00366993
Epoch [56/200], Train Loss: 0.004747
Validation Loss: 0.00367065
Epoch [57/200], Train Loss: 0.004693
Validation Loss: 0.00363466
Epoch [58/200], Train Loss: 0.004703
Validation Loss: 0.00363958
Epoch [59/200], Train Loss: 0.004694
Validation Loss: 0.00361712
Epoch [60/200], Train Loss: 0.004661
Validation Loss: 0.00362421
Epoch [61/200], Train Loss: 0.004617
Validation Loss: 0.00362325
Epoch [62/200], Train Loss: 0.004642
Validation Loss: 0.00355174
Epoch [63/200], Train Loss: 0.004589
Validation Loss: 0.00353701
Epoch [64/200], Train Loss: 0.004549
Validation Loss: 0.00352094
Epoch [65/200], Train Loss: 0.004547
Validation Loss: 0.00350322
Epoch [66/200], Train Loss: 0.004501
Validation Loss: 0.00347146
Epoch [67/200], Train Loss: 0.004489
Validation Loss: 0.00347396
Epoch [68/200], Train Loss: 0.004422
Validation Loss: 0.00341187
Epoch [69/200], Train Loss: 0.004410
Validation Loss: 0.00339817
Epoch [70/200], Train Loss: 0.004348
Validation Loss: 0.00336398
Epoch [71/200], Train Loss: 0.004365
Validation Loss: 0.00337819
Epoch [72/200], Train Loss: 0.004317
Validation Loss: 0.00333036
Epoch [73/200], Train Loss: 0.004276
Validation Loss: 0.00329139
Epoch [74/200], Train Loss: 0.004282
Validation Loss: 0.00326601
Epoch [75/200], Train Loss: 0.004239
Validation Loss: 0.00324551
Epoch [76/200], Train Loss: 0.004162
Validation Loss: 0.00320805
Epoch [77/200], Train Loss: 0.004132
Validation Loss: 0.00318565
Epoch [78/200], Train Loss: 0.004124
Validation Loss: 0.00315424
Epoch [79/200], Train Loss: 0.004075
Validation Loss: 0.00312587
Epoch [80/200], Train Loss: 0.004029
Validation Loss: 0.00309253
Epoch [81/200], Train Loss: 0.004023
Validation Loss: 0.00311611
Epoch [82/200], Train Loss: 0.003975
Validation Loss: 0.00302874
Epoch [83/200], Train Loss: 0.003910
Validation Loss: 0.00299778
Epoch [84/200], Train Loss: 0.003877
Validation Loss: 0.00295616
Epoch [85/200], Train Loss: 0.003805
Validation Loss: 0.00291259
Epoch [86/200], Train Loss: 0.003795
Validation Loss: 0.00287469
Epoch [87/200], Train Loss: 0.003725
Validation Loss: 0.00284036
Epoch [88/200], Train Loss: 0.003673
Validation Loss: 0.00278486
Epoch [89/200], Train Loss: 0.003602
Validation Loss: 0.00273521
Epoch [90/200], Train Loss: 0.003562
Validation Loss: 0.00268843
Epoch [91/200], Train Loss: 0.003526
Validation Loss: 0.00265338
Epoch [92/200], Train Loss: 0.003451
Validation Loss: 0.00259952
Epoch [93/200], Train Loss: 0.003367
Validation Loss: 0.00256431
Epoch [94/200], Train Loss: 0.003318
Validation Loss: 0.00252173
Epoch [95/200], Train Loss: 0.003242
Validation Loss: 0.00248373
Epoch [96/200], Train Loss: 0.003192
Validation Loss: 0.00244766
Epoch [97/200], Train Loss: 0.003176
Validation Loss: 0.00240993
Epoch [98/200], Train Loss: 0.003087
Validation Loss: 0.00236833
Epoch [99/200], Train Loss: 0.003091
Validation Loss: 0.00238721
Epoch [100/200], Train Loss: 0.003029
Validation Loss: 0.00231449
Epoch [101/200], Train Loss: 0.002974
Validation Loss: 0.00231265
Epoch [102/200], Train Loss: 0.002928
Validation Loss: 0.00225015
Epoch [103/200], Train Loss: 0.002873
Validation Loss: 0.00220673
Epoch [104/200], Train Loss: 0.002827
Validation Loss: 0.00217027
Epoch [105/200], Train Loss: 0.002779
Validation Loss: 0.00215207
Epoch [106/200], Train Loss: 0.002761
Validation Loss: 0.00211521
Epoch [107/200], Train Loss: 0.002725
Validation Loss: 0.00217521
Epoch [108/200], Train Loss: 0.002713
Validation Loss: 0.00210356
Epoch [109/200], Train Loss: 0.002657
Validation Loss: 0.00207557
Epoch [110/200], Train Loss: 0.002607
Validation Loss: 0.00205448
Epoch [111/200], Train Loss: 0.002603
Validation Loss: 0.00204596
Epoch [112/200], Train Loss: 0.002585
Validation Loss: 0.00203438
Epoch [113/200], Train Loss: 0.002565
Validation Loss: 0.00198571
Epoch [114/200], Train Loss: 0.002551
Validation Loss: 0.00197760
Epoch [115/200], Train Loss: 0.002503
Validation Loss: 0.00198566
Epoch [116/200], Train Loss: 0.002502
Validation Loss: 0.00195835
Epoch [117/200], Train Loss: 0.002459
Validation Loss: 0.00195577
Epoch [118/200], Train Loss: 0.002434
Validation Loss: 0.00192968
Epoch [119/200], Train Loss: 0.002436
Validation Loss: 0.00191531
Epoch [120/200], Train Loss: 0.002399
Validation Loss: 0.00190649
Epoch [121/200], Train Loss: 0.002376
Validation Loss: 0.00188607
Epoch [122/200], Train Loss: 0.002377
Validation Loss: 0.00186843
Epoch [123/200], Train Loss: 0.002366
Validation Loss: 0.00187023
Epoch [124/200], Train Loss: 0.002334
Validation Loss: 0.00185898
Epoch [125/200], Train Loss: 0.002330
Validation Loss: 0.00187187
Epoch [126/200], Train Loss: 0.002318
Validation Loss: 0.00182796
Epoch [127/200], Train Loss: 0.002280
Validation Loss: 0.00182645
Epoch [128/200], Train Loss: 0.002268
Validation Loss: 0.00181173
Epoch [129/200], Train Loss: 0.002254
Validation Loss: 0.00182449
Epoch [130/200], Train Loss: 0.002257
Validation Loss: 0.00182535
Epoch [131/200], Train Loss: 0.002238
Validation Loss: 0.00179901
Epoch [132/200], Train Loss: 0.002215
Validation Loss: 0.00177765
Epoch [133/200], Train Loss: 0.002214
Validation Loss: 0.00178194
Epoch [134/200], Train Loss: 0.002194
Validation Loss: 0.00176126
Epoch [135/200], Train Loss: 0.002208
Validation Loss: 0.00177516
Epoch [136/200], Train Loss: 0.002197
Validation Loss: 0.00176634
Epoch [137/200], Train Loss: 0.002162
Validation Loss: 0.00173780
Epoch [138/200], Train Loss: 0.002166
Validation Loss: 0.00175232
Epoch [139/200], Train Loss: 0.002142
Validation Loss: 0.00172798
Epoch [140/200], Train Loss: 0.002136
Validation Loss: 0.00170811
Epoch [141/200], Train Loss: 0.002121
Validation Loss: 0.00171484
Epoch [142/200], Train Loss: 0.002100
Validation Loss: 0.00171355
Epoch [143/200], Train Loss: 0.002110
Validation Loss: 0.00170645
Epoch [144/200], Train Loss: 0.002087
Validation Loss: 0.00168374
Epoch [145/200], Train Loss: 0.002088
Validation Loss: 0.00169681
Epoch [146/200], Train Loss: 0.002074
Validation Loss: 0.00168879
Epoch [147/200], Train Loss: 0.002098
Validation Loss: 0.00167258
Epoch [148/200], Train Loss: 0.002071
Validation Loss: 0.00167785
Epoch [149/200], Train Loss: 0.002067
Validation Loss: 0.00166120
Epoch [150/200], Train Loss: 0.002045
Validation Loss: 0.00165282
Epoch [151/200], Train Loss: 0.002036
Validation Loss: 0.00165343
Epoch [152/200], Train Loss: 0.002010
Validation Loss: 0.00163752
Epoch [153/200], Train Loss: 0.001998
Validation Loss: 0.00163690
Epoch [154/200], Train Loss: 0.001999
Validation Loss: 0.00162208
Epoch [155/200], Train Loss: 0.002002
Validation Loss: 0.00161617
Epoch [156/200], Train Loss: 0.001984
Validation Loss: 0.00161619
Epoch [157/200], Train Loss: 0.001978
Validation Loss: 0.00160222
Epoch [158/200], Train Loss: 0.001961
Validation Loss: 0.00160939
Epoch [159/200], Train Loss: 0.001966
Validation Loss: 0.00159287
Epoch [160/200], Train Loss: 0.001953
Validation Loss: 0.00160756
Epoch [161/200], Train Loss: 0.001966
Validation Loss: 0.00158200
Epoch [162/200], Train Loss: 0.001949
Validation Loss: 0.00158211
Epoch [163/200], Train Loss: 0.001931
Validation Loss: 0.00156541
Epoch [164/200], Train Loss: 0.001947
Validation Loss: 0.00155917
Epoch [165/200], Train Loss: 0.001935
Validation Loss: 0.00157809
Epoch [166/200], Train Loss: 0.001939
Validation Loss: 0.00154955
Epoch [167/200], Train Loss: 0.001927
Validation Loss: 0.00155976
Epoch [168/200], Train Loss: 0.001889
Validation Loss: 0.00155408
Epoch [169/200], Train Loss: 0.001898
Validation Loss: 0.00154733
Epoch [170/200], Train Loss: 0.001890
Validation Loss: 0.00153881
Epoch [171/200], Train Loss: 0.001857
Validation Loss: 0.00153532
Epoch [172/200], Train Loss: 0.001865
Validation Loss: 0.00154093
Epoch [173/200], Train Loss: 0.001883
Validation Loss: 0.00153657
Epoch [174/200], Train Loss: 0.001861
Validation Loss: 0.00152286
Epoch [175/200], Train Loss: 0.001856
Validation Loss: 0.00150969
Epoch [176/200], Train Loss: 0.001860
Validation Loss: 0.00152052
Epoch [177/200], Train Loss: 0.001838
Validation Loss: 0.00152298
Epoch [178/200], Train Loss: 0.001850
Validation Loss: 0.00149344
Epoch [179/200], Train Loss: 0.001838
Validation Loss: 0.00148442
Epoch [180/200], Train Loss: 0.001819
Validation Loss: 0.00149077
Epoch [181/200], Train Loss: 0.001818
Validation Loss: 0.00147684
Epoch [182/200], Train Loss: 0.001794
Validation Loss: 0.00148093
Epoch [183/200], Train Loss: 0.001798
Validation Loss: 0.00148915
Epoch [184/200], Train Loss: 0.001793
Validation Loss: 0.00147097
Epoch [185/200], Train Loss: 0.001804
Validation Loss: 0.00147162
Epoch [186/200], Train Loss: 0.001771
Validation Loss: 0.00147706
Epoch [187/200], Train Loss: 0.001795
Validation Loss: 0.00146004
Epoch [188/200], Train Loss: 0.001777
Validation Loss: 0.00145614
Epoch [189/200], Train Loss: 0.001743
Validation Loss: 0.00146088
Epoch [190/200], Train Loss: 0.001755
Validation Loss: 0.00144464
Epoch [191/200], Train Loss: 0.001782
Validation Loss: 0.00146302
Epoch [192/200], Train Loss: 0.001746
Validation Loss: 0.00145079
Epoch [193/200], Train Loss: 0.001764
Validation Loss: 0.00144711
Epoch [194/200], Train Loss: 0.001745
Validation Loss: 0.00144632
Epoch [195/200], Train Loss: 0.001724
Validation Loss: 0.00143168
Epoch [196/200], Train Loss: 0.001732
Validation Loss: 0.00143430
Epoch [197/200], Train Loss: 0.001709
Validation Loss: 0.00142443
Epoch [198/200], Train Loss: 0.001720
Validation Loss: 0.00142378
Epoch [199/200], Train Loss: 0.001709
Validation Loss: 0.00143254
Epoch [200/200], Train Loss: 0.001742
Validation Loss: 0.00141631

Evaluating model for: Lamp
Run 65/144 completed in 698.77 seconds with: {'MAE': np.float32(1.407663), 'MSE': np.float32(53.41226), 'RMSE': np.float32(7.308369), 'SAE': np.float32(0.015744135), 'NDE': np.float32(0.56554055)}

Run 66/144: hidden=128, seq_len=360, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.005746
Validation Loss: 0.00385853
Epoch [2/200], Train Loss: 0.004982
Validation Loss: 0.00385791
Epoch [3/200], Train Loss: 0.005014
Validation Loss: 0.00384821
Epoch [4/200], Train Loss: 0.004998
Validation Loss: 0.00384261
Epoch [5/200], Train Loss: 0.004980
Validation Loss: 0.00383647
Epoch [6/200], Train Loss: 0.004953
Validation Loss: 0.00382203
Epoch [7/200], Train Loss: 0.004919
Validation Loss: 0.00380247
Epoch [8/200], Train Loss: 0.004957
Validation Loss: 0.00378904
Epoch [9/200], Train Loss: 0.004914
Validation Loss: 0.00378602
Epoch [10/200], Train Loss: 0.004900
Validation Loss: 0.00378518
Epoch [11/200], Train Loss: 0.004879
Validation Loss: 0.00378154
Epoch [12/200], Train Loss: 0.004910
Validation Loss: 0.00378153
Epoch [13/200], Train Loss: 0.004895
Validation Loss: 0.00378067
Epoch [14/200], Train Loss: 0.004883
Validation Loss: 0.00378050
Epoch [15/200], Train Loss: 0.004946
Validation Loss: 0.00377637
Epoch [16/200], Train Loss: 0.004880
Validation Loss: 0.00377649
Epoch [17/200], Train Loss: 0.004911
Validation Loss: 0.00377552
Epoch [18/200], Train Loss: 0.004863
Validation Loss: 0.00378362
Epoch [19/200], Train Loss: 0.004878
Validation Loss: 0.00377715
Epoch [20/200], Train Loss: 0.004876
Validation Loss: 0.00377455
Epoch [21/200], Train Loss: 0.004884
Validation Loss: 0.00377473
Epoch [22/200], Train Loss: 0.004872
Validation Loss: 0.00378195
Epoch [23/200], Train Loss: 0.004861
Validation Loss: 0.00377255
Epoch [24/200], Train Loss: 0.004861
Validation Loss: 0.00377393
Epoch [25/200], Train Loss: 0.004839
Validation Loss: 0.00376945
Epoch [26/200], Train Loss: 0.004848
Validation Loss: 0.00376827
Epoch [27/200], Train Loss: 0.004854
Validation Loss: 0.00376842
Epoch [28/200], Train Loss: 0.004891
Validation Loss: 0.00376916
Epoch [29/200], Train Loss: 0.004850
Validation Loss: 0.00376927
Epoch [30/200], Train Loss: 0.004858
Validation Loss: 0.00376687
Epoch [31/200], Train Loss: 0.004887
Validation Loss: 0.00376629
Epoch [32/200], Train Loss: 0.004860
Validation Loss: 0.00376589
Epoch [33/200], Train Loss: 0.004905
Validation Loss: 0.00376385
Epoch [34/200], Train Loss: 0.004860
Validation Loss: 0.00377043
Epoch [35/200], Train Loss: 0.004883
Validation Loss: 0.00378385
Epoch [36/200], Train Loss: 0.004918
Validation Loss: 0.00376588
Epoch [37/200], Train Loss: 0.004855
Validation Loss: 0.00376246
Epoch [38/200], Train Loss: 0.004857
Validation Loss: 0.00376761
Epoch [39/200], Train Loss: 0.004842
Validation Loss: 0.00376172
Epoch [40/200], Train Loss: 0.004897
Validation Loss: 0.00376078
Epoch [41/200], Train Loss: 0.004892
Validation Loss: 0.00376229
Epoch [42/200], Train Loss: 0.004893
Validation Loss: 0.00376348
Epoch [43/200], Train Loss: 0.004855
Validation Loss: 0.00375841
Epoch [44/200], Train Loss: 0.004840
Validation Loss: 0.00375992
Epoch [45/200], Train Loss: 0.004867
Validation Loss: 0.00376115
Epoch [46/200], Train Loss: 0.004813
Validation Loss: 0.00375601
Epoch [47/200], Train Loss: 0.004853
Validation Loss: 0.00375751
Epoch [48/200], Train Loss: 0.004897
Validation Loss: 0.00375538
Epoch [49/200], Train Loss: 0.004914
Validation Loss: 0.00375479
Epoch [50/200], Train Loss: 0.004874
Validation Loss: 0.00376284
Epoch [51/200], Train Loss: 0.004869
Validation Loss: 0.00376763
Epoch [52/200], Train Loss: 0.004868
Validation Loss: 0.00375940
Epoch [53/200], Train Loss: 0.004885
Validation Loss: 0.00375614
Epoch [54/200], Train Loss: 0.004826
Validation Loss: 0.00375793
Epoch [55/200], Train Loss: 0.004907
Validation Loss: 0.00375402
Epoch [56/200], Train Loss: 0.004850
Validation Loss: 0.00375586
Epoch [57/200], Train Loss: 0.004837
Validation Loss: 0.00375391
Epoch [58/200], Train Loss: 0.004842
Validation Loss: 0.00375576
Epoch [59/200], Train Loss: 0.004837
Validation Loss: 0.00375520
Epoch [60/200], Train Loss: 0.004920
Validation Loss: 0.00375501
Epoch [61/200], Train Loss: 0.004856
Validation Loss: 0.00375274
Epoch [62/200], Train Loss: 0.004853
Validation Loss: 0.00375794
Epoch [63/200], Train Loss: 0.004833
Validation Loss: 0.00375214
Epoch [64/200], Train Loss: 0.004887
Validation Loss: 0.00375581
Epoch [65/200], Train Loss: 0.004819
Validation Loss: 0.00375763
Epoch [66/200], Train Loss: 0.004852
Validation Loss: 0.00375275
Epoch [67/200], Train Loss: 0.004906
Validation Loss: 0.00375195
Epoch [68/200], Train Loss: 0.004832
Validation Loss: 0.00375235
Epoch [69/200], Train Loss: 0.004855
Validation Loss: 0.00375805
Epoch [70/200], Train Loss: 0.004838
Validation Loss: 0.00376407
Epoch [71/200], Train Loss: 0.004867
Validation Loss: 0.00376301
Epoch [72/200], Train Loss: 0.004861
Validation Loss: 0.00375834
Epoch [73/200], Train Loss: 0.004846
Validation Loss: 0.00375928
Epoch [74/200], Train Loss: 0.004862
Validation Loss: 0.00375015
Epoch [75/200], Train Loss: 0.004882
Validation Loss: 0.00375025
Epoch [76/200], Train Loss: 0.004894
Validation Loss: 0.00375339
Epoch [77/200], Train Loss: 0.004848
Validation Loss: 0.00375226
Epoch [78/200], Train Loss: 0.004842
Validation Loss: 0.00374940
Epoch [79/200], Train Loss: 0.004847
Validation Loss: 0.00374925
Epoch [80/200], Train Loss: 0.004882
Validation Loss: 0.00374908
Epoch [81/200], Train Loss: 0.004898
Validation Loss: 0.00374938
Epoch [82/200], Train Loss: 0.004901
Validation Loss: 0.00375053
Epoch [83/200], Train Loss: 0.004832
Validation Loss: 0.00375006
Epoch [84/200], Train Loss: 0.004829
Validation Loss: 0.00374941
Epoch [85/200], Train Loss: 0.004871
Validation Loss: 0.00375004
Epoch [86/200], Train Loss: 0.004892
Validation Loss: 0.00375274
Epoch [87/200], Train Loss: 0.004878
Validation Loss: 0.00375107
Epoch [88/200], Train Loss: 0.004870
Validation Loss: 0.00376354
Epoch [89/200], Train Loss: 0.004867
Validation Loss: 0.00375220
Epoch [90/200], Train Loss: 0.004870
Validation Loss: 0.00375401
Early stopping triggered

Evaluating model for: Lamp
Run 66/144 completed in 329.90 seconds with: {'MAE': np.float32(3.125404), 'MSE': np.float32(161.16354), 'RMSE': np.float32(12.695021), 'SAE': np.float32(0.29474154), 'NDE': np.float32(0.9823742)}

Run 67/144: hidden=128, seq_len=360, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.005514
Validation Loss: 0.00385502
Epoch [2/200], Train Loss: 0.005032
Validation Loss: 0.00385564
Epoch [3/200], Train Loss: 0.005038
Validation Loss: 0.00384183
Epoch [4/200], Train Loss: 0.004947
Validation Loss: 0.00384159
Epoch [5/200], Train Loss: 0.005036
Validation Loss: 0.00383789
Epoch [6/200], Train Loss: 0.004985
Validation Loss: 0.00382919
Epoch [7/200], Train Loss: 0.005006
Validation Loss: 0.00380832
Epoch [8/200], Train Loss: 0.004903
Validation Loss: 0.00378104
Epoch [9/200], Train Loss: 0.004905
Validation Loss: 0.00378239
Epoch [10/200], Train Loss: 0.004948
Validation Loss: 0.00377771
Epoch [11/200], Train Loss: 0.004948
Validation Loss: 0.00377808
Epoch [12/200], Train Loss: 0.004915
Validation Loss: 0.00377558
Epoch [13/200], Train Loss: 0.004908
Validation Loss: 0.00377474
Epoch [14/200], Train Loss: 0.004912
Validation Loss: 0.00378021
Epoch [15/200], Train Loss: 0.004885
Validation Loss: 0.00378478
Epoch [16/200], Train Loss: 0.004879
Validation Loss: 0.00377337
Epoch [17/200], Train Loss: 0.004870
Validation Loss: 0.00377389
Epoch [18/200], Train Loss: 0.004866
Validation Loss: 0.00377012
Epoch [19/200], Train Loss: 0.004863
Validation Loss: 0.00376956
Epoch [20/200], Train Loss: 0.004860
Validation Loss: 0.00376888
Epoch [21/200], Train Loss: 0.004864
Validation Loss: 0.00376846
Epoch [22/200], Train Loss: 0.004879
Validation Loss: 0.00378017
Epoch [23/200], Train Loss: 0.004901
Validation Loss: 0.00376577
Epoch [24/200], Train Loss: 0.004887
Validation Loss: 0.00376737
Epoch [25/200], Train Loss: 0.004895
Validation Loss: 0.00376879
Epoch [26/200], Train Loss: 0.004833
Validation Loss: 0.00376645
Epoch [27/200], Train Loss: 0.004836
Validation Loss: 0.00377020
Epoch [28/200], Train Loss: 0.004891
Validation Loss: 0.00376837
Epoch [29/200], Train Loss: 0.004868
Validation Loss: 0.00377061
Epoch [30/200], Train Loss: 0.004855
Validation Loss: 0.00376908
Epoch [31/200], Train Loss: 0.004929
Validation Loss: 0.00376321
Epoch [32/200], Train Loss: 0.004878
Validation Loss: 0.00376160
Epoch [33/200], Train Loss: 0.004868
Validation Loss: 0.00376006
Epoch [34/200], Train Loss: 0.004852
Validation Loss: 0.00375922
Epoch [35/200], Train Loss: 0.004824
Validation Loss: 0.00376272
Epoch [36/200], Train Loss: 0.004869
Validation Loss: 0.00376319
Epoch [37/200], Train Loss: 0.004844
Validation Loss: 0.00376244
Epoch [38/200], Train Loss: 0.004847
Validation Loss: 0.00375680
Epoch [39/200], Train Loss: 0.004855
Validation Loss: 0.00375895
Epoch [40/200], Train Loss: 0.004865
Validation Loss: 0.00375576
Epoch [41/200], Train Loss: 0.004892
Validation Loss: 0.00375685
Epoch [42/200], Train Loss: 0.004849
Validation Loss: 0.00375738
Epoch [43/200], Train Loss: 0.004868
Validation Loss: 0.00375506
Epoch [44/200], Train Loss: 0.004847
Validation Loss: 0.00375643
Epoch [45/200], Train Loss: 0.004876
Validation Loss: 0.00376238
Epoch [46/200], Train Loss: 0.004863
Validation Loss: 0.00375722
Epoch [47/200], Train Loss: 0.004897
Validation Loss: 0.00376766
Epoch [48/200], Train Loss: 0.004858
Validation Loss: 0.00376061
Epoch [49/200], Train Loss: 0.004867
Validation Loss: 0.00376450
Epoch [50/200], Train Loss: 0.004828
Validation Loss: 0.00375424
Epoch [51/200], Train Loss: 0.004883
Validation Loss: 0.00375412
Epoch [52/200], Train Loss: 0.004863
Validation Loss: 0.00375503
Epoch [53/200], Train Loss: 0.004897
Validation Loss: 0.00375512
Epoch [54/200], Train Loss: 0.004875
Validation Loss: 0.00375314
Epoch [55/200], Train Loss: 0.004840
Validation Loss: 0.00375480
Epoch [56/200], Train Loss: 0.004823
Validation Loss: 0.00375361
Epoch [57/200], Train Loss: 0.004863
Validation Loss: 0.00375490
Epoch [58/200], Train Loss: 0.004867
Validation Loss: 0.00375297
Epoch [59/200], Train Loss: 0.004838
Validation Loss: 0.00375323
Epoch [60/200], Train Loss: 0.004878
Validation Loss: 0.00375272
Epoch [61/200], Train Loss: 0.004843
Validation Loss: 0.00375239
Epoch [62/200], Train Loss: 0.004846
Validation Loss: 0.00375358
Epoch [63/200], Train Loss: 0.004867
Validation Loss: 0.00375376
Epoch [64/200], Train Loss: 0.004868
Validation Loss: 0.00375424
Epoch [65/200], Train Loss: 0.004896
Validation Loss: 0.00375144
Epoch [66/200], Train Loss: 0.004812
Validation Loss: 0.00375206
Epoch [67/200], Train Loss: 0.004888
Validation Loss: 0.00375435
Epoch [68/200], Train Loss: 0.004834
Validation Loss: 0.00375264
Epoch [69/200], Train Loss: 0.004854
Validation Loss: 0.00375886
Epoch [70/200], Train Loss: 0.004848
Validation Loss: 0.00375562
Epoch [71/200], Train Loss: 0.004893
Validation Loss: 0.00375733
Epoch [72/200], Train Loss: 0.004848
Validation Loss: 0.00375238
Epoch [73/200], Train Loss: 0.004880
Validation Loss: 0.00375199
Epoch [74/200], Train Loss: 0.004834
Validation Loss: 0.00375097
Epoch [75/200], Train Loss: 0.004851
Validation Loss: 0.00375753
Epoch [76/200], Train Loss: 0.004862
Validation Loss: 0.00375228
Epoch [77/200], Train Loss: 0.004841
Validation Loss: 0.00375095
Epoch [78/200], Train Loss: 0.004906
Validation Loss: 0.00375163
Epoch [79/200], Train Loss: 0.004873
Validation Loss: 0.00375442
Epoch [80/200], Train Loss: 0.004852
Validation Loss: 0.00375288
Epoch [81/200], Train Loss: 0.004825
Validation Loss: 0.00375169
Epoch [82/200], Train Loss: 0.004872
Validation Loss: 0.00375252
Epoch [83/200], Train Loss: 0.004861
Validation Loss: 0.00375189
Epoch [84/200], Train Loss: 0.004865
Validation Loss: 0.00375056
Epoch [85/200], Train Loss: 0.004869
Validation Loss: 0.00375706
Epoch [86/200], Train Loss: 0.004823
Validation Loss: 0.00375217
Epoch [87/200], Train Loss: 0.004875
Validation Loss: 0.00375300
Epoch [88/200], Train Loss: 0.004866
Validation Loss: 0.00375018
Epoch [89/200], Train Loss: 0.004904
Validation Loss: 0.00375353
Epoch [90/200], Train Loss: 0.004836
Validation Loss: 0.00375587
Epoch [91/200], Train Loss: 0.004860
Validation Loss: 0.00374976
Epoch [92/200], Train Loss: 0.004882
Validation Loss: 0.00375007
Epoch [93/200], Train Loss: 0.004876
Validation Loss: 0.00374967
Epoch [94/200], Train Loss: 0.004847
Validation Loss: 0.00375741
Epoch [95/200], Train Loss: 0.004838
Validation Loss: 0.00375237
Epoch [96/200], Train Loss: 0.004856
Validation Loss: 0.00374893
Epoch [97/200], Train Loss: 0.004880
Validation Loss: 0.00375067
Epoch [98/200], Train Loss: 0.004911
Validation Loss: 0.00374867
Epoch [99/200], Train Loss: 0.004880
Validation Loss: 0.00374933
Epoch [100/200], Train Loss: 0.004833
Validation Loss: 0.00375071
Epoch [101/200], Train Loss: 0.004844
Validation Loss: 0.00374996
Epoch [102/200], Train Loss: 0.004822
Validation Loss: 0.00374898
Epoch [103/200], Train Loss: 0.004811
Validation Loss: 0.00375236
Epoch [104/200], Train Loss: 0.004861
Validation Loss: 0.00375083
Epoch [105/200], Train Loss: 0.004825
Validation Loss: 0.00374912
Epoch [106/200], Train Loss: 0.004944
Validation Loss: 0.00374940
Epoch [107/200], Train Loss: 0.004869
Validation Loss: 0.00374884
Epoch [108/200], Train Loss: 0.004861
Validation Loss: 0.00375151
Early stopping triggered

Evaluating model for: Lamp
Run 67/144 completed in 410.59 seconds with: {'MAE': np.float32(2.9382687), 'MSE': np.float32(161.0438), 'RMSE': np.float32(12.690303), 'SAE': np.float32(0.17539899), 'NDE': np.float32(0.9820085)}

Run 68/144: hidden=128, seq_len=360, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.005694
Validation Loss: 0.00390356
Epoch [2/200], Train Loss: 0.005029
Validation Loss: 0.00384479
Epoch [3/200], Train Loss: 0.005044
Validation Loss: 0.00384962
Epoch [4/200], Train Loss: 0.005036
Validation Loss: 0.00384414
Epoch [5/200], Train Loss: 0.005059
Validation Loss: 0.00385085
Epoch [6/200], Train Loss: 0.005001
Validation Loss: 0.00384126
Epoch [7/200], Train Loss: 0.004993
Validation Loss: 0.00383493
Epoch [8/200], Train Loss: 0.004961
Validation Loss: 0.00381935
Epoch [9/200], Train Loss: 0.004952
Validation Loss: 0.00379122
Epoch [10/200], Train Loss: 0.004908
Validation Loss: 0.00378384
Epoch [11/200], Train Loss: 0.004911
Validation Loss: 0.00378183
Epoch [12/200], Train Loss: 0.004878
Validation Loss: 0.00378349
Epoch [13/200], Train Loss: 0.004920
Validation Loss: 0.00377822
Epoch [14/200], Train Loss: 0.004931
Validation Loss: 0.00378056
Epoch [15/200], Train Loss: 0.004944
Validation Loss: 0.00377878
Epoch [16/200], Train Loss: 0.004917
Validation Loss: 0.00377380
Epoch [17/200], Train Loss: 0.004876
Validation Loss: 0.00377573
Epoch [18/200], Train Loss: 0.004943
Validation Loss: 0.00377457
Epoch [19/200], Train Loss: 0.004855
Validation Loss: 0.00377015
Epoch [20/200], Train Loss: 0.004890
Validation Loss: 0.00376941
Epoch [21/200], Train Loss: 0.004905
Validation Loss: 0.00376857
Epoch [22/200], Train Loss: 0.004898
Validation Loss: 0.00377021
Epoch [23/200], Train Loss: 0.004915
Validation Loss: 0.00377292
Epoch [24/200], Train Loss: 0.004898
Validation Loss: 0.00376861
Epoch [25/200], Train Loss: 0.004860
Validation Loss: 0.00376421
Epoch [26/200], Train Loss: 0.004842
Validation Loss: 0.00376485
Epoch [27/200], Train Loss: 0.004882
Validation Loss: 0.00376134
Epoch [28/200], Train Loss: 0.004873
Validation Loss: 0.00375953
Epoch [29/200], Train Loss: 0.004905
Validation Loss: 0.00377259
Epoch [30/200], Train Loss: 0.004879
Validation Loss: 0.00375855
Epoch [31/200], Train Loss: 0.004874
Validation Loss: 0.00375887
Epoch [32/200], Train Loss: 0.004857
Validation Loss: 0.00376153
Epoch [33/200], Train Loss: 0.004914
Validation Loss: 0.00375894
Epoch [34/200], Train Loss: 0.004883
Validation Loss: 0.00375814
Epoch [35/200], Train Loss: 0.004871
Validation Loss: 0.00375620
Epoch [36/200], Train Loss: 0.004920
Validation Loss: 0.00375878
Epoch [37/200], Train Loss: 0.004859
Validation Loss: 0.00375400
Epoch [38/200], Train Loss: 0.004870
Validation Loss: 0.00375443
Epoch [39/200], Train Loss: 0.004897
Validation Loss: 0.00375693
Epoch [40/200], Train Loss: 0.004895
Validation Loss: 0.00375311
Epoch [41/200], Train Loss: 0.004863
Validation Loss: 0.00375444
Epoch [42/200], Train Loss: 0.004896
Validation Loss: 0.00376201
Epoch [43/200], Train Loss: 0.004895
Validation Loss: 0.00375504
Epoch [44/200], Train Loss: 0.004865
Validation Loss: 0.00375774
Epoch [45/200], Train Loss: 0.004882
Validation Loss: 0.00375297
Epoch [46/200], Train Loss: 0.004867
Validation Loss: 0.00375122
Epoch [47/200], Train Loss: 0.004898
Validation Loss: 0.00375197
Epoch [48/200], Train Loss: 0.004905
Validation Loss: 0.00376081
Epoch [49/200], Train Loss: 0.004836
Validation Loss: 0.00375450
Epoch [50/200], Train Loss: 0.004859
Validation Loss: 0.00375049
Epoch [51/200], Train Loss: 0.004906
Validation Loss: 0.00375128
Epoch [52/200], Train Loss: 0.004891
Validation Loss: 0.00376209
Epoch [53/200], Train Loss: 0.004918
Validation Loss: 0.00375535
Epoch [54/200], Train Loss: 0.004906
Validation Loss: 0.00375352
Epoch [55/200], Train Loss: 0.004891
Validation Loss: 0.00375713
Epoch [56/200], Train Loss: 0.004870
Validation Loss: 0.00376293
Epoch [57/200], Train Loss: 0.004899
Validation Loss: 0.00375555
Epoch [58/200], Train Loss: 0.004863
Validation Loss: 0.00375170
Epoch [59/200], Train Loss: 0.004898
Validation Loss: 0.00374824
Epoch [60/200], Train Loss: 0.004866
Validation Loss: 0.00374964
Epoch [61/200], Train Loss: 0.004865
Validation Loss: 0.00374821
Epoch [62/200], Train Loss: 0.004839
Validation Loss: 0.00374944
Epoch [63/200], Train Loss: 0.004838
Validation Loss: 0.00374840
Epoch [64/200], Train Loss: 0.004889
Validation Loss: 0.00374829
Epoch [65/200], Train Loss: 0.004883
Validation Loss: 0.00374766
Epoch [66/200], Train Loss: 0.004825
Validation Loss: 0.00374904
Epoch [67/200], Train Loss: 0.004889
Validation Loss: 0.00374886
Epoch [68/200], Train Loss: 0.004847
Validation Loss: 0.00374757
Epoch [69/200], Train Loss: 0.004845
Validation Loss: 0.00375067
Epoch [70/200], Train Loss: 0.004878
Validation Loss: 0.00374721
Epoch [71/200], Train Loss: 0.004836
Validation Loss: 0.00375166
Epoch [72/200], Train Loss: 0.004874
Validation Loss: 0.00374716
Epoch [73/200], Train Loss: 0.004906
Validation Loss: 0.00374730
Epoch [74/200], Train Loss: 0.004897
Validation Loss: 0.00375162
Epoch [75/200], Train Loss: 0.004831
Validation Loss: 0.00374602
Epoch [76/200], Train Loss: 0.004883
Validation Loss: 0.00375185
Epoch [77/200], Train Loss: 0.004900
Validation Loss: 0.00375264
Epoch [78/200], Train Loss: 0.004880
Validation Loss: 0.00375247
Epoch [79/200], Train Loss: 0.004893
Validation Loss: 0.00374683
Epoch [80/200], Train Loss: 0.004839
Validation Loss: 0.00375066
Epoch [81/200], Train Loss: 0.004892
Validation Loss: 0.00376097
Epoch [82/200], Train Loss: 0.004881
Validation Loss: 0.00375396
Epoch [83/200], Train Loss: 0.004869
Validation Loss: 0.00375225
Epoch [84/200], Train Loss: 0.004848
Validation Loss: 0.00374602
Epoch [85/200], Train Loss: 0.004918
Validation Loss: 0.00374581
Epoch [86/200], Train Loss: 0.004847
Validation Loss: 0.00375499
Epoch [87/200], Train Loss: 0.004837
Validation Loss: 0.00375108
Epoch [88/200], Train Loss: 0.004857
Validation Loss: 0.00375031
Epoch [89/200], Train Loss: 0.004877
Validation Loss: 0.00374731
Epoch [90/200], Train Loss: 0.004847
Validation Loss: 0.00374702
Epoch [91/200], Train Loss: 0.004906
Validation Loss: 0.00374506
Epoch [92/200], Train Loss: 0.004854
Validation Loss: 0.00374449
Epoch [93/200], Train Loss: 0.004852
Validation Loss: 0.00374715
Epoch [94/200], Train Loss: 0.004896
Validation Loss: 0.00376711
Epoch [95/200], Train Loss: 0.004857
Validation Loss: 0.00374623
Epoch [96/200], Train Loss: 0.004858
Validation Loss: 0.00375126
Epoch [97/200], Train Loss: 0.004907
Validation Loss: 0.00374545
Epoch [98/200], Train Loss: 0.004845
Validation Loss: 0.00374450
Epoch [99/200], Train Loss: 0.004865
Validation Loss: 0.00374707
Epoch [100/200], Train Loss: 0.004870
Validation Loss: 0.00374405
Epoch [101/200], Train Loss: 0.004858
Validation Loss: 0.00374469
Epoch [102/200], Train Loss: 0.004881
Validation Loss: 0.00374661
Epoch [103/200], Train Loss: 0.004899
Validation Loss: 0.00374436
Epoch [104/200], Train Loss: 0.004844
Validation Loss: 0.00374503
Epoch [105/200], Train Loss: 0.004920
Validation Loss: 0.00374473
Epoch [106/200], Train Loss: 0.004897
Validation Loss: 0.00374580
Epoch [107/200], Train Loss: 0.004881
Validation Loss: 0.00374438
Epoch [108/200], Train Loss: 0.004866
Validation Loss: 0.00374364
Epoch [109/200], Train Loss: 0.004862
Validation Loss: 0.00374526
Epoch [110/200], Train Loss: 0.004863
Validation Loss: 0.00374700
Epoch [111/200], Train Loss: 0.004870
Validation Loss: 0.00375166
Epoch [112/200], Train Loss: 0.004879
Validation Loss: 0.00374499
Epoch [113/200], Train Loss: 0.004861
Validation Loss: 0.00375136
Epoch [114/200], Train Loss: 0.004879
Validation Loss: 0.00374328
Epoch [115/200], Train Loss: 0.004868
Validation Loss: 0.00374488
Epoch [116/200], Train Loss: 0.004870
Validation Loss: 0.00374833
Epoch [117/200], Train Loss: 0.004878
Validation Loss: 0.00374365
Epoch [118/200], Train Loss: 0.004883
Validation Loss: 0.00374398
Epoch [119/200], Train Loss: 0.004878
Validation Loss: 0.00374796
Epoch [120/200], Train Loss: 0.004896
Validation Loss: 0.00374822
Epoch [121/200], Train Loss: 0.004875
Validation Loss: 0.00375082
Epoch [122/200], Train Loss: 0.004835
Validation Loss: 0.00375112
Epoch [123/200], Train Loss: 0.004854
Validation Loss: 0.00374273
Epoch [124/200], Train Loss: 0.004895
Validation Loss: 0.00374359
Epoch [125/200], Train Loss: 0.004870
Validation Loss: 0.00374626
Epoch [126/200], Train Loss: 0.004914
Validation Loss: 0.00374300
Epoch [127/200], Train Loss: 0.004871
Validation Loss: 0.00374532
Epoch [128/200], Train Loss: 0.004843
Validation Loss: 0.00374372
Epoch [129/200], Train Loss: 0.004910
Validation Loss: 0.00374294
Epoch [130/200], Train Loss: 0.004888
Validation Loss: 0.00374409
Epoch [131/200], Train Loss: 0.004871
Validation Loss: 0.00374174
Epoch [132/200], Train Loss: 0.004861
Validation Loss: 0.00374302
Epoch [133/200], Train Loss: 0.004867
Validation Loss: 0.00374179
Epoch [134/200], Train Loss: 0.004914
Validation Loss: 0.00374154
Epoch [135/200], Train Loss: 0.004878
Validation Loss: 0.00374141
Epoch [136/200], Train Loss: 0.004865
Validation Loss: 0.00374205
Epoch [137/200], Train Loss: 0.004862
Validation Loss: 0.00374629
Epoch [138/200], Train Loss: 0.004850
Validation Loss: 0.00374080
Epoch [139/200], Train Loss: 0.004882
Validation Loss: 0.00374551
Epoch [140/200], Train Loss: 0.004863
Validation Loss: 0.00374064
Epoch [141/200], Train Loss: 0.004835
Validation Loss: 0.00374266
Epoch [142/200], Train Loss: 0.004855
Validation Loss: 0.00374119
Epoch [143/200], Train Loss: 0.004805
Validation Loss: 0.00374018
Epoch [144/200], Train Loss: 0.004844
Validation Loss: 0.00375008
Epoch [145/200], Train Loss: 0.004837
Validation Loss: 0.00374188
Epoch [146/200], Train Loss: 0.004831
Validation Loss: 0.00373964
Epoch [147/200], Train Loss: 0.004890
Validation Loss: 0.00373989
Epoch [148/200], Train Loss: 0.004870
Validation Loss: 0.00373948
Epoch [149/200], Train Loss: 0.004839
Validation Loss: 0.00373909
Epoch [150/200], Train Loss: 0.004843
Validation Loss: 0.00374455
Epoch [151/200], Train Loss: 0.004940
Validation Loss: 0.00374129
Epoch [152/200], Train Loss: 0.004877
Validation Loss: 0.00373905
Epoch [153/200], Train Loss: 0.004872
Validation Loss: 0.00374087
Epoch [154/200], Train Loss: 0.004829
Validation Loss: 0.00373957
Epoch [155/200], Train Loss: 0.004874
Validation Loss: 0.00374311
Epoch [156/200], Train Loss: 0.004877
Validation Loss: 0.00373806
Epoch [157/200], Train Loss: 0.004906
Validation Loss: 0.00373986
Epoch [158/200], Train Loss: 0.004833
Validation Loss: 0.00373835
Epoch [159/200], Train Loss: 0.004856
Validation Loss: 0.00373825
Epoch [160/200], Train Loss: 0.004874
Validation Loss: 0.00373728
Epoch [161/200], Train Loss: 0.004859
Validation Loss: 0.00374321
Epoch [162/200], Train Loss: 0.004864
Validation Loss: 0.00373682
Epoch [163/200], Train Loss: 0.004817
Validation Loss: 0.00373777
Epoch [164/200], Train Loss: 0.004875
Validation Loss: 0.00374061
Epoch [165/200], Train Loss: 0.004805
Validation Loss: 0.00373701
Epoch [166/200], Train Loss: 0.004851
Validation Loss: 0.00374115
Epoch [167/200], Train Loss: 0.004834
Validation Loss: 0.00373778
Epoch [168/200], Train Loss: 0.004871
Validation Loss: 0.00374014
Epoch [169/200], Train Loss: 0.004849
Validation Loss: 0.00373680
Epoch [170/200], Train Loss: 0.004850
Validation Loss: 0.00373660
Epoch [171/200], Train Loss: 0.004852
Validation Loss: 0.00373776
Epoch [172/200], Train Loss: 0.004852
Validation Loss: 0.00374026
Epoch [173/200], Train Loss: 0.004860
Validation Loss: 0.00373538
Epoch [174/200], Train Loss: 0.004820
Validation Loss: 0.00373715
Epoch [175/200], Train Loss: 0.004863
Validation Loss: 0.00374436
Epoch [176/200], Train Loss: 0.004855
Validation Loss: 0.00373664
Epoch [177/200], Train Loss: 0.004830
Validation Loss: 0.00373860
Epoch [178/200], Train Loss: 0.004865
Validation Loss: 0.00373614
Epoch [179/200], Train Loss: 0.004881
Validation Loss: 0.00373556
Epoch [180/200], Train Loss: 0.004882
Validation Loss: 0.00373902
Epoch [181/200], Train Loss: 0.004856
Validation Loss: 0.00373487
Epoch [182/200], Train Loss: 0.004830
Validation Loss: 0.00373481
Epoch [183/200], Train Loss: 0.004846
Validation Loss: 0.00373786
Epoch [184/200], Train Loss: 0.004830
Validation Loss: 0.00373441
Epoch [185/200], Train Loss: 0.004812
Validation Loss: 0.00373844
Epoch [186/200], Train Loss: 0.004888
Validation Loss: 0.00373446
Epoch [187/200], Train Loss: 0.004849
Validation Loss: 0.00373573
Epoch [188/200], Train Loss: 0.004826
Validation Loss: 0.00373695
Epoch [189/200], Train Loss: 0.004849
Validation Loss: 0.00373309
Epoch [190/200], Train Loss: 0.004844
Validation Loss: 0.00373498
Epoch [191/200], Train Loss: 0.004833
Validation Loss: 0.00373375
Epoch [192/200], Train Loss: 0.004836
Validation Loss: 0.00373504
Epoch [193/200], Train Loss: 0.004820
Validation Loss: 0.00373543
Epoch [194/200], Train Loss: 0.004870
Validation Loss: 0.00373344
Epoch [195/200], Train Loss: 0.004875
Validation Loss: 0.00373342
Epoch [196/200], Train Loss: 0.004842
Validation Loss: 0.00373310
Epoch [197/200], Train Loss: 0.004828
Validation Loss: 0.00373357
Epoch [198/200], Train Loss: 0.004827
Validation Loss: 0.00373705
Epoch [199/200], Train Loss: 0.004853
Validation Loss: 0.00373532
Early stopping triggered

Evaluating model for: Lamp
Run 68/144 completed in 797.15 seconds with: {'MAE': np.float32(2.8618486), 'MSE': np.float32(160.71225), 'RMSE': np.float32(12.677234), 'SAE': np.float32(0.106269635), 'NDE': np.float32(0.98099685)}

Run 69/144: hidden=128, seq_len=360, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.005219
Validation Loss: 0.00545630
Epoch [2/200], Train Loss: 0.004713
Validation Loss: 0.00535482
Epoch [3/200], Train Loss: 0.004734
Validation Loss: 0.00534604
Epoch [4/200], Train Loss: 0.004679
Validation Loss: 0.00533930
Epoch [5/200], Train Loss: 0.004683
Validation Loss: 0.00533443
Epoch [6/200], Train Loss: 0.004689
Validation Loss: 0.00532367
Epoch [7/200], Train Loss: 0.004673
Validation Loss: 0.00531608
Epoch [8/200], Train Loss: 0.004707
Validation Loss: 0.00531349
Epoch [9/200], Train Loss: 0.004689
Validation Loss: 0.00530252
Epoch [10/200], Train Loss: 0.004652
Validation Loss: 0.00529672
Epoch [11/200], Train Loss: 0.004653
Validation Loss: 0.00528924
Epoch [12/200], Train Loss: 0.004668
Validation Loss: 0.00528512
Epoch [13/200], Train Loss: 0.004629
Validation Loss: 0.00527898
Epoch [14/200], Train Loss: 0.004634
Validation Loss: 0.00527613
Epoch [15/200], Train Loss: 0.004649
Validation Loss: 0.00527214
Epoch [16/200], Train Loss: 0.004643
Validation Loss: 0.00527059
Epoch [17/200], Train Loss: 0.004667
Validation Loss: 0.00527119
Epoch [18/200], Train Loss: 0.004636
Validation Loss: 0.00526785
Epoch [19/200], Train Loss: 0.004620
Validation Loss: 0.00526769
Epoch [20/200], Train Loss: 0.004631
Validation Loss: 0.00526649
Epoch [21/200], Train Loss: 0.004643
Validation Loss: 0.00526608
Epoch [22/200], Train Loss: 0.004655
Validation Loss: 0.00526595
Epoch [23/200], Train Loss: 0.004629
Validation Loss: 0.00526483
Epoch [24/200], Train Loss: 0.004616
Validation Loss: 0.00526543
Epoch [25/200], Train Loss: 0.004631
Validation Loss: 0.00526502
Epoch [26/200], Train Loss: 0.004617
Validation Loss: 0.00526295
Epoch [27/200], Train Loss: 0.004615
Validation Loss: 0.00526513
Epoch [28/200], Train Loss: 0.004619
Validation Loss: 0.00526232
Epoch [29/200], Train Loss: 0.004626
Validation Loss: 0.00526232
Epoch [30/200], Train Loss: 0.004637
Validation Loss: 0.00526199
Epoch [31/200], Train Loss: 0.004631
Validation Loss: 0.00526134
Epoch [32/200], Train Loss: 0.004655
Validation Loss: 0.00526117
Epoch [33/200], Train Loss: 0.004654
Validation Loss: 0.00526158
Epoch [34/200], Train Loss: 0.004631
Validation Loss: 0.00525909
Epoch [35/200], Train Loss: 0.004621
Validation Loss: 0.00525915
Epoch [36/200], Train Loss: 0.004606
Validation Loss: 0.00525796
Epoch [37/200], Train Loss: 0.004627
Validation Loss: 0.00525871
Epoch [38/200], Train Loss: 0.004631
Validation Loss: 0.00525704
Epoch [39/200], Train Loss: 0.004627
Validation Loss: 0.00525780
Epoch [40/200], Train Loss: 0.004638
Validation Loss: 0.00525550
Epoch [41/200], Train Loss: 0.004627
Validation Loss: 0.00525608
Epoch [42/200], Train Loss: 0.004614
Validation Loss: 0.00525497
Epoch [43/200], Train Loss: 0.004621
Validation Loss: 0.00525727
Epoch [44/200], Train Loss: 0.004632
Validation Loss: 0.00525362
Epoch [45/200], Train Loss: 0.004608
Validation Loss: 0.00525250
Epoch [46/200], Train Loss: 0.004629
Validation Loss: 0.00525207
Epoch [47/200], Train Loss: 0.004594
Validation Loss: 0.00525091
Epoch [48/200], Train Loss: 0.004625
Validation Loss: 0.00525150
Epoch [49/200], Train Loss: 0.004626
Validation Loss: 0.00524937
Epoch [50/200], Train Loss: 0.004614
Validation Loss: 0.00524875
Epoch [51/200], Train Loss: 0.004646
Validation Loss: 0.00525191
Epoch [52/200], Train Loss: 0.004637
Validation Loss: 0.00524707
Epoch [53/200], Train Loss: 0.004604
Validation Loss: 0.00524631
Epoch [54/200], Train Loss: 0.004613
Validation Loss: 0.00524565
Epoch [55/200], Train Loss: 0.004604
Validation Loss: 0.00524623
Epoch [56/200], Train Loss: 0.004608
Validation Loss: 0.00524269
Epoch [57/200], Train Loss: 0.004620
Validation Loss: 0.00524446
Epoch [58/200], Train Loss: 0.004627
Validation Loss: 0.00524298
Epoch [59/200], Train Loss: 0.004587
Validation Loss: 0.00523979
Epoch [60/200], Train Loss: 0.004629
Validation Loss: 0.00523836
Epoch [61/200], Train Loss: 0.004625
Validation Loss: 0.00523568
Epoch [62/200], Train Loss: 0.004593
Validation Loss: 0.00523443
Epoch [63/200], Train Loss: 0.004596
Validation Loss: 0.00523467
Epoch [64/200], Train Loss: 0.004631
Validation Loss: 0.00523275
Epoch [65/200], Train Loss: 0.004632
Validation Loss: 0.00522948
Epoch [66/200], Train Loss: 0.004620
Validation Loss: 0.00522932
Epoch [67/200], Train Loss: 0.004611
Validation Loss: 0.00522592
Epoch [68/200], Train Loss: 0.004595
Validation Loss: 0.00522491
Epoch [69/200], Train Loss: 0.004605
Validation Loss: 0.00522277
Epoch [70/200], Train Loss: 0.004597
Validation Loss: 0.00522164
Epoch [71/200], Train Loss: 0.004599
Validation Loss: 0.00522021
Epoch [72/200], Train Loss: 0.004587
Validation Loss: 0.00521954
Epoch [73/200], Train Loss: 0.004618
Validation Loss: 0.00521610
Epoch [74/200], Train Loss: 0.004582
Validation Loss: 0.00522311
Epoch [75/200], Train Loss: 0.004585
Validation Loss: 0.00521267
Epoch [76/200], Train Loss: 0.004606
Validation Loss: 0.00521146
Epoch [77/200], Train Loss: 0.004577
Validation Loss: 0.00521437
Epoch [78/200], Train Loss: 0.004603
Validation Loss: 0.00520887
Epoch [79/200], Train Loss: 0.004599
Validation Loss: 0.00521337
Epoch [80/200], Train Loss: 0.004575
Validation Loss: 0.00520629
Epoch [81/200], Train Loss: 0.004564
Validation Loss: 0.00520844
Epoch [82/200], Train Loss: 0.004603
Validation Loss: 0.00520356
Epoch [83/200], Train Loss: 0.004617
Validation Loss: 0.00520318
Epoch [84/200], Train Loss: 0.004564
Validation Loss: 0.00520243
Epoch [85/200], Train Loss: 0.004594
Validation Loss: 0.00519952
Epoch [86/200], Train Loss: 0.004562
Validation Loss: 0.00519868
Epoch [87/200], Train Loss: 0.004579
Validation Loss: 0.00519830
Epoch [88/200], Train Loss: 0.004564
Validation Loss: 0.00519594
Epoch [89/200], Train Loss: 0.004575
Validation Loss: 0.00519624
Epoch [90/200], Train Loss: 0.004590
Validation Loss: 0.00520125
Epoch [91/200], Train Loss: 0.004579
Validation Loss: 0.00519145
Epoch [92/200], Train Loss: 0.004593
Validation Loss: 0.00519063
Epoch [93/200], Train Loss: 0.004580
Validation Loss: 0.00519224
Epoch [94/200], Train Loss: 0.004583
Validation Loss: 0.00518587
Epoch [95/200], Train Loss: 0.004559
Validation Loss: 0.00518666
Epoch [96/200], Train Loss: 0.004582
Validation Loss: 0.00518671
Epoch [97/200], Train Loss: 0.004574
Validation Loss: 0.00519143
Epoch [98/200], Train Loss: 0.004582
Validation Loss: 0.00517935
Epoch [99/200], Train Loss: 0.004549
Validation Loss: 0.00517922
Epoch [100/200], Train Loss: 0.004533
Validation Loss: 0.00518105
Epoch [101/200], Train Loss: 0.004551
Validation Loss: 0.00517382
Epoch [102/200], Train Loss: 0.004564
Validation Loss: 0.00517256
Epoch [103/200], Train Loss: 0.004576
Validation Loss: 0.00518882
Epoch [104/200], Train Loss: 0.004560
Validation Loss: 0.00516796
Epoch [105/200], Train Loss: 0.004567
Validation Loss: 0.00516766
Epoch [106/200], Train Loss: 0.004561
Validation Loss: 0.00517755
Epoch [107/200], Train Loss: 0.004544
Validation Loss: 0.00516490
Epoch [108/200], Train Loss: 0.004548
Validation Loss: 0.00516565
Epoch [109/200], Train Loss: 0.004560
Validation Loss: 0.00515866
Epoch [110/200], Train Loss: 0.004586
Validation Loss: 0.00516159
Epoch [111/200], Train Loss: 0.004547
Validation Loss: 0.00515593
Epoch [112/200], Train Loss: 0.004544
Validation Loss: 0.00515374
Epoch [113/200], Train Loss: 0.004537
Validation Loss: 0.00515354
Epoch [114/200], Train Loss: 0.004558
Validation Loss: 0.00514578
Epoch [115/200], Train Loss: 0.004540
Validation Loss: 0.00515547
Epoch [116/200], Train Loss: 0.004569
Validation Loss: 0.00514422
Epoch [117/200], Train Loss: 0.004547
Validation Loss: 0.00513858
Epoch [118/200], Train Loss: 0.004572
Validation Loss: 0.00514129
Epoch [119/200], Train Loss: 0.004550
Validation Loss: 0.00513542
Epoch [120/200], Train Loss: 0.004532
Validation Loss: 0.00513035
Epoch [121/200], Train Loss: 0.004528
Validation Loss: 0.00512699
Epoch [122/200], Train Loss: 0.004532
Validation Loss: 0.00512956
Epoch [123/200], Train Loss: 0.004536
Validation Loss: 0.00512392
Epoch [124/200], Train Loss: 0.004540
Validation Loss: 0.00511957
Epoch [125/200], Train Loss: 0.004523
Validation Loss: 0.00511555
Epoch [126/200], Train Loss: 0.004527
Validation Loss: 0.00511516
Epoch [127/200], Train Loss: 0.004525
Validation Loss: 0.00510488
Epoch [128/200], Train Loss: 0.004541
Validation Loss: 0.00510343
Epoch [129/200], Train Loss: 0.004522
Validation Loss: 0.00509945
Epoch [130/200], Train Loss: 0.004519
Validation Loss: 0.00509971
Epoch [131/200], Train Loss: 0.004500
Validation Loss: 0.00509700
Epoch [132/200], Train Loss: 0.004514
Validation Loss: 0.00508880
Epoch [133/200], Train Loss: 0.004483
Validation Loss: 0.00508381
Epoch [134/200], Train Loss: 0.004492
Validation Loss: 0.00508572
Epoch [135/200], Train Loss: 0.004487
Validation Loss: 0.00507522
Epoch [136/200], Train Loss: 0.004501
Validation Loss: 0.00507481
Epoch [137/200], Train Loss: 0.004482
Validation Loss: 0.00506776
Epoch [138/200], Train Loss: 0.004478
Validation Loss: 0.00506457
Epoch [139/200], Train Loss: 0.004479
Validation Loss: 0.00506342
Epoch [140/200], Train Loss: 0.004474
Validation Loss: 0.00505396
Epoch [141/200], Train Loss: 0.004500
Validation Loss: 0.00504862
Epoch [142/200], Train Loss: 0.004471
Validation Loss: 0.00504371
Epoch [143/200], Train Loss: 0.004465
Validation Loss: 0.00503823
Epoch [144/200], Train Loss: 0.004471
Validation Loss: 0.00504011
Epoch [145/200], Train Loss: 0.004485
Validation Loss: 0.00503401
Epoch [146/200], Train Loss: 0.004458
Validation Loss: 0.00502058
Epoch [147/200], Train Loss: 0.004453
Validation Loss: 0.00501623
Epoch [148/200], Train Loss: 0.004459
Validation Loss: 0.00500843
Epoch [149/200], Train Loss: 0.004439
Validation Loss: 0.00500335
Epoch [150/200], Train Loss: 0.004445
Validation Loss: 0.00499670
Epoch [151/200], Train Loss: 0.004452
Validation Loss: 0.00500483
Epoch [152/200], Train Loss: 0.004425
Validation Loss: 0.00500151
Epoch [153/200], Train Loss: 0.004425
Validation Loss: 0.00501232
Epoch [154/200], Train Loss: 0.004430
Validation Loss: 0.00498480
Epoch [155/200], Train Loss: 0.004446
Validation Loss: 0.00496902
Epoch [156/200], Train Loss: 0.004431
Validation Loss: 0.00496594
Epoch [157/200], Train Loss: 0.004426
Validation Loss: 0.00495564
Epoch [158/200], Train Loss: 0.004392
Validation Loss: 0.00494661
Epoch [159/200], Train Loss: 0.004392
Validation Loss: 0.00494262
Epoch [160/200], Train Loss: 0.004403
Validation Loss: 0.00493339
Epoch [161/200], Train Loss: 0.004403
Validation Loss: 0.00496420
Epoch [162/200], Train Loss: 0.004399
Validation Loss: 0.00492183
Epoch [163/200], Train Loss: 0.004381
Validation Loss: 0.00491299
Epoch [164/200], Train Loss: 0.004402
Validation Loss: 0.00490498
Epoch [165/200], Train Loss: 0.004394
Validation Loss: 0.00489509
Epoch [166/200], Train Loss: 0.004368
Validation Loss: 0.00489152
Epoch [167/200], Train Loss: 0.004380
Validation Loss: 0.00489339
Epoch [168/200], Train Loss: 0.004359
Validation Loss: 0.00487209
Epoch [169/200], Train Loss: 0.004344
Validation Loss: 0.00486812
Epoch [170/200], Train Loss: 0.004365
Validation Loss: 0.00485668
Epoch [171/200], Train Loss: 0.004365
Validation Loss: 0.00485983
Epoch [172/200], Train Loss: 0.004360
Validation Loss: 0.00484262
Epoch [173/200], Train Loss: 0.004347
Validation Loss: 0.00483580
Epoch [174/200], Train Loss: 0.004333
Validation Loss: 0.00482314
Epoch [175/200], Train Loss: 0.004315
Validation Loss: 0.00481508
Epoch [176/200], Train Loss: 0.004330
Validation Loss: 0.00487139
Epoch [177/200], Train Loss: 0.004292
Validation Loss: 0.00479936
Epoch [178/200], Train Loss: 0.004288
Validation Loss: 0.00478638
Epoch [179/200], Train Loss: 0.004281
Validation Loss: 0.00477207
Epoch [180/200], Train Loss: 0.004290
Validation Loss: 0.00477662
Epoch [181/200], Train Loss: 0.004268
Validation Loss: 0.00476424
Epoch [182/200], Train Loss: 0.004291
Validation Loss: 0.00478860
Epoch [183/200], Train Loss: 0.004278
Validation Loss: 0.00474278
Epoch [184/200], Train Loss: 0.004260
Validation Loss: 0.00472849
Epoch [185/200], Train Loss: 0.004243
Validation Loss: 0.00473883
Epoch [186/200], Train Loss: 0.004246
Validation Loss: 0.00470908
Epoch [187/200], Train Loss: 0.004245
Validation Loss: 0.00470267
Epoch [188/200], Train Loss: 0.004263
Validation Loss: 0.00472522
Epoch [189/200], Train Loss: 0.004222
Validation Loss: 0.00470206
Epoch [190/200], Train Loss: 0.004234
Validation Loss: 0.00467583
Epoch [191/200], Train Loss: 0.004256
Validation Loss: 0.00468519
Epoch [192/200], Train Loss: 0.004233
Validation Loss: 0.00465823
Epoch [193/200], Train Loss: 0.004222
Validation Loss: 0.00465415
Epoch [194/200], Train Loss: 0.004245
Validation Loss: 0.00464528
Epoch [195/200], Train Loss: 0.004192
Validation Loss: 0.00462958
Epoch [196/200], Train Loss: 0.004201
Validation Loss: 0.00463353
Epoch [197/200], Train Loss: 0.004194
Validation Loss: 0.00461764
Epoch [198/200], Train Loss: 0.004210
Validation Loss: 0.00460552
Epoch [199/200], Train Loss: 0.004206
Validation Loss: 0.00465339
Epoch [200/200], Train Loss: 0.004189
Validation Loss: 0.00459220

Evaluating model for: Lamp
Run 69/144 completed in 342.99 seconds with: {'MAE': np.float32(2.931263), 'MSE': np.float32(172.66547), 'RMSE': np.float32(13.1402235), 'SAE': np.float32(0.14817475), 'NDE': np.float32(0.9181205)}

Run 70/144: hidden=128, seq_len=360, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.006084
Validation Loss: 0.00583459
Epoch [2/200], Train Loss: 0.004918
Validation Loss: 0.00539991
Epoch [3/200], Train Loss: 0.004780
Validation Loss: 0.00544202
Epoch [4/200], Train Loss: 0.004727
Validation Loss: 0.00538807
Epoch [5/200], Train Loss: 0.004726
Validation Loss: 0.00538501
Epoch [6/200], Train Loss: 0.004734
Validation Loss: 0.00538412
Epoch [7/200], Train Loss: 0.004716
Validation Loss: 0.00537967
Epoch [8/200], Train Loss: 0.004741
Validation Loss: 0.00537594
Epoch [9/200], Train Loss: 0.004722
Validation Loss: 0.00537074
Epoch [10/200], Train Loss: 0.004703
Validation Loss: 0.00536592
Epoch [11/200], Train Loss: 0.004696
Validation Loss: 0.00535963
Epoch [12/200], Train Loss: 0.004708
Validation Loss: 0.00535064
Epoch [13/200], Train Loss: 0.004689
Validation Loss: 0.00533963
Epoch [14/200], Train Loss: 0.004700
Validation Loss: 0.00532488
Epoch [15/200], Train Loss: 0.004686
Validation Loss: 0.00530661
Epoch [16/200], Train Loss: 0.004656
Validation Loss: 0.00529006
Epoch [17/200], Train Loss: 0.004628
Validation Loss: 0.00527679
Epoch [18/200], Train Loss: 0.004636
Validation Loss: 0.00527518
Epoch [19/200], Train Loss: 0.004635
Validation Loss: 0.00527285
Epoch [20/200], Train Loss: 0.004648
Validation Loss: 0.00527360
Epoch [21/200], Train Loss: 0.004647
Validation Loss: 0.00527415
Epoch [22/200], Train Loss: 0.004626
Validation Loss: 0.00527090
Epoch [23/200], Train Loss: 0.004634
Validation Loss: 0.00527014
Epoch [24/200], Train Loss: 0.004636
Validation Loss: 0.00527145
Epoch [25/200], Train Loss: 0.004658
Validation Loss: 0.00527099
Epoch [26/200], Train Loss: 0.004640
Validation Loss: 0.00527155
Epoch [27/200], Train Loss: 0.004623
Validation Loss: 0.00526721
Epoch [28/200], Train Loss: 0.004641
Validation Loss: 0.00526795
Epoch [29/200], Train Loss: 0.004627
Validation Loss: 0.00526751
Epoch [30/200], Train Loss: 0.004678
Validation Loss: 0.00526616
Epoch [31/200], Train Loss: 0.004638
Validation Loss: 0.00526747
Epoch [32/200], Train Loss: 0.004639
Validation Loss: 0.00526395
Epoch [33/200], Train Loss: 0.004631
Validation Loss: 0.00526466
Epoch [34/200], Train Loss: 0.004651
Validation Loss: 0.00526352
Epoch [35/200], Train Loss: 0.004623
Validation Loss: 0.00526180
Epoch [36/200], Train Loss: 0.004657
Validation Loss: 0.00526479
Epoch [37/200], Train Loss: 0.004645
Validation Loss: 0.00526162
Epoch [38/200], Train Loss: 0.004647
Validation Loss: 0.00526296
Epoch [39/200], Train Loss: 0.004639
Validation Loss: 0.00526321
Epoch [40/200], Train Loss: 0.004650
Validation Loss: 0.00525882
Epoch [41/200], Train Loss: 0.004625
Validation Loss: 0.00525955
Epoch [42/200], Train Loss: 0.004634
Validation Loss: 0.00525931
Epoch [43/200], Train Loss: 0.004636
Validation Loss: 0.00525834
Epoch [44/200], Train Loss: 0.004619
Validation Loss: 0.00525716
Epoch [45/200], Train Loss: 0.004620
Validation Loss: 0.00525786
Epoch [46/200], Train Loss: 0.004628
Validation Loss: 0.00525567
Epoch [47/200], Train Loss: 0.004638
Validation Loss: 0.00525616
Epoch [48/200], Train Loss: 0.004598
Validation Loss: 0.00525509
Epoch [49/200], Train Loss: 0.004627
Validation Loss: 0.00526125
Epoch [50/200], Train Loss: 0.004624
Validation Loss: 0.00525433
Epoch [51/200], Train Loss: 0.004622
Validation Loss: 0.00525440
Epoch [52/200], Train Loss: 0.004615
Validation Loss: 0.00525517
Epoch [53/200], Train Loss: 0.004618
Validation Loss: 0.00525366
Epoch [54/200], Train Loss: 0.004602
Validation Loss: 0.00525472
Epoch [55/200], Train Loss: 0.004652
Validation Loss: 0.00525344
Epoch [56/200], Train Loss: 0.004637
Validation Loss: 0.00525507
Epoch [57/200], Train Loss: 0.004656
Validation Loss: 0.00525417
Epoch [58/200], Train Loss: 0.004620
Validation Loss: 0.00525210
Epoch [59/200], Train Loss: 0.004610
Validation Loss: 0.00525656
Epoch [60/200], Train Loss: 0.004625
Validation Loss: 0.00525299
Epoch [61/200], Train Loss: 0.004620
Validation Loss: 0.00525305
Epoch [62/200], Train Loss: 0.004668
Validation Loss: 0.00525553
Epoch [63/200], Train Loss: 0.004630
Validation Loss: 0.00525203
Epoch [64/200], Train Loss: 0.004605
Validation Loss: 0.00525153
Epoch [65/200], Train Loss: 0.004646
Validation Loss: 0.00525344
Epoch [66/200], Train Loss: 0.004595
Validation Loss: 0.00525254
Epoch [67/200], Train Loss: 0.004597
Validation Loss: 0.00525312
Epoch [68/200], Train Loss: 0.004613
Validation Loss: 0.00525299
Epoch [69/200], Train Loss: 0.004610
Validation Loss: 0.00525263
Epoch [70/200], Train Loss: 0.004619
Validation Loss: 0.00525319
Epoch [71/200], Train Loss: 0.004598
Validation Loss: 0.00525118
Epoch [72/200], Train Loss: 0.004618
Validation Loss: 0.00525205
Epoch [73/200], Train Loss: 0.004615
Validation Loss: 0.00525320
Epoch [74/200], Train Loss: 0.004599
Validation Loss: 0.00525160
Epoch [75/200], Train Loss: 0.004599
Validation Loss: 0.00525299
Epoch [76/200], Train Loss: 0.004606
Validation Loss: 0.00525192
Epoch [77/200], Train Loss: 0.004621
Validation Loss: 0.00525321
Epoch [78/200], Train Loss: 0.004620
Validation Loss: 0.00525151
Epoch [79/200], Train Loss: 0.004632
Validation Loss: 0.00525199
Epoch [80/200], Train Loss: 0.004619
Validation Loss: 0.00525156
Epoch [81/200], Train Loss: 0.004610
Validation Loss: 0.00525154
Early stopping triggered

Evaluating model for: Lamp
Run 70/144 completed in 147.65 seconds with: {'MAE': np.float32(2.9964268), 'MSE': np.float32(196.3412), 'RMSE': np.float32(14.01218), 'SAE': np.float32(0.16189282), 'NDE': np.float32(0.97904515)}

Run 71/144: hidden=128, seq_len=360, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.005706
Validation Loss: 0.00559640
Epoch [2/200], Train Loss: 0.004761
Validation Loss: 0.00542032
Epoch [3/200], Train Loss: 0.004749
Validation Loss: 0.00540183
Epoch [4/200], Train Loss: 0.004738
Validation Loss: 0.00537842
Epoch [5/200], Train Loss: 0.004730
Validation Loss: 0.00537692
Epoch [6/200], Train Loss: 0.004704
Validation Loss: 0.00537756
Epoch [7/200], Train Loss: 0.004697
Validation Loss: 0.00537654
Epoch [8/200], Train Loss: 0.004728
Validation Loss: 0.00537396
Epoch [9/200], Train Loss: 0.004713
Validation Loss: 0.00537156
Epoch [10/200], Train Loss: 0.004704
Validation Loss: 0.00536937
Epoch [11/200], Train Loss: 0.004737
Validation Loss: 0.00536760
Epoch [12/200], Train Loss: 0.004703
Validation Loss: 0.00535947
Epoch [13/200], Train Loss: 0.004729
Validation Loss: 0.00535504
Epoch [14/200], Train Loss: 0.004689
Validation Loss: 0.00533960
Epoch [15/200], Train Loss: 0.004655
Validation Loss: 0.00531949
Epoch [16/200], Train Loss: 0.004648
Validation Loss: 0.00529430
Epoch [17/200], Train Loss: 0.004649
Validation Loss: 0.00527451
Epoch [18/200], Train Loss: 0.004645
Validation Loss: 0.00526721
Epoch [19/200], Train Loss: 0.004639
Validation Loss: 0.00527012
Epoch [20/200], Train Loss: 0.004645
Validation Loss: 0.00526722
Epoch [21/200], Train Loss: 0.004644
Validation Loss: 0.00526732
Epoch [22/200], Train Loss: 0.004625
Validation Loss: 0.00526463
Epoch [23/200], Train Loss: 0.004630
Validation Loss: 0.00526440
Epoch [24/200], Train Loss: 0.004637
Validation Loss: 0.00526377
Epoch [25/200], Train Loss: 0.004635
Validation Loss: 0.00526350
Epoch [26/200], Train Loss: 0.004638
Validation Loss: 0.00526261
Epoch [27/200], Train Loss: 0.004635
Validation Loss: 0.00526215
Epoch [28/200], Train Loss: 0.004641
Validation Loss: 0.00526227
Epoch [29/200], Train Loss: 0.004639
Validation Loss: 0.00526176
Epoch [30/200], Train Loss: 0.004628
Validation Loss: 0.00526001
Epoch [31/200], Train Loss: 0.004621
Validation Loss: 0.00526386
Epoch [32/200], Train Loss: 0.004643
Validation Loss: 0.00526010
Epoch [33/200], Train Loss: 0.004639
Validation Loss: 0.00526149
Epoch [34/200], Train Loss: 0.004643
Validation Loss: 0.00525754
Epoch [35/200], Train Loss: 0.004626
Validation Loss: 0.00526234
Epoch [36/200], Train Loss: 0.004625
Validation Loss: 0.00525717
Epoch [37/200], Train Loss: 0.004637
Validation Loss: 0.00525695
Epoch [38/200], Train Loss: 0.004629
Validation Loss: 0.00525585
Epoch [39/200], Train Loss: 0.004615
Validation Loss: 0.00525472
Epoch [40/200], Train Loss: 0.004633
Validation Loss: 0.00525881
Epoch [41/200], Train Loss: 0.004629
Validation Loss: 0.00525457
Epoch [42/200], Train Loss: 0.004615
Validation Loss: 0.00526356
Epoch [43/200], Train Loss: 0.004617
Validation Loss: 0.00525453
Epoch [44/200], Train Loss: 0.004641
Validation Loss: 0.00525459
Epoch [45/200], Train Loss: 0.004603
Validation Loss: 0.00525796
Epoch [46/200], Train Loss: 0.004621
Validation Loss: 0.00525211
Epoch [47/200], Train Loss: 0.004619
Validation Loss: 0.00525553
Epoch [48/200], Train Loss: 0.004614
Validation Loss: 0.00525267
Epoch [49/200], Train Loss: 0.004632
Validation Loss: 0.00526120
Epoch [50/200], Train Loss: 0.004614
Validation Loss: 0.00525162
Epoch [51/200], Train Loss: 0.004626
Validation Loss: 0.00525097
Epoch [52/200], Train Loss: 0.004615
Validation Loss: 0.00525385
Epoch [53/200], Train Loss: 0.004616
Validation Loss: 0.00525195
Epoch [54/200], Train Loss: 0.004634
Validation Loss: 0.00525081
Epoch [55/200], Train Loss: 0.004613
Validation Loss: 0.00525146
Epoch [56/200], Train Loss: 0.004627
Validation Loss: 0.00525225
Epoch [57/200], Train Loss: 0.004659
Validation Loss: 0.00525078
Epoch [58/200], Train Loss: 0.004624
Validation Loss: 0.00525943
Epoch [59/200], Train Loss: 0.004593
Validation Loss: 0.00525067
Epoch [60/200], Train Loss: 0.004612
Validation Loss: 0.00525378
Epoch [61/200], Train Loss: 0.004605
Validation Loss: 0.00525070
Epoch [62/200], Train Loss: 0.004628
Validation Loss: 0.00525124
Epoch [63/200], Train Loss: 0.004622
Validation Loss: 0.00525553
Epoch [64/200], Train Loss: 0.004604
Validation Loss: 0.00525123
Epoch [65/200], Train Loss: 0.004614
Validation Loss: 0.00525590
Epoch [66/200], Train Loss: 0.004602
Validation Loss: 0.00525071
Epoch [67/200], Train Loss: 0.004621
Validation Loss: 0.00525186
Epoch [68/200], Train Loss: 0.004613
Validation Loss: 0.00525100
Epoch [69/200], Train Loss: 0.004613
Validation Loss: 0.00525271
Early stopping triggered

Evaluating model for: Lamp
Run 71/144 completed in 132.05 seconds with: {'MAE': np.float32(2.9693894), 'MSE': np.float32(196.28099), 'RMSE': np.float32(14.010032), 'SAE': np.float32(0.1779324), 'NDE': np.float32(0.97889507)}

Run 72/144: hidden=128, seq_len=360, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.005974
Validation Loss: 0.00578922
Epoch [2/200], Train Loss: 0.004873
Validation Loss: 0.00540012
Epoch [3/200], Train Loss: 0.004786
Validation Loss: 0.00539022
Epoch [4/200], Train Loss: 0.004743
Validation Loss: 0.00539511
Epoch [5/200], Train Loss: 0.004739
Validation Loss: 0.00538673
Epoch [6/200], Train Loss: 0.004749
Validation Loss: 0.00538134
Epoch [7/200], Train Loss: 0.004744
Validation Loss: 0.00538290
Epoch [8/200], Train Loss: 0.004776
Validation Loss: 0.00538270
Epoch [9/200], Train Loss: 0.004740
Validation Loss: 0.00537986
Epoch [10/200], Train Loss: 0.004745
Validation Loss: 0.00538075
Epoch [11/200], Train Loss: 0.004720
Validation Loss: 0.00537806
Epoch [12/200], Train Loss: 0.004763
Validation Loss: 0.00537831
Epoch [13/200], Train Loss: 0.004707
Validation Loss: 0.00537342
Epoch [14/200], Train Loss: 0.004743
Validation Loss: 0.00537080
Epoch [15/200], Train Loss: 0.004735
Validation Loss: 0.00536263
Epoch [16/200], Train Loss: 0.004729
Validation Loss: 0.00535194
Epoch [17/200], Train Loss: 0.004706
Validation Loss: 0.00532794
Epoch [18/200], Train Loss: 0.004688
Validation Loss: 0.00529735
Epoch [19/200], Train Loss: 0.004693
Validation Loss: 0.00527670
Epoch [20/200], Train Loss: 0.004666
Validation Loss: 0.00527512
Epoch [21/200], Train Loss: 0.004653
Validation Loss: 0.00527337
Epoch [22/200], Train Loss: 0.004642
Validation Loss: 0.00527270
Epoch [23/200], Train Loss: 0.004656
Validation Loss: 0.00527520
Epoch [24/200], Train Loss: 0.004660
Validation Loss: 0.00527006
Epoch [25/200], Train Loss: 0.004652
Validation Loss: 0.00527115
Epoch [26/200], Train Loss: 0.004641
Validation Loss: 0.00526915
Epoch [27/200], Train Loss: 0.004644
Validation Loss: 0.00526627
Epoch [28/200], Train Loss: 0.004665
Validation Loss: 0.00527640
Epoch [29/200], Train Loss: 0.004636
Validation Loss: 0.00527507
Epoch [30/200], Train Loss: 0.004635
Validation Loss: 0.00526639
Epoch [31/200], Train Loss: 0.004660
Validation Loss: 0.00527132
Epoch [32/200], Train Loss: 0.004640
Validation Loss: 0.00526606
Epoch [33/200], Train Loss: 0.004648
Validation Loss: 0.00526459
Epoch [34/200], Train Loss: 0.004656
Validation Loss: 0.00526389
Epoch [35/200], Train Loss: 0.004626
Validation Loss: 0.00526680
Epoch [36/200], Train Loss: 0.004650
Validation Loss: 0.00526129
Epoch [37/200], Train Loss: 0.004638
Validation Loss: 0.00526423
Epoch [38/200], Train Loss: 0.004651
Validation Loss: 0.00526617
Epoch [39/200], Train Loss: 0.004637
Validation Loss: 0.00526237
Epoch [40/200], Train Loss: 0.004643
Validation Loss: 0.00526230
Epoch [41/200], Train Loss: 0.004630
Validation Loss: 0.00525977
Epoch [42/200], Train Loss: 0.004637
Validation Loss: 0.00526659
Epoch [43/200], Train Loss: 0.004631
Validation Loss: 0.00525955
Epoch [44/200], Train Loss: 0.004635
Validation Loss: 0.00525922
Epoch [45/200], Train Loss: 0.004660
Validation Loss: 0.00527564
Epoch [46/200], Train Loss: 0.004611
Validation Loss: 0.00526093
Epoch [47/200], Train Loss: 0.004611
Validation Loss: 0.00525911
Epoch [48/200], Train Loss: 0.004625
Validation Loss: 0.00526221
Epoch [49/200], Train Loss: 0.004625
Validation Loss: 0.00526286
Epoch [50/200], Train Loss: 0.004627
Validation Loss: 0.00526200
Epoch [51/200], Train Loss: 0.004649
Validation Loss: 0.00527136
Epoch [52/200], Train Loss: 0.004633
Validation Loss: 0.00526301
Epoch [53/200], Train Loss: 0.004636
Validation Loss: 0.00526144
Epoch [54/200], Train Loss: 0.004613
Validation Loss: 0.00527066
Epoch [55/200], Train Loss: 0.004620
Validation Loss: 0.00526280
Epoch [56/200], Train Loss: 0.004615
Validation Loss: 0.00526461
Epoch [57/200], Train Loss: 0.004639
Validation Loss: 0.00526481
Early stopping triggered

Evaluating model for: Lamp
Run 72/144 completed in 115.16 seconds with: {'MAE': np.float32(3.0251946), 'MSE': np.float32(196.3689), 'RMSE': np.float32(14.013168), 'SAE': np.float32(0.20002441), 'NDE': np.float32(0.9791143)}

Run 73/144: hidden=128, seq_len=720, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.005067
Validation Loss: 0.00499634
Epoch [2/200], Train Loss: 0.004853
Validation Loss: 0.00495474
Epoch [3/200], Train Loss: 0.004832
Validation Loss: 0.00493916
Epoch [4/200], Train Loss: 0.004830
Validation Loss: 0.00492445
Epoch [5/200], Train Loss: 0.004805
Validation Loss: 0.00491145
Epoch [6/200], Train Loss: 0.004794
Validation Loss: 0.00490395
Epoch [7/200], Train Loss: 0.004798
Validation Loss: 0.00490158
Epoch [8/200], Train Loss: 0.004801
Validation Loss: 0.00490060
Epoch [9/200], Train Loss: 0.004789
Validation Loss: 0.00489890
Epoch [10/200], Train Loss: 0.004793
Validation Loss: 0.00489751
Epoch [11/200], Train Loss: 0.004788
Validation Loss: 0.00489712
Epoch [12/200], Train Loss: 0.004793
Validation Loss: 0.00489692
Epoch [13/200], Train Loss: 0.004789
Validation Loss: 0.00489402
Epoch [14/200], Train Loss: 0.004812
Validation Loss: 0.00489239
Epoch [15/200], Train Loss: 0.004787
Validation Loss: 0.00489152
Epoch [16/200], Train Loss: 0.004781
Validation Loss: 0.00488898
Epoch [17/200], Train Loss: 0.004789
Validation Loss: 0.00488720
Epoch [18/200], Train Loss: 0.004791
Validation Loss: 0.00488522
Epoch [19/200], Train Loss: 0.004793
Validation Loss: 0.00488337
Epoch [20/200], Train Loss: 0.004780
Validation Loss: 0.00488185
Epoch [21/200], Train Loss: 0.004788
Validation Loss: 0.00488153
Epoch [22/200], Train Loss: 0.004757
Validation Loss: 0.00487474
Epoch [23/200], Train Loss: 0.004757
Validation Loss: 0.00486949
Epoch [24/200], Train Loss: 0.004758
Validation Loss: 0.00486450
Epoch [25/200], Train Loss: 0.004753
Validation Loss: 0.00486292
Epoch [26/200], Train Loss: 0.004749
Validation Loss: 0.00485330
Epoch [27/200], Train Loss: 0.004745
Validation Loss: 0.00484890
Epoch [28/200], Train Loss: 0.004744
Validation Loss: 0.00484239
Epoch [29/200], Train Loss: 0.004748
Validation Loss: 0.00483739
Epoch [30/200], Train Loss: 0.004723
Validation Loss: 0.00483416
Epoch [31/200], Train Loss: 0.004728
Validation Loss: 0.00481807
Epoch [32/200], Train Loss: 0.004711
Validation Loss: 0.00481441
Epoch [33/200], Train Loss: 0.004724
Validation Loss: 0.00479705
Epoch [34/200], Train Loss: 0.004685
Validation Loss: 0.00480006
Epoch [35/200], Train Loss: 0.004691
Validation Loss: 0.00477954
Epoch [36/200], Train Loss: 0.004681
Validation Loss: 0.00474952
Epoch [37/200], Train Loss: 0.004666
Validation Loss: 0.00472145
Epoch [38/200], Train Loss: 0.004631
Validation Loss: 0.00470281
Epoch [39/200], Train Loss: 0.004598
Validation Loss: 0.00468442
Epoch [40/200], Train Loss: 0.004583
Validation Loss: 0.00463499
Epoch [41/200], Train Loss: 0.004568
Validation Loss: 0.00459600
Epoch [42/200], Train Loss: 0.004521
Validation Loss: 0.00458628
Epoch [43/200], Train Loss: 0.004470
Validation Loss: 0.00450713
Epoch [44/200], Train Loss: 0.004438
Validation Loss: 0.00444835
Epoch [45/200], Train Loss: 0.004378
Validation Loss: 0.00438700
Epoch [46/200], Train Loss: 0.004353
Validation Loss: 0.00434001
Epoch [47/200], Train Loss: 0.004303
Validation Loss: 0.00430996
Epoch [48/200], Train Loss: 0.004281
Validation Loss: 0.00438595
Epoch [49/200], Train Loss: 0.004239
Validation Loss: 0.00422318
Epoch [50/200], Train Loss: 0.004165
Validation Loss: 0.00412115
Epoch [51/200], Train Loss: 0.004118
Validation Loss: 0.00406377
Epoch [52/200], Train Loss: 0.004077
Validation Loss: 0.00399349
Epoch [53/200], Train Loss: 0.004049
Validation Loss: 0.00395256
Epoch [54/200], Train Loss: 0.003955
Validation Loss: 0.00387923
Epoch [55/200], Train Loss: 0.003898
Validation Loss: 0.00379216
Epoch [56/200], Train Loss: 0.003834
Validation Loss: 0.00369885
Epoch [57/200], Train Loss: 0.003754
Validation Loss: 0.00363466
Epoch [58/200], Train Loss: 0.003693
Validation Loss: 0.00353954
Epoch [59/200], Train Loss: 0.003598
Validation Loss: 0.00340896
Epoch [60/200], Train Loss: 0.003512
Validation Loss: 0.00331086
Epoch [61/200], Train Loss: 0.003430
Validation Loss: 0.00322727
Epoch [62/200], Train Loss: 0.003338
Validation Loss: 0.00314023
Epoch [63/200], Train Loss: 0.003256
Validation Loss: 0.00305746
Epoch [64/200], Train Loss: 0.003173
Validation Loss: 0.00297501
Epoch [65/200], Train Loss: 0.003084
Validation Loss: 0.00292489
Epoch [66/200], Train Loss: 0.002999
Validation Loss: 0.00279080
Epoch [67/200], Train Loss: 0.002938
Validation Loss: 0.00275575
Epoch [68/200], Train Loss: 0.002859
Validation Loss: 0.00266164
Epoch [69/200], Train Loss: 0.002794
Validation Loss: 0.00260039
Epoch [70/200], Train Loss: 0.002754
Validation Loss: 0.00257110
Epoch [71/200], Train Loss: 0.002679
Validation Loss: 0.00249514
Epoch [72/200], Train Loss: 0.002613
Validation Loss: 0.00245268
Epoch [73/200], Train Loss: 0.002562
Validation Loss: 0.00240803
Epoch [74/200], Train Loss: 0.002498
Validation Loss: 0.00237764
Epoch [75/200], Train Loss: 0.002450
Validation Loss: 0.00234940
Epoch [76/200], Train Loss: 0.002420
Validation Loss: 0.00230797
Epoch [77/200], Train Loss: 0.002391
Validation Loss: 0.00226899
Epoch [78/200], Train Loss: 0.002346
Validation Loss: 0.00224683
Epoch [79/200], Train Loss: 0.002314
Validation Loss: 0.00221795
Epoch [80/200], Train Loss: 0.002275
Validation Loss: 0.00221424
Epoch [81/200], Train Loss: 0.002264
Validation Loss: 0.00221327
Epoch [82/200], Train Loss: 0.002224
Validation Loss: 0.00215831
Epoch [83/200], Train Loss: 0.002205
Validation Loss: 0.00212771
Epoch [84/200], Train Loss: 0.002176
Validation Loss: 0.00209651
Epoch [85/200], Train Loss: 0.002150
Validation Loss: 0.00207101
Epoch [86/200], Train Loss: 0.002118
Validation Loss: 0.00205455
Epoch [87/200], Train Loss: 0.002100
Validation Loss: 0.00202708
Epoch [88/200], Train Loss: 0.002089
Validation Loss: 0.00204873
Epoch [89/200], Train Loss: 0.002064
Validation Loss: 0.00200112
Epoch [90/200], Train Loss: 0.002033
Validation Loss: 0.00198874
Epoch [91/200], Train Loss: 0.002020
Validation Loss: 0.00198033
Epoch [92/200], Train Loss: 0.002009
Validation Loss: 0.00195339
Epoch [93/200], Train Loss: 0.001981
Validation Loss: 0.00193880
Epoch [94/200], Train Loss: 0.001970
Validation Loss: 0.00193589
Epoch [95/200], Train Loss: 0.001962
Validation Loss: 0.00195373
Epoch [96/200], Train Loss: 0.001942
Validation Loss: 0.00190963
Epoch [97/200], Train Loss: 0.001930
Validation Loss: 0.00189917
Epoch [98/200], Train Loss: 0.001908
Validation Loss: 0.00187843
Epoch [99/200], Train Loss: 0.001906
Validation Loss: 0.00189701
Epoch [100/200], Train Loss: 0.001893
Validation Loss: 0.00187407
Epoch [101/200], Train Loss: 0.001876
Validation Loss: 0.00187030
Epoch [102/200], Train Loss: 0.001869
Validation Loss: 0.00184230
Epoch [103/200], Train Loss: 0.001841
Validation Loss: 0.00181901
Epoch [104/200], Train Loss: 0.001838
Validation Loss: 0.00180909
Epoch [105/200], Train Loss: 0.001818
Validation Loss: 0.00180572
Epoch [106/200], Train Loss: 0.001811
Validation Loss: 0.00179240
Epoch [107/200], Train Loss: 0.001803
Validation Loss: 0.00180039
Epoch [108/200], Train Loss: 0.001801
Validation Loss: 0.00179903
Epoch [109/200], Train Loss: 0.001777
Validation Loss: 0.00176795
Epoch [110/200], Train Loss: 0.001764
Validation Loss: 0.00175336
Epoch [111/200], Train Loss: 0.001757
Validation Loss: 0.00175422
Epoch [112/200], Train Loss: 0.001749
Validation Loss: 0.00173738
Epoch [113/200], Train Loss: 0.001744
Validation Loss: 0.00173122
Epoch [114/200], Train Loss: 0.001727
Validation Loss: 0.00173877
Epoch [115/200], Train Loss: 0.001720
Validation Loss: 0.00170888
Epoch [116/200], Train Loss: 0.001704
Validation Loss: 0.00170269
Epoch [117/200], Train Loss: 0.001697
Validation Loss: 0.00169819
Epoch [118/200], Train Loss: 0.001690
Validation Loss: 0.00168676
Epoch [119/200], Train Loss: 0.001676
Validation Loss: 0.00166907
Epoch [120/200], Train Loss: 0.001671
Validation Loss: 0.00167297
Epoch [121/200], Train Loss: 0.001662
Validation Loss: 0.00165670
Epoch [122/200], Train Loss: 0.001650
Validation Loss: 0.00166405
Epoch [123/200], Train Loss: 0.001644
Validation Loss: 0.00164725
Epoch [124/200], Train Loss: 0.001631
Validation Loss: 0.00165888
Epoch [125/200], Train Loss: 0.001630
Validation Loss: 0.00164435
Epoch [126/200], Train Loss: 0.001620
Validation Loss: 0.00163430
Epoch [127/200], Train Loss: 0.001614
Validation Loss: 0.00161557
Epoch [128/200], Train Loss: 0.001598
Validation Loss: 0.00161482
Epoch [129/200], Train Loss: 0.001607
Validation Loss: 0.00162169
Epoch [130/200], Train Loss: 0.001600
Validation Loss: 0.00160386
Epoch [131/200], Train Loss: 0.001586
Validation Loss: 0.00160176
Epoch [132/200], Train Loss: 0.001579
Validation Loss: 0.00159340
Epoch [133/200], Train Loss: 0.001572
Validation Loss: 0.00159924
Epoch [134/200], Train Loss: 0.001559
Validation Loss: 0.00158114
Epoch [135/200], Train Loss: 0.001559
Validation Loss: 0.00156099
Epoch [136/200], Train Loss: 0.001543
Validation Loss: 0.00155670
Epoch [137/200], Train Loss: 0.001544
Validation Loss: 0.00155484
Epoch [138/200], Train Loss: 0.001540
Validation Loss: 0.00154578
Epoch [139/200], Train Loss: 0.001524
Validation Loss: 0.00154412
Epoch [140/200], Train Loss: 0.001528
Validation Loss: 0.00153886
Epoch [141/200], Train Loss: 0.001514
Validation Loss: 0.00153999
Epoch [142/200], Train Loss: 0.001518
Validation Loss: 0.00152931
Epoch [143/200], Train Loss: 0.001493
Validation Loss: 0.00152500
Epoch [144/200], Train Loss: 0.001506
Validation Loss: 0.00152826
Epoch [145/200], Train Loss: 0.001488
Validation Loss: 0.00151177
Epoch [146/200], Train Loss: 0.001478
Validation Loss: 0.00151461
Epoch [147/200], Train Loss: 0.001480
Validation Loss: 0.00151244
Epoch [148/200], Train Loss: 0.001476
Validation Loss: 0.00150499
Epoch [149/200], Train Loss: 0.001460
Validation Loss: 0.00149894
Epoch [150/200], Train Loss: 0.001466
Validation Loss: 0.00149133
Epoch [151/200], Train Loss: 0.001463
Validation Loss: 0.00147848
Epoch [152/200], Train Loss: 0.001449
Validation Loss: 0.00148010
Epoch [153/200], Train Loss: 0.001439
Validation Loss: 0.00147053
Epoch [154/200], Train Loss: 0.001456
Validation Loss: 0.00149269
Epoch [155/200], Train Loss: 0.001439
Validation Loss: 0.00146661
Epoch [156/200], Train Loss: 0.001424
Validation Loss: 0.00146081
Epoch [157/200], Train Loss: 0.001430
Validation Loss: 0.00145757
Epoch [158/200], Train Loss: 0.001410
Validation Loss: 0.00145328
Epoch [159/200], Train Loss: 0.001414
Validation Loss: 0.00144596
Epoch [160/200], Train Loss: 0.001405
Validation Loss: 0.00144059
Epoch [161/200], Train Loss: 0.001397
Validation Loss: 0.00144262
Epoch [162/200], Train Loss: 0.001390
Validation Loss: 0.00143319
Epoch [163/200], Train Loss: 0.001395
Validation Loss: 0.00143745
Epoch [164/200], Train Loss: 0.001382
Validation Loss: 0.00142891
Epoch [165/200], Train Loss: 0.001376
Validation Loss: 0.00142576
Epoch [166/200], Train Loss: 0.001378
Validation Loss: 0.00141515
Epoch [167/200], Train Loss: 0.001368
Validation Loss: 0.00141151
Epoch [168/200], Train Loss: 0.001363
Validation Loss: 0.00140519
Epoch [169/200], Train Loss: 0.001357
Validation Loss: 0.00141022
Epoch [170/200], Train Loss: 0.001366
Validation Loss: 0.00141165
Epoch [171/200], Train Loss: 0.001358
Validation Loss: 0.00140271
Epoch [172/200], Train Loss: 0.001382
Validation Loss: 0.00149693
Epoch [173/200], Train Loss: 0.001372
Validation Loss: 0.00139021
Epoch [174/200], Train Loss: 0.001347
Validation Loss: 0.00139389
Epoch [175/200], Train Loss: 0.001335
Validation Loss: 0.00138179
Epoch [176/200], Train Loss: 0.001332
Validation Loss: 0.00138299
Epoch [177/200], Train Loss: 0.001331
Validation Loss: 0.00138127
Epoch [178/200], Train Loss: 0.001327
Validation Loss: 0.00137545
Epoch [179/200], Train Loss: 0.001325
Validation Loss: 0.00137312
Epoch [180/200], Train Loss: 0.001322
Validation Loss: 0.00136691
Epoch [181/200], Train Loss: 0.001308
Validation Loss: 0.00135820
Epoch [182/200], Train Loss: 0.001306
Validation Loss: 0.00136496
Epoch [183/200], Train Loss: 0.001310
Validation Loss: 0.00135954
Epoch [184/200], Train Loss: 0.001298
Validation Loss: 0.00135547
Epoch [185/200], Train Loss: 0.001296
Validation Loss: 0.00136706
Epoch [186/200], Train Loss: 0.001298
Validation Loss: 0.00134472
Epoch [187/200], Train Loss: 0.001294
Validation Loss: 0.00134530
Epoch [188/200], Train Loss: 0.001289
Validation Loss: 0.00134138
Epoch [189/200], Train Loss: 0.001290
Validation Loss: 0.00133465
Epoch [190/200], Train Loss: 0.001284
Validation Loss: 0.00133875
Epoch [191/200], Train Loss: 0.001281
Validation Loss: 0.00133338
Epoch [192/200], Train Loss: 0.001280
Validation Loss: 0.00133028
Epoch [193/200], Train Loss: 0.001276
Validation Loss: 0.00132371
Epoch [194/200], Train Loss: 0.001267
Validation Loss: 0.00136988
Epoch [195/200], Train Loss: 0.001268
Validation Loss: 0.00131927
Epoch [196/200], Train Loss: 0.001255
Validation Loss: 0.00131502
Epoch [197/200], Train Loss: 0.001259
Validation Loss: 0.00131616
Epoch [198/200], Train Loss: 0.001263
Validation Loss: 0.00131207
Epoch [199/200], Train Loss: 0.001255
Validation Loss: 0.00131587
Epoch [200/200], Train Loss: 0.001288
Validation Loss: 0.00137834

Evaluating model for: Lamp
Run 73/144 completed in 938.60 seconds with: {'MAE': np.float32(1.0355765), 'MSE': np.float32(42.832096), 'RMSE': np.float32(6.5446234), 'SAE': np.float32(0.06279814), 'NDE': np.float32(0.5094444)}

Run 74/144: hidden=128, seq_len=720, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.005499
Validation Loss: 0.00504123
Epoch [2/200], Train Loss: 0.004911
Validation Loss: 0.00499561
Epoch [3/200], Train Loss: 0.004879
Validation Loss: 0.00498654
Epoch [4/200], Train Loss: 0.004913
Validation Loss: 0.00497706
Epoch [5/200], Train Loss: 0.004860
Validation Loss: 0.00496008
Epoch [6/200], Train Loss: 0.004841
Validation Loss: 0.00492897
Epoch [7/200], Train Loss: 0.004829
Validation Loss: 0.00490863
Epoch [8/200], Train Loss: 0.004814
Validation Loss: 0.00490439
Epoch [9/200], Train Loss: 0.004801
Validation Loss: 0.00490227
Epoch [10/200], Train Loss: 0.004800
Validation Loss: 0.00490003
Epoch [11/200], Train Loss: 0.004802
Validation Loss: 0.00489820
Epoch [12/200], Train Loss: 0.004796
Validation Loss: 0.00489618
Epoch [13/200], Train Loss: 0.004800
Validation Loss: 0.00489683
Epoch [14/200], Train Loss: 0.004793
Validation Loss: 0.00489449
Epoch [15/200], Train Loss: 0.004792
Validation Loss: 0.00489202
Epoch [16/200], Train Loss: 0.004779
Validation Loss: 0.00489370
Epoch [17/200], Train Loss: 0.004775
Validation Loss: 0.00488984
Epoch [18/200], Train Loss: 0.004776
Validation Loss: 0.00488873
Epoch [19/200], Train Loss: 0.004791
Validation Loss: 0.00489385
Epoch [20/200], Train Loss: 0.004773
Validation Loss: 0.00489089
Epoch [21/200], Train Loss: 0.004778
Validation Loss: 0.00488720
Epoch [22/200], Train Loss: 0.004780
Validation Loss: 0.00488685
Epoch [23/200], Train Loss: 0.004777
Validation Loss: 0.00488508
Epoch [24/200], Train Loss: 0.004766
Validation Loss: 0.00488462
Epoch [25/200], Train Loss: 0.004783
Validation Loss: 0.00488464
Epoch [26/200], Train Loss: 0.004787
Validation Loss: 0.00488326
Epoch [27/200], Train Loss: 0.004772
Validation Loss: 0.00488679
Epoch [28/200], Train Loss: 0.004770
Validation Loss: 0.00488209
Epoch [29/200], Train Loss: 0.004755
Validation Loss: 0.00488209
Epoch [30/200], Train Loss: 0.004777
Validation Loss: 0.00488061
Epoch [31/200], Train Loss: 0.004779
Validation Loss: 0.00488108
Epoch [32/200], Train Loss: 0.004784
Validation Loss: 0.00488029
Epoch [33/200], Train Loss: 0.004759
Validation Loss: 0.00488211
Epoch [34/200], Train Loss: 0.004761
Validation Loss: 0.00487964
Epoch [35/200], Train Loss: 0.004753
Validation Loss: 0.00487946
Epoch [36/200], Train Loss: 0.004780
Validation Loss: 0.00487905
Epoch [37/200], Train Loss: 0.004770
Validation Loss: 0.00487805
Epoch [38/200], Train Loss: 0.004757
Validation Loss: 0.00487734
Epoch [39/200], Train Loss: 0.004757
Validation Loss: 0.00488011
Epoch [40/200], Train Loss: 0.004768
Validation Loss: 0.00487735
Epoch [41/200], Train Loss: 0.004776
Validation Loss: 0.00487773
Epoch [42/200], Train Loss: 0.004760
Validation Loss: 0.00487612
Epoch [43/200], Train Loss: 0.004768
Validation Loss: 0.00487526
Epoch [44/200], Train Loss: 0.004763
Validation Loss: 0.00487554
Epoch [45/200], Train Loss: 0.004752
Validation Loss: 0.00487492
Epoch [46/200], Train Loss: 0.004770
Validation Loss: 0.00487410
Epoch [47/200], Train Loss: 0.004767
Validation Loss: 0.00487503
Epoch [48/200], Train Loss: 0.004757
Validation Loss: 0.00487428
Epoch [49/200], Train Loss: 0.004761
Validation Loss: 0.00487368
Epoch [50/200], Train Loss: 0.004771
Validation Loss: 0.00487302
Epoch [51/200], Train Loss: 0.004763
Validation Loss: 0.00487243
Epoch [52/200], Train Loss: 0.004767
Validation Loss: 0.00487077
Epoch [53/200], Train Loss: 0.004760
Validation Loss: 0.00487222
Epoch [54/200], Train Loss: 0.004767
Validation Loss: 0.00486950
Epoch [55/200], Train Loss: 0.004756
Validation Loss: 0.00487009
Epoch [56/200], Train Loss: 0.004783
Validation Loss: 0.00486785
Epoch [57/200], Train Loss: 0.004761
Validation Loss: 0.00487275
Epoch [58/200], Train Loss: 0.004749
Validation Loss: 0.00487662
Epoch [59/200], Train Loss: 0.004763
Validation Loss: 0.00486426
Epoch [60/200], Train Loss: 0.004748
Validation Loss: 0.00486206
Epoch [61/200], Train Loss: 0.004787
Validation Loss: 0.00486066
Epoch [62/200], Train Loss: 0.004757
Validation Loss: 0.00486068
Epoch [63/200], Train Loss: 0.004756
Validation Loss: 0.00485792
Epoch [64/200], Train Loss: 0.004758
Validation Loss: 0.00485720
Epoch [65/200], Train Loss: 0.004747
Validation Loss: 0.00485108
Epoch [66/200], Train Loss: 0.004727
Validation Loss: 0.00484858
Epoch [67/200], Train Loss: 0.004747
Validation Loss: 0.00484601
Epoch [68/200], Train Loss: 0.004736
Validation Loss: 0.00484384
Epoch [69/200], Train Loss: 0.004742
Validation Loss: 0.00483907
Epoch [70/200], Train Loss: 0.004728
Validation Loss: 0.00484105
Epoch [71/200], Train Loss: 0.004746
Validation Loss: 0.00483705
Epoch [72/200], Train Loss: 0.004719
Validation Loss: 0.00483636
Epoch [73/200], Train Loss: 0.004735
Validation Loss: 0.00482913
Epoch [74/200], Train Loss: 0.004724
Validation Loss: 0.00483048
Epoch [75/200], Train Loss: 0.004724
Validation Loss: 0.00482129
Epoch [76/200], Train Loss: 0.004715
Validation Loss: 0.00482076
Epoch [77/200], Train Loss: 0.004708
Validation Loss: 0.00482073
Epoch [78/200], Train Loss: 0.004724
Validation Loss: 0.00481429
Epoch [79/200], Train Loss: 0.004702
Validation Loss: 0.00481368
Epoch [80/200], Train Loss: 0.004711
Validation Loss: 0.00481193
Epoch [81/200], Train Loss: 0.004706
Validation Loss: 0.00480106
Epoch [82/200], Train Loss: 0.004694
Validation Loss: 0.00480596
Epoch [83/200], Train Loss: 0.004690
Validation Loss: 0.00479666
Epoch [84/200], Train Loss: 0.004698
Validation Loss: 0.00478846
Epoch [85/200], Train Loss: 0.004685
Validation Loss: 0.00478654
Epoch [86/200], Train Loss: 0.004682
Validation Loss: 0.00477735
Epoch [87/200], Train Loss: 0.004677
Validation Loss: 0.00476535
Epoch [88/200], Train Loss: 0.004658
Validation Loss: 0.00475904
Epoch [89/200], Train Loss: 0.004671
Validation Loss: 0.00475358
Epoch [90/200], Train Loss: 0.004662
Validation Loss: 0.00475870
Epoch [91/200], Train Loss: 0.004636
Validation Loss: 0.00472126
Epoch [92/200], Train Loss: 0.004647
Validation Loss: 0.00470213
Epoch [93/200], Train Loss: 0.004608
Validation Loss: 0.00471883
Epoch [94/200], Train Loss: 0.004598
Validation Loss: 0.00465368
Epoch [95/200], Train Loss: 0.004560
Validation Loss: 0.00464143
Epoch [96/200], Train Loss: 0.004566
Validation Loss: 0.00461039
Epoch [97/200], Train Loss: 0.004524
Validation Loss: 0.00457630
Epoch [98/200], Train Loss: 0.004513
Validation Loss: 0.00454704
Epoch [99/200], Train Loss: 0.004494
Validation Loss: 0.00456095
Epoch [100/200], Train Loss: 0.004507
Validation Loss: 0.00452553
Epoch [101/200], Train Loss: 0.004464
Validation Loss: 0.00449498
Epoch [102/200], Train Loss: 0.004453
Validation Loss: 0.00449865
Epoch [103/200], Train Loss: 0.004428
Validation Loss: 0.00448287
Epoch [104/200], Train Loss: 0.004439
Validation Loss: 0.00442787
Epoch [105/200], Train Loss: 0.004404
Validation Loss: 0.00442366
Epoch [106/200], Train Loss: 0.004391
Validation Loss: 0.00440261
Epoch [107/200], Train Loss: 0.004349
Validation Loss: 0.00434751
Epoch [108/200], Train Loss: 0.004310
Validation Loss: 0.00430569
Epoch [109/200], Train Loss: 0.004299
Validation Loss: 0.00428200
Epoch [110/200], Train Loss: 0.004271
Validation Loss: 0.00425270
Epoch [111/200], Train Loss: 0.004223
Validation Loss: 0.00420837
Epoch [112/200], Train Loss: 0.004217
Validation Loss: 0.00420705
Epoch [113/200], Train Loss: 0.004185
Validation Loss: 0.00412704
Epoch [114/200], Train Loss: 0.004133
Validation Loss: 0.00414463
Epoch [115/200], Train Loss: 0.004121
Validation Loss: 0.00404237
Epoch [116/200], Train Loss: 0.004063
Validation Loss: 0.00400901
Epoch [117/200], Train Loss: 0.004041
Validation Loss: 0.00395319
Epoch [118/200], Train Loss: 0.003984
Validation Loss: 0.00388440
Epoch [119/200], Train Loss: 0.003933
Validation Loss: 0.00382583
Epoch [120/200], Train Loss: 0.003906
Validation Loss: 0.00379476
Epoch [121/200], Train Loss: 0.003859
Validation Loss: 0.00371870
Epoch [122/200], Train Loss: 0.003800
Validation Loss: 0.00364594
Epoch [123/200], Train Loss: 0.003740
Validation Loss: 0.00356775
Epoch [124/200], Train Loss: 0.003699
Validation Loss: 0.00352134
Epoch [125/200], Train Loss: 0.003640
Validation Loss: 0.00348260
Epoch [126/200], Train Loss: 0.003579
Validation Loss: 0.00335542
Epoch [127/200], Train Loss: 0.003496
Validation Loss: 0.00327232
Epoch [128/200], Train Loss: 0.003422
Validation Loss: 0.00319825
Epoch [129/200], Train Loss: 0.003366
Validation Loss: 0.00320149
Epoch [130/200], Train Loss: 0.003306
Validation Loss: 0.00304479
Epoch [131/200], Train Loss: 0.003228
Validation Loss: 0.00296744
Epoch [132/200], Train Loss: 0.003150
Validation Loss: 0.00291243
Epoch [133/200], Train Loss: 0.003080
Validation Loss: 0.00281739
Epoch [134/200], Train Loss: 0.003018
Validation Loss: 0.00278735
Epoch [135/200], Train Loss: 0.002979
Validation Loss: 0.00270770
Epoch [136/200], Train Loss: 0.002888
Validation Loss: 0.00264264
Epoch [137/200], Train Loss: 0.002829
Validation Loss: 0.00265142
Epoch [138/200], Train Loss: 0.002782
Validation Loss: 0.00253776
Epoch [139/200], Train Loss: 0.002740
Validation Loss: 0.00252200
Epoch [140/200], Train Loss: 0.002699
Validation Loss: 0.00251637
Epoch [141/200], Train Loss: 0.002659
Validation Loss: 0.00243179
Epoch [142/200], Train Loss: 0.002621
Validation Loss: 0.00240567
Epoch [143/200], Train Loss: 0.002586
Validation Loss: 0.00237099
Epoch [144/200], Train Loss: 0.002547
Validation Loss: 0.00232988
Epoch [145/200], Train Loss: 0.002526
Validation Loss: 0.00238309
Epoch [146/200], Train Loss: 0.002470
Validation Loss: 0.00230718
Epoch [147/200], Train Loss: 0.002433
Validation Loss: 0.00226352
Epoch [148/200], Train Loss: 0.002404
Validation Loss: 0.00226677
Epoch [149/200], Train Loss: 0.002387
Validation Loss: 0.00221195
Epoch [150/200], Train Loss: 0.002365
Validation Loss: 0.00219709
Epoch [151/200], Train Loss: 0.002337
Validation Loss: 0.00220550
Epoch [152/200], Train Loss: 0.002310
Validation Loss: 0.00219988
Epoch [153/200], Train Loss: 0.002284
Validation Loss: 0.00217442
Epoch [154/200], Train Loss: 0.002268
Validation Loss: 0.00215367
Epoch [155/200], Train Loss: 0.002250
Validation Loss: 0.00215022
Epoch [156/200], Train Loss: 0.002236
Validation Loss: 0.00210740
Epoch [157/200], Train Loss: 0.002216
Validation Loss: 0.00207097
Epoch [158/200], Train Loss: 0.002192
Validation Loss: 0.00206069
Epoch [159/200], Train Loss: 0.002180
Validation Loss: 0.00212333
Epoch [160/200], Train Loss: 0.002153
Validation Loss: 0.00202495
Epoch [161/200], Train Loss: 0.002128
Validation Loss: 0.00201329
Epoch [162/200], Train Loss: 0.002122
Validation Loss: 0.00205068
Epoch [163/200], Train Loss: 0.002086
Validation Loss: 0.00199564
Epoch [164/200], Train Loss: 0.002075
Validation Loss: 0.00200660
Epoch [165/200], Train Loss: 0.002054
Validation Loss: 0.00196464
Epoch [166/200], Train Loss: 0.002044
Validation Loss: 0.00197348
Epoch [167/200], Train Loss: 0.002021
Validation Loss: 0.00192924
Epoch [168/200], Train Loss: 0.002016
Validation Loss: 0.00194541
Epoch [169/200], Train Loss: 0.001986
Validation Loss: 0.00192221
Epoch [170/200], Train Loss: 0.001975
Validation Loss: 0.00191557
Epoch [171/200], Train Loss: 0.001969
Validation Loss: 0.00189322
Epoch [172/200], Train Loss: 0.001951
Validation Loss: 0.00188665
Epoch [173/200], Train Loss: 0.001934
Validation Loss: 0.00188876
Epoch [174/200], Train Loss: 0.001929
Validation Loss: 0.00185921
Epoch [175/200], Train Loss: 0.001916
Validation Loss: 0.00185545
Epoch [176/200], Train Loss: 0.001900
Validation Loss: 0.00184593
Epoch [177/200], Train Loss: 0.001885
Validation Loss: 0.00184054
Epoch [178/200], Train Loss: 0.001881
Validation Loss: 0.00183119
Epoch [179/200], Train Loss: 0.001866
Validation Loss: 0.00181713
Epoch [180/200], Train Loss: 0.001858
Validation Loss: 0.00182428
Epoch [181/200], Train Loss: 0.001838
Validation Loss: 0.00179315
Epoch [182/200], Train Loss: 0.001831
Validation Loss: 0.00179799
Epoch [183/200], Train Loss: 0.001825
Validation Loss: 0.00177626
Epoch [184/200], Train Loss: 0.001812
Validation Loss: 0.00176345
Epoch [185/200], Train Loss: 0.001801
Validation Loss: 0.00177943
Epoch [186/200], Train Loss: 0.001796
Validation Loss: 0.00174914
Epoch [187/200], Train Loss: 0.001780
Validation Loss: 0.00175161
Epoch [188/200], Train Loss: 0.001770
Validation Loss: 0.00174317
Epoch [189/200], Train Loss: 0.001764
Validation Loss: 0.00176250
Epoch [190/200], Train Loss: 0.001770
Validation Loss: 0.00171504
Epoch [191/200], Train Loss: 0.001740
Validation Loss: 0.00171869
Epoch [192/200], Train Loss: 0.001731
Validation Loss: 0.00171692
Epoch [193/200], Train Loss: 0.001731
Validation Loss: 0.00171659
Epoch [194/200], Train Loss: 0.001734
Validation Loss: 0.00168660
Epoch [195/200], Train Loss: 0.001709
Validation Loss: 0.00172421
Epoch [196/200], Train Loss: 0.001701
Validation Loss: 0.00168914
Epoch [197/200], Train Loss: 0.001695
Validation Loss: 0.00168218
Epoch [198/200], Train Loss: 0.001685
Validation Loss: 0.00167963
Epoch [199/200], Train Loss: 0.001678
Validation Loss: 0.00166412
Epoch [200/200], Train Loss: 0.001673
Validation Loss: 0.00166642

Evaluating model for: Lamp
Run 74/144 completed in 991.00 seconds with: {'MAE': np.float32(1.1793114), 'MSE': np.float32(51.24297), 'RMSE': np.float32(7.1584196), 'SAE': np.float32(0.11349531), 'NDE': np.float32(0.55722404)}

Run 75/144: hidden=128, seq_len=720, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.005333
Validation Loss: 0.00504221
Epoch [2/200], Train Loss: 0.004911
Validation Loss: 0.00499182
Epoch [3/200], Train Loss: 0.004874
Validation Loss: 0.00498628
Epoch [4/200], Train Loss: 0.004880
Validation Loss: 0.00498092
Epoch [5/200], Train Loss: 0.004883
Validation Loss: 0.00496991
Epoch [6/200], Train Loss: 0.004837
Validation Loss: 0.00493488
Epoch [7/200], Train Loss: 0.004805
Validation Loss: 0.00490252
Epoch [8/200], Train Loss: 0.004827
Validation Loss: 0.00490118
Epoch [9/200], Train Loss: 0.004800
Validation Loss: 0.00489873
Epoch [10/200], Train Loss: 0.004790
Validation Loss: 0.00490013
Epoch [11/200], Train Loss: 0.004780
Validation Loss: 0.00489694
Epoch [12/200], Train Loss: 0.004801
Validation Loss: 0.00489590
Epoch [13/200], Train Loss: 0.004789
Validation Loss: 0.00489463
Epoch [14/200], Train Loss: 0.004778
Validation Loss: 0.00489344
Epoch [15/200], Train Loss: 0.004793
Validation Loss: 0.00489193
Epoch [16/200], Train Loss: 0.004791
Validation Loss: 0.00489374
Epoch [17/200], Train Loss: 0.004779
Validation Loss: 0.00488850
Epoch [18/200], Train Loss: 0.004769
Validation Loss: 0.00488833
Epoch [19/200], Train Loss: 0.004763
Validation Loss: 0.00489080
Epoch [20/200], Train Loss: 0.004764
Validation Loss: 0.00488861
Epoch [21/200], Train Loss: 0.004762
Validation Loss: 0.00488554
Epoch [22/200], Train Loss: 0.004776
Validation Loss: 0.00488654
Epoch [23/200], Train Loss: 0.004771
Validation Loss: 0.00488688
Epoch [24/200], Train Loss: 0.004757
Validation Loss: 0.00488387
Epoch [25/200], Train Loss: 0.004777
Validation Loss: 0.00488400
Epoch [26/200], Train Loss: 0.004778
Validation Loss: 0.00488326
Epoch [27/200], Train Loss: 0.004789
Validation Loss: 0.00488491
Epoch [28/200], Train Loss: 0.004771
Validation Loss: 0.00488228
Epoch [29/200], Train Loss: 0.004754
Validation Loss: 0.00488288
Epoch [30/200], Train Loss: 0.004765
Validation Loss: 0.00488358
Epoch [31/200], Train Loss: 0.004770
Validation Loss: 0.00488155
Epoch [32/200], Train Loss: 0.004769
Validation Loss: 0.00488481
Epoch [33/200], Train Loss: 0.004782
Validation Loss: 0.00488410
Epoch [34/200], Train Loss: 0.004767
Validation Loss: 0.00488087
Epoch [35/200], Train Loss: 0.004768
Validation Loss: 0.00488023
Epoch [36/200], Train Loss: 0.004790
Validation Loss: 0.00488009
Epoch [37/200], Train Loss: 0.004772
Validation Loss: 0.00488142
Epoch [38/200], Train Loss: 0.004761
Validation Loss: 0.00488296
Epoch [39/200], Train Loss: 0.004754
Validation Loss: 0.00487988
Epoch [40/200], Train Loss: 0.004763
Validation Loss: 0.00488058
Epoch [41/200], Train Loss: 0.004765
Validation Loss: 0.00488074
Epoch [42/200], Train Loss: 0.004759
Validation Loss: 0.00487848
Epoch [43/200], Train Loss: 0.004753
Validation Loss: 0.00487898
Epoch [44/200], Train Loss: 0.004775
Validation Loss: 0.00487946
Epoch [45/200], Train Loss: 0.004761
Validation Loss: 0.00487780
Epoch [46/200], Train Loss: 0.004759
Validation Loss: 0.00488066
Epoch [47/200], Train Loss: 0.004757
Validation Loss: 0.00487750
Epoch [48/200], Train Loss: 0.004750
Validation Loss: 0.00487750
Epoch [49/200], Train Loss: 0.004754
Validation Loss: 0.00487732
Epoch [50/200], Train Loss: 0.004756
Validation Loss: 0.00487735
Epoch [51/200], Train Loss: 0.004756
Validation Loss: 0.00487819
Epoch [52/200], Train Loss: 0.004761
Validation Loss: 0.00487704
Epoch [53/200], Train Loss: 0.004766
Validation Loss: 0.00487673
Epoch [54/200], Train Loss: 0.004768
Validation Loss: 0.00488126
Epoch [55/200], Train Loss: 0.004747
Validation Loss: 0.00487607
Epoch [56/200], Train Loss: 0.004766
Validation Loss: 0.00487645
Epoch [57/200], Train Loss: 0.004776
Validation Loss: 0.00487767
Epoch [58/200], Train Loss: 0.004767
Validation Loss: 0.00487627
Epoch [59/200], Train Loss: 0.004771
Validation Loss: 0.00487733
Epoch [60/200], Train Loss: 0.004767
Validation Loss: 0.00487526
Epoch [61/200], Train Loss: 0.004777
Validation Loss: 0.00487548
Epoch [62/200], Train Loss: 0.004765
Validation Loss: 0.00487471
Epoch [63/200], Train Loss: 0.004758
Validation Loss: 0.00487574
Epoch [64/200], Train Loss: 0.004762
Validation Loss: 0.00487550
Epoch [65/200], Train Loss: 0.004749
Validation Loss: 0.00487477
Epoch [66/200], Train Loss: 0.004752
Validation Loss: 0.00487443
Epoch [67/200], Train Loss: 0.004778
Validation Loss: 0.00487585
Epoch [68/200], Train Loss: 0.004768
Validation Loss: 0.00487750
Epoch [69/200], Train Loss: 0.004746
Validation Loss: 0.00487589
Epoch [70/200], Train Loss: 0.004747
Validation Loss: 0.00487376
Epoch [71/200], Train Loss: 0.004762
Validation Loss: 0.00488532
Epoch [72/200], Train Loss: 0.004769
Validation Loss: 0.00487359
Epoch [73/200], Train Loss: 0.004771
Validation Loss: 0.00487424
Epoch [74/200], Train Loss: 0.004748
Validation Loss: 0.00487280
Epoch [75/200], Train Loss: 0.004749
Validation Loss: 0.00487398
Epoch [76/200], Train Loss: 0.004770
Validation Loss: 0.00487225
Epoch [77/200], Train Loss: 0.004762
Validation Loss: 0.00487157
Epoch [78/200], Train Loss: 0.004750
Validation Loss: 0.00487159
Epoch [79/200], Train Loss: 0.004765
Validation Loss: 0.00487235
Epoch [80/200], Train Loss: 0.004758
Validation Loss: 0.00487075
Epoch [81/200], Train Loss: 0.004776
Validation Loss: 0.00487001
Epoch [82/200], Train Loss: 0.004760
Validation Loss: 0.00487007
Epoch [83/200], Train Loss: 0.004764
Validation Loss: 0.00486917
Epoch [84/200], Train Loss: 0.004751
Validation Loss: 0.00487384
Epoch [85/200], Train Loss: 0.004748
Validation Loss: 0.00486881
Epoch [86/200], Train Loss: 0.004754
Validation Loss: 0.00486778
Epoch [87/200], Train Loss: 0.004753
Validation Loss: 0.00486686
Epoch [88/200], Train Loss: 0.004749
Validation Loss: 0.00486920
Epoch [89/200], Train Loss: 0.004763
Validation Loss: 0.00486669
Epoch [90/200], Train Loss: 0.004763
Validation Loss: 0.00486489
Epoch [91/200], Train Loss: 0.004764
Validation Loss: 0.00486418
Epoch [92/200], Train Loss: 0.004748
Validation Loss: 0.00486341
Epoch [93/200], Train Loss: 0.004736
Validation Loss: 0.00486787
Epoch [94/200], Train Loss: 0.004760
Validation Loss: 0.00486342
Epoch [95/200], Train Loss: 0.004757
Validation Loss: 0.00486257
Epoch [96/200], Train Loss: 0.004754
Validation Loss: 0.00486587
Epoch [97/200], Train Loss: 0.004752
Validation Loss: 0.00486155
Epoch [98/200], Train Loss: 0.004760
Validation Loss: 0.00486241
Epoch [99/200], Train Loss: 0.004749
Validation Loss: 0.00485910
Epoch [100/200], Train Loss: 0.004746
Validation Loss: 0.00486024
Epoch [101/200], Train Loss: 0.004743
Validation Loss: 0.00486066
Epoch [102/200], Train Loss: 0.004735
Validation Loss: 0.00485705
Epoch [103/200], Train Loss: 0.004738
Validation Loss: 0.00486076
Epoch [104/200], Train Loss: 0.004756
Validation Loss: 0.00485525
Epoch [105/200], Train Loss: 0.004736
Validation Loss: 0.00485591
Epoch [106/200], Train Loss: 0.004761
Validation Loss: 0.00485435
Epoch [107/200], Train Loss: 0.004752
Validation Loss: 0.00485226
Epoch [108/200], Train Loss: 0.004743
Validation Loss: 0.00485089
Epoch [109/200], Train Loss: 0.004736
Validation Loss: 0.00484802
Epoch [110/200], Train Loss: 0.004741
Validation Loss: 0.00484998
Epoch [111/200], Train Loss: 0.004749
Validation Loss: 0.00484406
Epoch [112/200], Train Loss: 0.004739
Validation Loss: 0.00484414
Epoch [113/200], Train Loss: 0.004740
Validation Loss: 0.00483985
Epoch [114/200], Train Loss: 0.004721
Validation Loss: 0.00483782
Epoch [115/200], Train Loss: 0.004716
Validation Loss: 0.00483169
Epoch [116/200], Train Loss: 0.004717
Validation Loss: 0.00482828
Epoch [117/200], Train Loss: 0.004704
Validation Loss: 0.00482515
Epoch [118/200], Train Loss: 0.004721
Validation Loss: 0.00482225
Epoch [119/200], Train Loss: 0.004718
Validation Loss: 0.00482077
Epoch [120/200], Train Loss: 0.004720
Validation Loss: 0.00481354
Epoch [121/200], Train Loss: 0.004708
Validation Loss: 0.00480695
Epoch [122/200], Train Loss: 0.004700
Validation Loss: 0.00480531
Epoch [123/200], Train Loss: 0.004693
Validation Loss: 0.00480961
Epoch [124/200], Train Loss: 0.004717
Validation Loss: 0.00479995
Epoch [125/200], Train Loss: 0.004694
Validation Loss: 0.00479024
Epoch [126/200], Train Loss: 0.004688
Validation Loss: 0.00478942
Epoch [127/200], Train Loss: 0.004671
Validation Loss: 0.00477839
Epoch [128/200], Train Loss: 0.004660
Validation Loss: 0.00476835
Epoch [129/200], Train Loss: 0.004703
Validation Loss: 0.00477074
Epoch [130/200], Train Loss: 0.004669
Validation Loss: 0.00476635
Epoch [131/200], Train Loss: 0.004662
Validation Loss: 0.00475371
Epoch [132/200], Train Loss: 0.004648
Validation Loss: 0.00473686
Epoch [133/200], Train Loss: 0.004645
Validation Loss: 0.00472396
Epoch [134/200], Train Loss: 0.004644
Validation Loss: 0.00470232
Epoch [135/200], Train Loss: 0.004631
Validation Loss: 0.00471583
Epoch [136/200], Train Loss: 0.004618
Validation Loss: 0.00469688
Epoch [137/200], Train Loss: 0.004611
Validation Loss: 0.00469176
Epoch [138/200], Train Loss: 0.004572
Validation Loss: 0.00464659
Epoch [139/200], Train Loss: 0.004564
Validation Loss: 0.00464043
Epoch [140/200], Train Loss: 0.004591
Validation Loss: 0.00472375
Epoch [141/200], Train Loss: 0.004566
Validation Loss: 0.00463394
Epoch [142/200], Train Loss: 0.004559
Validation Loss: 0.00461328
Epoch [143/200], Train Loss: 0.004527
Validation Loss: 0.00460591
Epoch [144/200], Train Loss: 0.004516
Validation Loss: 0.00458068
Epoch [145/200], Train Loss: 0.004525
Validation Loss: 0.00456462
Epoch [146/200], Train Loss: 0.004491
Validation Loss: 0.00454986
Epoch [147/200], Train Loss: 0.004485
Validation Loss: 0.00453967
Epoch [148/200], Train Loss: 0.004485
Validation Loss: 0.00454054
Epoch [149/200], Train Loss: 0.004470
Validation Loss: 0.00451379
Epoch [150/200], Train Loss: 0.004453
Validation Loss: 0.00449649
Epoch [151/200], Train Loss: 0.004445
Validation Loss: 0.00448398
Epoch [152/200], Train Loss: 0.004439
Validation Loss: 0.00447172
Epoch [153/200], Train Loss: 0.004438
Validation Loss: 0.00446315
Epoch [154/200], Train Loss: 0.004416
Validation Loss: 0.00446094
Epoch [155/200], Train Loss: 0.004411
Validation Loss: 0.00445396
Epoch [156/200], Train Loss: 0.004407
Validation Loss: 0.00442106
Epoch [157/200], Train Loss: 0.004387
Validation Loss: 0.00441692
Epoch [158/200], Train Loss: 0.004391
Validation Loss: 0.00442125
Epoch [159/200], Train Loss: 0.004373
Validation Loss: 0.00438403
Epoch [160/200], Train Loss: 0.004354
Validation Loss: 0.00435881
Epoch [161/200], Train Loss: 0.004369
Validation Loss: 0.00439707
Epoch [162/200], Train Loss: 0.004347
Validation Loss: 0.00439548
Epoch [163/200], Train Loss: 0.004331
Validation Loss: 0.00433649
Epoch [164/200], Train Loss: 0.004320
Validation Loss: 0.00431417
Epoch [165/200], Train Loss: 0.004289
Validation Loss: 0.00428678
Epoch [166/200], Train Loss: 0.004288
Validation Loss: 0.00426345
Epoch [167/200], Train Loss: 0.004269
Validation Loss: 0.00424784
Epoch [168/200], Train Loss: 0.004234
Validation Loss: 0.00423946
Epoch [169/200], Train Loss: 0.004251
Validation Loss: 0.00420886
Epoch [170/200], Train Loss: 0.004235
Validation Loss: 0.00418352
Epoch [171/200], Train Loss: 0.004213
Validation Loss: 0.00416111
Epoch [172/200], Train Loss: 0.004184
Validation Loss: 0.00413702
Epoch [173/200], Train Loss: 0.004175
Validation Loss: 0.00411318
Epoch [174/200], Train Loss: 0.004141
Validation Loss: 0.00409495
Epoch [175/200], Train Loss: 0.004117
Validation Loss: 0.00406084
Epoch [176/200], Train Loss: 0.004104
Validation Loss: 0.00404165
Epoch [177/200], Train Loss: 0.004079
Validation Loss: 0.00399612
Epoch [178/200], Train Loss: 0.004056
Validation Loss: 0.00396622
Epoch [179/200], Train Loss: 0.004029
Validation Loss: 0.00394396
Epoch [180/200], Train Loss: 0.004009
Validation Loss: 0.00388413
Epoch [181/200], Train Loss: 0.003979
Validation Loss: 0.00384806
Epoch [182/200], Train Loss: 0.003963
Validation Loss: 0.00382709
Epoch [183/200], Train Loss: 0.003919
Validation Loss: 0.00376356
Epoch [184/200], Train Loss: 0.003877
Validation Loss: 0.00373644
Epoch [185/200], Train Loss: 0.003850
Validation Loss: 0.00368794
Epoch [186/200], Train Loss: 0.003823
Validation Loss: 0.00364114
Epoch [187/200], Train Loss: 0.003749
Validation Loss: 0.00355233
Epoch [188/200], Train Loss: 0.003741
Validation Loss: 0.00359189
Epoch [189/200], Train Loss: 0.003670
Validation Loss: 0.00349441
Epoch [190/200], Train Loss: 0.003635
Validation Loss: 0.00338401
Epoch [191/200], Train Loss: 0.003582
Validation Loss: 0.00331805
Epoch [192/200], Train Loss: 0.003506
Validation Loss: 0.00337735
Epoch [193/200], Train Loss: 0.003462
Validation Loss: 0.00324854
Epoch [194/200], Train Loss: 0.003421
Validation Loss: 0.00323673
Epoch [195/200], Train Loss: 0.003379
Validation Loss: 0.00324829
Epoch [196/200], Train Loss: 0.003328
Validation Loss: 0.00308753
Epoch [197/200], Train Loss: 0.003273
Validation Loss: 0.00310108
Epoch [198/200], Train Loss: 0.003247
Validation Loss: 0.00302856
Epoch [199/200], Train Loss: 0.003214
Validation Loss: 0.00293065
Epoch [200/200], Train Loss: 0.003178
Validation Loss: 0.00292403

Evaluating model for: Lamp
Run 75/144 completed in 1054.51 seconds with: {'MAE': np.float32(1.6953737), 'MSE': np.float32(100.24783), 'RMSE': np.float32(10.012384), 'SAE': np.float32(0.17990999), 'NDE': np.float32(0.7793811)}

Run 76/144: hidden=128, seq_len=720, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.005466
Validation Loss: 0.00505624
Epoch [2/200], Train Loss: 0.004933
Validation Loss: 0.00499548
Epoch [3/200], Train Loss: 0.004914
Validation Loss: 0.00499245
Epoch [4/200], Train Loss: 0.004900
Validation Loss: 0.00499165
Epoch [5/200], Train Loss: 0.004914
Validation Loss: 0.00498745
Epoch [6/200], Train Loss: 0.004888
Validation Loss: 0.00497648
Epoch [7/200], Train Loss: 0.004866
Validation Loss: 0.00493335
Epoch [8/200], Train Loss: 0.004806
Validation Loss: 0.00491398
Epoch [9/200], Train Loss: 0.004810
Validation Loss: 0.00490473
Epoch [10/200], Train Loss: 0.004797
Validation Loss: 0.00490279
Epoch [11/200], Train Loss: 0.004794
Validation Loss: 0.00490473
Epoch [12/200], Train Loss: 0.004801
Validation Loss: 0.00490169
Epoch [13/200], Train Loss: 0.004790
Validation Loss: 0.00490442
Epoch [14/200], Train Loss: 0.004793
Validation Loss: 0.00489849
Epoch [15/200], Train Loss: 0.004794
Validation Loss: 0.00489509
Epoch [16/200], Train Loss: 0.004788
Validation Loss: 0.00489430
Epoch [17/200], Train Loss: 0.004799
Validation Loss: 0.00489331
Epoch [18/200], Train Loss: 0.004775
Validation Loss: 0.00489458
Epoch [19/200], Train Loss: 0.004772
Validation Loss: 0.00489233
Epoch [20/200], Train Loss: 0.004787
Validation Loss: 0.00489057
Epoch [21/200], Train Loss: 0.004785
Validation Loss: 0.00489145
Epoch [22/200], Train Loss: 0.004783
Validation Loss: 0.00488996
Epoch [23/200], Train Loss: 0.004788
Validation Loss: 0.00489107
Epoch [24/200], Train Loss: 0.004791
Validation Loss: 0.00488996
Epoch [25/200], Train Loss: 0.004772
Validation Loss: 0.00488875
Epoch [26/200], Train Loss: 0.004784
Validation Loss: 0.00488658
Epoch [27/200], Train Loss: 0.004777
Validation Loss: 0.00488684
Epoch [28/200], Train Loss: 0.004786
Validation Loss: 0.00488758
Epoch [29/200], Train Loss: 0.004780
Validation Loss: 0.00488656
Epoch [30/200], Train Loss: 0.004763
Validation Loss: 0.00488778
Epoch [31/200], Train Loss: 0.004787
Validation Loss: 0.00488858
Epoch [32/200], Train Loss: 0.004768
Validation Loss: 0.00491239
Epoch [33/200], Train Loss: 0.004786
Validation Loss: 0.00488517
Epoch [34/200], Train Loss: 0.004781
Validation Loss: 0.00489023
Epoch [35/200], Train Loss: 0.004773
Validation Loss: 0.00488559
Epoch [36/200], Train Loss: 0.004787
Validation Loss: 0.00488499
Epoch [37/200], Train Loss: 0.004785
Validation Loss: 0.00488576
Epoch [38/200], Train Loss: 0.004787
Validation Loss: 0.00488439
Epoch [39/200], Train Loss: 0.004759
Validation Loss: 0.00488619
Epoch [40/200], Train Loss: 0.004767
Validation Loss: 0.00488370
Epoch [41/200], Train Loss: 0.004767
Validation Loss: 0.00488531
Epoch [42/200], Train Loss: 0.004780
Validation Loss: 0.00488740
Epoch [43/200], Train Loss: 0.004776
Validation Loss: 0.00488711
Epoch [44/200], Train Loss: 0.004770
Validation Loss: 0.00488304
Epoch [45/200], Train Loss: 0.004783
Validation Loss: 0.00488624
Epoch [46/200], Train Loss: 0.004781
Validation Loss: 0.00488779
Epoch [47/200], Train Loss: 0.004767
Validation Loss: 0.00488271
Epoch [48/200], Train Loss: 0.004774
Validation Loss: 0.00488283
Epoch [49/200], Train Loss: 0.004768
Validation Loss: 0.00488800
Epoch [50/200], Train Loss: 0.004784
Validation Loss: 0.00488404
Epoch [51/200], Train Loss: 0.004760
Validation Loss: 0.00488243
Epoch [52/200], Train Loss: 0.004768
Validation Loss: 0.00488201
Epoch [53/200], Train Loss: 0.004775
Validation Loss: 0.00488350
Epoch [54/200], Train Loss: 0.004770
Validation Loss: 0.00488258
Epoch [55/200], Train Loss: 0.004779
Validation Loss: 0.00488262
Epoch [56/200], Train Loss: 0.004793
Validation Loss: 0.00488302
Epoch [57/200], Train Loss: 0.004763
Validation Loss: 0.00488229
Epoch [58/200], Train Loss: 0.004775
Validation Loss: 0.00488174
Epoch [59/200], Train Loss: 0.004770
Validation Loss: 0.00488176
Epoch [60/200], Train Loss: 0.004760
Validation Loss: 0.00488155
Epoch [61/200], Train Loss: 0.004778
Validation Loss: 0.00488170
Epoch [62/200], Train Loss: 0.004768
Validation Loss: 0.00488510
Epoch [63/200], Train Loss: 0.004799
Validation Loss: 0.00488344
Epoch [64/200], Train Loss: 0.004796
Validation Loss: 0.00488213
Epoch [65/200], Train Loss: 0.004773
Validation Loss: 0.00488153
Epoch [66/200], Train Loss: 0.004760
Validation Loss: 0.00488914
Epoch [67/200], Train Loss: 0.004775
Validation Loss: 0.00488158
Epoch [68/200], Train Loss: 0.004770
Validation Loss: 0.00488084
Epoch [69/200], Train Loss: 0.004772
Validation Loss: 0.00488082
Epoch [70/200], Train Loss: 0.004758
Validation Loss: 0.00488257
Epoch [71/200], Train Loss: 0.004777
Validation Loss: 0.00488018
Epoch [72/200], Train Loss: 0.004779
Validation Loss: 0.00488005
Epoch [73/200], Train Loss: 0.004765
Validation Loss: 0.00488112
Epoch [74/200], Train Loss: 0.004750
Validation Loss: 0.00488105
Epoch [75/200], Train Loss: 0.004773
Validation Loss: 0.00488484
Epoch [76/200], Train Loss: 0.004770
Validation Loss: 0.00487966
Epoch [77/200], Train Loss: 0.004757
Validation Loss: 0.00488008
Epoch [78/200], Train Loss: 0.004771
Validation Loss: 0.00488067
Epoch [79/200], Train Loss: 0.004757
Validation Loss: 0.00487937
Epoch [80/200], Train Loss: 0.004784
Validation Loss: 0.00488331
Epoch [81/200], Train Loss: 0.004754
Validation Loss: 0.00488071
Epoch [82/200], Train Loss: 0.004768
Validation Loss: 0.00487921
Epoch [83/200], Train Loss: 0.004774
Validation Loss: 0.00487923
Epoch [84/200], Train Loss: 0.004777
Validation Loss: 0.00488204
Epoch [85/200], Train Loss: 0.004764
Validation Loss: 0.00488228
Epoch [86/200], Train Loss: 0.004755
Validation Loss: 0.00488108
Epoch [87/200], Train Loss: 0.004777
Validation Loss: 0.00487940
Epoch [88/200], Train Loss: 0.004764
Validation Loss: 0.00487951
Epoch [89/200], Train Loss: 0.004775
Validation Loss: 0.00487778
Epoch [90/200], Train Loss: 0.004747
Validation Loss: 0.00488170
Epoch [91/200], Train Loss: 0.004769
Validation Loss: 0.00487851
Epoch [92/200], Train Loss: 0.004744
Validation Loss: 0.00487771
Epoch [93/200], Train Loss: 0.004770
Validation Loss: 0.00487933
Epoch [94/200], Train Loss: 0.004766
Validation Loss: 0.00487733
Epoch [95/200], Train Loss: 0.004760
Validation Loss: 0.00487755
Epoch [96/200], Train Loss: 0.004759
Validation Loss: 0.00487761
Epoch [97/200], Train Loss: 0.004769
Validation Loss: 0.00487853
Epoch [98/200], Train Loss: 0.004776
Validation Loss: 0.00487753
Epoch [99/200], Train Loss: 0.004747
Validation Loss: 0.00488224
Epoch [100/200], Train Loss: 0.004769
Validation Loss: 0.00487749
Epoch [101/200], Train Loss: 0.004773
Validation Loss: 0.00487670
Epoch [102/200], Train Loss: 0.004751
Validation Loss: 0.00488259
Epoch [103/200], Train Loss: 0.004777
Validation Loss: 0.00487760
Epoch [104/200], Train Loss: 0.004773
Validation Loss: 0.00487633
Epoch [105/200], Train Loss: 0.004763
Validation Loss: 0.00487941
Epoch [106/200], Train Loss: 0.004755
Validation Loss: 0.00487801
Epoch [107/200], Train Loss: 0.004761
Validation Loss: 0.00487731
Epoch [108/200], Train Loss: 0.004765
Validation Loss: 0.00487645
Epoch [109/200], Train Loss: 0.004754
Validation Loss: 0.00487600
Epoch [110/200], Train Loss: 0.004751
Validation Loss: 0.00487541
Epoch [111/200], Train Loss: 0.004757
Validation Loss: 0.00487524
Epoch [112/200], Train Loss: 0.004759
Validation Loss: 0.00487572
Epoch [113/200], Train Loss: 0.004763
Validation Loss: 0.00487551
Epoch [114/200], Train Loss: 0.004771
Validation Loss: 0.00487540
Epoch [115/200], Train Loss: 0.004769
Validation Loss: 0.00487501
Epoch [116/200], Train Loss: 0.004771
Validation Loss: 0.00487456
Epoch [117/200], Train Loss: 0.004770
Validation Loss: 0.00487603
Epoch [118/200], Train Loss: 0.004791
Validation Loss: 0.00487599
Epoch [119/200], Train Loss: 0.004766
Validation Loss: 0.00487615
Epoch [120/200], Train Loss: 0.004754
Validation Loss: 0.00487510
Epoch [121/200], Train Loss: 0.004761
Validation Loss: 0.00487427
Epoch [122/200], Train Loss: 0.004752
Validation Loss: 0.00487635
Epoch [123/200], Train Loss: 0.004754
Validation Loss: 0.00487406
Epoch [124/200], Train Loss: 0.004766
Validation Loss: 0.00487428
Epoch [125/200], Train Loss: 0.004768
Validation Loss: 0.00487352
Epoch [126/200], Train Loss: 0.004770
Validation Loss: 0.00487310
Epoch [127/200], Train Loss: 0.004766
Validation Loss: 0.00487316
Epoch [128/200], Train Loss: 0.004766
Validation Loss: 0.00487242
Epoch [129/200], Train Loss: 0.004749
Validation Loss: 0.00487307
Epoch [130/200], Train Loss: 0.004754
Validation Loss: 0.00487269
Epoch [131/200], Train Loss: 0.004752
Validation Loss: 0.00487560
Epoch [132/200], Train Loss: 0.004754
Validation Loss: 0.00487242
Epoch [133/200], Train Loss: 0.004750
Validation Loss: 0.00487095
Epoch [134/200], Train Loss: 0.004756
Validation Loss: 0.00487350
Epoch [135/200], Train Loss: 0.004757
Validation Loss: 0.00487065
Epoch [136/200], Train Loss: 0.004758
Validation Loss: 0.00487015
Epoch [137/200], Train Loss: 0.004747
Validation Loss: 0.00487037
Epoch [138/200], Train Loss: 0.004754
Validation Loss: 0.00486948
Epoch [139/200], Train Loss: 0.004757
Validation Loss: 0.00486918
Epoch [140/200], Train Loss: 0.004754
Validation Loss: 0.00486893
Epoch [141/200], Train Loss: 0.004749
Validation Loss: 0.00487130
Epoch [142/200], Train Loss: 0.004746
Validation Loss: 0.00486949
Epoch [143/200], Train Loss: 0.004757
Validation Loss: 0.00486935
Epoch [144/200], Train Loss: 0.004761
Validation Loss: 0.00486825
Epoch [145/200], Train Loss: 0.004765
Validation Loss: 0.00486815
Epoch [146/200], Train Loss: 0.004733
Validation Loss: 0.00487209
Epoch [147/200], Train Loss: 0.004759
Validation Loss: 0.00486981
Epoch [148/200], Train Loss: 0.004771
Validation Loss: 0.00486720
Epoch [149/200], Train Loss: 0.004766
Validation Loss: 0.00486662
Epoch [150/200], Train Loss: 0.004749
Validation Loss: 0.00486800
Epoch [151/200], Train Loss: 0.004744
Validation Loss: 0.00486817
Epoch [152/200], Train Loss: 0.004744
Validation Loss: 0.00486584
Epoch [153/200], Train Loss: 0.004756
Validation Loss: 0.00486648
Epoch [154/200], Train Loss: 0.004753
Validation Loss: 0.00486652
Epoch [155/200], Train Loss: 0.004774
Validation Loss: 0.00486694
Epoch [156/200], Train Loss: 0.004750
Validation Loss: 0.00486516
Epoch [157/200], Train Loss: 0.004761
Validation Loss: 0.00486515
Epoch [158/200], Train Loss: 0.004763
Validation Loss: 0.00486456
Epoch [159/200], Train Loss: 0.004758
Validation Loss: 0.00486560
Epoch [160/200], Train Loss: 0.004754
Validation Loss: 0.00486483
Epoch [161/200], Train Loss: 0.004763
Validation Loss: 0.00486255
Epoch [162/200], Train Loss: 0.004746
Validation Loss: 0.00486300
Epoch [163/200], Train Loss: 0.004752
Validation Loss: 0.00486832
Epoch [164/200], Train Loss: 0.004758
Validation Loss: 0.00486332
Epoch [165/200], Train Loss: 0.004750
Validation Loss: 0.00486281
Epoch [166/200], Train Loss: 0.004744
Validation Loss: 0.00486407
Epoch [167/200], Train Loss: 0.004740
Validation Loss: 0.00486098
Epoch [168/200], Train Loss: 0.004750
Validation Loss: 0.00486137
Epoch [169/200], Train Loss: 0.004744
Validation Loss: 0.00486388
Epoch [170/200], Train Loss: 0.004734
Validation Loss: 0.00486150
Epoch [171/200], Train Loss: 0.004737
Validation Loss: 0.00485911
Epoch [172/200], Train Loss: 0.004737
Validation Loss: 0.00485940
Epoch [173/200], Train Loss: 0.004738
Validation Loss: 0.00485827
Epoch [174/200], Train Loss: 0.004746
Validation Loss: 0.00485980
Epoch [175/200], Train Loss: 0.004745
Validation Loss: 0.00486177
Epoch [176/200], Train Loss: 0.004764
Validation Loss: 0.00485845
Epoch [177/200], Train Loss: 0.004755
Validation Loss: 0.00485829
Epoch [178/200], Train Loss: 0.004745
Validation Loss: 0.00485828
Epoch [179/200], Train Loss: 0.004748
Validation Loss: 0.00485683
Epoch [180/200], Train Loss: 0.004739
Validation Loss: 0.00485664
Epoch [181/200], Train Loss: 0.004740
Validation Loss: 0.00485659
Epoch [182/200], Train Loss: 0.004748
Validation Loss: 0.00485645
Epoch [183/200], Train Loss: 0.004747
Validation Loss: 0.00485612
Epoch [184/200], Train Loss: 0.004754
Validation Loss: 0.00485500
Epoch [185/200], Train Loss: 0.004746
Validation Loss: 0.00485760
Epoch [186/200], Train Loss: 0.004743
Validation Loss: 0.00485916
Epoch [187/200], Train Loss: 0.004741
Validation Loss: 0.00485508
Epoch [188/200], Train Loss: 0.004755
Validation Loss: 0.00485412
Epoch [189/200], Train Loss: 0.004732
Validation Loss: 0.00485441
Epoch [190/200], Train Loss: 0.004755
Validation Loss: 0.00485266
Epoch [191/200], Train Loss: 0.004736
Validation Loss: 0.00485369
Epoch [192/200], Train Loss: 0.004734
Validation Loss: 0.00485338
Epoch [193/200], Train Loss: 0.004745
Validation Loss: 0.00485273
Epoch [194/200], Train Loss: 0.004739
Validation Loss: 0.00485247
Epoch [195/200], Train Loss: 0.004743
Validation Loss: 0.00484932
Epoch [196/200], Train Loss: 0.004731
Validation Loss: 0.00485006
Epoch [197/200], Train Loss: 0.004742
Validation Loss: 0.00485090
Epoch [198/200], Train Loss: 0.004729
Validation Loss: 0.00485105
Epoch [199/200], Train Loss: 0.004730
Validation Loss: 0.00484673
Epoch [200/200], Train Loss: 0.004748
Validation Loss: 0.00484692

Evaluating model for: Lamp
Run 76/144 completed in 1132.70 seconds with: {'MAE': np.float32(2.598161), 'MSE': np.float32(157.98262), 'RMSE': np.float32(12.569114), 'SAE': np.float32(0.014545775), 'NDE': np.float32(0.97840273)}

Run 77/144: hidden=128, seq_len=720, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.005405
Validation Loss: 0.00520095
Epoch [2/200], Train Loss: 0.004894
Validation Loss: 0.00510909
Epoch [3/200], Train Loss: 0.004907
Validation Loss: 0.00510347
Epoch [4/200], Train Loss: 0.004938
Validation Loss: 0.00508989
Epoch [5/200], Train Loss: 0.004802
Validation Loss: 0.00508476
Epoch [6/200], Train Loss: 0.004832
Validation Loss: 0.00507631
Epoch [7/200], Train Loss: 0.004782
Validation Loss: 0.00507044
Epoch [8/200], Train Loss: 0.004910
Validation Loss: 0.00506460
Epoch [9/200], Train Loss: 0.004886
Validation Loss: 0.00505719
Epoch [10/200], Train Loss: 0.004790
Validation Loss: 0.00505069
Epoch [11/200], Train Loss: 0.004844
Validation Loss: 0.00504472
Epoch [12/200], Train Loss: 0.004837
Validation Loss: 0.00503945
Epoch [13/200], Train Loss: 0.004813
Validation Loss: 0.00503422
Epoch [14/200], Train Loss: 0.004818
Validation Loss: 0.00503084
Epoch [15/200], Train Loss: 0.004839
Validation Loss: 0.00503020
Epoch [16/200], Train Loss: 0.004799
Validation Loss: 0.00502702
Epoch [17/200], Train Loss: 0.004820
Validation Loss: 0.00502614
Epoch [18/200], Train Loss: 0.004763
Validation Loss: 0.00502661
Epoch [19/200], Train Loss: 0.004837
Validation Loss: 0.00502653
Epoch [20/200], Train Loss: 0.004898
Validation Loss: 0.00502487
Epoch [21/200], Train Loss: 0.004904
Validation Loss: 0.00502554
Epoch [22/200], Train Loss: 0.004762
Validation Loss: 0.00502383
Epoch [23/200], Train Loss: 0.004762
Validation Loss: 0.00502434
Epoch [24/200], Train Loss: 0.004835
Validation Loss: 0.00502276
Epoch [25/200], Train Loss: 0.004802
Validation Loss: 0.00502264
Epoch [26/200], Train Loss: 0.004810
Validation Loss: 0.00502232
Epoch [27/200], Train Loss: 0.004816
Validation Loss: 0.00502167
Epoch [28/200], Train Loss: 0.004831
Validation Loss: 0.00502330
Epoch [29/200], Train Loss: 0.004799
Validation Loss: 0.00502040
Epoch [30/200], Train Loss: 0.004872
Validation Loss: 0.00502180
Epoch [31/200], Train Loss: 0.004919
Validation Loss: 0.00502078
Epoch [32/200], Train Loss: 0.004836
Validation Loss: 0.00501990
Epoch [33/200], Train Loss: 0.004802
Validation Loss: 0.00502130
Epoch [34/200], Train Loss: 0.004858
Validation Loss: 0.00501902
Epoch [35/200], Train Loss: 0.004851
Validation Loss: 0.00502015
Epoch [36/200], Train Loss: 0.004865
Validation Loss: 0.00501791
Epoch [37/200], Train Loss: 0.004823
Validation Loss: 0.00501810
Epoch [38/200], Train Loss: 0.004813
Validation Loss: 0.00501687
Epoch [39/200], Train Loss: 0.004795
Validation Loss: 0.00501945
Epoch [40/200], Train Loss: 0.004857
Validation Loss: 0.00501572
Epoch [41/200], Train Loss: 0.004786
Validation Loss: 0.00501736
Epoch [42/200], Train Loss: 0.004848
Validation Loss: 0.00501550
Epoch [43/200], Train Loss: 0.004787
Validation Loss: 0.00501502
Epoch [44/200], Train Loss: 0.004825
Validation Loss: 0.00501447
Epoch [45/200], Train Loss: 0.004782
Validation Loss: 0.00501652
Epoch [46/200], Train Loss: 0.004772
Validation Loss: 0.00501353
Epoch [47/200], Train Loss: 0.004782
Validation Loss: 0.00501293
Epoch [48/200], Train Loss: 0.004802
Validation Loss: 0.00501232
Epoch [49/200], Train Loss: 0.004762
Validation Loss: 0.00501180
Epoch [50/200], Train Loss: 0.004873
Validation Loss: 0.00501092
Epoch [51/200], Train Loss: 0.004751
Validation Loss: 0.00501080
Epoch [52/200], Train Loss: 0.004812
Validation Loss: 0.00501049
Epoch [53/200], Train Loss: 0.004826
Validation Loss: 0.00500949
Epoch [54/200], Train Loss: 0.004824
Validation Loss: 0.00501013
Epoch [55/200], Train Loss: 0.004849
Validation Loss: 0.00500661
Epoch [56/200], Train Loss: 0.004766
Validation Loss: 0.00500783
Epoch [57/200], Train Loss: 0.004814
Validation Loss: 0.00500756
Epoch [58/200], Train Loss: 0.004758
Validation Loss: 0.00500430
Epoch [59/200], Train Loss: 0.004775
Validation Loss: 0.00500636
Epoch [60/200], Train Loss: 0.004775
Validation Loss: 0.00500204
Epoch [61/200], Train Loss: 0.004806
Validation Loss: 0.00500030
Epoch [62/200], Train Loss: 0.004815
Validation Loss: 0.00500207
Epoch [63/200], Train Loss: 0.004737
Validation Loss: 0.00499776
Epoch [64/200], Train Loss: 0.004803
Validation Loss: 0.00499553
Epoch [65/200], Train Loss: 0.004728
Validation Loss: 0.00499887
Epoch [66/200], Train Loss: 0.004861
Validation Loss: 0.00499616
Epoch [67/200], Train Loss: 0.004821
Validation Loss: 0.00499339
Epoch [68/200], Train Loss: 0.004736
Validation Loss: 0.00499003
Epoch [69/200], Train Loss: 0.004794
Validation Loss: 0.00498920
Epoch [70/200], Train Loss: 0.004774
Validation Loss: 0.00498985
Epoch [71/200], Train Loss: 0.004748
Validation Loss: 0.00499180
Epoch [72/200], Train Loss: 0.004820
Validation Loss: 0.00498246
Epoch [73/200], Train Loss: 0.004719
Validation Loss: 0.00498046
Epoch [74/200], Train Loss: 0.004804
Validation Loss: 0.00497957
Epoch [75/200], Train Loss: 0.004785
Validation Loss: 0.00497873
Epoch [76/200], Train Loss: 0.004772
Validation Loss: 0.00497697
Epoch [77/200], Train Loss: 0.004731
Validation Loss: 0.00497317
Epoch [78/200], Train Loss: 0.004735
Validation Loss: 0.00497336
Epoch [79/200], Train Loss: 0.004714
Validation Loss: 0.00497482
Epoch [80/200], Train Loss: 0.004782
Validation Loss: 0.00496939
Epoch [81/200], Train Loss: 0.004721
Validation Loss: 0.00496968
Epoch [82/200], Train Loss: 0.004770
Validation Loss: 0.00496830
Epoch [83/200], Train Loss: 0.004734
Validation Loss: 0.00496492
Epoch [84/200], Train Loss: 0.004716
Validation Loss: 0.00496396
Epoch [85/200], Train Loss: 0.004763
Validation Loss: 0.00496842
Epoch [86/200], Train Loss: 0.004718
Validation Loss: 0.00496358
Epoch [87/200], Train Loss: 0.004785
Validation Loss: 0.00496080
Epoch [88/200], Train Loss: 0.004736
Validation Loss: 0.00495707
Epoch [89/200], Train Loss: 0.004677
Validation Loss: 0.00495425
Epoch [90/200], Train Loss: 0.004808
Validation Loss: 0.00495495
Epoch [91/200], Train Loss: 0.004812
Validation Loss: 0.00495261
Epoch [92/200], Train Loss: 0.004752
Validation Loss: 0.00495812
Epoch [93/200], Train Loss: 0.004701
Validation Loss: 0.00495198
Epoch [94/200], Train Loss: 0.004723
Validation Loss: 0.00494494
Epoch [95/200], Train Loss: 0.004735
Validation Loss: 0.00494110
Epoch [96/200], Train Loss: 0.004709
Validation Loss: 0.00494267
Epoch [97/200], Train Loss: 0.004693
Validation Loss: 0.00493801
Epoch [98/200], Train Loss: 0.004733
Validation Loss: 0.00493443
Epoch [99/200], Train Loss: 0.004817
Validation Loss: 0.00493191
Epoch [100/200], Train Loss: 0.004673
Validation Loss: 0.00492999
Epoch [101/200], Train Loss: 0.004640
Validation Loss: 0.00493322
Epoch [102/200], Train Loss: 0.004730
Validation Loss: 0.00492335
Epoch [103/200], Train Loss: 0.004637
Validation Loss: 0.00492910
Epoch [104/200], Train Loss: 0.004714
Validation Loss: 0.00493523
Epoch [105/200], Train Loss: 0.004689
Validation Loss: 0.00491259
Epoch [106/200], Train Loss: 0.004669
Validation Loss: 0.00491118
Epoch [107/200], Train Loss: 0.004719
Validation Loss: 0.00490868
Epoch [108/200], Train Loss: 0.004681
Validation Loss: 0.00490468
Epoch [109/200], Train Loss: 0.004700
Validation Loss: 0.00489932
Epoch [110/200], Train Loss: 0.004638
Validation Loss: 0.00489795
Epoch [111/200], Train Loss: 0.004656
Validation Loss: 0.00489720
Epoch [112/200], Train Loss: 0.004619
Validation Loss: 0.00488914
Epoch [113/200], Train Loss: 0.004686
Validation Loss: 0.00488221
Epoch [114/200], Train Loss: 0.004714
Validation Loss: 0.00487908
Epoch [115/200], Train Loss: 0.004645
Validation Loss: 0.00487440
Epoch [116/200], Train Loss: 0.004732
Validation Loss: 0.00487056
Epoch [117/200], Train Loss: 0.004745
Validation Loss: 0.00487078
Epoch [118/200], Train Loss: 0.004635
Validation Loss: 0.00485860
Epoch [119/200], Train Loss: 0.004625
Validation Loss: 0.00485697
Epoch [120/200], Train Loss: 0.004645
Validation Loss: 0.00486380
Epoch [121/200], Train Loss: 0.004657
Validation Loss: 0.00484721
Epoch [122/200], Train Loss: 0.004655
Validation Loss: 0.00485265
Epoch [123/200], Train Loss: 0.004633
Validation Loss: 0.00482883
Epoch [124/200], Train Loss: 0.004651
Validation Loss: 0.00482588
Epoch [125/200], Train Loss: 0.004614
Validation Loss: 0.00482968
Epoch [126/200], Train Loss: 0.004559
Validation Loss: 0.00481142
Epoch [127/200], Train Loss: 0.004508
Validation Loss: 0.00480235
Epoch [128/200], Train Loss: 0.004505
Validation Loss: 0.00479701
Epoch [129/200], Train Loss: 0.004587
Validation Loss: 0.00478922
Epoch [130/200], Train Loss: 0.004510
Validation Loss: 0.00478120
Epoch [131/200], Train Loss: 0.004503
Validation Loss: 0.00481079
Epoch [132/200], Train Loss: 0.004618
Validation Loss: 0.00476818
Epoch [133/200], Train Loss: 0.004480
Validation Loss: 0.00476753
Epoch [134/200], Train Loss: 0.004527
Validation Loss: 0.00475002
Epoch [135/200], Train Loss: 0.004493
Validation Loss: 0.00473813
Epoch [136/200], Train Loss: 0.004497
Validation Loss: 0.00472863
Epoch [137/200], Train Loss: 0.004488
Validation Loss: 0.00472088
Epoch [138/200], Train Loss: 0.004535
Validation Loss: 0.00471435
Epoch [139/200], Train Loss: 0.004454
Validation Loss: 0.00469604
Epoch [140/200], Train Loss: 0.004520
Validation Loss: 0.00469070
Epoch [141/200], Train Loss: 0.004461
Validation Loss: 0.00469275
Epoch [142/200], Train Loss: 0.004479
Validation Loss: 0.00467541
Epoch [143/200], Train Loss: 0.004470
Validation Loss: 0.00465970
Epoch [144/200], Train Loss: 0.004464
Validation Loss: 0.00464644
Epoch [145/200], Train Loss: 0.004395
Validation Loss: 0.00465868
Epoch [146/200], Train Loss: 0.004450
Validation Loss: 0.00463972
Epoch [147/200], Train Loss: 0.004437
Validation Loss: 0.00462735
Epoch [148/200], Train Loss: 0.004505
Validation Loss: 0.00460862
Epoch [149/200], Train Loss: 0.004395
Validation Loss: 0.00462008
Epoch [150/200], Train Loss: 0.004504
Validation Loss: 0.00459603
Epoch [151/200], Train Loss: 0.004338
Validation Loss: 0.00458364
Epoch [152/200], Train Loss: 0.004349
Validation Loss: 0.00457290
Epoch [153/200], Train Loss: 0.004381
Validation Loss: 0.00456439
Epoch [154/200], Train Loss: 0.004376
Validation Loss: 0.00456054
Epoch [155/200], Train Loss: 0.004438
Validation Loss: 0.00454444
Epoch [156/200], Train Loss: 0.004327
Validation Loss: 0.00456733
Epoch [157/200], Train Loss: 0.004350
Validation Loss: 0.00452761
Epoch [158/200], Train Loss: 0.004366
Validation Loss: 0.00451622
Epoch [159/200], Train Loss: 0.004286
Validation Loss: 0.00452886
Epoch [160/200], Train Loss: 0.004283
Validation Loss: 0.00451191
Epoch [161/200], Train Loss: 0.004304
Validation Loss: 0.00449754
Epoch [162/200], Train Loss: 0.004309
Validation Loss: 0.00448315
Epoch [163/200], Train Loss: 0.004264
Validation Loss: 0.00452101
Epoch [164/200], Train Loss: 0.004248
Validation Loss: 0.00447748
Epoch [165/200], Train Loss: 0.004285
Validation Loss: 0.00446151
Epoch [166/200], Train Loss: 0.004232
Validation Loss: 0.00445623
Epoch [167/200], Train Loss: 0.004267
Validation Loss: 0.00444804
Epoch [168/200], Train Loss: 0.004239
Validation Loss: 0.00446011
Epoch [169/200], Train Loss: 0.004297
Validation Loss: 0.00443967
Epoch [170/200], Train Loss: 0.004218
Validation Loss: 0.00444538
Epoch [171/200], Train Loss: 0.004137
Validation Loss: 0.00443292
Epoch [172/200], Train Loss: 0.004167
Validation Loss: 0.00441250
Epoch [173/200], Train Loss: 0.004193
Validation Loss: 0.00440748
Epoch [174/200], Train Loss: 0.004201
Validation Loss: 0.00441278
Epoch [175/200], Train Loss: 0.004172
Validation Loss: 0.00439247
Epoch [176/200], Train Loss: 0.004220
Validation Loss: 0.00438136
Epoch [177/200], Train Loss: 0.004199
Validation Loss: 0.00437504
Epoch [178/200], Train Loss: 0.004147
Validation Loss: 0.00438382
Epoch [179/200], Train Loss: 0.004133
Validation Loss: 0.00436063
Epoch [180/200], Train Loss: 0.004097
Validation Loss: 0.00435634
Epoch [181/200], Train Loss: 0.004115
Validation Loss: 0.00435392
Epoch [182/200], Train Loss: 0.004215
Validation Loss: 0.00433544
Epoch [183/200], Train Loss: 0.004073
Validation Loss: 0.00434223
Epoch [184/200], Train Loss: 0.004099
Validation Loss: 0.00432296
Epoch [185/200], Train Loss: 0.004102
Validation Loss: 0.00431728
Epoch [186/200], Train Loss: 0.004077
Validation Loss: 0.00431226
Epoch [187/200], Train Loss: 0.004099
Validation Loss: 0.00429662
Epoch [188/200], Train Loss: 0.004035
Validation Loss: 0.00433949
Epoch [189/200], Train Loss: 0.004099
Validation Loss: 0.00431824
Epoch [190/200], Train Loss: 0.004127
Validation Loss: 0.00432334
Epoch [191/200], Train Loss: 0.004024
Validation Loss: 0.00427168
Epoch [192/200], Train Loss: 0.004093
Validation Loss: 0.00427345
Epoch [193/200], Train Loss: 0.004010
Validation Loss: 0.00425820
Epoch [194/200], Train Loss: 0.004055
Validation Loss: 0.00424959
Epoch [195/200], Train Loss: 0.004005
Validation Loss: 0.00424111
Epoch [196/200], Train Loss: 0.004089
Validation Loss: 0.00425217
Epoch [197/200], Train Loss: 0.003985
Validation Loss: 0.00422148
Epoch [198/200], Train Loss: 0.004052
Validation Loss: 0.00422660
Epoch [199/200], Train Loss: 0.004058
Validation Loss: 0.00421084
Epoch [200/200], Train Loss: 0.003977
Validation Loss: 0.00422428

Evaluating model for: Lamp
Run 77/144 completed in 385.54 seconds with: {'MAE': np.float32(2.1939726), 'MSE': np.float32(120.87314), 'RMSE': np.float32(10.994232), 'SAE': np.float32(0.3426079), 'NDE': np.float32(0.87843)}

Run 78/144: hidden=128, seq_len=720, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.006122
Validation Loss: 0.00559497
Epoch [2/200], Train Loss: 0.005067
Validation Loss: 0.00513561
Epoch [3/200], Train Loss: 0.004925
Validation Loss: 0.00516309
Epoch [4/200], Train Loss: 0.004925
Validation Loss: 0.00512747
Epoch [5/200], Train Loss: 0.004857
Validation Loss: 0.00512722
Epoch [6/200], Train Loss: 0.004853
Validation Loss: 0.00512176
Epoch [7/200], Train Loss: 0.004922
Validation Loss: 0.00511878
Epoch [8/200], Train Loss: 0.004902
Validation Loss: 0.00511523
Epoch [9/200], Train Loss: 0.004909
Validation Loss: 0.00511105
Epoch [10/200], Train Loss: 0.004877
Validation Loss: 0.00510610
Epoch [11/200], Train Loss: 0.004981
Validation Loss: 0.00509960
Epoch [12/200], Train Loss: 0.004843
Validation Loss: 0.00509105
Epoch [13/200], Train Loss: 0.004819
Validation Loss: 0.00507988
Epoch [14/200], Train Loss: 0.004892
Validation Loss: 0.00506568
Epoch [15/200], Train Loss: 0.004826
Validation Loss: 0.00504847
Epoch [16/200], Train Loss: 0.004805
Validation Loss: 0.00503481
Epoch [17/200], Train Loss: 0.004811
Validation Loss: 0.00503062
Epoch [18/200], Train Loss: 0.004815
Validation Loss: 0.00503093
Epoch [19/200], Train Loss: 0.004854
Validation Loss: 0.00502832
Epoch [20/200], Train Loss: 0.004833
Validation Loss: 0.00502889
Epoch [21/200], Train Loss: 0.004790
Validation Loss: 0.00502685
Epoch [22/200], Train Loss: 0.004815
Validation Loss: 0.00502650
Epoch [23/200], Train Loss: 0.004784
Validation Loss: 0.00502484
Epoch [24/200], Train Loss: 0.004863
Validation Loss: 0.00502948
Epoch [25/200], Train Loss: 0.004766
Validation Loss: 0.00502252
Epoch [26/200], Train Loss: 0.004844
Validation Loss: 0.00502302
Epoch [27/200], Train Loss: 0.004853
Validation Loss: 0.00502209
Epoch [28/200], Train Loss: 0.004893
Validation Loss: 0.00502100
Epoch [29/200], Train Loss: 0.004817
Validation Loss: 0.00502276
Epoch [30/200], Train Loss: 0.004853
Validation Loss: 0.00501890
Epoch [31/200], Train Loss: 0.004882
Validation Loss: 0.00502276
Epoch [32/200], Train Loss: 0.004836
Validation Loss: 0.00501704
Epoch [33/200], Train Loss: 0.004802
Validation Loss: 0.00502911
Epoch [34/200], Train Loss: 0.004816
Validation Loss: 0.00501777
Epoch [35/200], Train Loss: 0.004775
Validation Loss: 0.00502314
Epoch [36/200], Train Loss: 0.004833
Validation Loss: 0.00501513
Epoch [37/200], Train Loss: 0.004829
Validation Loss: 0.00501980
Epoch [38/200], Train Loss: 0.004833
Validation Loss: 0.00501355
Epoch [39/200], Train Loss: 0.004823
Validation Loss: 0.00501483
Epoch [40/200], Train Loss: 0.004781
Validation Loss: 0.00501488
Epoch [41/200], Train Loss: 0.004776
Validation Loss: 0.00501178
Epoch [42/200], Train Loss: 0.004759
Validation Loss: 0.00501311
Epoch [43/200], Train Loss: 0.004864
Validation Loss: 0.00501033
Epoch [44/200], Train Loss: 0.004808
Validation Loss: 0.00500956
Epoch [45/200], Train Loss: 0.004802
Validation Loss: 0.00501388
Epoch [46/200], Train Loss: 0.004811
Validation Loss: 0.00500840
Epoch [47/200], Train Loss: 0.004793
Validation Loss: 0.00500945
Epoch [48/200], Train Loss: 0.004794
Validation Loss: 0.00501067
Epoch [49/200], Train Loss: 0.004784
Validation Loss: 0.00500824
Epoch [50/200], Train Loss: 0.004851
Validation Loss: 0.00500938
Epoch [51/200], Train Loss: 0.004810
Validation Loss: 0.00501170
Epoch [52/200], Train Loss: 0.004882
Validation Loss: 0.00500806
Epoch [53/200], Train Loss: 0.004782
Validation Loss: 0.00500800
Epoch [54/200], Train Loss: 0.004774
Validation Loss: 0.00500452
Epoch [55/200], Train Loss: 0.004813
Validation Loss: 0.00500785
Epoch [56/200], Train Loss: 0.004748
Validation Loss: 0.00500445
Epoch [57/200], Train Loss: 0.004763
Validation Loss: 0.00500909
Epoch [58/200], Train Loss: 0.004812
Validation Loss: 0.00500267
Epoch [59/200], Train Loss: 0.004714
Validation Loss: 0.00500406
Epoch [60/200], Train Loss: 0.004733
Validation Loss: 0.00500408
Epoch [61/200], Train Loss: 0.004788
Validation Loss: 0.00500290
Epoch [62/200], Train Loss: 0.004800
Validation Loss: 0.00500185
Epoch [63/200], Train Loss: 0.004796
Validation Loss: 0.00500238
Epoch [64/200], Train Loss: 0.004802
Validation Loss: 0.00499988
Epoch [65/200], Train Loss: 0.004803
Validation Loss: 0.00500758
Epoch [66/200], Train Loss: 0.004801
Validation Loss: 0.00499987
Epoch [67/200], Train Loss: 0.004813
Validation Loss: 0.00500206
Epoch [68/200], Train Loss: 0.004839
Validation Loss: 0.00499814
Epoch [69/200], Train Loss: 0.004812
Validation Loss: 0.00501343
Epoch [70/200], Train Loss: 0.004812
Validation Loss: 0.00499842
Epoch [71/200], Train Loss: 0.004841
Validation Loss: 0.00499858
Epoch [72/200], Train Loss: 0.004853
Validation Loss: 0.00499645
Epoch [73/200], Train Loss: 0.004754
Validation Loss: 0.00500736
Epoch [74/200], Train Loss: 0.004812
Validation Loss: 0.00499770
Epoch [75/200], Train Loss: 0.004789
Validation Loss: 0.00499841
Epoch [76/200], Train Loss: 0.004794
Validation Loss: 0.00499587
Epoch [77/200], Train Loss: 0.004741
Validation Loss: 0.00499602
Epoch [78/200], Train Loss: 0.004816
Validation Loss: 0.00499607
Epoch [79/200], Train Loss: 0.004799
Validation Loss: 0.00499443
Epoch [80/200], Train Loss: 0.004815
Validation Loss: 0.00499570
Epoch [81/200], Train Loss: 0.004788
Validation Loss: 0.00499442
Epoch [82/200], Train Loss: 0.004808
Validation Loss: 0.00499516
Epoch [83/200], Train Loss: 0.004765
Validation Loss: 0.00499349
Epoch [84/200], Train Loss: 0.004772
Validation Loss: 0.00499490
Epoch [85/200], Train Loss: 0.004792
Validation Loss: 0.00499439
Epoch [86/200], Train Loss: 0.004810
Validation Loss: 0.00499464
Epoch [87/200], Train Loss: 0.004776
Validation Loss: 0.00499526
Epoch [88/200], Train Loss: 0.004802
Validation Loss: 0.00499386
Epoch [89/200], Train Loss: 0.004761
Validation Loss: 0.00499263
Epoch [90/200], Train Loss: 0.004767
Validation Loss: 0.00499687
Epoch [91/200], Train Loss: 0.004750
Validation Loss: 0.00499239
Epoch [92/200], Train Loss: 0.004872
Validation Loss: 0.00499839
Epoch [93/200], Train Loss: 0.004763
Validation Loss: 0.00499258
Epoch [94/200], Train Loss: 0.004816
Validation Loss: 0.00499527
Epoch [95/200], Train Loss: 0.004735
Validation Loss: 0.00499261
Epoch [96/200], Train Loss: 0.004772
Validation Loss: 0.00499217
Epoch [97/200], Train Loss: 0.004755
Validation Loss: 0.00499264
Epoch [98/200], Train Loss: 0.004779
Validation Loss: 0.00499201
Epoch [99/200], Train Loss: 0.004775
Validation Loss: 0.00499358
Epoch [100/200], Train Loss: 0.004837
Validation Loss: 0.00499162
Epoch [101/200], Train Loss: 0.004740
Validation Loss: 0.00499555
Epoch [102/200], Train Loss: 0.004756
Validation Loss: 0.00499186
Epoch [103/200], Train Loss: 0.004742
Validation Loss: 0.00499365
Epoch [104/200], Train Loss: 0.004820
Validation Loss: 0.00499160
Epoch [105/200], Train Loss: 0.004810
Validation Loss: 0.00499369
Epoch [106/200], Train Loss: 0.004790
Validation Loss: 0.00499051
Epoch [107/200], Train Loss: 0.004798
Validation Loss: 0.00499222
Epoch [108/200], Train Loss: 0.004813
Validation Loss: 0.00499092
Epoch [109/200], Train Loss: 0.004803
Validation Loss: 0.00499168
Epoch [110/200], Train Loss: 0.004805
Validation Loss: 0.00499102
Epoch [111/200], Train Loss: 0.004785
Validation Loss: 0.00499180
Epoch [112/200], Train Loss: 0.004793
Validation Loss: 0.00499080
Epoch [113/200], Train Loss: 0.004788
Validation Loss: 0.00499133
Epoch [114/200], Train Loss: 0.004778
Validation Loss: 0.00499226
Epoch [115/200], Train Loss: 0.004870
Validation Loss: 0.00499055
Epoch [116/200], Train Loss: 0.004751
Validation Loss: 0.00499258
Early stopping triggered

Evaluating model for: Lamp
Run 78/144 completed in 243.97 seconds with: {'MAE': np.float32(2.4480565), 'MSE': np.float32(150.89096), 'RMSE': np.float32(12.283769), 'SAE': np.float32(0.16815765), 'NDE': np.float32(0.9814629)}

Run 79/144: hidden=128, seq_len=720, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.005955
Validation Loss: 0.00535530
Epoch [2/200], Train Loss: 0.004899
Validation Loss: 0.00514970
Epoch [3/200], Train Loss: 0.004949
Validation Loss: 0.00513456
Epoch [4/200], Train Loss: 0.004986
Validation Loss: 0.00512212
Epoch [5/200], Train Loss: 0.004891
Validation Loss: 0.00512053
Epoch [6/200], Train Loss: 0.004946
Validation Loss: 0.00511902
Epoch [7/200], Train Loss: 0.004928
Validation Loss: 0.00511738
Epoch [8/200], Train Loss: 0.004905
Validation Loss: 0.00511576
Epoch [9/200], Train Loss: 0.004917
Validation Loss: 0.00511356
Epoch [10/200], Train Loss: 0.004950
Validation Loss: 0.00511074
Epoch [11/200], Train Loss: 0.004834
Validation Loss: 0.00510657
Epoch [12/200], Train Loss: 0.004968
Validation Loss: 0.00510137
Epoch [13/200], Train Loss: 0.004906
Validation Loss: 0.00509097
Epoch [14/200], Train Loss: 0.004934
Validation Loss: 0.00507436
Epoch [15/200], Train Loss: 0.004904
Validation Loss: 0.00504887
Epoch [16/200], Train Loss: 0.004887
Validation Loss: 0.00502947
Epoch [17/200], Train Loss: 0.004785
Validation Loss: 0.00503623
Epoch [18/200], Train Loss: 0.004835
Validation Loss: 0.00502479
Epoch [19/200], Train Loss: 0.004803
Validation Loss: 0.00502476
Epoch [20/200], Train Loss: 0.004803
Validation Loss: 0.00502359
Epoch [21/200], Train Loss: 0.004756
Validation Loss: 0.00502280
Epoch [22/200], Train Loss: 0.004856
Validation Loss: 0.00502243
Epoch [23/200], Train Loss: 0.004721
Validation Loss: 0.00502604
Epoch [24/200], Train Loss: 0.004844
Validation Loss: 0.00502001
Epoch [25/200], Train Loss: 0.004816
Validation Loss: 0.00502032
Epoch [26/200], Train Loss: 0.004806
Validation Loss: 0.00501944
Epoch [27/200], Train Loss: 0.004840
Validation Loss: 0.00501864
Epoch [28/200], Train Loss: 0.004929
Validation Loss: 0.00502339
Epoch [29/200], Train Loss: 0.004879
Validation Loss: 0.00501698
Epoch [30/200], Train Loss: 0.004830
Validation Loss: 0.00501761
Epoch [31/200], Train Loss: 0.004822
Validation Loss: 0.00501574
Epoch [32/200], Train Loss: 0.004850
Validation Loss: 0.00501841
Epoch [33/200], Train Loss: 0.004778
Validation Loss: 0.00501577
Epoch [34/200], Train Loss: 0.004879
Validation Loss: 0.00501505
Epoch [35/200], Train Loss: 0.004786
Validation Loss: 0.00501399
Epoch [36/200], Train Loss: 0.004867
Validation Loss: 0.00501991
Epoch [37/200], Train Loss: 0.004780
Validation Loss: 0.00501238
Epoch [38/200], Train Loss: 0.004790
Validation Loss: 0.00501582
Epoch [39/200], Train Loss: 0.004791
Validation Loss: 0.00501098
Epoch [40/200], Train Loss: 0.004827
Validation Loss: 0.00501064
Epoch [41/200], Train Loss: 0.004894
Validation Loss: 0.00500950
Epoch [42/200], Train Loss: 0.004806
Validation Loss: 0.00501057
Epoch [43/200], Train Loss: 0.004790
Validation Loss: 0.00501026
Epoch [44/200], Train Loss: 0.004812
Validation Loss: 0.00500775
Epoch [45/200], Train Loss: 0.004805
Validation Loss: 0.00501183
Epoch [46/200], Train Loss: 0.004755
Validation Loss: 0.00500726
Epoch [47/200], Train Loss: 0.004875
Validation Loss: 0.00500884
Epoch [48/200], Train Loss: 0.004804
Validation Loss: 0.00500643
Epoch [49/200], Train Loss: 0.004779
Validation Loss: 0.00500660
Epoch [50/200], Train Loss: 0.004854
Validation Loss: 0.00500543
Epoch [51/200], Train Loss: 0.004747
Validation Loss: 0.00501012
Epoch [52/200], Train Loss: 0.004751
Validation Loss: 0.00500516
Epoch [53/200], Train Loss: 0.004883
Validation Loss: 0.00500912
Epoch [54/200], Train Loss: 0.004795
Validation Loss: 0.00500289
Epoch [55/200], Train Loss: 0.004840
Validation Loss: 0.00500789
Epoch [56/200], Train Loss: 0.004859
Validation Loss: 0.00500137
Epoch [57/200], Train Loss: 0.004789
Validation Loss: 0.00500436
Epoch [58/200], Train Loss: 0.004817
Validation Loss: 0.00500142
Epoch [59/200], Train Loss: 0.004751
Validation Loss: 0.00500647
Epoch [60/200], Train Loss: 0.004795
Validation Loss: 0.00500075
Epoch [61/200], Train Loss: 0.004764
Validation Loss: 0.00500622
Epoch [62/200], Train Loss: 0.004763
Validation Loss: 0.00499909
Epoch [63/200], Train Loss: 0.004824
Validation Loss: 0.00500315
Epoch [64/200], Train Loss: 0.004860
Validation Loss: 0.00499854
Epoch [65/200], Train Loss: 0.004769
Validation Loss: 0.00500158
Epoch [66/200], Train Loss: 0.004815
Validation Loss: 0.00499771
Epoch [67/200], Train Loss: 0.004744
Validation Loss: 0.00500345
Epoch [68/200], Train Loss: 0.004769
Validation Loss: 0.00499710
Epoch [69/200], Train Loss: 0.004789
Validation Loss: 0.00500071
Epoch [70/200], Train Loss: 0.004801
Validation Loss: 0.00499776
Epoch [71/200], Train Loss: 0.004786
Validation Loss: 0.00499727
Epoch [72/200], Train Loss: 0.004776
Validation Loss: 0.00499982
Epoch [73/200], Train Loss: 0.004766
Validation Loss: 0.00499658
Epoch [74/200], Train Loss: 0.004799
Validation Loss: 0.00499733
Epoch [75/200], Train Loss: 0.004840
Validation Loss: 0.00499648
Epoch [76/200], Train Loss: 0.004781
Validation Loss: 0.00499835
Epoch [77/200], Train Loss: 0.004767
Validation Loss: 0.00499641
Epoch [78/200], Train Loss: 0.004846
Validation Loss: 0.00499525
Epoch [79/200], Train Loss: 0.004738
Validation Loss: 0.00500025
Epoch [80/200], Train Loss: 0.004848
Validation Loss: 0.00499519
Epoch [81/200], Train Loss: 0.004825
Validation Loss: 0.00500017
Epoch [82/200], Train Loss: 0.004796
Validation Loss: 0.00499472
Epoch [83/200], Train Loss: 0.004768
Validation Loss: 0.00500400
Epoch [84/200], Train Loss: 0.004791
Validation Loss: 0.00499441
Epoch [85/200], Train Loss: 0.004830
Validation Loss: 0.00499898
Epoch [86/200], Train Loss: 0.004852
Validation Loss: 0.00499354
Epoch [87/200], Train Loss: 0.004743
Validation Loss: 0.00499540
Epoch [88/200], Train Loss: 0.004756
Validation Loss: 0.00499467
Epoch [89/200], Train Loss: 0.004782
Validation Loss: 0.00499563
Epoch [90/200], Train Loss: 0.004821
Validation Loss: 0.00499381
Epoch [91/200], Train Loss: 0.004796
Validation Loss: 0.00499578
Epoch [92/200], Train Loss: 0.004902
Validation Loss: 0.00499301
Epoch [93/200], Train Loss: 0.004732
Validation Loss: 0.00499700
Epoch [94/200], Train Loss: 0.004789
Validation Loss: 0.00499395
Epoch [95/200], Train Loss: 0.004782
Validation Loss: 0.00499298
Epoch [96/200], Train Loss: 0.004776
Validation Loss: 0.00499259
Epoch [97/200], Train Loss: 0.004808
Validation Loss: 0.00499421
Epoch [98/200], Train Loss: 0.004755
Validation Loss: 0.00499231
Epoch [99/200], Train Loss: 0.004852
Validation Loss: 0.00499602
Epoch [100/200], Train Loss: 0.004735
Validation Loss: 0.00499174
Epoch [101/200], Train Loss: 0.004793
Validation Loss: 0.00499543
Epoch [102/200], Train Loss: 0.004841
Validation Loss: 0.00499206
Epoch [103/200], Train Loss: 0.004773
Validation Loss: 0.00499238
Epoch [104/200], Train Loss: 0.004735
Validation Loss: 0.00499283
Epoch [105/200], Train Loss: 0.004808
Validation Loss: 0.00499281
Epoch [106/200], Train Loss: 0.004798
Validation Loss: 0.00499206
Epoch [107/200], Train Loss: 0.004748
Validation Loss: 0.00499305
Epoch [108/200], Train Loss: 0.004871
Validation Loss: 0.00499119
Epoch [109/200], Train Loss: 0.004759
Validation Loss: 0.00499214
Epoch [110/200], Train Loss: 0.004730
Validation Loss: 0.00499259
Epoch [111/200], Train Loss: 0.004751
Validation Loss: 0.00499153
Epoch [112/200], Train Loss: 0.004856
Validation Loss: 0.00499198
Epoch [113/200], Train Loss: 0.004796
Validation Loss: 0.00499251
Epoch [114/200], Train Loss: 0.004787
Validation Loss: 0.00499018
Epoch [115/200], Train Loss: 0.004739
Validation Loss: 0.00499196
Epoch [116/200], Train Loss: 0.004740
Validation Loss: 0.00499089
Epoch [117/200], Train Loss: 0.004804
Validation Loss: 0.00499096
Epoch [118/200], Train Loss: 0.004753
Validation Loss: 0.00499146
Epoch [119/200], Train Loss: 0.004793
Validation Loss: 0.00499018
Epoch [120/200], Train Loss: 0.004716
Validation Loss: 0.00499059
Epoch [121/200], Train Loss: 0.004689
Validation Loss: 0.00498988
Epoch [122/200], Train Loss: 0.004812
Validation Loss: 0.00499138
Epoch [123/200], Train Loss: 0.004735
Validation Loss: 0.00499002
Epoch [124/200], Train Loss: 0.004781
Validation Loss: 0.00499017
Epoch [125/200], Train Loss: 0.004762
Validation Loss: 0.00499073
Epoch [126/200], Train Loss: 0.004824
Validation Loss: 0.00499009
Epoch [127/200], Train Loss: 0.004752
Validation Loss: 0.00498920
Epoch [128/200], Train Loss: 0.004738
Validation Loss: 0.00499083
Epoch [129/200], Train Loss: 0.004829
Validation Loss: 0.00498957
Epoch [130/200], Train Loss: 0.004746
Validation Loss: 0.00498917
Epoch [131/200], Train Loss: 0.004769
Validation Loss: 0.00498929
Epoch [132/200], Train Loss: 0.004832
Validation Loss: 0.00498983
Epoch [133/200], Train Loss: 0.004730
Validation Loss: 0.00498918
Epoch [134/200], Train Loss: 0.004836
Validation Loss: 0.00498974
Epoch [135/200], Train Loss: 0.004802
Validation Loss: 0.00498937
Epoch [136/200], Train Loss: 0.004757
Validation Loss: 0.00498883
Epoch [137/200], Train Loss: 0.004698
Validation Loss: 0.00498925
Epoch [138/200], Train Loss: 0.004698
Validation Loss: 0.00499047
Epoch [139/200], Train Loss: 0.004751
Validation Loss: 0.00498888
Epoch [140/200], Train Loss: 0.004864
Validation Loss: 0.00498963
Epoch [141/200], Train Loss: 0.004785
Validation Loss: 0.00498808
Epoch [142/200], Train Loss: 0.004822
Validation Loss: 0.00498902
Epoch [143/200], Train Loss: 0.004817
Validation Loss: 0.00498851
Epoch [144/200], Train Loss: 0.004803
Validation Loss: 0.00498831
Epoch [145/200], Train Loss: 0.004771
Validation Loss: 0.00498772
Epoch [146/200], Train Loss: 0.004778
Validation Loss: 0.00499008
Epoch [147/200], Train Loss: 0.004737
Validation Loss: 0.00498737
Epoch [148/200], Train Loss: 0.004757
Validation Loss: 0.00498864
Epoch [149/200], Train Loss: 0.004880
Validation Loss: 0.00498830
Epoch [150/200], Train Loss: 0.004839
Validation Loss: 0.00498772
Epoch [151/200], Train Loss: 0.004766
Validation Loss: 0.00498700
Epoch [152/200], Train Loss: 0.004818
Validation Loss: 0.00498822
Epoch [153/200], Train Loss: 0.004825
Validation Loss: 0.00498662
Epoch [154/200], Train Loss: 0.004780
Validation Loss: 0.00498655
Epoch [155/200], Train Loss: 0.004834
Validation Loss: 0.00498669
Epoch [156/200], Train Loss: 0.004792
Validation Loss: 0.00498702
Epoch [157/200], Train Loss: 0.004732
Validation Loss: 0.00498745
Epoch [158/200], Train Loss: 0.004840
Validation Loss: 0.00498690
Epoch [159/200], Train Loss: 0.004789
Validation Loss: 0.00498640
Epoch [160/200], Train Loss: 0.004761
Validation Loss: 0.00498762
Epoch [161/200], Train Loss: 0.004861
Validation Loss: 0.00498676
Epoch [162/200], Train Loss: 0.004782
Validation Loss: 0.00498621
Epoch [163/200], Train Loss: 0.004743
Validation Loss: 0.00498587
Epoch [164/200], Train Loss: 0.004780
Validation Loss: 0.00498596
Epoch [165/200], Train Loss: 0.004731
Validation Loss: 0.00498615
Epoch [166/200], Train Loss: 0.004825
Validation Loss: 0.00498664
Epoch [167/200], Train Loss: 0.004804
Validation Loss: 0.00498529
Epoch [168/200], Train Loss: 0.004742
Validation Loss: 0.00498747
Epoch [169/200], Train Loss: 0.004812
Validation Loss: 0.00498536
Epoch [170/200], Train Loss: 0.004857
Validation Loss: 0.00498628
Epoch [171/200], Train Loss: 0.004841
Validation Loss: 0.00498521
Epoch [172/200], Train Loss: 0.004779
Validation Loss: 0.00498624
Epoch [173/200], Train Loss: 0.004759
Validation Loss: 0.00498655
Epoch [174/200], Train Loss: 0.004746
Validation Loss: 0.00498523
Epoch [175/200], Train Loss: 0.004737
Validation Loss: 0.00498647
Epoch [176/200], Train Loss: 0.004813
Validation Loss: 0.00498613
Epoch [177/200], Train Loss: 0.004759
Validation Loss: 0.00498508
Epoch [178/200], Train Loss: 0.004764
Validation Loss: 0.00498700
Epoch [179/200], Train Loss: 0.004782
Validation Loss: 0.00498492
Epoch [180/200], Train Loss: 0.004784
Validation Loss: 0.00498663
Epoch [181/200], Train Loss: 0.004830
Validation Loss: 0.00498455
Epoch [182/200], Train Loss: 0.004787
Validation Loss: 0.00498541
Epoch [183/200], Train Loss: 0.004751
Validation Loss: 0.00498445
Epoch [184/200], Train Loss: 0.004765
Validation Loss: 0.00498517
Epoch [185/200], Train Loss: 0.004838
Validation Loss: 0.00498498
Epoch [186/200], Train Loss: 0.004861
Validation Loss: 0.00498500
Epoch [187/200], Train Loss: 0.004810
Validation Loss: 0.00498521
Epoch [188/200], Train Loss: 0.004811
Validation Loss: 0.00498477
Epoch [189/200], Train Loss: 0.004833
Validation Loss: 0.00498488
Epoch [190/200], Train Loss: 0.004770
Validation Loss: 0.00498520
Epoch [191/200], Train Loss: 0.004782
Validation Loss: 0.00498678
Epoch [192/200], Train Loss: 0.004777
Validation Loss: 0.00498405
Epoch [193/200], Train Loss: 0.004768
Validation Loss: 0.00498430
Epoch [194/200], Train Loss: 0.004778
Validation Loss: 0.00498506
Epoch [195/200], Train Loss: 0.004825
Validation Loss: 0.00498434
Epoch [196/200], Train Loss: 0.004757
Validation Loss: 0.00498480
Epoch [197/200], Train Loss: 0.004823
Validation Loss: 0.00498442
Epoch [198/200], Train Loss: 0.004788
Validation Loss: 0.00498466
Epoch [199/200], Train Loss: 0.004779
Validation Loss: 0.00498362
Epoch [200/200], Train Loss: 0.004735
Validation Loss: 0.00498405

Evaluating model for: Lamp
Run 79/144 completed in 418.17 seconds with: {'MAE': np.float32(2.4917698), 'MSE': np.float32(150.71762), 'RMSE': np.float32(12.2767105), 'SAE': np.float32(0.0621886), 'NDE': np.float32(0.98089904)}

Run 80/144: hidden=128, seq_len=720, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.006169
Validation Loss: 0.00551426
Epoch [2/200], Train Loss: 0.004978
Validation Loss: 0.00515204
Epoch [3/200], Train Loss: 0.004949
Validation Loss: 0.00513999
Epoch [4/200], Train Loss: 0.004823
Validation Loss: 0.00513435
Epoch [5/200], Train Loss: 0.004929
Validation Loss: 0.00512780
Epoch [6/200], Train Loss: 0.004906
Validation Loss: 0.00512525
Epoch [7/200], Train Loss: 0.004871
Validation Loss: 0.00512471
Epoch [8/200], Train Loss: 0.004924
Validation Loss: 0.00512523
Epoch [9/200], Train Loss: 0.004911
Validation Loss: 0.00512359
Epoch [10/200], Train Loss: 0.004911
Validation Loss: 0.00512269
Epoch [11/200], Train Loss: 0.004938
Validation Loss: 0.00512211
Epoch [12/200], Train Loss: 0.004896
Validation Loss: 0.00511983
Epoch [13/200], Train Loss: 0.004893
Validation Loss: 0.00511774
Epoch [14/200], Train Loss: 0.004963
Validation Loss: 0.00511280
Epoch [15/200], Train Loss: 0.004923
Validation Loss: 0.00510451
Epoch [16/200], Train Loss: 0.004869
Validation Loss: 0.00508940
Epoch [17/200], Train Loss: 0.004838
Validation Loss: 0.00506284
Epoch [18/200], Train Loss: 0.004890
Validation Loss: 0.00503452
Epoch [19/200], Train Loss: 0.004847
Validation Loss: 0.00502922
Epoch [20/200], Train Loss: 0.004867
Validation Loss: 0.00502979
Epoch [21/200], Train Loss: 0.004814
Validation Loss: 0.00504192
Epoch [22/200], Train Loss: 0.004878
Validation Loss: 0.00502801
Epoch [23/200], Train Loss: 0.004815
Validation Loss: 0.00502706
Epoch [24/200], Train Loss: 0.004888
Validation Loss: 0.00502479
Epoch [25/200], Train Loss: 0.004784
Validation Loss: 0.00502410
Epoch [26/200], Train Loss: 0.004886
Validation Loss: 0.00502519
Epoch [27/200], Train Loss: 0.004782
Validation Loss: 0.00502371
Epoch [28/200], Train Loss: 0.004856
Validation Loss: 0.00503340
Epoch [29/200], Train Loss: 0.004862
Validation Loss: 0.00502283
Epoch [30/200], Train Loss: 0.004841
Validation Loss: 0.00501982
Epoch [31/200], Train Loss: 0.004820
Validation Loss: 0.00503228
Epoch [32/200], Train Loss: 0.004790
Validation Loss: 0.00501859
Epoch [33/200], Train Loss: 0.004787
Validation Loss: 0.00501849
Epoch [34/200], Train Loss: 0.004810
Validation Loss: 0.00501809
Epoch [35/200], Train Loss: 0.004822
Validation Loss: 0.00502428
Epoch [36/200], Train Loss: 0.004832
Validation Loss: 0.00501608
Epoch [37/200], Train Loss: 0.004895
Validation Loss: 0.00501549
Epoch [38/200], Train Loss: 0.004820
Validation Loss: 0.00501365
Epoch [39/200], Train Loss: 0.004790
Validation Loss: 0.00501607
Epoch [40/200], Train Loss: 0.004803
Validation Loss: 0.00501551
Epoch [41/200], Train Loss: 0.004792
Validation Loss: 0.00501356
Epoch [42/200], Train Loss: 0.004745
Validation Loss: 0.00501198
Epoch [43/200], Train Loss: 0.004871
Validation Loss: 0.00501198
Epoch [44/200], Train Loss: 0.004789
Validation Loss: 0.00501436
Epoch [45/200], Train Loss: 0.004775
Validation Loss: 0.00501099
Epoch [46/200], Train Loss: 0.004908
Validation Loss: 0.00501035
Epoch [47/200], Train Loss: 0.004847
Validation Loss: 0.00500868
Epoch [48/200], Train Loss: 0.004791
Validation Loss: 0.00500784
Epoch [49/200], Train Loss: 0.004831
Validation Loss: 0.00500578
Epoch [50/200], Train Loss: 0.004810
Validation Loss: 0.00500722
Epoch [51/200], Train Loss: 0.004794
Validation Loss: 0.00501176
Epoch [52/200], Train Loss: 0.004798
Validation Loss: 0.00500440
Epoch [53/200], Train Loss: 0.004803
Validation Loss: 0.00500805
Epoch [54/200], Train Loss: 0.004830
Validation Loss: 0.00500130
Epoch [55/200], Train Loss: 0.004776
Validation Loss: 0.00500108
Epoch [56/200], Train Loss: 0.004876
Validation Loss: 0.00500590
Epoch [57/200], Train Loss: 0.004795
Validation Loss: 0.00500117
Epoch [58/200], Train Loss: 0.004763
Validation Loss: 0.00499938
Epoch [59/200], Train Loss: 0.004852
Validation Loss: 0.00500218
Epoch [60/200], Train Loss: 0.004845
Validation Loss: 0.00500192
Epoch [61/200], Train Loss: 0.004824
Validation Loss: 0.00499886
Epoch [62/200], Train Loss: 0.004780
Validation Loss: 0.00499845
Epoch [63/200], Train Loss: 0.004757
Validation Loss: 0.00499545
Epoch [64/200], Train Loss: 0.004826
Validation Loss: 0.00500035
Epoch [65/200], Train Loss: 0.004781
Validation Loss: 0.00499714
Epoch [66/200], Train Loss: 0.004820
Validation Loss: 0.00499874
Epoch [67/200], Train Loss: 0.004794
Validation Loss: 0.00499940
Epoch [68/200], Train Loss: 0.004772
Validation Loss: 0.00499393
Epoch [69/200], Train Loss: 0.004817
Validation Loss: 0.00499582
Epoch [70/200], Train Loss: 0.004821
Validation Loss: 0.00499402
Epoch [71/200], Train Loss: 0.004860
Validation Loss: 0.00499380
Epoch [72/200], Train Loss: 0.004799
Validation Loss: 0.00499915
Epoch [73/200], Train Loss: 0.004761
Validation Loss: 0.00499270
Epoch [74/200], Train Loss: 0.004757
Validation Loss: 0.00499587
Epoch [75/200], Train Loss: 0.004751
Validation Loss: 0.00499390
Epoch [76/200], Train Loss: 0.004806
Validation Loss: 0.00499347
Epoch [77/200], Train Loss: 0.004828
Validation Loss: 0.00499689
Epoch [78/200], Train Loss: 0.004802
Validation Loss: 0.00499331
Epoch [79/200], Train Loss: 0.004804
Validation Loss: 0.00499381
Epoch [80/200], Train Loss: 0.004851
Validation Loss: 0.00499656
Epoch [81/200], Train Loss: 0.004791
Validation Loss: 0.00499368
Epoch [82/200], Train Loss: 0.004789
Validation Loss: 0.00499681
Epoch [83/200], Train Loss: 0.004806
Validation Loss: 0.00499333
Early stopping triggered

Evaluating model for: Lamp
Run 80/144 completed in 188.72 seconds with: {'MAE': np.float32(2.6949196), 'MSE': np.float32(150.81497), 'RMSE': np.float32(12.280675), 'SAE': np.float32(0.11442152), 'NDE': np.float32(0.9812158)}

Run 81/144: hidden=128, seq_len=720, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.005717
Validation Loss: 0.00389911
Epoch [2/200], Train Loss: 0.005199
Validation Loss: 0.00361513
Epoch [3/200], Train Loss: 0.005010
Validation Loss: 0.00355818
Epoch [4/200], Train Loss: 0.004950
Validation Loss: 0.00359589
Epoch [5/200], Train Loss: 0.004919
Validation Loss: 0.00360988
Epoch [6/200], Train Loss: 0.004925
Validation Loss: 0.00358704
Epoch [7/200], Train Loss: 0.004951
Validation Loss: 0.00356251
Epoch [8/200], Train Loss: 0.004953
Validation Loss: 0.00355163
Epoch [9/200], Train Loss: 0.004906
Validation Loss: 0.00354824
Epoch [10/200], Train Loss: 0.004898
Validation Loss: 0.00354643
Epoch [11/200], Train Loss: 0.004915
Validation Loss: 0.00354573
Epoch [12/200], Train Loss: 0.004911
Validation Loss: 0.00354547
Epoch [13/200], Train Loss: 0.004870
Validation Loss: 0.00354363
Epoch [14/200], Train Loss: 0.004871
Validation Loss: 0.00353876
Epoch [15/200], Train Loss: 0.004888
Validation Loss: 0.00353499
Epoch [16/200], Train Loss: 0.004973
Validation Loss: 0.00353155
Epoch [17/200], Train Loss: 0.004892
Validation Loss: 0.00353030
Epoch [18/200], Train Loss: 0.004883
Validation Loss: 0.00352888
Epoch [19/200], Train Loss: 0.004952
Validation Loss: 0.00352569
Epoch [20/200], Train Loss: 0.004893
Validation Loss: 0.00352402
Epoch [21/200], Train Loss: 0.004896
Validation Loss: 0.00352092
Epoch [22/200], Train Loss: 0.004886
Validation Loss: 0.00351882
Epoch [23/200], Train Loss: 0.004873
Validation Loss: 0.00351559
Epoch [24/200], Train Loss: 0.004877
Validation Loss: 0.00351349
Epoch [25/200], Train Loss: 0.004864
Validation Loss: 0.00351290
Epoch [26/200], Train Loss: 0.004896
Validation Loss: 0.00351320
Epoch [27/200], Train Loss: 0.004878
Validation Loss: 0.00351035
Epoch [28/200], Train Loss: 0.004893
Validation Loss: 0.00350764
Epoch [29/200], Train Loss: 0.004900
Validation Loss: 0.00350881
Epoch [30/200], Train Loss: 0.004859
Validation Loss: 0.00350738
Epoch [31/200], Train Loss: 0.004830
Validation Loss: 0.00350842
Epoch [32/200], Train Loss: 0.004891
Validation Loss: 0.00350545
Epoch [33/200], Train Loss: 0.004849
Validation Loss: 0.00350765
Epoch [34/200], Train Loss: 0.004857
Validation Loss: 0.00350582
Epoch [35/200], Train Loss: 0.004846
Validation Loss: 0.00350554
Epoch [36/200], Train Loss: 0.004869
Validation Loss: 0.00350465
Epoch [37/200], Train Loss: 0.004873
Validation Loss: 0.00350524
Epoch [38/200], Train Loss: 0.004863
Validation Loss: 0.00350654
Epoch [39/200], Train Loss: 0.004844
Validation Loss: 0.00350579
Epoch [40/200], Train Loss: 0.004898
Validation Loss: 0.00350369
Epoch [41/200], Train Loss: 0.004853
Validation Loss: 0.00350612
Epoch [42/200], Train Loss: 0.004883
Validation Loss: 0.00350532
Epoch [43/200], Train Loss: 0.004820
Validation Loss: 0.00350433
Epoch [44/200], Train Loss: 0.004878
Validation Loss: 0.00350333
Epoch [45/200], Train Loss: 0.004855
Validation Loss: 0.00350583
Epoch [46/200], Train Loss: 0.004842
Validation Loss: 0.00350357
Epoch [47/200], Train Loss: 0.004838
Validation Loss: 0.00350336
Epoch [48/200], Train Loss: 0.004829
Validation Loss: 0.00350655
Epoch [49/200], Train Loss: 0.004876
Validation Loss: 0.00350401
Epoch [50/200], Train Loss: 0.004886
Validation Loss: 0.00350207
Epoch [51/200], Train Loss: 0.004837
Validation Loss: 0.00350442
Epoch [52/200], Train Loss: 0.004842
Validation Loss: 0.00350432
Epoch [53/200], Train Loss: 0.004844
Validation Loss: 0.00350335
Epoch [54/200], Train Loss: 0.004866
Validation Loss: 0.00350118
Epoch [55/200], Train Loss: 0.004851
Validation Loss: 0.00350392
Epoch [56/200], Train Loss: 0.004844
Validation Loss: 0.00350567
Epoch [57/200], Train Loss: 0.004836
Validation Loss: 0.00350369
Epoch [58/200], Train Loss: 0.004824
Validation Loss: 0.00350101
Epoch [59/200], Train Loss: 0.004879
Validation Loss: 0.00350176
Epoch [60/200], Train Loss: 0.004849
Validation Loss: 0.00350447
Epoch [61/200], Train Loss: 0.004847
Validation Loss: 0.00350194
Epoch [62/200], Train Loss: 0.004854
Validation Loss: 0.00350263
Epoch [63/200], Train Loss: 0.004841
Validation Loss: 0.00350168
Epoch [64/200], Train Loss: 0.004879
Validation Loss: 0.00350115
Epoch [65/200], Train Loss: 0.004864
Validation Loss: 0.00350314
Epoch [66/200], Train Loss: 0.004828
Validation Loss: 0.00350511
Epoch [67/200], Train Loss: 0.004858
Validation Loss: 0.00350004
Epoch [68/200], Train Loss: 0.004885
Validation Loss: 0.00349972
Epoch [69/200], Train Loss: 0.004860
Validation Loss: 0.00350328
Epoch [70/200], Train Loss: 0.004866
Validation Loss: 0.00350323
Epoch [71/200], Train Loss: 0.004882
Validation Loss: 0.00349978
Epoch [72/200], Train Loss: 0.004861
Validation Loss: 0.00350139
Epoch [73/200], Train Loss: 0.004835
Validation Loss: 0.00350365
Epoch [74/200], Train Loss: 0.004893
Validation Loss: 0.00349926
Epoch [75/200], Train Loss: 0.004858
Validation Loss: 0.00349893
Epoch [76/200], Train Loss: 0.004812
Validation Loss: 0.00350256
Epoch [77/200], Train Loss: 0.004846
Validation Loss: 0.00350138
Epoch [78/200], Train Loss: 0.004883
Validation Loss: 0.00349800
Epoch [79/200], Train Loss: 0.004833
Validation Loss: 0.00349973
Epoch [80/200], Train Loss: 0.004888
Validation Loss: 0.00350144
Epoch [81/200], Train Loss: 0.004869
Validation Loss: 0.00350034
Epoch [82/200], Train Loss: 0.004845
Validation Loss: 0.00349952
Epoch [83/200], Train Loss: 0.004848
Validation Loss: 0.00349806
Epoch [84/200], Train Loss: 0.004880
Validation Loss: 0.00349847
Epoch [85/200], Train Loss: 0.004849
Validation Loss: 0.00349953
Epoch [86/200], Train Loss: 0.004838
Validation Loss: 0.00350062
Epoch [87/200], Train Loss: 0.004866
Validation Loss: 0.00349808
Epoch [88/200], Train Loss: 0.004840
Validation Loss: 0.00349829
Early stopping triggered

Evaluating model for: Lamp
Run 81/144 completed in 83.33 seconds with: {'MAE': np.float32(3.1832693), 'MSE': np.float32(179.40866), 'RMSE': np.float32(13.394352), 'SAE': np.float32(0.05171616), 'NDE': np.float32(0.9837727)}

Run 82/144: hidden=128, seq_len=720, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.006697
Validation Loss: 0.00493049
Epoch [2/200], Train Loss: 0.005814
Validation Loss: 0.00416263
Epoch [3/200], Train Loss: 0.005257
Validation Loss: 0.00372767
Epoch [4/200], Train Loss: 0.004971
Validation Loss: 0.00359419
Epoch [5/200], Train Loss: 0.005019
Validation Loss: 0.00360787
Epoch [6/200], Train Loss: 0.005014
Validation Loss: 0.00359990
Epoch [7/200], Train Loss: 0.004968
Validation Loss: 0.00358706
Epoch [8/200], Train Loss: 0.004997
Validation Loss: 0.00359518
Epoch [9/200], Train Loss: 0.004967
Validation Loss: 0.00360363
Epoch [10/200], Train Loss: 0.005036
Validation Loss: 0.00359851
Epoch [11/200], Train Loss: 0.004949
Validation Loss: 0.00359096
Epoch [12/200], Train Loss: 0.004967
Validation Loss: 0.00358494
Epoch [13/200], Train Loss: 0.004972
Validation Loss: 0.00358203
Epoch [14/200], Train Loss: 0.004976
Validation Loss: 0.00358082
Epoch [15/200], Train Loss: 0.004977
Validation Loss: 0.00358067
Epoch [16/200], Train Loss: 0.004985
Validation Loss: 0.00358013
Epoch [17/200], Train Loss: 0.004937
Validation Loss: 0.00357964
Epoch [18/200], Train Loss: 0.004968
Validation Loss: 0.00357705
Epoch [19/200], Train Loss: 0.004987
Validation Loss: 0.00357368
Epoch [20/200], Train Loss: 0.004911
Validation Loss: 0.00357230
Epoch [21/200], Train Loss: 0.004952
Validation Loss: 0.00356953
Epoch [22/200], Train Loss: 0.004945
Validation Loss: 0.00356671
Epoch [23/200], Train Loss: 0.004939
Validation Loss: 0.00356369
Epoch [24/200], Train Loss: 0.004922
Validation Loss: 0.00355960
Epoch [25/200], Train Loss: 0.004952
Validation Loss: 0.00355541
Epoch [26/200], Train Loss: 0.004885
Validation Loss: 0.00355238
Epoch [27/200], Train Loss: 0.004949
Validation Loss: 0.00354514
Epoch [28/200], Train Loss: 0.004951
Validation Loss: 0.00353948
Epoch [29/200], Train Loss: 0.004887
Validation Loss: 0.00353585
Epoch [30/200], Train Loss: 0.004907
Validation Loss: 0.00352925
Epoch [31/200], Train Loss: 0.004946
Validation Loss: 0.00351984
Epoch [32/200], Train Loss: 0.004855
Validation Loss: 0.00351927
Epoch [33/200], Train Loss: 0.004888
Validation Loss: 0.00351085
Epoch [34/200], Train Loss: 0.004902
Validation Loss: 0.00351013
Epoch [35/200], Train Loss: 0.004868
Validation Loss: 0.00351350
Epoch [36/200], Train Loss: 0.004908
Validation Loss: 0.00350999
Epoch [37/200], Train Loss: 0.004872
Validation Loss: 0.00350924
Epoch [38/200], Train Loss: 0.004865
Validation Loss: 0.00351168
Epoch [39/200], Train Loss: 0.004874
Validation Loss: 0.00351061
Epoch [40/200], Train Loss: 0.004844
Validation Loss: 0.00350892
Epoch [41/200], Train Loss: 0.004850
Validation Loss: 0.00351019
Epoch [42/200], Train Loss: 0.004862
Validation Loss: 0.00350807
Epoch [43/200], Train Loss: 0.004854
Validation Loss: 0.00350835
Epoch [44/200], Train Loss: 0.004892
Validation Loss: 0.00350753
Epoch [45/200], Train Loss: 0.004886
Validation Loss: 0.00350734
Epoch [46/200], Train Loss: 0.004868
Validation Loss: 0.00350811
Epoch [47/200], Train Loss: 0.004871
Validation Loss: 0.00350756
Epoch [48/200], Train Loss: 0.004873
Validation Loss: 0.00350838
Epoch [49/200], Train Loss: 0.004916
Validation Loss: 0.00350400
Epoch [50/200], Train Loss: 0.004881
Validation Loss: 0.00351054
Epoch [51/200], Train Loss: 0.004880
Validation Loss: 0.00350752
Epoch [52/200], Train Loss: 0.004880
Validation Loss: 0.00350321
Epoch [53/200], Train Loss: 0.004850
Validation Loss: 0.00350771
Epoch [54/200], Train Loss: 0.004882
Validation Loss: 0.00350552
Epoch [55/200], Train Loss: 0.004832
Validation Loss: 0.00350720
Epoch [56/200], Train Loss: 0.004906
Validation Loss: 0.00350274
Epoch [57/200], Train Loss: 0.004832
Validation Loss: 0.00350626
Epoch [58/200], Train Loss: 0.004860
Validation Loss: 0.00350653
Epoch [59/200], Train Loss: 0.004889
Validation Loss: 0.00350247
Epoch [60/200], Train Loss: 0.004854
Validation Loss: 0.00350712
Epoch [61/200], Train Loss: 0.004879
Validation Loss: 0.00350319
Epoch [62/200], Train Loss: 0.004846
Validation Loss: 0.00350251
Epoch [63/200], Train Loss: 0.004833
Validation Loss: 0.00350284
Epoch [64/200], Train Loss: 0.004890
Validation Loss: 0.00350257
Epoch [65/200], Train Loss: 0.004878
Validation Loss: 0.00350503
Epoch [66/200], Train Loss: 0.004880
Validation Loss: 0.00350203
Epoch [67/200], Train Loss: 0.004851
Validation Loss: 0.00350249
Epoch [68/200], Train Loss: 0.004852
Validation Loss: 0.00350615
Epoch [69/200], Train Loss: 0.004848
Validation Loss: 0.00349961
Epoch [70/200], Train Loss: 0.004851
Validation Loss: 0.00350172
Epoch [71/200], Train Loss: 0.004850
Validation Loss: 0.00350487
Epoch [72/200], Train Loss: 0.004816
Validation Loss: 0.00350264
Epoch [73/200], Train Loss: 0.004831
Validation Loss: 0.00349965
Epoch [74/200], Train Loss: 0.004860
Validation Loss: 0.00350195
Epoch [75/200], Train Loss: 0.004880
Validation Loss: 0.00350333
Epoch [76/200], Train Loss: 0.004837
Validation Loss: 0.00350195
Epoch [77/200], Train Loss: 0.004868
Validation Loss: 0.00349861
Epoch [78/200], Train Loss: 0.004841
Validation Loss: 0.00350213
Epoch [79/200], Train Loss: 0.004856
Validation Loss: 0.00350280
Epoch [80/200], Train Loss: 0.004833
Validation Loss: 0.00350007
Epoch [81/200], Train Loss: 0.004876
Validation Loss: 0.00349846
Epoch [82/200], Train Loss: 0.004823
Validation Loss: 0.00350346
Epoch [83/200], Train Loss: 0.004868
Validation Loss: 0.00349817
Epoch [84/200], Train Loss: 0.004815
Validation Loss: 0.00349890
Epoch [85/200], Train Loss: 0.004867
Validation Loss: 0.00350028
Epoch [86/200], Train Loss: 0.004857
Validation Loss: 0.00349979
Epoch [87/200], Train Loss: 0.004855
Validation Loss: 0.00349893
Epoch [88/200], Train Loss: 0.004844
Validation Loss: 0.00349993
Epoch [89/200], Train Loss: 0.004837
Validation Loss: 0.00349964
Epoch [90/200], Train Loss: 0.004829
Validation Loss: 0.00349860
Epoch [91/200], Train Loss: 0.004835
Validation Loss: 0.00349762
Epoch [92/200], Train Loss: 0.004828
Validation Loss: 0.00349875
Epoch [93/200], Train Loss: 0.004898
Validation Loss: 0.00349693
Epoch [94/200], Train Loss: 0.004883
Validation Loss: 0.00349786
Epoch [95/200], Train Loss: 0.004843
Validation Loss: 0.00350108
Epoch [96/200], Train Loss: 0.004879
Validation Loss: 0.00349734
Epoch [97/200], Train Loss: 0.004894
Validation Loss: 0.00349579
Epoch [98/200], Train Loss: 0.004844
Validation Loss: 0.00350136
Epoch [99/200], Train Loss: 0.004845
Validation Loss: 0.00349725
Epoch [100/200], Train Loss: 0.004833
Validation Loss: 0.00349713
Epoch [101/200], Train Loss: 0.004881
Validation Loss: 0.00349490
Epoch [102/200], Train Loss: 0.004889
Validation Loss: 0.00349662
Epoch [103/200], Train Loss: 0.004835
Validation Loss: 0.00350080
Epoch [104/200], Train Loss: 0.004840
Validation Loss: 0.00349682
Epoch [105/200], Train Loss: 0.004835
Validation Loss: 0.00349591
Epoch [106/200], Train Loss: 0.004855
Validation Loss: 0.00349759
Epoch [107/200], Train Loss: 0.004822
Validation Loss: 0.00349650
Epoch [108/200], Train Loss: 0.004877
Validation Loss: 0.00349524
Epoch [109/200], Train Loss: 0.004857
Validation Loss: 0.00349624
Epoch [110/200], Train Loss: 0.004864
Validation Loss: 0.00349785
Epoch [111/200], Train Loss: 0.004867
Validation Loss: 0.00349577
Early stopping triggered

Evaluating model for: Lamp
Run 82/144 completed in 113.27 seconds with: {'MAE': np.float32(3.1473665), 'MSE': np.float32(179.54582), 'RMSE': np.float32(13.399471), 'SAE': np.float32(0.0653373), 'NDE': np.float32(0.9841486)}

Run 83/144: hidden=128, seq_len=720, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.006353
Validation Loss: 0.00455228
Epoch [2/200], Train Loss: 0.005557
Validation Loss: 0.00389545
Epoch [3/200], Train Loss: 0.005082
Validation Loss: 0.00361275
Epoch [4/200], Train Loss: 0.004964
Validation Loss: 0.00358989
Epoch [5/200], Train Loss: 0.005006
Validation Loss: 0.00359872
Epoch [6/200], Train Loss: 0.004998
Validation Loss: 0.00358175
Epoch [7/200], Train Loss: 0.004962
Validation Loss: 0.00358451
Epoch [8/200], Train Loss: 0.004918
Validation Loss: 0.00359615
Epoch [9/200], Train Loss: 0.004957
Validation Loss: 0.00359562
Epoch [10/200], Train Loss: 0.004981
Validation Loss: 0.00358768
Epoch [11/200], Train Loss: 0.004914
Validation Loss: 0.00358239
Epoch [12/200], Train Loss: 0.004971
Validation Loss: 0.00357996
Epoch [13/200], Train Loss: 0.004977
Validation Loss: 0.00357892
Epoch [14/200], Train Loss: 0.004939
Validation Loss: 0.00358042
Epoch [15/200], Train Loss: 0.004931
Validation Loss: 0.00358265
Epoch [16/200], Train Loss: 0.004936
Validation Loss: 0.00358183
Epoch [17/200], Train Loss: 0.004922
Validation Loss: 0.00357948
Epoch [18/200], Train Loss: 0.004969
Validation Loss: 0.00357665
Epoch [19/200], Train Loss: 0.004960
Validation Loss: 0.00357604
Epoch [20/200], Train Loss: 0.004920
Validation Loss: 0.00357604
Epoch [21/200], Train Loss: 0.004920
Validation Loss: 0.00357645
Epoch [22/200], Train Loss: 0.004926
Validation Loss: 0.00357338
Epoch [23/200], Train Loss: 0.004919
Validation Loss: 0.00357072
Epoch [24/200], Train Loss: 0.004953
Validation Loss: 0.00356737
Epoch [25/200], Train Loss: 0.004962
Validation Loss: 0.00356526
Epoch [26/200], Train Loss: 0.004952
Validation Loss: 0.00356129
Epoch [27/200], Train Loss: 0.004964
Validation Loss: 0.00355677
Epoch [28/200], Train Loss: 0.004920
Validation Loss: 0.00355233
Epoch [29/200], Train Loss: 0.004888
Validation Loss: 0.00354551
Epoch [30/200], Train Loss: 0.004885
Validation Loss: 0.00353347
Epoch [31/200], Train Loss: 0.004868
Validation Loss: 0.00352380
Epoch [32/200], Train Loss: 0.004948
Validation Loss: 0.00351607
Epoch [33/200], Train Loss: 0.004870
Validation Loss: 0.00351404
Epoch [34/200], Train Loss: 0.004865
Validation Loss: 0.00350907
Epoch [35/200], Train Loss: 0.004843
Validation Loss: 0.00350799
Epoch [36/200], Train Loss: 0.004866
Validation Loss: 0.00350657
Epoch [37/200], Train Loss: 0.004857
Validation Loss: 0.00350890
Epoch [38/200], Train Loss: 0.004835
Validation Loss: 0.00350836
Epoch [39/200], Train Loss: 0.004886
Validation Loss: 0.00350312
Epoch [40/200], Train Loss: 0.004860
Validation Loss: 0.00350680
Epoch [41/200], Train Loss: 0.004859
Validation Loss: 0.00350624
Epoch [42/200], Train Loss: 0.004906
Validation Loss: 0.00350262
Epoch [43/200], Train Loss: 0.004854
Validation Loss: 0.00350595
Epoch [44/200], Train Loss: 0.004853
Validation Loss: 0.00350661
Epoch [45/200], Train Loss: 0.004866
Validation Loss: 0.00350391
Epoch [46/200], Train Loss: 0.004844
Validation Loss: 0.00350488
Epoch [47/200], Train Loss: 0.004869
Validation Loss: 0.00350310
Epoch [48/200], Train Loss: 0.004824
Validation Loss: 0.00350627
Epoch [49/200], Train Loss: 0.004846
Validation Loss: 0.00350283
Epoch [50/200], Train Loss: 0.004897
Validation Loss: 0.00350430
Epoch [51/200], Train Loss: 0.004844
Validation Loss: 0.00350540
Epoch [52/200], Train Loss: 0.004836
Validation Loss: 0.00350386
Early stopping triggered

Evaluating model for: Lamp
Run 83/144 completed in 56.45 seconds with: {'MAE': np.float32(3.2695453), 'MSE': np.float32(179.84181), 'RMSE': np.float32(13.410511), 'SAE': np.float32(0.09468534), 'NDE': np.float32(0.98495954)}

Run 84/144: hidden=128, seq_len=720, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.006687
Validation Loss: 0.00457241
Epoch [2/200], Train Loss: 0.005767
Validation Loss: 0.00387988
Epoch [3/200], Train Loss: 0.005173
Validation Loss: 0.00359462
Epoch [4/200], Train Loss: 0.004991
Validation Loss: 0.00363931
Epoch [5/200], Train Loss: 0.005057
Validation Loss: 0.00368141
Epoch [6/200], Train Loss: 0.004984
Validation Loss: 0.00362156
Epoch [7/200], Train Loss: 0.004986
Validation Loss: 0.00358545
Epoch [8/200], Train Loss: 0.004974
Validation Loss: 0.00358301
Epoch [9/200], Train Loss: 0.005000
Validation Loss: 0.00358266
Epoch [10/200], Train Loss: 0.004993
Validation Loss: 0.00358398
Epoch [11/200], Train Loss: 0.005001
Validation Loss: 0.00358874
Epoch [12/200], Train Loss: 0.004967
Validation Loss: 0.00359264
Epoch [13/200], Train Loss: 0.004995
Validation Loss: 0.00358875
Epoch [14/200], Train Loss: 0.004997
Validation Loss: 0.00358488
Epoch [15/200], Train Loss: 0.004988
Validation Loss: 0.00358443
Epoch [16/200], Train Loss: 0.004988
Validation Loss: 0.00358500
Epoch [17/200], Train Loss: 0.004961
Validation Loss: 0.00358667
Epoch [18/200], Train Loss: 0.004956
Validation Loss: 0.00358565
Epoch [19/200], Train Loss: 0.004982
Validation Loss: 0.00358448
Early stopping triggered

Evaluating model for: Lamp
Run 84/144 completed in 22.65 seconds with: {'MAE': np.float32(2.871059), 'MSE': np.float32(182.17952), 'RMSE': np.float32(13.49739), 'SAE': np.float32(0.103110164), 'NDE': np.float32(0.9913406)}

Run 85/144: hidden=128, seq_len=1080, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.005310
Validation Loss: 0.00475488
Epoch [2/200], Train Loss: 0.004988
Validation Loss: 0.00475953
Epoch [3/200], Train Loss: 0.004966
Validation Loss: 0.00474444
Epoch [4/200], Train Loss: 0.004928
Validation Loss: 0.00473626
Epoch [5/200], Train Loss: 0.004922
Validation Loss: 0.00472622
Epoch [6/200], Train Loss: 0.004939
Validation Loss: 0.00471790
Epoch [7/200], Train Loss: 0.004927
Validation Loss: 0.00471192
Epoch [8/200], Train Loss: 0.004906
Validation Loss: 0.00470627
Epoch [9/200], Train Loss: 0.004913
Validation Loss: 0.00470513
Epoch [10/200], Train Loss: 0.004912
Validation Loss: 0.00470380
Epoch [11/200], Train Loss: 0.004912
Validation Loss: 0.00470383
Epoch [12/200], Train Loss: 0.004880
Validation Loss: 0.00470297
Epoch [13/200], Train Loss: 0.004896
Validation Loss: 0.00470206
Epoch [14/200], Train Loss: 0.004901
Validation Loss: 0.00470127
Epoch [15/200], Train Loss: 0.004875
Validation Loss: 0.00470111
Epoch [16/200], Train Loss: 0.004889
Validation Loss: 0.00470125
Epoch [17/200], Train Loss: 0.004888
Validation Loss: 0.00470213
Epoch [18/200], Train Loss: 0.004878
Validation Loss: 0.00469859
Epoch [19/200], Train Loss: 0.004889
Validation Loss: 0.00470542
Epoch [20/200], Train Loss: 0.004885
Validation Loss: 0.00469980
Epoch [21/200], Train Loss: 0.004866
Validation Loss: 0.00469600
Epoch [22/200], Train Loss: 0.004869
Validation Loss: 0.00469434
Epoch [23/200], Train Loss: 0.004883
Validation Loss: 0.00469368
Epoch [24/200], Train Loss: 0.004878
Validation Loss: 0.00469883
Epoch [25/200], Train Loss: 0.004885
Validation Loss: 0.00469246
Epoch [26/200], Train Loss: 0.004875
Validation Loss: 0.00469069
Epoch [27/200], Train Loss: 0.004880
Validation Loss: 0.00468815
Epoch [28/200], Train Loss: 0.004878
Validation Loss: 0.00468689
Epoch [29/200], Train Loss: 0.004870
Validation Loss: 0.00468482
Epoch [30/200], Train Loss: 0.004866
Validation Loss: 0.00468247
Epoch [31/200], Train Loss: 0.004869
Validation Loss: 0.00468078
Epoch [32/200], Train Loss: 0.004853
Validation Loss: 0.00467825
Epoch [33/200], Train Loss: 0.004868
Validation Loss: 0.00467906
Epoch [34/200], Train Loss: 0.004847
Validation Loss: 0.00467638
Epoch [35/200], Train Loss: 0.004846
Validation Loss: 0.00467016
Epoch [36/200], Train Loss: 0.004847
Validation Loss: 0.00467831
Epoch [37/200], Train Loss: 0.004863
Validation Loss: 0.00466707
Epoch [38/200], Train Loss: 0.004854
Validation Loss: 0.00466825
Epoch [39/200], Train Loss: 0.004841
Validation Loss: 0.00466176
Epoch [40/200], Train Loss: 0.004849
Validation Loss: 0.00465810
Epoch [41/200], Train Loss: 0.004880
Validation Loss: 0.00465974
Epoch [42/200], Train Loss: 0.004821
Validation Loss: 0.00465091
Epoch [43/200], Train Loss: 0.004851
Validation Loss: 0.00464852
Epoch [44/200], Train Loss: 0.004853
Validation Loss: 0.00464780
Epoch [45/200], Train Loss: 0.004815
Validation Loss: 0.00464489
Epoch [46/200], Train Loss: 0.004847
Validation Loss: 0.00463474
Epoch [47/200], Train Loss: 0.004812
Validation Loss: 0.00462806
Epoch [48/200], Train Loss: 0.004819
Validation Loss: 0.00462662
Epoch [49/200], Train Loss: 0.004839
Validation Loss: 0.00461899
Epoch [50/200], Train Loss: 0.004815
Validation Loss: 0.00461006
Epoch [51/200], Train Loss: 0.004809
Validation Loss: 0.00460115
Epoch [52/200], Train Loss: 0.004808
Validation Loss: 0.00460774
Epoch [53/200], Train Loss: 0.004789
Validation Loss: 0.00459432
Epoch [54/200], Train Loss: 0.004752
Validation Loss: 0.00457305
Epoch [55/200], Train Loss: 0.004751
Validation Loss: 0.00456440
Epoch [56/200], Train Loss: 0.004754
Validation Loss: 0.00454552
Epoch [57/200], Train Loss: 0.004745
Validation Loss: 0.00453067
Epoch [58/200], Train Loss: 0.004701
Validation Loss: 0.00451444
Epoch [59/200], Train Loss: 0.004700
Validation Loss: 0.00449742
Epoch [60/200], Train Loss: 0.004713
Validation Loss: 0.00450047
Epoch [61/200], Train Loss: 0.004652
Validation Loss: 0.00446198
Epoch [62/200], Train Loss: 0.004663
Validation Loss: 0.00444489
Epoch [63/200], Train Loss: 0.004632
Validation Loss: 0.00443429
Epoch [64/200], Train Loss: 0.004626
Validation Loss: 0.00440895
Epoch [65/200], Train Loss: 0.004596
Validation Loss: 0.00437222
Epoch [66/200], Train Loss: 0.004569
Validation Loss: 0.00440140
Epoch [67/200], Train Loss: 0.004555
Validation Loss: 0.00433458
Epoch [68/200], Train Loss: 0.004523
Validation Loss: 0.00429007
Epoch [69/200], Train Loss: 0.004494
Validation Loss: 0.00425830
Epoch [70/200], Train Loss: 0.004471
Validation Loss: 0.00422865
Epoch [71/200], Train Loss: 0.004448
Validation Loss: 0.00420144
Epoch [72/200], Train Loss: 0.004410
Validation Loss: 0.00417369
Epoch [73/200], Train Loss: 0.004381
Validation Loss: 0.00413421
Epoch [74/200], Train Loss: 0.004354
Validation Loss: 0.00409797
Epoch [75/200], Train Loss: 0.004378
Validation Loss: 0.00406813
Epoch [76/200], Train Loss: 0.004294
Validation Loss: 0.00403359
Epoch [77/200], Train Loss: 0.004266
Validation Loss: 0.00399937
Epoch [78/200], Train Loss: 0.004239
Validation Loss: 0.00404160
Epoch [79/200], Train Loss: 0.004193
Validation Loss: 0.00393513
Epoch [80/200], Train Loss: 0.004168
Validation Loss: 0.00389939
Epoch [81/200], Train Loss: 0.004153
Validation Loss: 0.00386397
Epoch [82/200], Train Loss: 0.004106
Validation Loss: 0.00385692
Epoch [83/200], Train Loss: 0.004054
Validation Loss: 0.00380043
Epoch [84/200], Train Loss: 0.004039
Validation Loss: 0.00375437
Epoch [85/200], Train Loss: 0.003990
Validation Loss: 0.00369997
Epoch [86/200], Train Loss: 0.003946
Validation Loss: 0.00365121
Epoch [87/200], Train Loss: 0.003912
Validation Loss: 0.00364180
Epoch [88/200], Train Loss: 0.003862
Validation Loss: 0.00356926
Epoch [89/200], Train Loss: 0.003821
Validation Loss: 0.00350072
Epoch [90/200], Train Loss: 0.003778
Validation Loss: 0.00344648
Epoch [91/200], Train Loss: 0.003721
Validation Loss: 0.00340232
Epoch [92/200], Train Loss: 0.003700
Validation Loss: 0.00334675
Epoch [93/200], Train Loss: 0.003619
Validation Loss: 0.00329567
Epoch [94/200], Train Loss: 0.003582
Validation Loss: 0.00329835
Epoch [95/200], Train Loss: 0.003543
Validation Loss: 0.00323867
Epoch [96/200], Train Loss: 0.003489
Validation Loss: 0.00317780
Epoch [97/200], Train Loss: 0.003441
Validation Loss: 0.00308478
Epoch [98/200], Train Loss: 0.003377
Validation Loss: 0.00306092
Epoch [99/200], Train Loss: 0.003339
Validation Loss: 0.00298586
Epoch [100/200], Train Loss: 0.003311
Validation Loss: 0.00295526
Epoch [101/200], Train Loss: 0.003272
Validation Loss: 0.00293432
Epoch [102/200], Train Loss: 0.003216
Validation Loss: 0.00284414
Epoch [103/200], Train Loss: 0.003147
Validation Loss: 0.00283607
Epoch [104/200], Train Loss: 0.003128
Validation Loss: 0.00277696
Epoch [105/200], Train Loss: 0.003063
Validation Loss: 0.00275591
Epoch [106/200], Train Loss: 0.003029
Validation Loss: 0.00270696
Epoch [107/200], Train Loss: 0.002998
Validation Loss: 0.00267073
Epoch [108/200], Train Loss: 0.002962
Validation Loss: 0.00263700
Epoch [109/200], Train Loss: 0.002919
Validation Loss: 0.00259718
Epoch [110/200], Train Loss: 0.002878
Validation Loss: 0.00258830
Epoch [111/200], Train Loss: 0.002863
Validation Loss: 0.00256262
Epoch [112/200], Train Loss: 0.002828
Validation Loss: 0.00251079
Epoch [113/200], Train Loss: 0.002780
Validation Loss: 0.00248116
Epoch [114/200], Train Loss: 0.002762
Validation Loss: 0.00246517
Epoch [115/200], Train Loss: 0.002729
Validation Loss: 0.00245083
Epoch [116/200], Train Loss: 0.002705
Validation Loss: 0.00244417
Epoch [117/200], Train Loss: 0.002665
Validation Loss: 0.00240522
Epoch [118/200], Train Loss: 0.002638
Validation Loss: 0.00238207
Epoch [119/200], Train Loss: 0.002629
Validation Loss: 0.00237758
Epoch [120/200], Train Loss: 0.002594
Validation Loss: 0.00233092
Epoch [121/200], Train Loss: 0.002580
Validation Loss: 0.00233219
Epoch [122/200], Train Loss: 0.002569
Validation Loss: 0.00231496
Epoch [123/200], Train Loss: 0.002522
Validation Loss: 0.00231247
Epoch [124/200], Train Loss: 0.002527
Validation Loss: 0.00228113
Epoch [125/200], Train Loss: 0.002501
Validation Loss: 0.00226231
Epoch [126/200], Train Loss: 0.002469
Validation Loss: 0.00223659
Epoch [127/200], Train Loss: 0.002454
Validation Loss: 0.00222136
Epoch [128/200], Train Loss: 0.002429
Validation Loss: 0.00221341
Epoch [129/200], Train Loss: 0.002414
Validation Loss: 0.00219590
Epoch [130/200], Train Loss: 0.002400
Validation Loss: 0.00218546
Epoch [131/200], Train Loss: 0.002378
Validation Loss: 0.00216816
Epoch [132/200], Train Loss: 0.002363
Validation Loss: 0.00214393
Epoch [133/200], Train Loss: 0.002349
Validation Loss: 0.00214524
Epoch [134/200], Train Loss: 0.002336
Validation Loss: 0.00212537
Epoch [135/200], Train Loss: 0.002318
Validation Loss: 0.00211097
Epoch [136/200], Train Loss: 0.002307
Validation Loss: 0.00208684
Epoch [137/200], Train Loss: 0.002281
Validation Loss: 0.00208487
Epoch [138/200], Train Loss: 0.002278
Validation Loss: 0.00210511
Epoch [139/200], Train Loss: 0.002255
Validation Loss: 0.00205961
Epoch [140/200], Train Loss: 0.002252
Validation Loss: 0.00206227
Epoch [141/200], Train Loss: 0.002238
Validation Loss: 0.00205059
Epoch [142/200], Train Loss: 0.002225
Validation Loss: 0.00203489
Epoch [143/200], Train Loss: 0.002214
Validation Loss: 0.00201803
Epoch [144/200], Train Loss: 0.002198
Validation Loss: 0.00200376
Epoch [145/200], Train Loss: 0.002181
Validation Loss: 0.00199727
Epoch [146/200], Train Loss: 0.002173
Validation Loss: 0.00197769
Epoch [147/200], Train Loss: 0.002167
Validation Loss: 0.00199623
Epoch [148/200], Train Loss: 0.002154
Validation Loss: 0.00196557
Epoch [149/200], Train Loss: 0.002137
Validation Loss: 0.00194752
Epoch [150/200], Train Loss: 0.002140
Validation Loss: 0.00196862
Epoch [151/200], Train Loss: 0.002122
Validation Loss: 0.00192878
Epoch [152/200], Train Loss: 0.002122
Validation Loss: 0.00192669
Epoch [153/200], Train Loss: 0.002090
Validation Loss: 0.00194411
Epoch [154/200], Train Loss: 0.002091
Validation Loss: 0.00193438
Epoch [155/200], Train Loss: 0.002091
Validation Loss: 0.00191318
Epoch [156/200], Train Loss: 0.002080
Validation Loss: 0.00189647
Epoch [157/200], Train Loss: 0.002072
Validation Loss: 0.00190191
Epoch [158/200], Train Loss: 0.002049
Validation Loss: 0.00191081
Epoch [159/200], Train Loss: 0.002066
Validation Loss: 0.00187259
Epoch [160/200], Train Loss: 0.002051
Validation Loss: 0.00187903
Epoch [161/200], Train Loss: 0.002031
Validation Loss: 0.00185668
Epoch [162/200], Train Loss: 0.002026
Validation Loss: 0.00186568
Epoch [163/200], Train Loss: 0.002013
Validation Loss: 0.00185784
Epoch [164/200], Train Loss: 0.002010
Validation Loss: 0.00183628
Epoch [165/200], Train Loss: 0.002002
Validation Loss: 0.00184670
Epoch [166/200], Train Loss: 0.001993
Validation Loss: 0.00184621
Epoch [167/200], Train Loss: 0.001985
Validation Loss: 0.00183170
Epoch [168/200], Train Loss: 0.001970
Validation Loss: 0.00182585
Epoch [169/200], Train Loss: 0.001969
Validation Loss: 0.00181178
Epoch [170/200], Train Loss: 0.001965
Validation Loss: 0.00180107
Epoch [171/200], Train Loss: 0.001959
Validation Loss: 0.00182049
Epoch [172/200], Train Loss: 0.001940
Validation Loss: 0.00180065
Epoch [173/200], Train Loss: 0.001949
Validation Loss: 0.00178305
Epoch [174/200], Train Loss: 0.001942
Validation Loss: 0.00178210
Epoch [175/200], Train Loss: 0.001930
Validation Loss: 0.00179185
Epoch [176/200], Train Loss: 0.001912
Validation Loss: 0.00179390
Epoch [177/200], Train Loss: 0.001912
Validation Loss: 0.00178792
Epoch [178/200], Train Loss: 0.001902
Validation Loss: 0.00176532
Epoch [179/200], Train Loss: 0.001906
Validation Loss: 0.00176476
Epoch [180/200], Train Loss: 0.001904
Validation Loss: 0.00176236
Epoch [181/200], Train Loss: 0.001885
Validation Loss: 0.00174991
Epoch [182/200], Train Loss: 0.001884
Validation Loss: 0.00177934
Epoch [183/200], Train Loss: 0.001870
Validation Loss: 0.00176229
Epoch [184/200], Train Loss: 0.001876
Validation Loss: 0.00172855
Epoch [185/200], Train Loss: 0.001870
Validation Loss: 0.00174313
Epoch [186/200], Train Loss: 0.001865
Validation Loss: 0.00172031
Epoch [187/200], Train Loss: 0.001856
Validation Loss: 0.00173531
Epoch [188/200], Train Loss: 0.001846
Validation Loss: 0.00175456
Epoch [189/200], Train Loss: 0.001856
Validation Loss: 0.00171592
Epoch [190/200], Train Loss: 0.001841
Validation Loss: 0.00172183
Epoch [191/200], Train Loss: 0.001831
Validation Loss: 0.00171096
Epoch [192/200], Train Loss: 0.001824
Validation Loss: 0.00171572
Epoch [193/200], Train Loss: 0.001820
Validation Loss: 0.00173855
Epoch [194/200], Train Loss: 0.001831
Validation Loss: 0.00171675
Epoch [195/200], Train Loss: 0.001819
Validation Loss: 0.00170659
Epoch [196/200], Train Loss: 0.001813
Validation Loss: 0.00169006
Epoch [197/200], Train Loss: 0.001811
Validation Loss: 0.00167682
Epoch [198/200], Train Loss: 0.001796
Validation Loss: 0.00168144
Epoch [199/200], Train Loss: 0.001800
Validation Loss: 0.00168623
Epoch [200/200], Train Loss: 0.001802
Validation Loss: 0.00166343

Evaluating model for: Lamp
Run 85/144 completed in 657.65 seconds with: {'MAE': np.float32(1.3357155), 'MSE': np.float32(49.055916), 'RMSE': np.float32(7.003993), 'SAE': np.float32(0.022678511), 'NDE': np.float32(0.5657766)}

Run 86/144: hidden=128, seq_len=1080, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.005907
Validation Loss: 0.00484889
Epoch [2/200], Train Loss: 0.005026
Validation Loss: 0.00480593
Epoch [3/200], Train Loss: 0.005010
Validation Loss: 0.00478829
Epoch [4/200], Train Loss: 0.004995
Validation Loss: 0.00477924
Epoch [5/200], Train Loss: 0.004981
Validation Loss: 0.00477414
Epoch [6/200], Train Loss: 0.004991
Validation Loss: 0.00476700
Epoch [7/200], Train Loss: 0.004973
Validation Loss: 0.00475737
Epoch [8/200], Train Loss: 0.004942
Validation Loss: 0.00474086
Epoch [9/200], Train Loss: 0.004937
Validation Loss: 0.00471961
Epoch [10/200], Train Loss: 0.004914
Validation Loss: 0.00470921
Epoch [11/200], Train Loss: 0.004898
Validation Loss: 0.00471101
Epoch [12/200], Train Loss: 0.004893
Validation Loss: 0.00470780
Epoch [13/200], Train Loss: 0.004880
Validation Loss: 0.00470607
Epoch [14/200], Train Loss: 0.004888
Validation Loss: 0.00470491
Epoch [15/200], Train Loss: 0.004905
Validation Loss: 0.00470340
Epoch [16/200], Train Loss: 0.004909
Validation Loss: 0.00470255
Epoch [17/200], Train Loss: 0.004908
Validation Loss: 0.00470372
Epoch [18/200], Train Loss: 0.004902
Validation Loss: 0.00470281
Epoch [19/200], Train Loss: 0.004902
Validation Loss: 0.00470197
Epoch [20/200], Train Loss: 0.004887
Validation Loss: 0.00470190
Epoch [21/200], Train Loss: 0.004902
Validation Loss: 0.00470014
Epoch [22/200], Train Loss: 0.004905
Validation Loss: 0.00470487
Epoch [23/200], Train Loss: 0.004881
Validation Loss: 0.00469490
Epoch [24/200], Train Loss: 0.004907
Validation Loss: 0.00469986
Epoch [25/200], Train Loss: 0.004899
Validation Loss: 0.00469541
Epoch [26/200], Train Loss: 0.004881
Validation Loss: 0.00470166
Epoch [27/200], Train Loss: 0.004919
Validation Loss: 0.00469893
Epoch [28/200], Train Loss: 0.004887
Validation Loss: 0.00469508
Epoch [29/200], Train Loss: 0.004891
Validation Loss: 0.00469303
Epoch [30/200], Train Loss: 0.004897
Validation Loss: 0.00469444
Epoch [31/200], Train Loss: 0.004893
Validation Loss: 0.00469140
Epoch [32/200], Train Loss: 0.004874
Validation Loss: 0.00468885
Epoch [33/200], Train Loss: 0.004892
Validation Loss: 0.00469185
Epoch [34/200], Train Loss: 0.004896
Validation Loss: 0.00468915
Epoch [35/200], Train Loss: 0.004875
Validation Loss: 0.00469114
Epoch [36/200], Train Loss: 0.004901
Validation Loss: 0.00469008
Epoch [37/200], Train Loss: 0.004882
Validation Loss: 0.00468758
Epoch [38/200], Train Loss: 0.004880
Validation Loss: 0.00468635
Epoch [39/200], Train Loss: 0.004879
Validation Loss: 0.00468749
Epoch [40/200], Train Loss: 0.004868
Validation Loss: 0.00468653
Epoch [41/200], Train Loss: 0.004898
Validation Loss: 0.00468751
Epoch [42/200], Train Loss: 0.004871
Validation Loss: 0.00468695
Epoch [43/200], Train Loss: 0.004876
Validation Loss: 0.00468639
Epoch [44/200], Train Loss: 0.004857
Validation Loss: 0.00468649
Epoch [45/200], Train Loss: 0.004876
Validation Loss: 0.00468947
Epoch [46/200], Train Loss: 0.004869
Validation Loss: 0.00468560
Epoch [47/200], Train Loss: 0.004868
Validation Loss: 0.00468633
Epoch [48/200], Train Loss: 0.004869
Validation Loss: 0.00468621
Epoch [49/200], Train Loss: 0.004866
Validation Loss: 0.00468673
Epoch [50/200], Train Loss: 0.004871
Validation Loss: 0.00468616
Epoch [51/200], Train Loss: 0.004895
Validation Loss: 0.00468659
Epoch [52/200], Train Loss: 0.004883
Validation Loss: 0.00469062
Epoch [53/200], Train Loss: 0.004860
Validation Loss: 0.00468538
Epoch [54/200], Train Loss: 0.004870
Validation Loss: 0.00469024
Epoch [55/200], Train Loss: 0.004877
Validation Loss: 0.00469688
Epoch [56/200], Train Loss: 0.004843
Validation Loss: 0.00469973
Epoch [57/200], Train Loss: 0.004860
Validation Loss: 0.00469606
Epoch [58/200], Train Loss: 0.004893
Validation Loss: 0.00469040
Epoch [59/200], Train Loss: 0.004889
Validation Loss: 0.00468686
Epoch [60/200], Train Loss: 0.004860
Validation Loss: 0.00468982
Epoch [61/200], Train Loss: 0.004871
Validation Loss: 0.00469025
Epoch [62/200], Train Loss: 0.004864
Validation Loss: 0.00468836
Epoch [63/200], Train Loss: 0.004882
Validation Loss: 0.00468564
Early stopping triggered

Evaluating model for: Lamp
Run 86/144 completed in 229.10 seconds with: {'MAE': np.float32(2.4358132), 'MSE': np.float32(147.07207), 'RMSE': np.float32(12.127327), 'SAE': np.float32(0.024746722), 'NDE': np.float32(0.9796338)}

Run 87/144: hidden=128, seq_len=1080, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.005668
Validation Loss: 0.00478479
Epoch [2/200], Train Loss: 0.005046
Validation Loss: 0.00478378
Epoch [3/200], Train Loss: 0.004985
Validation Loss: 0.00478250
Epoch [4/200], Train Loss: 0.004989
Validation Loss: 0.00477788
Epoch [5/200], Train Loss: 0.004980
Validation Loss: 0.00477627
Epoch [6/200], Train Loss: 0.004998
Validation Loss: 0.00477250
Epoch [7/200], Train Loss: 0.004974
Validation Loss: 0.00476743
Epoch [8/200], Train Loss: 0.004989
Validation Loss: 0.00475698
Epoch [9/200], Train Loss: 0.004953
Validation Loss: 0.00473327
Epoch [10/200], Train Loss: 0.004933
Validation Loss: 0.00470988
Epoch [11/200], Train Loss: 0.004880
Validation Loss: 0.00470752
Epoch [12/200], Train Loss: 0.004902
Validation Loss: 0.00470549
Epoch [13/200], Train Loss: 0.004922
Validation Loss: 0.00470537
Epoch [14/200], Train Loss: 0.004906
Validation Loss: 0.00470817
Epoch [15/200], Train Loss: 0.004885
Validation Loss: 0.00470253
Epoch [16/200], Train Loss: 0.004878
Validation Loss: 0.00470381
Epoch [17/200], Train Loss: 0.004902
Validation Loss: 0.00470309
Epoch [18/200], Train Loss: 0.004885
Validation Loss: 0.00470156
Epoch [19/200], Train Loss: 0.004876
Validation Loss: 0.00469721
Epoch [20/200], Train Loss: 0.004904
Validation Loss: 0.00469843
Epoch [21/200], Train Loss: 0.004894
Validation Loss: 0.00469730
Epoch [22/200], Train Loss: 0.004880
Validation Loss: 0.00469452
Epoch [23/200], Train Loss: 0.004884
Validation Loss: 0.00470233
Epoch [24/200], Train Loss: 0.004892
Validation Loss: 0.00469329
Epoch [25/200], Train Loss: 0.004894
Validation Loss: 0.00469521
Epoch [26/200], Train Loss: 0.004876
Validation Loss: 0.00469274
Epoch [27/200], Train Loss: 0.004871
Validation Loss: 0.00469085
Epoch [28/200], Train Loss: 0.004880
Validation Loss: 0.00469009
Epoch [29/200], Train Loss: 0.004893
Validation Loss: 0.00468907
Epoch [30/200], Train Loss: 0.004880
Validation Loss: 0.00468821
Epoch [31/200], Train Loss: 0.004887
Validation Loss: 0.00468764
Epoch [32/200], Train Loss: 0.004885
Validation Loss: 0.00468733
Epoch [33/200], Train Loss: 0.004884
Validation Loss: 0.00469370
Epoch [34/200], Train Loss: 0.004880
Validation Loss: 0.00469681
Epoch [35/200], Train Loss: 0.004880
Validation Loss: 0.00469074
Epoch [36/200], Train Loss: 0.004866
Validation Loss: 0.00469652
Epoch [37/200], Train Loss: 0.004865
Validation Loss: 0.00468671
Epoch [38/200], Train Loss: 0.004873
Validation Loss: 0.00469499
Epoch [39/200], Train Loss: 0.004853
Validation Loss: 0.00468624
Epoch [40/200], Train Loss: 0.004874
Validation Loss: 0.00469001
Epoch [41/200], Train Loss: 0.004855
Validation Loss: 0.00469306
Epoch [42/200], Train Loss: 0.004871
Validation Loss: 0.00468553
Epoch [43/200], Train Loss: 0.004868
Validation Loss: 0.00468730
Epoch [44/200], Train Loss: 0.004880
Validation Loss: 0.00468601
Epoch [45/200], Train Loss: 0.004866
Validation Loss: 0.00468780
Epoch [46/200], Train Loss: 0.004865
Validation Loss: 0.00468695
Epoch [47/200], Train Loss: 0.004870
Validation Loss: 0.00468651
Epoch [48/200], Train Loss: 0.004860
Validation Loss: 0.00469051
Epoch [49/200], Train Loss: 0.004880
Validation Loss: 0.00468538
Epoch [50/200], Train Loss: 0.004871
Validation Loss: 0.00469301
Epoch [51/200], Train Loss: 0.004900
Validation Loss: 0.00468740
Epoch [52/200], Train Loss: 0.004864
Validation Loss: 0.00468596
Epoch [53/200], Train Loss: 0.004861
Validation Loss: 0.00468737
Epoch [54/200], Train Loss: 0.004850
Validation Loss: 0.00468645
Epoch [55/200], Train Loss: 0.004877
Validation Loss: 0.00469001
Epoch [56/200], Train Loss: 0.004878
Validation Loss: 0.00469857
Epoch [57/200], Train Loss: 0.004897
Validation Loss: 0.00469167
Epoch [58/200], Train Loss: 0.004859
Validation Loss: 0.00469178
Epoch [59/200], Train Loss: 0.004862
Validation Loss: 0.00468922
Early stopping triggered

Evaluating model for: Lamp
Run 87/144 completed in 232.61 seconds with: {'MAE': np.float32(2.575215), 'MSE': np.float32(147.23584), 'RMSE': np.float32(12.134078), 'SAE': np.float32(0.09375728), 'NDE': np.float32(0.9801801)}

Run 88/144: hidden=128, seq_len=1080, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.005873
Validation Loss: 0.00479233
Epoch [2/200], Train Loss: 0.005039
Validation Loss: 0.00478735
Epoch [3/200], Train Loss: 0.005039
Validation Loss: 0.00478305
Epoch [4/200], Train Loss: 0.004999
Validation Loss: 0.00478044
Epoch [5/200], Train Loss: 0.005001
Validation Loss: 0.00477906
Epoch [6/200], Train Loss: 0.005027
Validation Loss: 0.00477813
Epoch [7/200], Train Loss: 0.005016
Validation Loss: 0.00477585
Epoch [8/200], Train Loss: 0.005000
Validation Loss: 0.00477235
Epoch [9/200], Train Loss: 0.004978
Validation Loss: 0.00476287
Epoch [10/200], Train Loss: 0.004977
Validation Loss: 0.00473646
Epoch [11/200], Train Loss: 0.004935
Validation Loss: 0.00471169
Epoch [12/200], Train Loss: 0.004939
Validation Loss: 0.00471192
Epoch [13/200], Train Loss: 0.004927
Validation Loss: 0.00471475
Epoch [14/200], Train Loss: 0.004911
Validation Loss: 0.00470603
Epoch [15/200], Train Loss: 0.004902
Validation Loss: 0.00470438
Epoch [16/200], Train Loss: 0.004892
Validation Loss: 0.00470338
Epoch [17/200], Train Loss: 0.004887
Validation Loss: 0.00470266
Epoch [18/200], Train Loss: 0.004886
Validation Loss: 0.00470059
Epoch [19/200], Train Loss: 0.004912
Validation Loss: 0.00470080
Epoch [20/200], Train Loss: 0.004909
Validation Loss: 0.00470103
Epoch [21/200], Train Loss: 0.004902
Validation Loss: 0.00470892
Epoch [22/200], Train Loss: 0.004879
Validation Loss: 0.00469631
Epoch [23/200], Train Loss: 0.004895
Validation Loss: 0.00470257
Epoch [24/200], Train Loss: 0.004895
Validation Loss: 0.00469671
Epoch [25/200], Train Loss: 0.004877
Validation Loss: 0.00469456
Epoch [26/200], Train Loss: 0.004900
Validation Loss: 0.00469333
Epoch [27/200], Train Loss: 0.004893
Validation Loss: 0.00469578
Epoch [28/200], Train Loss: 0.004882
Validation Loss: 0.00469166
Epoch [29/200], Train Loss: 0.004890
Validation Loss: 0.00469138
Epoch [30/200], Train Loss: 0.004895
Validation Loss: 0.00469413
Epoch [31/200], Train Loss: 0.004895
Validation Loss: 0.00469101
Epoch [32/200], Train Loss: 0.004879
Validation Loss: 0.00468986
Epoch [33/200], Train Loss: 0.004887
Validation Loss: 0.00469938
Epoch [34/200], Train Loss: 0.004860
Validation Loss: 0.00469691
Epoch [35/200], Train Loss: 0.004878
Validation Loss: 0.00469029
Epoch [36/200], Train Loss: 0.004881
Validation Loss: 0.00469459
Epoch [37/200], Train Loss: 0.004908
Validation Loss: 0.00469483
Epoch [38/200], Train Loss: 0.004879
Validation Loss: 0.00469395
Epoch [39/200], Train Loss: 0.004893
Validation Loss: 0.00469329
Epoch [40/200], Train Loss: 0.004870
Validation Loss: 0.00469347
Epoch [41/200], Train Loss: 0.004898
Validation Loss: 0.00469560
Epoch [42/200], Train Loss: 0.004886
Validation Loss: 0.00468784
Epoch [43/200], Train Loss: 0.004858
Validation Loss: 0.00468823
Epoch [44/200], Train Loss: 0.004857
Validation Loss: 0.00468916
Epoch [45/200], Train Loss: 0.004877
Validation Loss: 0.00468768
Epoch [46/200], Train Loss: 0.004889
Validation Loss: 0.00469139
Epoch [47/200], Train Loss: 0.004895
Validation Loss: 0.00468781
Epoch [48/200], Train Loss: 0.004882
Validation Loss: 0.00468801
Epoch [49/200], Train Loss: 0.004893
Validation Loss: 0.00468863
Epoch [50/200], Train Loss: 0.004889
Validation Loss: 0.00468779
Epoch [51/200], Train Loss: 0.004879
Validation Loss: 0.00468838
Epoch [52/200], Train Loss: 0.004858
Validation Loss: 0.00468991
Epoch [53/200], Train Loss: 0.004867
Validation Loss: 0.00468864
Epoch [54/200], Train Loss: 0.004891
Validation Loss: 0.00469481
Epoch [55/200], Train Loss: 0.004885
Validation Loss: 0.00469249
Early stopping triggered

Evaluating model for: Lamp
Run 88/144 completed in 234.07 seconds with: {'MAE': np.float32(2.6907856), 'MSE': np.float32(147.28928), 'RMSE': np.float32(12.136279), 'SAE': np.float32(0.1525298), 'NDE': np.float32(0.9803585)}

Run 89/144: hidden=128, seq_len=1080, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.005632
Validation Loss: 0.00260673
Epoch [2/200], Train Loss: 0.005105
Validation Loss: 0.00243829
Epoch [3/200], Train Loss: 0.004897
Validation Loss: 0.00250042
Epoch [4/200], Train Loss: 0.005023
Validation Loss: 0.00251142
Epoch [5/200], Train Loss: 0.005024
Validation Loss: 0.00246967
Epoch [6/200], Train Loss: 0.004958
Validation Loss: 0.00244510
Epoch [7/200], Train Loss: 0.004930
Validation Loss: 0.00243957
Epoch [8/200], Train Loss: 0.004943
Validation Loss: 0.00244286
Epoch [9/200], Train Loss: 0.004888
Validation Loss: 0.00244646
Epoch [10/200], Train Loss: 0.004974
Validation Loss: 0.00244416
Epoch [11/200], Train Loss: 0.004858
Validation Loss: 0.00244247
Epoch [12/200], Train Loss: 0.005004
Validation Loss: 0.00243609
Epoch [13/200], Train Loss: 0.005020
Validation Loss: 0.00243727
Epoch [14/200], Train Loss: 0.004904
Validation Loss: 0.00244094
Epoch [15/200], Train Loss: 0.004910
Validation Loss: 0.00243370
Epoch [16/200], Train Loss: 0.004914
Validation Loss: 0.00243315
Epoch [17/200], Train Loss: 0.004878
Validation Loss: 0.00243420
Epoch [18/200], Train Loss: 0.004899
Validation Loss: 0.00244012
Epoch [19/200], Train Loss: 0.004865
Validation Loss: 0.00243365
Epoch [20/200], Train Loss: 0.004974
Validation Loss: 0.00243350
Epoch [21/200], Train Loss: 0.004938
Validation Loss: 0.00243936
Epoch [22/200], Train Loss: 0.004951
Validation Loss: 0.00244117
Epoch [23/200], Train Loss: 0.004877
Validation Loss: 0.00243860
Epoch [24/200], Train Loss: 0.004922
Validation Loss: 0.00243768
Epoch [25/200], Train Loss: 0.004914
Validation Loss: 0.00244129
Epoch [26/200], Train Loss: 0.004849
Validation Loss: 0.00244227
Early stopping triggered

Evaluating model for: Lamp
Run 89/144 completed in 35.14 seconds with: {'MAE': np.float32(3.7146096), 'MSE': np.float32(225.94884), 'RMSE': np.float32(15.031594), 'SAE': np.float32(0.08055302), 'NDE': np.float32(0.9792109)}

Run 90/144: hidden=128, seq_len=1080, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.006668
Validation Loss: 0.00364171
Epoch [2/200], Train Loss: 0.005531
Validation Loss: 0.00277662
Epoch [3/200], Train Loss: 0.005051
Validation Loss: 0.00247223
Epoch [4/200], Train Loss: 0.005152
Validation Loss: 0.00246505
Epoch [5/200], Train Loss: 0.005115
Validation Loss: 0.00245887
Epoch [6/200], Train Loss: 0.005004
Validation Loss: 0.00248781
Epoch [7/200], Train Loss: 0.004985
Validation Loss: 0.00249773
Epoch [8/200], Train Loss: 0.004996
Validation Loss: 0.00248131
Epoch [9/200], Train Loss: 0.005002
Validation Loss: 0.00246719
Epoch [10/200], Train Loss: 0.005083
Validation Loss: 0.00246689
Epoch [11/200], Train Loss: 0.005052
Validation Loss: 0.00246983
Epoch [12/200], Train Loss: 0.005004
Validation Loss: 0.00247242
Epoch [13/200], Train Loss: 0.004952
Validation Loss: 0.00246689
Epoch [14/200], Train Loss: 0.004902
Validation Loss: 0.00246030
Epoch [15/200], Train Loss: 0.004920
Validation Loss: 0.00246024
Early stopping triggered

Evaluating model for: Lamp
Run 90/144 completed in 22.78 seconds with: {'MAE': np.float32(3.2792819), 'MSE': np.float32(230.27017), 'RMSE': np.float32(15.174656), 'SAE': np.float32(0.23456821), 'NDE': np.float32(0.9885308)}

Run 91/144: hidden=128, seq_len=1080, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.006336
Validation Loss: 0.00326938
Epoch [2/200], Train Loss: 0.005305
Validation Loss: 0.00258095
Epoch [3/200], Train Loss: 0.004966
Validation Loss: 0.00245414
Epoch [4/200], Train Loss: 0.005044
Validation Loss: 0.00245574
Epoch [5/200], Train Loss: 0.004982
Validation Loss: 0.00246208
Epoch [6/200], Train Loss: 0.004982
Validation Loss: 0.00248848
Epoch [7/200], Train Loss: 0.004927
Validation Loss: 0.00248604
Epoch [8/200], Train Loss: 0.004961
Validation Loss: 0.00246876
Epoch [9/200], Train Loss: 0.004967
Validation Loss: 0.00246192
Epoch [10/200], Train Loss: 0.005083
Validation Loss: 0.00246448
Epoch [11/200], Train Loss: 0.005054
Validation Loss: 0.00246998
Epoch [12/200], Train Loss: 0.004897
Validation Loss: 0.00247073
Epoch [13/200], Train Loss: 0.004960
Validation Loss: 0.00246533
Early stopping triggered

Evaluating model for: Lamp
Run 91/144 completed in 20.88 seconds with: {'MAE': np.float32(3.263516), 'MSE': np.float32(230.72603), 'RMSE': np.float32(15.189669), 'SAE': np.float32(0.24385813), 'NDE': np.float32(0.9895087)}

Run 92/144: hidden=128, seq_len=1080, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.006646
Validation Loss: 0.00307187
Epoch [2/200], Train Loss: 0.005402
Validation Loss: 0.00248663
Epoch [3/200], Train Loss: 0.005075
Validation Loss: 0.00253939
Epoch [4/200], Train Loss: 0.005023
Validation Loss: 0.00257516
Epoch [5/200], Train Loss: 0.005011
Validation Loss: 0.00247554
Epoch [6/200], Train Loss: 0.005083
Validation Loss: 0.00245356
Epoch [7/200], Train Loss: 0.004930
Validation Loss: 0.00245634
Epoch [8/200], Train Loss: 0.005153
Validation Loss: 0.00246785
Epoch [9/200], Train Loss: 0.005189
Validation Loss: 0.00247625
Epoch [10/200], Train Loss: 0.004955
Validation Loss: 0.00247621
Epoch [11/200], Train Loss: 0.005031
Validation Loss: 0.00246353
Epoch [12/200], Train Loss: 0.005014
Validation Loss: 0.00246256
Epoch [13/200], Train Loss: 0.005080
Validation Loss: 0.00246748
Epoch [14/200], Train Loss: 0.005005
Validation Loss: 0.00247241
Epoch [15/200], Train Loss: 0.004983
Validation Loss: 0.00246730
Epoch [16/200], Train Loss: 0.004917
Validation Loss: 0.00246583
Early stopping triggered

Evaluating model for: Lamp
Run 92/144 completed in 27.78 seconds with: {'MAE': np.float32(3.2466764), 'MSE': np.float32(230.84224), 'RMSE': np.float32(15.193493), 'SAE': np.float32(0.26645175), 'NDE': np.float32(0.9897579)}

Run 93/144: hidden=128, seq_len=1080, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.005536
Validation Loss: 0.00582346
Epoch [2/200], Train Loss: 0.005132
Validation Loss: 0.00549340
Epoch [3/200], Train Loss: 0.004850
Validation Loss: 0.00529699
Epoch [4/200], Train Loss: 0.004642
Validation Loss: 0.00520731
Epoch [5/200], Train Loss: 0.004615
Validation Loss: 0.00518847
Epoch [6/200], Train Loss: 0.004638
Validation Loss: 0.00519781
Epoch [7/200], Train Loss: 0.004642
Validation Loss: 0.00520420
Epoch [8/200], Train Loss: 0.004588
Validation Loss: 0.00519956
Epoch [9/200], Train Loss: 0.004621
Validation Loss: 0.00518956
Epoch [10/200], Train Loss: 0.004623
Validation Loss: 0.00518291
Epoch [11/200], Train Loss: 0.004587
Validation Loss: 0.00518166
Epoch [12/200], Train Loss: 0.004655
Validation Loss: 0.00518324
Epoch [13/200], Train Loss: 0.004624
Validation Loss: 0.00518347
Epoch [14/200], Train Loss: 0.004670
Validation Loss: 0.00518178
Epoch [15/200], Train Loss: 0.004578
Validation Loss: 0.00517680
Epoch [16/200], Train Loss: 0.004641
Validation Loss: 0.00517288
Epoch [17/200], Train Loss: 0.004656
Validation Loss: 0.00516928
Epoch [18/200], Train Loss: 0.004581
Validation Loss: 0.00516617
Epoch [19/200], Train Loss: 0.004598
Validation Loss: 0.00516371
Epoch [20/200], Train Loss: 0.004653
Validation Loss: 0.00516184
Epoch [21/200], Train Loss: 0.004636
Validation Loss: 0.00516032
Epoch [22/200], Train Loss: 0.004575
Validation Loss: 0.00515836
Epoch [23/200], Train Loss: 0.004634
Validation Loss: 0.00515647
Epoch [24/200], Train Loss: 0.004577
Validation Loss: 0.00515379
Epoch [25/200], Train Loss: 0.004612
Validation Loss: 0.00515158
Epoch [26/200], Train Loss: 0.004601
Validation Loss: 0.00514912
Epoch [27/200], Train Loss: 0.004585
Validation Loss: 0.00514670
Epoch [28/200], Train Loss: 0.004647
Validation Loss: 0.00514473
Epoch [29/200], Train Loss: 0.004593
Validation Loss: 0.00514216
Epoch [30/200], Train Loss: 0.004562
Validation Loss: 0.00513952
Epoch [31/200], Train Loss: 0.004634
Validation Loss: 0.00513779
Epoch [32/200], Train Loss: 0.004585
Validation Loss: 0.00513558
Epoch [33/200], Train Loss: 0.004582
Validation Loss: 0.00513400
Epoch [34/200], Train Loss: 0.004580
Validation Loss: 0.00513284
Epoch [35/200], Train Loss: 0.004623
Validation Loss: 0.00513112
Epoch [36/200], Train Loss: 0.004604
Validation Loss: 0.00512907
Epoch [37/200], Train Loss: 0.004592
Validation Loss: 0.00512712
Epoch [38/200], Train Loss: 0.004578
Validation Loss: 0.00512502
Epoch [39/200], Train Loss: 0.004570
Validation Loss: 0.00512359
Epoch [40/200], Train Loss: 0.004584
Validation Loss: 0.00512293
Epoch [41/200], Train Loss: 0.004584
Validation Loss: 0.00512154
Epoch [42/200], Train Loss: 0.004596
Validation Loss: 0.00512073
Epoch [43/200], Train Loss: 0.004555
Validation Loss: 0.00511960
Epoch [44/200], Train Loss: 0.004585
Validation Loss: 0.00511851
Epoch [45/200], Train Loss: 0.004564
Validation Loss: 0.00511788
Epoch [46/200], Train Loss: 0.004539
Validation Loss: 0.00511695
Epoch [47/200], Train Loss: 0.004578
Validation Loss: 0.00511658
Epoch [48/200], Train Loss: 0.004570
Validation Loss: 0.00511569
Epoch [49/200], Train Loss: 0.004529
Validation Loss: 0.00511458
Epoch [50/200], Train Loss: 0.004596
Validation Loss: 0.00511530
Epoch [51/200], Train Loss: 0.004548
Validation Loss: 0.00511432
Epoch [52/200], Train Loss: 0.004631
Validation Loss: 0.00511500
Epoch [53/200], Train Loss: 0.004556
Validation Loss: 0.00511383
Epoch [54/200], Train Loss: 0.004556
Validation Loss: 0.00511282
Epoch [55/200], Train Loss: 0.004604
Validation Loss: 0.00511280
Epoch [56/200], Train Loss: 0.004557
Validation Loss: 0.00511272
Epoch [57/200], Train Loss: 0.004576
Validation Loss: 0.00511294
Epoch [58/200], Train Loss: 0.004558
Validation Loss: 0.00511215
Epoch [59/200], Train Loss: 0.004528
Validation Loss: 0.00511273
Epoch [60/200], Train Loss: 0.004606
Validation Loss: 0.00511323
Epoch [61/200], Train Loss: 0.004576
Validation Loss: 0.00511250
Epoch [62/200], Train Loss: 0.004578
Validation Loss: 0.00511168
Epoch [63/200], Train Loss: 0.004581
Validation Loss: 0.00511117
Epoch [64/200], Train Loss: 0.004641
Validation Loss: 0.00511149
Epoch [65/200], Train Loss: 0.004546
Validation Loss: 0.00511169
Epoch [66/200], Train Loss: 0.004569
Validation Loss: 0.00511194
Epoch [67/200], Train Loss: 0.004581
Validation Loss: 0.00511174
Epoch [68/200], Train Loss: 0.004506
Validation Loss: 0.00511076
Epoch [69/200], Train Loss: 0.004578
Validation Loss: 0.00511076
Epoch [70/200], Train Loss: 0.004561
Validation Loss: 0.00511130
Epoch [71/200], Train Loss: 0.004576
Validation Loss: 0.00511169
Epoch [72/200], Train Loss: 0.004610
Validation Loss: 0.00511133
Epoch [73/200], Train Loss: 0.004561
Validation Loss: 0.00511027
Epoch [74/200], Train Loss: 0.004605
Validation Loss: 0.00511041
Epoch [75/200], Train Loss: 0.004593
Validation Loss: 0.00511049
Epoch [76/200], Train Loss: 0.004587
Validation Loss: 0.00511034
Epoch [77/200], Train Loss: 0.004634
Validation Loss: 0.00511102
Epoch [78/200], Train Loss: 0.004642
Validation Loss: 0.00511051
Epoch [79/200], Train Loss: 0.004580
Validation Loss: 0.00510922
Epoch [80/200], Train Loss: 0.004533
Validation Loss: 0.00510898
Epoch [81/200], Train Loss: 0.004584
Validation Loss: 0.00510985
Epoch [82/200], Train Loss: 0.004535
Validation Loss: 0.00510958
Epoch [83/200], Train Loss: 0.004556
Validation Loss: 0.00511018
Epoch [84/200], Train Loss: 0.004516
Validation Loss: 0.00510936
Epoch [85/200], Train Loss: 0.004574
Validation Loss: 0.00510957
Epoch [86/200], Train Loss: 0.004598
Validation Loss: 0.00510937
Epoch [87/200], Train Loss: 0.004576
Validation Loss: 0.00510953
Epoch [88/200], Train Loss: 0.004568
Validation Loss: 0.00510889
Epoch [89/200], Train Loss: 0.004598
Validation Loss: 0.00510910
Epoch [90/200], Train Loss: 0.004531
Validation Loss: 0.00510855
Epoch [91/200], Train Loss: 0.004630
Validation Loss: 0.00510943
Epoch [92/200], Train Loss: 0.004560
Validation Loss: 0.00510902
Epoch [93/200], Train Loss: 0.004599
Validation Loss: 0.00510875
Epoch [94/200], Train Loss: 0.004595
Validation Loss: 0.00510890
Epoch [95/200], Train Loss: 0.004589
Validation Loss: 0.00510792
Epoch [96/200], Train Loss: 0.004580
Validation Loss: 0.00510840
Epoch [97/200], Train Loss: 0.004585
Validation Loss: 0.00510802
Epoch [98/200], Train Loss: 0.004591
Validation Loss: 0.00510810
Epoch [99/200], Train Loss: 0.004613
Validation Loss: 0.00510866
Epoch [100/200], Train Loss: 0.004573
Validation Loss: 0.00510820
Epoch [101/200], Train Loss: 0.004567
Validation Loss: 0.00510746
Epoch [102/200], Train Loss: 0.004546
Validation Loss: 0.00510727
Epoch [103/200], Train Loss: 0.004573
Validation Loss: 0.00510780
Epoch [104/200], Train Loss: 0.004577
Validation Loss: 0.00510911
Epoch [105/200], Train Loss: 0.004578
Validation Loss: 0.00510850
Epoch [106/200], Train Loss: 0.004558
Validation Loss: 0.00510701
Epoch [107/200], Train Loss: 0.004632
Validation Loss: 0.00510684
Epoch [108/200], Train Loss: 0.004567
Validation Loss: 0.00510654
Epoch [109/200], Train Loss: 0.004543
Validation Loss: 0.00510659
Epoch [110/200], Train Loss: 0.004529
Validation Loss: 0.00510654
Epoch [111/200], Train Loss: 0.004575
Validation Loss: 0.00510690
Epoch [112/200], Train Loss: 0.004544
Validation Loss: 0.00510675
Epoch [113/200], Train Loss: 0.004609
Validation Loss: 0.00510726
Epoch [114/200], Train Loss: 0.004536
Validation Loss: 0.00510612
Epoch [115/200], Train Loss: 0.004601
Validation Loss: 0.00510642
Epoch [116/200], Train Loss: 0.004510
Validation Loss: 0.00510488
Epoch [117/200], Train Loss: 0.004580
Validation Loss: 0.00510484
Epoch [118/200], Train Loss: 0.004623
Validation Loss: 0.00510594
Epoch [119/200], Train Loss: 0.004588
Validation Loss: 0.00510635
Epoch [120/200], Train Loss: 0.004558
Validation Loss: 0.00510581
Epoch [121/200], Train Loss: 0.004614
Validation Loss: 0.00510525
Epoch [122/200], Train Loss: 0.004570
Validation Loss: 0.00510431
Epoch [123/200], Train Loss: 0.004578
Validation Loss: 0.00510439
Epoch [124/200], Train Loss: 0.004575
Validation Loss: 0.00510463
Epoch [125/200], Train Loss: 0.004563
Validation Loss: 0.00510473
Epoch [126/200], Train Loss: 0.004580
Validation Loss: 0.00510451
Epoch [127/200], Train Loss: 0.004561
Validation Loss: 0.00510423
Epoch [128/200], Train Loss: 0.004590
Validation Loss: 0.00510463
Epoch [129/200], Train Loss: 0.004607
Validation Loss: 0.00510446
Epoch [130/200], Train Loss: 0.004616
Validation Loss: 0.00510401
Epoch [131/200], Train Loss: 0.004636
Validation Loss: 0.00510379
Epoch [132/200], Train Loss: 0.004528
Validation Loss: 0.00510236
Epoch [133/200], Train Loss: 0.004641
Validation Loss: 0.00510293
Epoch [134/200], Train Loss: 0.004592
Validation Loss: 0.00510310
Epoch [135/200], Train Loss: 0.004511
Validation Loss: 0.00510221
Epoch [136/200], Train Loss: 0.004540
Validation Loss: 0.00510249
Epoch [137/200], Train Loss: 0.004572
Validation Loss: 0.00510337
Epoch [138/200], Train Loss: 0.004566
Validation Loss: 0.00510284
Epoch [139/200], Train Loss: 0.004542
Validation Loss: 0.00510243
Epoch [140/200], Train Loss: 0.004538
Validation Loss: 0.00510255
Epoch [141/200], Train Loss: 0.004568
Validation Loss: 0.00510267
Epoch [142/200], Train Loss: 0.004582
Validation Loss: 0.00510255
Epoch [143/200], Train Loss: 0.004554
Validation Loss: 0.00510188
Epoch [144/200], Train Loss: 0.004564
Validation Loss: 0.00510152
Epoch [145/200], Train Loss: 0.004547
Validation Loss: 0.00510129
Epoch [146/200], Train Loss: 0.004529
Validation Loss: 0.00510140
Epoch [147/200], Train Loss: 0.004548
Validation Loss: 0.00510184
Epoch [148/200], Train Loss: 0.004547
Validation Loss: 0.00510163
Epoch [149/200], Train Loss: 0.004594
Validation Loss: 0.00510162
Epoch [150/200], Train Loss: 0.004560
Validation Loss: 0.00510085
Epoch [151/200], Train Loss: 0.004587
Validation Loss: 0.00510050
Epoch [152/200], Train Loss: 0.004545
Validation Loss: 0.00510019
Epoch [153/200], Train Loss: 0.004515
Validation Loss: 0.00510016
Epoch [154/200], Train Loss: 0.004556
Validation Loss: 0.00510054
Epoch [155/200], Train Loss: 0.004534
Validation Loss: 0.00510045
Epoch [156/200], Train Loss: 0.004529
Validation Loss: 0.00510074
Epoch [157/200], Train Loss: 0.004533
Validation Loss: 0.00510088
Epoch [158/200], Train Loss: 0.004537
Validation Loss: 0.00510025
Epoch [159/200], Train Loss: 0.004606
Validation Loss: 0.00510037
Epoch [160/200], Train Loss: 0.004617
Validation Loss: 0.00510015
Epoch [161/200], Train Loss: 0.004525
Validation Loss: 0.00509913
Epoch [162/200], Train Loss: 0.004630
Validation Loss: 0.00509928
Epoch [163/200], Train Loss: 0.004546
Validation Loss: 0.00509912
Epoch [164/200], Train Loss: 0.004553
Validation Loss: 0.00509875
Epoch [165/200], Train Loss: 0.004519
Validation Loss: 0.00509857
Epoch [166/200], Train Loss: 0.004621
Validation Loss: 0.00509909
Epoch [167/200], Train Loss: 0.004571
Validation Loss: 0.00509893
Epoch [168/200], Train Loss: 0.004589
Validation Loss: 0.00509870
Epoch [169/200], Train Loss: 0.004525
Validation Loss: 0.00509826
Epoch [170/200], Train Loss: 0.004560
Validation Loss: 0.00509796
Epoch [171/200], Train Loss: 0.004549
Validation Loss: 0.00509781
Epoch [172/200], Train Loss: 0.004574
Validation Loss: 0.00509842
Epoch [173/200], Train Loss: 0.004564
Validation Loss: 0.00509889
Epoch [174/200], Train Loss: 0.004525
Validation Loss: 0.00509827
Epoch [175/200], Train Loss: 0.004515
Validation Loss: 0.00509807
Epoch [176/200], Train Loss: 0.004573
Validation Loss: 0.00509798
Epoch [177/200], Train Loss: 0.004605
Validation Loss: 0.00509816
Epoch [178/200], Train Loss: 0.004611
Validation Loss: 0.00509784
Epoch [179/200], Train Loss: 0.004576
Validation Loss: 0.00509709
Epoch [180/200], Train Loss: 0.004538
Validation Loss: 0.00509629
Epoch [181/200], Train Loss: 0.004622
Validation Loss: 0.00509666
Epoch [182/200], Train Loss: 0.004570
Validation Loss: 0.00509672
Epoch [183/200], Train Loss: 0.004566
Validation Loss: 0.00509665
Epoch [184/200], Train Loss: 0.004504
Validation Loss: 0.00509637
Epoch [185/200], Train Loss: 0.004565
Validation Loss: 0.00509669
Epoch [186/200], Train Loss: 0.004541
Validation Loss: 0.00509655
Epoch [187/200], Train Loss: 0.004578
Validation Loss: 0.00509631
Epoch [188/200], Train Loss: 0.004563
Validation Loss: 0.00509612
Epoch [189/200], Train Loss: 0.004585
Validation Loss: 0.00509612
Epoch [190/200], Train Loss: 0.004528
Validation Loss: 0.00509566
Epoch [191/200], Train Loss: 0.004584
Validation Loss: 0.00509540
Epoch [192/200], Train Loss: 0.004580
Validation Loss: 0.00509572
Epoch [193/200], Train Loss: 0.004554
Validation Loss: 0.00509531
Epoch [194/200], Train Loss: 0.004635
Validation Loss: 0.00509552
Epoch [195/200], Train Loss: 0.004615
Validation Loss: 0.00509541
Epoch [196/200], Train Loss: 0.004561
Validation Loss: 0.00509481
Epoch [197/200], Train Loss: 0.004560
Validation Loss: 0.00509469
Epoch [198/200], Train Loss: 0.004536
Validation Loss: 0.00509444
Epoch [199/200], Train Loss: 0.004587
Validation Loss: 0.00509482
Epoch [200/200], Train Loss: 0.004547
Validation Loss: 0.00509486

Evaluating model for: Lamp
Run 93/144 completed in 137.82 seconds with: {'MAE': np.float32(3.0600872), 'MSE': np.float32(208.40445), 'RMSE': np.float32(14.43622), 'SAE': np.float32(0.28376597), 'NDE': np.float32(0.979126)}

Run 94/144: hidden=128, seq_len=1080, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.006587
Validation Loss: 0.00669324
Epoch [2/200], Train Loss: 0.005945
Validation Loss: 0.00612357
Epoch [3/200], Train Loss: 0.005425
Validation Loss: 0.00569087
Epoch [4/200], Train Loss: 0.005001
Validation Loss: 0.00540898
Epoch [5/200], Train Loss: 0.004776
Validation Loss: 0.00526640
Epoch [6/200], Train Loss: 0.004691
Validation Loss: 0.00524233
Epoch [7/200], Train Loss: 0.004700
Validation Loss: 0.00527525
Epoch [8/200], Train Loss: 0.004728
Validation Loss: 0.00529349
Epoch [9/200], Train Loss: 0.004714
Validation Loss: 0.00527746
Epoch [10/200], Train Loss: 0.004693
Validation Loss: 0.00525119
Epoch [11/200], Train Loss: 0.004702
Validation Loss: 0.00523554
Epoch [12/200], Train Loss: 0.004657
Validation Loss: 0.00523120
Epoch [13/200], Train Loss: 0.004646
Validation Loss: 0.00523148
Epoch [14/200], Train Loss: 0.004615
Validation Loss: 0.00523098
Epoch [15/200], Train Loss: 0.004644
Validation Loss: 0.00522854
Epoch [16/200], Train Loss: 0.004693
Validation Loss: 0.00522660
Epoch [17/200], Train Loss: 0.004667
Validation Loss: 0.00522583
Epoch [18/200], Train Loss: 0.004701
Validation Loss: 0.00522565
Epoch [19/200], Train Loss: 0.004614
Validation Loss: 0.00522501
Epoch [20/200], Train Loss: 0.004689
Validation Loss: 0.00522414
Epoch [21/200], Train Loss: 0.004718
Validation Loss: 0.00522279
Epoch [22/200], Train Loss: 0.004709
Validation Loss: 0.00522129
Epoch [23/200], Train Loss: 0.004619
Validation Loss: 0.00521949
Epoch [24/200], Train Loss: 0.004654
Validation Loss: 0.00521804
Epoch [25/200], Train Loss: 0.004701
Validation Loss: 0.00521674
Epoch [26/200], Train Loss: 0.004667
Validation Loss: 0.00521533
Epoch [27/200], Train Loss: 0.004663
Validation Loss: 0.00521397
Epoch [28/200], Train Loss: 0.004618
Validation Loss: 0.00521247
Epoch [29/200], Train Loss: 0.004689
Validation Loss: 0.00521125
Epoch [30/200], Train Loss: 0.004637
Validation Loss: 0.00520955
Epoch [31/200], Train Loss: 0.004723
Validation Loss: 0.00520806
Epoch [32/200], Train Loss: 0.004586
Validation Loss: 0.00520571
Epoch [33/200], Train Loss: 0.004612
Validation Loss: 0.00520380
Epoch [34/200], Train Loss: 0.004672
Validation Loss: 0.00520185
Epoch [35/200], Train Loss: 0.004729
Validation Loss: 0.00519988
Epoch [36/200], Train Loss: 0.004679
Validation Loss: 0.00519755
Epoch [37/200], Train Loss: 0.004642
Validation Loss: 0.00519469
Epoch [38/200], Train Loss: 0.004615
Validation Loss: 0.00519173
Epoch [39/200], Train Loss: 0.004681
Validation Loss: 0.00518894
Epoch [40/200], Train Loss: 0.004651
Validation Loss: 0.00518602
Epoch [41/200], Train Loss: 0.004655
Validation Loss: 0.00518273
Epoch [42/200], Train Loss: 0.004628
Validation Loss: 0.00517866
Epoch [43/200], Train Loss: 0.004623
Validation Loss: 0.00517468
Epoch [44/200], Train Loss: 0.004631
Validation Loss: 0.00517026
Epoch [45/200], Train Loss: 0.004656
Validation Loss: 0.00516561
Epoch [46/200], Train Loss: 0.004606
Validation Loss: 0.00516095
Epoch [47/200], Train Loss: 0.004610
Validation Loss: 0.00515634
Epoch [48/200], Train Loss: 0.004605
Validation Loss: 0.00515139
Epoch [49/200], Train Loss: 0.004637
Validation Loss: 0.00514639
Epoch [50/200], Train Loss: 0.004659
Validation Loss: 0.00514181
Epoch [51/200], Train Loss: 0.004662
Validation Loss: 0.00513667
Epoch [52/200], Train Loss: 0.004600
Validation Loss: 0.00513038
Epoch [53/200], Train Loss: 0.004608
Validation Loss: 0.00512546
Epoch [54/200], Train Loss: 0.004608
Validation Loss: 0.00512195
Epoch [55/200], Train Loss: 0.004576
Validation Loss: 0.00511921
Epoch [56/200], Train Loss: 0.004590
Validation Loss: 0.00511891
Epoch [57/200], Train Loss: 0.004569
Validation Loss: 0.00511523
Epoch [58/200], Train Loss: 0.004613
Validation Loss: 0.00511476
Epoch [59/200], Train Loss: 0.004603
Validation Loss: 0.00511486
Epoch [60/200], Train Loss: 0.004689
Validation Loss: 0.00511479
Epoch [61/200], Train Loss: 0.004543
Validation Loss: 0.00511257
Epoch [62/200], Train Loss: 0.004624
Validation Loss: 0.00511338
Epoch [63/200], Train Loss: 0.004551
Validation Loss: 0.00511363
Epoch [64/200], Train Loss: 0.004578
Validation Loss: 0.00511478
Epoch [65/200], Train Loss: 0.004575
Validation Loss: 0.00511430
Epoch [66/200], Train Loss: 0.004649
Validation Loss: 0.00511439
Epoch [67/200], Train Loss: 0.004576
Validation Loss: 0.00511386
Epoch [68/200], Train Loss: 0.004565
Validation Loss: 0.00511379
Epoch [69/200], Train Loss: 0.004574
Validation Loss: 0.00511355
Epoch [70/200], Train Loss: 0.004569
Validation Loss: 0.00511398
Epoch [71/200], Train Loss: 0.004622
Validation Loss: 0.00511664
Early stopping triggered

Evaluating model for: Lamp
Run 94/144 completed in 52.88 seconds with: {'MAE': np.float32(3.0859883), 'MSE': np.float32(209.27559), 'RMSE': np.float32(14.466361), 'SAE': np.float32(0.35614404), 'NDE': np.float32(0.9811704)}

Run 95/144: hidden=128, seq_len=1080, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.006271
Validation Loss: 0.00632550
Epoch [2/200], Train Loss: 0.005587
Validation Loss: 0.00580427
Epoch [3/200], Train Loss: 0.005141
Validation Loss: 0.00545453
Epoch [4/200], Train Loss: 0.004798
Validation Loss: 0.00527154
Epoch [5/200], Train Loss: 0.004695
Validation Loss: 0.00522815
Epoch [6/200], Train Loss: 0.004710
Validation Loss: 0.00526195
Epoch [7/200], Train Loss: 0.004703
Validation Loss: 0.00528471
Epoch [8/200], Train Loss: 0.004683
Validation Loss: 0.00526941
Epoch [9/200], Train Loss: 0.004659
Validation Loss: 0.00524396
Epoch [10/200], Train Loss: 0.004653
Validation Loss: 0.00522814
Epoch [11/200], Train Loss: 0.004674
Validation Loss: 0.00522401
Epoch [12/200], Train Loss: 0.004664
Validation Loss: 0.00522453
Epoch [13/200], Train Loss: 0.004662
Validation Loss: 0.00522469
Epoch [14/200], Train Loss: 0.004632
Validation Loss: 0.00522361
Epoch [15/200], Train Loss: 0.004638
Validation Loss: 0.00522252
Epoch [16/200], Train Loss: 0.004669
Validation Loss: 0.00522252
Epoch [17/200], Train Loss: 0.004624
Validation Loss: 0.00522295
Epoch [18/200], Train Loss: 0.004665
Validation Loss: 0.00522337
Epoch [19/200], Train Loss: 0.004647
Validation Loss: 0.00522300
Epoch [20/200], Train Loss: 0.004733
Validation Loss: 0.00522237
Epoch [21/200], Train Loss: 0.004694
Validation Loss: 0.00522123
Epoch [22/200], Train Loss: 0.004639
Validation Loss: 0.00522017
Epoch [23/200], Train Loss: 0.004679
Validation Loss: 0.00521950
Epoch [24/200], Train Loss: 0.004632
Validation Loss: 0.00521897
Epoch [25/200], Train Loss: 0.004645
Validation Loss: 0.00521854
Epoch [26/200], Train Loss: 0.004661
Validation Loss: 0.00521818
Epoch [27/200], Train Loss: 0.004627
Validation Loss: 0.00521774
Epoch [28/200], Train Loss: 0.004675
Validation Loss: 0.00521729
Epoch [29/200], Train Loss: 0.004639
Validation Loss: 0.00521628
Epoch [30/200], Train Loss: 0.004682
Validation Loss: 0.00521521
Epoch [31/200], Train Loss: 0.004595
Validation Loss: 0.00521387
Epoch [32/200], Train Loss: 0.004629
Validation Loss: 0.00521279
Epoch [33/200], Train Loss: 0.004692
Validation Loss: 0.00521186
Epoch [34/200], Train Loss: 0.004647
Validation Loss: 0.00521082
Epoch [35/200], Train Loss: 0.004630
Validation Loss: 0.00520947
Epoch [36/200], Train Loss: 0.004668
Validation Loss: 0.00520812
Epoch [37/200], Train Loss: 0.004671
Validation Loss: 0.00520676
Epoch [38/200], Train Loss: 0.004677
Validation Loss: 0.00520476
Epoch [39/200], Train Loss: 0.004649
Validation Loss: 0.00520265
Epoch [40/200], Train Loss: 0.004656
Validation Loss: 0.00520043
Epoch [41/200], Train Loss: 0.004717
Validation Loss: 0.00519793
Epoch [42/200], Train Loss: 0.004644
Validation Loss: 0.00519439
Epoch [43/200], Train Loss: 0.004643
Validation Loss: 0.00519059
Epoch [44/200], Train Loss: 0.004668
Validation Loss: 0.00518642
Epoch [45/200], Train Loss: 0.004636
Validation Loss: 0.00518206
Epoch [46/200], Train Loss: 0.004604
Validation Loss: 0.00517735
Epoch [47/200], Train Loss: 0.004617
Validation Loss: 0.00517167
Epoch [48/200], Train Loss: 0.004630
Validation Loss: 0.00516530
Epoch [49/200], Train Loss: 0.004651
Validation Loss: 0.00515811
Epoch [50/200], Train Loss: 0.004598
Validation Loss: 0.00515037
Epoch [51/200], Train Loss: 0.004608
Validation Loss: 0.00514224
Epoch [52/200], Train Loss: 0.004602
Validation Loss: 0.00513462
Epoch [53/200], Train Loss: 0.004627
Validation Loss: 0.00512763
Epoch [54/200], Train Loss: 0.004634
Validation Loss: 0.00512134
Epoch [55/200], Train Loss: 0.004546
Validation Loss: 0.00511618
Epoch [56/200], Train Loss: 0.004547
Validation Loss: 0.00511650
Epoch [57/200], Train Loss: 0.004523
Validation Loss: 0.00511599
Epoch [58/200], Train Loss: 0.004594
Validation Loss: 0.00511508
Epoch [59/200], Train Loss: 0.004577
Validation Loss: 0.00511392
Epoch [60/200], Train Loss: 0.004527
Validation Loss: 0.00511237
Epoch [61/200], Train Loss: 0.004582
Validation Loss: 0.00511409
Epoch [62/200], Train Loss: 0.004607
Validation Loss: 0.00511471
Epoch [63/200], Train Loss: 0.004598
Validation Loss: 0.00511345
Epoch [64/200], Train Loss: 0.004539
Validation Loss: 0.00511167
Epoch [65/200], Train Loss: 0.004578
Validation Loss: 0.00511269
Epoch [66/200], Train Loss: 0.004553
Validation Loss: 0.00511276
Epoch [67/200], Train Loss: 0.004583
Validation Loss: 0.00511206
Epoch [68/200], Train Loss: 0.004600
Validation Loss: 0.00511297
Epoch [69/200], Train Loss: 0.004575
Validation Loss: 0.00511172
Epoch [70/200], Train Loss: 0.004613
Validation Loss: 0.00511174
Epoch [71/200], Train Loss: 0.004608
Validation Loss: 0.00511133
Epoch [72/200], Train Loss: 0.004556
Validation Loss: 0.00510971
Epoch [73/200], Train Loss: 0.004558
Validation Loss: 0.00510872
Epoch [74/200], Train Loss: 0.004568
Validation Loss: 0.00510973
Epoch [75/200], Train Loss: 0.004565
Validation Loss: 0.00511195
Epoch [76/200], Train Loss: 0.004536
Validation Loss: 0.00511117
Epoch [77/200], Train Loss: 0.004607
Validation Loss: 0.00511002
Epoch [78/200], Train Loss: 0.004625
Validation Loss: 0.00510923
Epoch [79/200], Train Loss: 0.004617
Validation Loss: 0.00510779
Epoch [80/200], Train Loss: 0.004575
Validation Loss: 0.00510785
Epoch [81/200], Train Loss: 0.004639
Validation Loss: 0.00511045
Epoch [82/200], Train Loss: 0.004576
Validation Loss: 0.00510901
Epoch [83/200], Train Loss: 0.004564
Validation Loss: 0.00510738
Epoch [84/200], Train Loss: 0.004515
Validation Loss: 0.00510654
Epoch [85/200], Train Loss: 0.004616
Validation Loss: 0.00510910
Epoch [86/200], Train Loss: 0.004594
Validation Loss: 0.00510921
Epoch [87/200], Train Loss: 0.004586
Validation Loss: 0.00510742
Epoch [88/200], Train Loss: 0.004558
Validation Loss: 0.00510682
Epoch [89/200], Train Loss: 0.004597
Validation Loss: 0.00510700
Epoch [90/200], Train Loss: 0.004588
Validation Loss: 0.00510721
Epoch [91/200], Train Loss: 0.004572
Validation Loss: 0.00510671
Epoch [92/200], Train Loss: 0.004519
Validation Loss: 0.00510614
Epoch [93/200], Train Loss: 0.004582
Validation Loss: 0.00510712
Epoch [94/200], Train Loss: 0.004559
Validation Loss: 0.00510795
Epoch [95/200], Train Loss: 0.004589
Validation Loss: 0.00510792
Epoch [96/200], Train Loss: 0.004596
Validation Loss: 0.00510586
Epoch [97/200], Train Loss: 0.004563
Validation Loss: 0.00510493
Epoch [98/200], Train Loss: 0.004532
Validation Loss: 0.00510636
Epoch [99/200], Train Loss: 0.004579
Validation Loss: 0.00510738
Epoch [100/200], Train Loss: 0.004529
Validation Loss: 0.00510628
Epoch [101/200], Train Loss: 0.004584
Validation Loss: 0.00510573
Epoch [102/200], Train Loss: 0.004567
Validation Loss: 0.00510572
Epoch [103/200], Train Loss: 0.004607
Validation Loss: 0.00510550
Epoch [104/200], Train Loss: 0.004546
Validation Loss: 0.00510480
Epoch [105/200], Train Loss: 0.004550
Validation Loss: 0.00510465
Epoch [106/200], Train Loss: 0.004569
Validation Loss: 0.00510478
Epoch [107/200], Train Loss: 0.004564
Validation Loss: 0.00510494
Epoch [108/200], Train Loss: 0.004555
Validation Loss: 0.00510462
Epoch [109/200], Train Loss: 0.004534
Validation Loss: 0.00510450
Epoch [110/200], Train Loss: 0.004567
Validation Loss: 0.00510452
Epoch [111/200], Train Loss: 0.004588
Validation Loss: 0.00510493
Epoch [112/200], Train Loss: 0.004588
Validation Loss: 0.00510463
Epoch [113/200], Train Loss: 0.004555
Validation Loss: 0.00510380
Epoch [114/200], Train Loss: 0.004578
Validation Loss: 0.00510405
Epoch [115/200], Train Loss: 0.004521
Validation Loss: 0.00510300
Epoch [116/200], Train Loss: 0.004537
Validation Loss: 0.00510309
Epoch [117/200], Train Loss: 0.004531
Validation Loss: 0.00510397
Epoch [118/200], Train Loss: 0.004571
Validation Loss: 0.00510440
Epoch [119/200], Train Loss: 0.004571
Validation Loss: 0.00510468
Epoch [120/200], Train Loss: 0.004609
Validation Loss: 0.00510339
Epoch [121/200], Train Loss: 0.004564
Validation Loss: 0.00510208
Epoch [122/200], Train Loss: 0.004583
Validation Loss: 0.00510198
Epoch [123/200], Train Loss: 0.004556
Validation Loss: 0.00510251
Epoch [124/200], Train Loss: 0.004544
Validation Loss: 0.00510298
Epoch [125/200], Train Loss: 0.004584
Validation Loss: 0.00510408
Epoch [126/200], Train Loss: 0.004573
Validation Loss: 0.00510306
Epoch [127/200], Train Loss: 0.004583
Validation Loss: 0.00510268
Epoch [128/200], Train Loss: 0.004567
Validation Loss: 0.00510173
Epoch [129/200], Train Loss: 0.004583
Validation Loss: 0.00510185
Epoch [130/200], Train Loss: 0.004606
Validation Loss: 0.00510238
Epoch [131/200], Train Loss: 0.004580
Validation Loss: 0.00510231
Epoch [132/200], Train Loss: 0.004553
Validation Loss: 0.00510188
Epoch [133/200], Train Loss: 0.004592
Validation Loss: 0.00510149
Epoch [134/200], Train Loss: 0.004583
Validation Loss: 0.00510194
Epoch [135/200], Train Loss: 0.004549
Validation Loss: 0.00510077
Epoch [136/200], Train Loss: 0.004566
Validation Loss: 0.00510091
Epoch [137/200], Train Loss: 0.004546
Validation Loss: 0.00510065
Epoch [138/200], Train Loss: 0.004586
Validation Loss: 0.00510106
Epoch [139/200], Train Loss: 0.004571
Validation Loss: 0.00510140
Epoch [140/200], Train Loss: 0.004622
Validation Loss: 0.00510204
Epoch [141/200], Train Loss: 0.004534
Validation Loss: 0.00510055
Epoch [142/200], Train Loss: 0.004526
Validation Loss: 0.00509928
Epoch [143/200], Train Loss: 0.004623
Validation Loss: 0.00510046
Epoch [144/200], Train Loss: 0.004614
Validation Loss: 0.00510185
Epoch [145/200], Train Loss: 0.004578
Validation Loss: 0.00510065
Epoch [146/200], Train Loss: 0.004571
Validation Loss: 0.00509998
Epoch [147/200], Train Loss: 0.004558
Validation Loss: 0.00509907
Epoch [148/200], Train Loss: 0.004569
Validation Loss: 0.00509915
Epoch [149/200], Train Loss: 0.004538
Validation Loss: 0.00509957
Epoch [150/200], Train Loss: 0.004593
Validation Loss: 0.00510042
Epoch [151/200], Train Loss: 0.004600
Validation Loss: 0.00510029
Epoch [152/200], Train Loss: 0.004541
Validation Loss: 0.00509864
Epoch [153/200], Train Loss: 0.004567
Validation Loss: 0.00509874
Epoch [154/200], Train Loss: 0.004570
Validation Loss: 0.00509857
Epoch [155/200], Train Loss: 0.004547
Validation Loss: 0.00509881
Epoch [156/200], Train Loss: 0.004545
Validation Loss: 0.00509865
Epoch [157/200], Train Loss: 0.004554
Validation Loss: 0.00509868
Epoch [158/200], Train Loss: 0.004614
Validation Loss: 0.00509899
Epoch [159/200], Train Loss: 0.004516
Validation Loss: 0.00509772
Epoch [160/200], Train Loss: 0.004573
Validation Loss: 0.00509827
Epoch [161/200], Train Loss: 0.004521
Validation Loss: 0.00509744
Epoch [162/200], Train Loss: 0.004551
Validation Loss: 0.00509831
Epoch [163/200], Train Loss: 0.004541
Validation Loss: 0.00509802
Epoch [164/200], Train Loss: 0.004575
Validation Loss: 0.00509846
Epoch [165/200], Train Loss: 0.004561
Validation Loss: 0.00509863
Epoch [166/200], Train Loss: 0.004538
Validation Loss: 0.00509857
Epoch [167/200], Train Loss: 0.004566
Validation Loss: 0.00509754
Epoch [168/200], Train Loss: 0.004522
Validation Loss: 0.00509721
Epoch [169/200], Train Loss: 0.004598
Validation Loss: 0.00509754
Epoch [170/200], Train Loss: 0.004579
Validation Loss: 0.00509676
Epoch [171/200], Train Loss: 0.004568
Validation Loss: 0.00509644
Epoch [172/200], Train Loss: 0.004591
Validation Loss: 0.00509775
Epoch [173/200], Train Loss: 0.004588
Validation Loss: 0.00509776
Epoch [174/200], Train Loss: 0.004546
Validation Loss: 0.00509724
Epoch [175/200], Train Loss: 0.004576
Validation Loss: 0.00509625
Epoch [176/200], Train Loss: 0.004578
Validation Loss: 0.00509609
Epoch [177/200], Train Loss: 0.004517
Validation Loss: 0.00509578
Epoch [178/200], Train Loss: 0.004539
Validation Loss: 0.00509666
Epoch [179/200], Train Loss: 0.004541
Validation Loss: 0.00509708
Epoch [180/200], Train Loss: 0.004586
Validation Loss: 0.00509701
Epoch [181/200], Train Loss: 0.004575
Validation Loss: 0.00509664
Epoch [182/200], Train Loss: 0.004618
Validation Loss: 0.00509624
Epoch [183/200], Train Loss: 0.004614
Validation Loss: 0.00509539
Epoch [184/200], Train Loss: 0.004558
Validation Loss: 0.00509494
Epoch [185/200], Train Loss: 0.004605
Validation Loss: 0.00509547
Epoch [186/200], Train Loss: 0.004529
Validation Loss: 0.00509599
Epoch [187/200], Train Loss: 0.004540
Validation Loss: 0.00509566
Epoch [188/200], Train Loss: 0.004529
Validation Loss: 0.00509491
Epoch [189/200], Train Loss: 0.004566
Validation Loss: 0.00509634
Epoch [190/200], Train Loss: 0.004574
Validation Loss: 0.00509590
Epoch [191/200], Train Loss: 0.004545
Validation Loss: 0.00509500
Epoch [192/200], Train Loss: 0.004535
Validation Loss: 0.00509482
Epoch [193/200], Train Loss: 0.004564
Validation Loss: 0.00509573
Epoch [194/200], Train Loss: 0.004551
Validation Loss: 0.00509497
Epoch [195/200], Train Loss: 0.004601
Validation Loss: 0.00509501
Epoch [196/200], Train Loss: 0.004530
Validation Loss: 0.00509454
Epoch [197/200], Train Loss: 0.004487
Validation Loss: 0.00509413
Epoch [198/200], Train Loss: 0.004571
Validation Loss: 0.00509461
Epoch [199/200], Train Loss: 0.004630
Validation Loss: 0.00509560
Epoch [200/200], Train Loss: 0.004551
Validation Loss: 0.00509461

Evaluating model for: Lamp
Run 95/144 completed in 160.33 seconds with: {'MAE': np.float32(3.036199), 'MSE': np.float32(208.45728), 'RMSE': np.float32(14.438049), 'SAE': np.float32(0.28248265), 'NDE': np.float32(0.97925013)}

Run 96/144: hidden=128, seq_len=1080, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.006534
Validation Loss: 0.00671068
Epoch [2/200], Train Loss: 0.005837
Validation Loss: 0.00608654
Epoch [3/200], Train Loss: 0.005284
Validation Loss: 0.00561530
Epoch [4/200], Train Loss: 0.004925
Validation Loss: 0.00533174
Epoch [5/200], Train Loss: 0.004724
Validation Loss: 0.00522626
Epoch [6/200], Train Loss: 0.004706
Validation Loss: 0.00524457
Epoch [7/200], Train Loss: 0.004748
Validation Loss: 0.00527248
Epoch [8/200], Train Loss: 0.004694
Validation Loss: 0.00525686
Epoch [9/200], Train Loss: 0.004703
Validation Loss: 0.00523121
Epoch [10/200], Train Loss: 0.004722
Validation Loss: 0.00522284
Epoch [11/200], Train Loss: 0.004694
Validation Loss: 0.00522856
Epoch [12/200], Train Loss: 0.004727
Validation Loss: 0.00523490
Epoch [13/200], Train Loss: 0.004741
Validation Loss: 0.00523479
Epoch [14/200], Train Loss: 0.004655
Validation Loss: 0.00522952
Epoch [15/200], Train Loss: 0.004659
Validation Loss: 0.00522456
Epoch [16/200], Train Loss: 0.004680
Validation Loss: 0.00522260
Epoch [17/200], Train Loss: 0.004663
Validation Loss: 0.00522204
Epoch [18/200], Train Loss: 0.004665
Validation Loss: 0.00522191
Epoch [19/200], Train Loss: 0.004673
Validation Loss: 0.00522187
Epoch [20/200], Train Loss: 0.004700
Validation Loss: 0.00522210
Epoch [21/200], Train Loss: 0.004632
Validation Loss: 0.00522252
Epoch [22/200], Train Loss: 0.004702
Validation Loss: 0.00522301
Epoch [23/200], Train Loss: 0.004731
Validation Loss: 0.00522310
Epoch [24/200], Train Loss: 0.004682
Validation Loss: 0.00522233
Epoch [25/200], Train Loss: 0.004669
Validation Loss: 0.00522166
Epoch [26/200], Train Loss: 0.004664
Validation Loss: 0.00522110
Epoch [27/200], Train Loss: 0.004719
Validation Loss: 0.00522102
Epoch [28/200], Train Loss: 0.004621
Validation Loss: 0.00522056
Epoch [29/200], Train Loss: 0.004653
Validation Loss: 0.00522055
Epoch [30/200], Train Loss: 0.004664
Validation Loss: 0.00522064
Epoch [31/200], Train Loss: 0.004694
Validation Loss: 0.00522031
Epoch [32/200], Train Loss: 0.004680
Validation Loss: 0.00521973
Epoch [33/200], Train Loss: 0.004727
Validation Loss: 0.00521947
Epoch [34/200], Train Loss: 0.004661
Validation Loss: 0.00521870
Epoch [35/200], Train Loss: 0.004696
Validation Loss: 0.00521837
Epoch [36/200], Train Loss: 0.004670
Validation Loss: 0.00521765
Epoch [37/200], Train Loss: 0.004676
Validation Loss: 0.00521703
Epoch [38/200], Train Loss: 0.004652
Validation Loss: 0.00521627
Epoch [39/200], Train Loss: 0.004710
Validation Loss: 0.00521595
Epoch [40/200], Train Loss: 0.004668
Validation Loss: 0.00521495
Epoch [41/200], Train Loss: 0.004657
Validation Loss: 0.00521372
Epoch [42/200], Train Loss: 0.004693
Validation Loss: 0.00521253
Epoch [43/200], Train Loss: 0.004641
Validation Loss: 0.00521113
Epoch [44/200], Train Loss: 0.004660
Validation Loss: 0.00520980
Epoch [45/200], Train Loss: 0.004668
Validation Loss: 0.00520809
Epoch [46/200], Train Loss: 0.004697
Validation Loss: 0.00520619
Epoch [47/200], Train Loss: 0.004637
Validation Loss: 0.00520344
Epoch [48/200], Train Loss: 0.004657
Validation Loss: 0.00520113
Epoch [49/200], Train Loss: 0.004687
Validation Loss: 0.00519781
Epoch [50/200], Train Loss: 0.004639
Validation Loss: 0.00519315
Epoch [51/200], Train Loss: 0.004647
Validation Loss: 0.00518822
Epoch [52/200], Train Loss: 0.004658
Validation Loss: 0.00518306
Epoch [53/200], Train Loss: 0.004672
Validation Loss: 0.00517681
Epoch [54/200], Train Loss: 0.004550
Validation Loss: 0.00516727
Epoch [55/200], Train Loss: 0.004649
Validation Loss: 0.00516093
Epoch [56/200], Train Loss: 0.004620
Validation Loss: 0.00515267
Epoch [57/200], Train Loss: 0.004650
Validation Loss: 0.00514262
Epoch [58/200], Train Loss: 0.004631
Validation Loss: 0.00513092
Epoch [59/200], Train Loss: 0.004612
Validation Loss: 0.00512443
Epoch [60/200], Train Loss: 0.004605
Validation Loss: 0.00512190
Epoch [61/200], Train Loss: 0.004600
Validation Loss: 0.00511596
Epoch [62/200], Train Loss: 0.004557
Validation Loss: 0.00511391
Epoch [63/200], Train Loss: 0.004540
Validation Loss: 0.00511465
Epoch [64/200], Train Loss: 0.004588
Validation Loss: 0.00512055
Epoch [65/200], Train Loss: 0.004597
Validation Loss: 0.00511407
Epoch [66/200], Train Loss: 0.004589
Validation Loss: 0.00511191
Epoch [67/200], Train Loss: 0.004574
Validation Loss: 0.00511334
Epoch [68/200], Train Loss: 0.004606
Validation Loss: 0.00511443
Epoch [69/200], Train Loss: 0.004600
Validation Loss: 0.00511230
Epoch [70/200], Train Loss: 0.004598
Validation Loss: 0.00511130
Epoch [71/200], Train Loss: 0.004588
Validation Loss: 0.00511167
Epoch [72/200], Train Loss: 0.004603
Validation Loss: 0.00511371
Epoch [73/200], Train Loss: 0.004606
Validation Loss: 0.00511074
Epoch [74/200], Train Loss: 0.004586
Validation Loss: 0.00510842
Epoch [75/200], Train Loss: 0.004562
Validation Loss: 0.00510933
Epoch [76/200], Train Loss: 0.004566
Validation Loss: 0.00511153
Epoch [77/200], Train Loss: 0.004620
Validation Loss: 0.00510992
Epoch [78/200], Train Loss: 0.004593
Validation Loss: 0.00510743
Epoch [79/200], Train Loss: 0.004531
Validation Loss: 0.00510682
Epoch [80/200], Train Loss: 0.004543
Validation Loss: 0.00510819
Epoch [81/200], Train Loss: 0.004638
Validation Loss: 0.00511176
Epoch [82/200], Train Loss: 0.004556
Validation Loss: 0.00510638
Epoch [83/200], Train Loss: 0.004547
Validation Loss: 0.00510575
Epoch [84/200], Train Loss: 0.004563
Validation Loss: 0.00510917
Epoch [85/200], Train Loss: 0.004629
Validation Loss: 0.00511114
Epoch [86/200], Train Loss: 0.004581
Validation Loss: 0.00510586
Epoch [87/200], Train Loss: 0.004614
Validation Loss: 0.00510570
Epoch [88/200], Train Loss: 0.004599
Validation Loss: 0.00510665
Epoch [89/200], Train Loss: 0.004597
Validation Loss: 0.00510685
Epoch [90/200], Train Loss: 0.004649
Validation Loss: 0.00510824
Epoch [91/200], Train Loss: 0.004575
Validation Loss: 0.00510487
Epoch [92/200], Train Loss: 0.004622
Validation Loss: 0.00510499
Epoch [93/200], Train Loss: 0.004529
Validation Loss: 0.00510479
Epoch [94/200], Train Loss: 0.004559
Validation Loss: 0.00510747
Epoch [95/200], Train Loss: 0.004620
Validation Loss: 0.00510697
Epoch [96/200], Train Loss: 0.004565
Validation Loss: 0.00510484
Epoch [97/200], Train Loss: 0.004598
Validation Loss: 0.00510476
Epoch [98/200], Train Loss: 0.004558
Validation Loss: 0.00510413
Epoch [99/200], Train Loss: 0.004609
Validation Loss: 0.00510512
Epoch [100/200], Train Loss: 0.004570
Validation Loss: 0.00510396
Epoch [101/200], Train Loss: 0.004565
Validation Loss: 0.00510396
Epoch [102/200], Train Loss: 0.004513
Validation Loss: 0.00510447
Epoch [103/200], Train Loss: 0.004644
Validation Loss: 0.00510651
Epoch [104/200], Train Loss: 0.004565
Validation Loss: 0.00510235
Epoch [105/200], Train Loss: 0.004595
Validation Loss: 0.00510307
Epoch [106/200], Train Loss: 0.004554
Validation Loss: 0.00510299
Epoch [107/200], Train Loss: 0.004597
Validation Loss: 0.00510420
Epoch [108/200], Train Loss: 0.004603
Validation Loss: 0.00510406
Epoch [109/200], Train Loss: 0.004562
Validation Loss: 0.00510212
Epoch [110/200], Train Loss: 0.004595
Validation Loss: 0.00510196
Epoch [111/200], Train Loss: 0.004593
Validation Loss: 0.00510234
Epoch [112/200], Train Loss: 0.004531
Validation Loss: 0.00510241
Epoch [113/200], Train Loss: 0.004566
Validation Loss: 0.00510337
Epoch [114/200], Train Loss: 0.004562
Validation Loss: 0.00510300
Epoch [115/200], Train Loss: 0.004612
Validation Loss: 0.00510129
Epoch [116/200], Train Loss: 0.004539
Validation Loss: 0.00510088
Epoch [117/200], Train Loss: 0.004543
Validation Loss: 0.00510202
Epoch [118/200], Train Loss: 0.004604
Validation Loss: 0.00510359
Epoch [119/200], Train Loss: 0.004562
Validation Loss: 0.00510116
Epoch [120/200], Train Loss: 0.004543
Validation Loss: 0.00509989
Epoch [121/200], Train Loss: 0.004579
Validation Loss: 0.00510144
Epoch [122/200], Train Loss: 0.004557
Validation Loss: 0.00510137
Epoch [123/200], Train Loss: 0.004580
Validation Loss: 0.00510089
Epoch [124/200], Train Loss: 0.004572
Validation Loss: 0.00510053
Epoch [125/200], Train Loss: 0.004573
Validation Loss: 0.00510066
Epoch [126/200], Train Loss: 0.004589
Validation Loss: 0.00510104
Epoch [127/200], Train Loss: 0.004615
Validation Loss: 0.00510087
Epoch [128/200], Train Loss: 0.004631
Validation Loss: 0.00510039
Epoch [129/200], Train Loss: 0.004569
Validation Loss: 0.00509907
Epoch [130/200], Train Loss: 0.004579
Validation Loss: 0.00509871
Epoch [131/200], Train Loss: 0.004517
Validation Loss: 0.00509981
Epoch [132/200], Train Loss: 0.004543
Validation Loss: 0.00510281
Epoch [133/200], Train Loss: 0.004612
Validation Loss: 0.00510139
Epoch [134/200], Train Loss: 0.004630
Validation Loss: 0.00509914
Epoch [135/200], Train Loss: 0.004499
Validation Loss: 0.00509738
Epoch [136/200], Train Loss: 0.004591
Validation Loss: 0.00509914
Epoch [137/200], Train Loss: 0.004588
Validation Loss: 0.00510223
Epoch [138/200], Train Loss: 0.004614
Validation Loss: 0.00510149
Epoch [139/200], Train Loss: 0.004552
Validation Loss: 0.00509768
Epoch [140/200], Train Loss: 0.004631
Validation Loss: 0.00509821
Epoch [141/200], Train Loss: 0.004580
Validation Loss: 0.00509862
Epoch [142/200], Train Loss: 0.004570
Validation Loss: 0.00509784
Epoch [143/200], Train Loss: 0.004579
Validation Loss: 0.00509956
Epoch [144/200], Train Loss: 0.004571
Validation Loss: 0.00509903
Epoch [145/200], Train Loss: 0.004549
Validation Loss: 0.00509789
Early stopping triggered

Evaluating model for: Lamp
Run 96/144 completed in 127.39 seconds with: {'MAE': np.float32(3.0491247), 'MSE': np.float32(208.76967), 'RMSE': np.float32(14.448864), 'SAE': np.float32(0.2905003), 'NDE': np.float32(0.9799836)}

Run 97/144: hidden=256, seq_len=120, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004803
Validation Loss: 0.00474365
Epoch [2/200], Train Loss: 0.004732
Validation Loss: 0.00473794
Epoch [3/200], Train Loss: 0.004721
Validation Loss: 0.00473660
Epoch [4/200], Train Loss: 0.004711
Validation Loss: 0.00470897
Epoch [5/200], Train Loss: 0.004684
Validation Loss: 0.00467548
Epoch [6/200], Train Loss: 0.004643
Validation Loss: 0.00461927
Epoch [7/200], Train Loss: 0.004564
Validation Loss: 0.00447065
Epoch [8/200], Train Loss: 0.004358
Validation Loss: 0.00411299
Epoch [9/200], Train Loss: 0.003944
Validation Loss: 0.00354590
Epoch [10/200], Train Loss: 0.003224
Validation Loss: 0.00264629
Epoch [11/200], Train Loss: 0.002589
Validation Loss: 0.00226519
Epoch [12/200], Train Loss: 0.002258
Validation Loss: 0.00225269
Epoch [13/200], Train Loss: 0.002051
Validation Loss: 0.00190355
Epoch [14/200], Train Loss: 0.001887
Validation Loss: 0.00180693
Epoch [15/200], Train Loss: 0.001773
Validation Loss: 0.00172078
Epoch [16/200], Train Loss: 0.001663
Validation Loss: 0.00161260
Epoch [17/200], Train Loss: 0.001585
Validation Loss: 0.00156667
Epoch [18/200], Train Loss: 0.001509
Validation Loss: 0.00152920
Epoch [19/200], Train Loss: 0.001444
Validation Loss: 0.00143277
Epoch [20/200], Train Loss: 0.001385
Validation Loss: 0.00138405
Epoch [21/200], Train Loss: 0.001327
Validation Loss: 0.00131460
Epoch [22/200], Train Loss: 0.001492
Validation Loss: 0.00129219
Epoch [23/200], Train Loss: 0.001418
Validation Loss: 0.00139830
Epoch [24/200], Train Loss: 0.001221
Validation Loss: 0.00121725
Epoch [25/200], Train Loss: 0.001169
Validation Loss: 0.00118771
Epoch [26/200], Train Loss: 0.001133
Validation Loss: 0.00114509
Epoch [27/200], Train Loss: 0.001098
Validation Loss: 0.00110958
Epoch [28/200], Train Loss: 0.001065
Validation Loss: 0.00112591
Epoch [29/200], Train Loss: 0.001050
Validation Loss: 0.00103296
Epoch [30/200], Train Loss: 0.001005
Validation Loss: 0.00103431
Epoch [31/200], Train Loss: 0.001024
Validation Loss: 0.00099104
Epoch [32/200], Train Loss: 0.000960
Validation Loss: 0.00098673
Epoch [33/200], Train Loss: 0.000934
Validation Loss: 0.00096330
Epoch [34/200], Train Loss: 0.000913
Validation Loss: 0.00094627
Epoch [35/200], Train Loss: 0.000888
Validation Loss: 0.00091475
Epoch [36/200], Train Loss: 0.000867
Validation Loss: 0.00092488
Epoch [37/200], Train Loss: 0.000844
Validation Loss: 0.00084595
Epoch [38/200], Train Loss: 0.000820
Validation Loss: 0.00080943
Epoch [39/200], Train Loss: 0.000848
Validation Loss: 0.00084084
Epoch [40/200], Train Loss: 0.000800
Validation Loss: 0.00078053
Epoch [41/200], Train Loss: 0.000769
Validation Loss: 0.00075699
Epoch [42/200], Train Loss: 0.001140
Validation Loss: 0.00114855
Epoch [43/200], Train Loss: 0.000899
Validation Loss: 0.00081939
Epoch [44/200], Train Loss: 0.000785
Validation Loss: 0.00075351
Epoch [45/200], Train Loss: 0.000727
Validation Loss: 0.00072276
Epoch [46/200], Train Loss: 0.000704
Validation Loss: 0.00072765
Epoch [47/200], Train Loss: 0.000677
Validation Loss: 0.00068467
Epoch [48/200], Train Loss: 0.000655
Validation Loss: 0.00067191
Epoch [49/200], Train Loss: 0.000640
Validation Loss: 0.00066721
Epoch [50/200], Train Loss: 0.000631
Validation Loss: 0.00065031
Epoch [51/200], Train Loss: 0.000621
Validation Loss: 0.00064409
Epoch [52/200], Train Loss: 0.000640
Validation Loss: 0.00063794
Epoch [53/200], Train Loss: 0.000601
Validation Loss: 0.00061722
Epoch [54/200], Train Loss: 0.000587
Validation Loss: 0.00059068
Epoch [55/200], Train Loss: 0.000579
Validation Loss: 0.00057982
Epoch [56/200], Train Loss: 0.000569
Validation Loss: 0.00056647
Epoch [57/200], Train Loss: 0.000567
Validation Loss: 0.00055997
Epoch [58/200], Train Loss: 0.000558
Validation Loss: 0.00057147
Epoch [59/200], Train Loss: 0.000561
Validation Loss: 0.00055508
Epoch [60/200], Train Loss: 0.000540
Validation Loss: 0.00056297
Epoch [61/200], Train Loss: 0.000532
Validation Loss: 0.00054328
Epoch [62/200], Train Loss: 0.000534
Validation Loss: 0.00053000
Epoch [63/200], Train Loss: 0.000519
Validation Loss: 0.00052769
Epoch [64/200], Train Loss: 0.000532
Validation Loss: 0.00061525
Epoch [65/200], Train Loss: 0.000611
Validation Loss: 0.00053722
Epoch [66/200], Train Loss: 0.000512
Validation Loss: 0.00052392
Epoch [67/200], Train Loss: 0.000503
Validation Loss: 0.00050932
Epoch [68/200], Train Loss: 0.000498
Validation Loss: 0.00050972
Epoch [69/200], Train Loss: 0.000493
Validation Loss: 0.00050079
Epoch [70/200], Train Loss: 0.000488
Validation Loss: 0.00050904
Epoch [71/200], Train Loss: 0.000490
Validation Loss: 0.00048127
Epoch [72/200], Train Loss: 0.000480
Validation Loss: 0.00049263
Epoch [73/200], Train Loss: 0.000482
Validation Loss: 0.00048277
Epoch [74/200], Train Loss: 0.000471
Validation Loss: 0.00046865
Epoch [75/200], Train Loss: 0.000471
Validation Loss: 0.00049292
Epoch [76/200], Train Loss: 0.000457
Validation Loss: 0.00046575
Epoch [77/200], Train Loss: 0.000461
Validation Loss: 0.00046601
Epoch [78/200], Train Loss: 0.000451
Validation Loss: 0.00046398
Epoch [79/200], Train Loss: 0.000451
Validation Loss: 0.00046696
Epoch [80/200], Train Loss: 0.000444
Validation Loss: 0.00045048
Epoch [81/200], Train Loss: 0.000455
Validation Loss: 0.00044148
Epoch [82/200], Train Loss: 0.000429
Validation Loss: 0.00043421
Epoch [83/200], Train Loss: 0.000430
Validation Loss: 0.00044037
Epoch [84/200], Train Loss: 0.000427
Validation Loss: 0.00043699
Epoch [85/200], Train Loss: 0.000422
Validation Loss: 0.00044021
Epoch [86/200], Train Loss: 0.000421
Validation Loss: 0.00042757
Epoch [87/200], Train Loss: 0.000430
Validation Loss: 0.00044421
Epoch [88/200], Train Loss: 0.000416
Validation Loss: 0.00043577
Epoch [89/200], Train Loss: 0.000410
Validation Loss: 0.00042344
Epoch [90/200], Train Loss: 0.000412
Validation Loss: 0.00041195
Epoch [91/200], Train Loss: 0.000448
Validation Loss: 0.00043196
Epoch [92/200], Train Loss: 0.000403
Validation Loss: 0.00040932
Epoch [93/200], Train Loss: 0.000396
Validation Loss: 0.00041723
Epoch [94/200], Train Loss: 0.000391
Validation Loss: 0.00040232
Epoch [95/200], Train Loss: 0.000406
Validation Loss: 0.00043165
Epoch [96/200], Train Loss: 0.000397
Validation Loss: 0.00039900
Epoch [97/200], Train Loss: 0.000383
Validation Loss: 0.00040073
Epoch [98/200], Train Loss: 0.000388
Validation Loss: 0.00039431
Epoch [99/200], Train Loss: 0.000393
Validation Loss: 0.00040569
Epoch [100/200], Train Loss: 0.000376
Validation Loss: 0.00038313
Epoch [101/200], Train Loss: 0.000381
Validation Loss: 0.00036974
Epoch [102/200], Train Loss: 0.000369
Validation Loss: 0.00037538
Epoch [103/200], Train Loss: 0.000367
Validation Loss: 0.00036785
Epoch [104/200], Train Loss: 0.000364
Validation Loss: 0.00038005
Epoch [105/200], Train Loss: 0.000358
Validation Loss: 0.00036870
Epoch [106/200], Train Loss: 0.000386
Validation Loss: 0.00037876
Epoch [107/200], Train Loss: 0.000363
Validation Loss: 0.00035888
Epoch [108/200], Train Loss: 0.000355
Validation Loss: 0.00035334
Epoch [109/200], Train Loss: 0.000349
Validation Loss: 0.00034574
Epoch [110/200], Train Loss: 0.000354
Validation Loss: 0.00034830
Epoch [111/200], Train Loss: 0.000347
Validation Loss: 0.00034602
Epoch [112/200], Train Loss: 0.000347
Validation Loss: 0.00034895
Epoch [113/200], Train Loss: 0.000339
Validation Loss: 0.00034278
Epoch [114/200], Train Loss: 0.000339
Validation Loss: 0.00033539
Epoch [115/200], Train Loss: 0.000337
Validation Loss: 0.00033850
Epoch [116/200], Train Loss: 0.000337
Validation Loss: 0.00032802
Epoch [117/200], Train Loss: 0.000342
Validation Loss: 0.00035461
Epoch [118/200], Train Loss: 0.000336
Validation Loss: 0.00034347
Epoch [119/200], Train Loss: 0.000375
Validation Loss: 0.00038195
Epoch [120/200], Train Loss: 0.000357
Validation Loss: 0.00033964
Epoch [121/200], Train Loss: 0.000327
Validation Loss: 0.00031934
Epoch [122/200], Train Loss: 0.000319
Validation Loss: 0.00031829
Epoch [123/200], Train Loss: 0.000324
Validation Loss: 0.00040263
Epoch [124/200], Train Loss: 0.000327
Validation Loss: 0.00031098
Epoch [125/200], Train Loss: 0.000311
Validation Loss: 0.00030952
Epoch [126/200], Train Loss: 0.000311
Validation Loss: 0.00030270
Epoch [127/200], Train Loss: 0.000313
Validation Loss: 0.00030217
Epoch [128/200], Train Loss: 0.000306
Validation Loss: 0.00030572
Epoch [129/200], Train Loss: 0.000311
Validation Loss: 0.00029295
Epoch [130/200], Train Loss: 0.000305
Validation Loss: 0.00030357
Epoch [131/200], Train Loss: 0.000303
Validation Loss: 0.00029454
Epoch [132/200], Train Loss: 0.000296
Validation Loss: 0.00029862
Epoch [133/200], Train Loss: 0.000310
Validation Loss: 0.00029624
Epoch [134/200], Train Loss: 0.000292
Validation Loss: 0.00028726
Epoch [135/200], Train Loss: 0.000288
Validation Loss: 0.00028527
Epoch [136/200], Train Loss: 0.000287
Validation Loss: 0.00027699
Epoch [137/200], Train Loss: 0.000295
Validation Loss: 0.00028410
Epoch [138/200], Train Loss: 0.000298
Validation Loss: 0.00028775
Epoch [139/200], Train Loss: 0.000289
Validation Loss: 0.00027774
Epoch [140/200], Train Loss: 0.000291
Validation Loss: 0.00050821
Epoch [141/200], Train Loss: 0.000311
Validation Loss: 0.00028010
Epoch [142/200], Train Loss: 0.000280
Validation Loss: 0.00027464
Epoch [143/200], Train Loss: 0.000278
Validation Loss: 0.00027208
Epoch [144/200], Train Loss: 0.000275
Validation Loss: 0.00030159
Epoch [145/200], Train Loss: 0.000291
Validation Loss: 0.00026814
Epoch [146/200], Train Loss: 0.000277
Validation Loss: 0.00026661
Epoch [147/200], Train Loss: 0.000272
Validation Loss: 0.00028456
Epoch [148/200], Train Loss: 0.000272
Validation Loss: 0.00026974
Epoch [149/200], Train Loss: 0.000280
Validation Loss: 0.00026387
Epoch [150/200], Train Loss: 0.000268
Validation Loss: 0.00025808
Epoch [151/200], Train Loss: 0.000277
Validation Loss: 0.00026077
Epoch [152/200], Train Loss: 0.000266
Validation Loss: 0.00026568
Epoch [153/200], Train Loss: 0.000264
Validation Loss: 0.00026075
Epoch [154/200], Train Loss: 0.000263
Validation Loss: 0.00025297
Epoch [155/200], Train Loss: 0.000263
Validation Loss: 0.00025868
Epoch [156/200], Train Loss: 0.000263
Validation Loss: 0.00025212
Epoch [157/200], Train Loss: 0.000271
Validation Loss: 0.00026465
Epoch [158/200], Train Loss: 0.000264
Validation Loss: 0.00024729
Epoch [159/200], Train Loss: 0.000258
Validation Loss: 0.00025373
Epoch [160/200], Train Loss: 0.000259
Validation Loss: 0.00024056
Epoch [161/200], Train Loss: 0.000254
Validation Loss: 0.00024593
Epoch [162/200], Train Loss: 0.000252
Validation Loss: 0.00024227
Epoch [163/200], Train Loss: 0.000251
Validation Loss: 0.00023660
Epoch [164/200], Train Loss: 0.000258
Validation Loss: 0.00024029
Epoch [165/200], Train Loss: 0.000252
Validation Loss: 0.00023435
Epoch [166/200], Train Loss: 0.000249
Validation Loss: 0.00023048
Epoch [167/200], Train Loss: 0.000244
Validation Loss: 0.00023903
Epoch [168/200], Train Loss: 0.000245
Validation Loss: 0.00022798
Epoch [169/200], Train Loss: 0.000250
Validation Loss: 0.00022595
Epoch [170/200], Train Loss: 0.000240
Validation Loss: 0.00023008
Epoch [171/200], Train Loss: 0.000252
Validation Loss: 0.00032418
Epoch [172/200], Train Loss: 0.000254
Validation Loss: 0.00022570
Epoch [173/200], Train Loss: 0.000237
Validation Loss: 0.00022197
Epoch [174/200], Train Loss: 0.000236
Validation Loss: 0.00022025
Epoch [175/200], Train Loss: 0.000236
Validation Loss: 0.00021903
Epoch [176/200], Train Loss: 0.000237
Validation Loss: 0.00022077
Epoch [177/200], Train Loss: 0.000234
Validation Loss: 0.00021558
Epoch [178/200], Train Loss: 0.000231
Validation Loss: 0.00022588
Epoch [179/200], Train Loss: 0.000238
Validation Loss: 0.00022068
Epoch [180/200], Train Loss: 0.000232
Validation Loss: 0.00021453
Epoch [181/200], Train Loss: 0.000231
Validation Loss: 0.00021329
Epoch [182/200], Train Loss: 0.000230
Validation Loss: 0.00020960
Epoch [183/200], Train Loss: 0.000253
Validation Loss: 0.00022100
Epoch [184/200], Train Loss: 0.000228
Validation Loss: 0.00021528
Epoch [185/200], Train Loss: 0.000229
Validation Loss: 0.00021798
Epoch [186/200], Train Loss: 0.000224
Validation Loss: 0.00020666
Epoch [187/200], Train Loss: 0.000220
Validation Loss: 0.00020502
Epoch [188/200], Train Loss: 0.000257
Validation Loss: 0.00021780
Epoch [189/200], Train Loss: 0.000227
Validation Loss: 0.00020891
Epoch [190/200], Train Loss: 0.000222
Validation Loss: 0.00020549
Epoch [191/200], Train Loss: 0.000219
Validation Loss: 0.00020261
Epoch [192/200], Train Loss: 0.000226
Validation Loss: 0.00020310
Epoch [193/200], Train Loss: 0.000217
Validation Loss: 0.00020071
Epoch [194/200], Train Loss: 0.000219
Validation Loss: 0.00019813
Epoch [195/200], Train Loss: 0.000219
Validation Loss: 0.00020518
Epoch [196/200], Train Loss: 0.000218
Validation Loss: 0.00019982
Epoch [197/200], Train Loss: 0.000215
Validation Loss: 0.00019889
Epoch [198/200], Train Loss: 0.000217
Validation Loss: 0.00019682
Epoch [199/200], Train Loss: 0.000217
Validation Loss: 0.00021065
Epoch [200/200], Train Loss: 0.000216
Validation Loss: 0.00019391

Evaluating model for: Lamp
Run 97/144 completed in 5476.62 seconds with: {'MAE': np.float32(0.30776423), 'MSE': np.float32(9.093838), 'RMSE': np.float32(3.015599), 'SAE': np.float32(0.02073858), 'NDE': np.float32(0.23160829)}

Run 98/144: hidden=256, seq_len=120, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004740
Validation Loss: 0.00473486
Epoch [2/200], Train Loss: 0.004717
Validation Loss: 0.00472262
Epoch [3/200], Train Loss: 0.004715
Validation Loss: 0.00472864
Epoch [4/200], Train Loss: 0.004712
Validation Loss: 0.00471645
Epoch [5/200], Train Loss: 0.004707
Validation Loss: 0.00471731
Epoch [6/200], Train Loss: 0.004706
Validation Loss: 0.00471191
Epoch [7/200], Train Loss: 0.004693
Validation Loss: 0.00469526
Epoch [8/200], Train Loss: 0.004676
Validation Loss: 0.00466297
Epoch [9/200], Train Loss: 0.004595
Validation Loss: 0.00444592
Epoch [10/200], Train Loss: 0.004315
Validation Loss: 0.00403694
Epoch [11/200], Train Loss: 0.003280
Validation Loss: 0.00249337
Epoch [12/200], Train Loss: 0.002256
Validation Loss: 0.00202951
Epoch [13/200], Train Loss: 0.001891
Validation Loss: 0.00173359
Epoch [14/200], Train Loss: 0.001642
Validation Loss: 0.00156018
Epoch [15/200], Train Loss: 0.001459
Validation Loss: 0.00143267
Epoch [16/200], Train Loss: 0.001328
Validation Loss: 0.00128336
Epoch [17/200], Train Loss: 0.001263
Validation Loss: 0.00122681
Epoch [18/200], Train Loss: 0.001165
Validation Loss: 0.00115990
Epoch [19/200], Train Loss: 0.001140
Validation Loss: 0.00114023
Epoch [20/200], Train Loss: 0.001027
Validation Loss: 0.00102225
Epoch [21/200], Train Loss: 0.000970
Validation Loss: 0.00109205
Epoch [22/200], Train Loss: 0.000914
Validation Loss: 0.00091760
Epoch [23/200], Train Loss: 0.000865
Validation Loss: 0.00085636
Epoch [24/200], Train Loss: 0.000824
Validation Loss: 0.00084078
Epoch [25/200], Train Loss: 0.000804
Validation Loss: 0.00086523
Epoch [26/200], Train Loss: 0.000795
Validation Loss: 0.00086865
Epoch [27/200], Train Loss: 0.000771
Validation Loss: 0.00082424
Epoch [28/200], Train Loss: 0.000733
Validation Loss: 0.00072319
Epoch [29/200], Train Loss: 0.000702
Validation Loss: 0.00076507
Epoch [30/200], Train Loss: 0.000680
Validation Loss: 0.00066799
Epoch [31/200], Train Loss: 0.000652
Validation Loss: 0.00068972
Epoch [32/200], Train Loss: 0.000661
Validation Loss: 0.00064387
Epoch [33/200], Train Loss: 0.000627
Validation Loss: 0.00064433
Epoch [34/200], Train Loss: 0.000611
Validation Loss: 0.00062671
Epoch [35/200], Train Loss: 0.000600
Validation Loss: 0.00060377
Epoch [36/200], Train Loss: 0.000585
Validation Loss: 0.00060636
Epoch [37/200], Train Loss: 0.000574
Validation Loss: 0.00060922
Epoch [38/200], Train Loss: 0.000565
Validation Loss: 0.00065319
Epoch [39/200], Train Loss: 0.000570
Validation Loss: 0.00059098
Epoch [40/200], Train Loss: 0.000622
Validation Loss: 0.00055740
Epoch [41/200], Train Loss: 0.000534
Validation Loss: 0.00055279
Epoch [42/200], Train Loss: 0.000525
Validation Loss: 0.00054825
Epoch [43/200], Train Loss: 0.000521
Validation Loss: 0.00052975
Epoch [44/200], Train Loss: 0.000505
Validation Loss: 0.00052151
Epoch [45/200], Train Loss: 0.000508
Validation Loss: 0.00055271
Epoch [46/200], Train Loss: 0.000504
Validation Loss: 0.00050794
Epoch [47/200], Train Loss: 0.000487
Validation Loss: 0.00049865
Epoch [48/200], Train Loss: 0.000481
Validation Loss: 0.00054022
Epoch [49/200], Train Loss: 0.000480
Validation Loss: 0.00048350
Epoch [50/200], Train Loss: 0.000476
Validation Loss: 0.00091718
Epoch [51/200], Train Loss: 0.000508
Validation Loss: 0.00046999
Epoch [52/200], Train Loss: 0.000457
Validation Loss: 0.00045413
Epoch [53/200], Train Loss: 0.000455
Validation Loss: 0.00045980
Epoch [54/200], Train Loss: 0.000461
Validation Loss: 0.00046782
Epoch [55/200], Train Loss: 0.000444
Validation Loss: 0.00044706
Epoch [56/200], Train Loss: 0.000432
Validation Loss: 0.00045299
Epoch [57/200], Train Loss: 0.000440
Validation Loss: 0.00043308
Epoch [58/200], Train Loss: 0.000423
Validation Loss: 0.00044568
Epoch [59/200], Train Loss: 0.000440
Validation Loss: 0.00043524
Epoch [60/200], Train Loss: 0.000425
Validation Loss: 0.00044828
Epoch [61/200], Train Loss: 0.000421
Validation Loss: 0.00043586
Epoch [62/200], Train Loss: 0.000407
Validation Loss: 0.00041315
Epoch [63/200], Train Loss: 0.000409
Validation Loss: 0.00041157
Epoch [64/200], Train Loss: 0.000408
Validation Loss: 0.00044071
Epoch [65/200], Train Loss: 0.000402
Validation Loss: 0.00040520
Epoch [66/200], Train Loss: 0.000397
Validation Loss: 0.00041339
Epoch [67/200], Train Loss: 0.000390
Validation Loss: 0.00041201
Epoch [68/200], Train Loss: 0.000391
Validation Loss: 0.00040384
Epoch [69/200], Train Loss: 0.000391
Validation Loss: 0.00041989
Epoch [70/200], Train Loss: 0.000381
Validation Loss: 0.00041010
Epoch [71/200], Train Loss: 0.000380
Validation Loss: 0.00038277
Epoch [72/200], Train Loss: 0.000371
Validation Loss: 0.00040434
Epoch [73/200], Train Loss: 0.000373
Validation Loss: 0.00041467
Epoch [74/200], Train Loss: 0.000371
Validation Loss: 0.00037783
Epoch [75/200], Train Loss: 0.000394
Validation Loss: 0.00041201
Epoch [76/200], Train Loss: 0.000366
Validation Loss: 0.00037488
Epoch [77/200], Train Loss: 0.000358
Validation Loss: 0.00039011
Epoch [78/200], Train Loss: 0.000362
Validation Loss: 0.00036013
Epoch [79/200], Train Loss: 0.000350
Validation Loss: 0.00035957
Epoch [80/200], Train Loss: 0.000351
Validation Loss: 0.00036850
Epoch [81/200], Train Loss: 0.000347
Validation Loss: 0.00035859
Epoch [82/200], Train Loss: 0.000342
Validation Loss: 0.00035071
Epoch [83/200], Train Loss: 0.000339
Validation Loss: 0.00035153
Epoch [84/200], Train Loss: 0.000335
Validation Loss: 0.00034750
Epoch [85/200], Train Loss: 0.000338
Validation Loss: 0.00035105
Epoch [86/200], Train Loss: 0.000334
Validation Loss: 0.00033240
Epoch [87/200], Train Loss: 0.000326
Validation Loss: 0.00033247
Epoch [88/200], Train Loss: 0.000335
Validation Loss: 0.00035318
Epoch [89/200], Train Loss: 0.000331
Validation Loss: 0.00042273
Epoch [90/200], Train Loss: 0.000363
Validation Loss: 0.00033416
Epoch [91/200], Train Loss: 0.000317
Validation Loss: 0.00033438
Epoch [92/200], Train Loss: 0.000314
Validation Loss: 0.00032098
Epoch [93/200], Train Loss: 0.000310
Validation Loss: 0.00031730
Epoch [94/200], Train Loss: 0.000311
Validation Loss: 0.00034125
Epoch [95/200], Train Loss: 0.000308
Validation Loss: 0.00036504
Epoch [96/200], Train Loss: 0.000315
Validation Loss: 0.00032716
Epoch [97/200], Train Loss: 0.000312
Validation Loss: 0.00031300
Epoch [98/200], Train Loss: 0.000306
Validation Loss: 0.00030692
Epoch [99/200], Train Loss: 0.000312
Validation Loss: 0.00030495
Epoch [100/200], Train Loss: 0.000293
Validation Loss: 0.00029469
Epoch [101/200], Train Loss: 0.000291
Validation Loss: 0.00030197
Epoch [102/200], Train Loss: 0.000292
Validation Loss: 0.00029830
Epoch [103/200], Train Loss: 0.000290
Validation Loss: 0.00031155
Epoch [104/200], Train Loss: 0.000287
Validation Loss: 0.00032770
Epoch [105/200], Train Loss: 0.000284
Validation Loss: 0.00027333
Epoch [106/200], Train Loss: 0.000283
Validation Loss: 0.00029313
Epoch [107/200], Train Loss: 0.000269
Validation Loss: 0.00026850
Epoch [108/200], Train Loss: 0.000269
Validation Loss: 0.00026251
Epoch [109/200], Train Loss: 0.000270
Validation Loss: 0.00025937
Epoch [110/200], Train Loss: 0.000273
Validation Loss: 0.00026628
Epoch [111/200], Train Loss: 0.000261
Validation Loss: 0.00026628
Epoch [112/200], Train Loss: 0.000259
Validation Loss: 0.00025227
Epoch [113/200], Train Loss: 0.000262
Validation Loss: 0.00025375
Epoch [114/200], Train Loss: 0.000258
Validation Loss: 0.00025407
Epoch [115/200], Train Loss: 0.000254
Validation Loss: 0.00029677
Epoch [116/200], Train Loss: 0.000258
Validation Loss: 0.00027113
Epoch [117/200], Train Loss: 0.000249
Validation Loss: 0.00022754
Epoch [118/200], Train Loss: 0.000258
Validation Loss: 0.00025064
Epoch [119/200], Train Loss: 0.000262
Validation Loss: 0.00031588
Epoch [120/200], Train Loss: 0.000254
Validation Loss: 0.00023578
Epoch [121/200], Train Loss: 0.000240
Validation Loss: 0.00022467
Epoch [122/200], Train Loss: 0.000238
Validation Loss: 0.00025422
Epoch [123/200], Train Loss: 0.000246
Validation Loss: 0.00023385
Epoch [124/200], Train Loss: 0.000234
Validation Loss: 0.00023002
Epoch [125/200], Train Loss: 0.000233
Validation Loss: 0.00021863
Epoch [126/200], Train Loss: 0.000230
Validation Loss: 0.00021567
Epoch [127/200], Train Loss: 0.000239
Validation Loss: 0.00022543
Epoch [128/200], Train Loss: 0.000230
Validation Loss: 0.00023607
Epoch [129/200], Train Loss: 0.000228
Validation Loss: 0.00021508
Epoch [130/200], Train Loss: 0.000239
Validation Loss: 0.00023692
Epoch [131/200], Train Loss: 0.000238
Validation Loss: 0.00024056
Epoch [132/200], Train Loss: 0.000236
Validation Loss: 0.00021735
Epoch [133/200], Train Loss: 0.000225
Validation Loss: 0.00021054
Epoch [134/200], Train Loss: 0.000222
Validation Loss: 0.00021535
Epoch [135/200], Train Loss: 0.000226
Validation Loss: 0.00022436
Epoch [136/200], Train Loss: 0.000218
Validation Loss: 0.00020281
Epoch [137/200], Train Loss: 0.000254
Validation Loss: 0.00020806
Epoch [138/200], Train Loss: 0.000216
Validation Loss: 0.00021778
Epoch [139/200], Train Loss: 0.000213
Validation Loss: 0.00022403
Epoch [140/200], Train Loss: 0.000212
Validation Loss: 0.00021213
Epoch [141/200], Train Loss: 0.000214
Validation Loss: 0.00024042
Epoch [142/200], Train Loss: 0.000239
Validation Loss: 0.00021646
Epoch [143/200], Train Loss: 0.000216
Validation Loss: 0.00020044
Epoch [144/200], Train Loss: 0.000214
Validation Loss: 0.00019675
Epoch [145/200], Train Loss: 0.000212
Validation Loss: 0.00020656
Epoch [146/200], Train Loss: 0.000216
Validation Loss: 0.00020535
Epoch [147/200], Train Loss: 0.000209
Validation Loss: 0.00019357
Epoch [148/200], Train Loss: 0.000210
Validation Loss: 0.00020792
Epoch [149/200], Train Loss: 0.000206
Validation Loss: 0.00019499
Epoch [150/200], Train Loss: 0.000207
Validation Loss: 0.00020596
Epoch [151/200], Train Loss: 0.000207
Validation Loss: 0.00019611
Epoch [152/200], Train Loss: 0.000207
Validation Loss: 0.00020267
Epoch [153/200], Train Loss: 0.000201
Validation Loss: 0.00020532
Epoch [154/200], Train Loss: 0.000205
Validation Loss: 0.00019676
Epoch [155/200], Train Loss: 0.000204
Validation Loss: 0.00018710
Epoch [156/200], Train Loss: 0.000208
Validation Loss: 0.00019811
Epoch [157/200], Train Loss: 0.000196
Validation Loss: 0.00020367
Epoch [158/200], Train Loss: 0.000198
Validation Loss: 0.00021937
Epoch [159/200], Train Loss: 0.000198
Validation Loss: 0.00018826
Epoch [160/200], Train Loss: 0.000199
Validation Loss: 0.00019060
Epoch [161/200], Train Loss: 0.000197
Validation Loss: 0.00018988
Epoch [162/200], Train Loss: 0.000203
Validation Loss: 0.00019845
Epoch [163/200], Train Loss: 0.000195
Validation Loss: 0.00020673
Epoch [164/200], Train Loss: 0.000202
Validation Loss: 0.00019704
Epoch [165/200], Train Loss: 0.000195
Validation Loss: 0.00018212
Epoch [166/200], Train Loss: 0.000197
Validation Loss: 0.00018656
Epoch [167/200], Train Loss: 0.000199
Validation Loss: 0.00018662
Epoch [168/200], Train Loss: 0.000190
Validation Loss: 0.00018217
Epoch [169/200], Train Loss: 0.000188
Validation Loss: 0.00018789
Epoch [170/200], Train Loss: 0.000188
Validation Loss: 0.00017894
Epoch [171/200], Train Loss: 0.000187
Validation Loss: 0.00017977
Epoch [172/200], Train Loss: 0.000190
Validation Loss: 0.00021119
Epoch [173/200], Train Loss: 0.000188
Validation Loss: 0.00017575
Epoch [174/200], Train Loss: 0.000185
Validation Loss: 0.00018576
Epoch [175/200], Train Loss: 0.000188
Validation Loss: 0.00017968
Epoch [176/200], Train Loss: 0.000188
Validation Loss: 0.00017717
Epoch [177/200], Train Loss: 0.000184
Validation Loss: 0.00017923
Epoch [178/200], Train Loss: 0.000195
Validation Loss: 0.00020872
Epoch [179/200], Train Loss: 0.000227
Validation Loss: 0.00018121
Epoch [180/200], Train Loss: 0.000184
Validation Loss: 0.00017575
Epoch [181/200], Train Loss: 0.000180
Validation Loss: 0.00017118
Epoch [182/200], Train Loss: 0.000183
Validation Loss: 0.00018224
Epoch [183/200], Train Loss: 0.000180
Validation Loss: 0.00016841
Epoch [184/200], Train Loss: 0.000179
Validation Loss: 0.00016926
Epoch [185/200], Train Loss: 0.000178
Validation Loss: 0.00017744
Epoch [186/200], Train Loss: 0.000179
Validation Loss: 0.00017639
Epoch [187/200], Train Loss: 0.000178
Validation Loss: 0.00016835
Epoch [188/200], Train Loss: 0.000179
Validation Loss: 0.00017233
Epoch [189/200], Train Loss: 0.000175
Validation Loss: 0.00017064
Epoch [190/200], Train Loss: 0.000177
Validation Loss: 0.00016700
Epoch [191/200], Train Loss: 0.000183
Validation Loss: 0.00017038
Epoch [192/200], Train Loss: 0.000174
Validation Loss: 0.00018130
Epoch [193/200], Train Loss: 0.000176
Validation Loss: 0.00016844
Epoch [194/200], Train Loss: 0.000176
Validation Loss: 0.00016654
Epoch [195/200], Train Loss: 0.000175
Validation Loss: 0.00016566
Epoch [196/200], Train Loss: 0.000170
Validation Loss: 0.00016708
Epoch [197/200], Train Loss: 0.000174
Validation Loss: 0.00020388
Epoch [198/200], Train Loss: 0.000174
Validation Loss: 0.00016640
Epoch [199/200], Train Loss: 0.000172
Validation Loss: 0.00016774
Epoch [200/200], Train Loss: 0.000172
Validation Loss: 0.00017534

Evaluating model for: Lamp
Run 98/144 completed in 6088.83 seconds with: {'MAE': np.float32(0.264457), 'MSE': np.float32(7.907274), 'RMSE': np.float32(2.8119874), 'SAE': np.float32(9.431166e-05), 'NDE': np.float32(0.21597014)}

Run 99/144: hidden=256, seq_len=120, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004776
Validation Loss: 0.00475005
Epoch [2/200], Train Loss: 0.004728
Validation Loss: 0.00472853
Epoch [3/200], Train Loss: 0.004711
Validation Loss: 0.00475260
Epoch [4/200], Train Loss: 0.004715
Validation Loss: 0.00471658
Epoch [5/200], Train Loss: 0.004706
Validation Loss: 0.00471574
Epoch [6/200], Train Loss: 0.004703
Validation Loss: 0.00471344
Epoch [7/200], Train Loss: 0.004704
Validation Loss: 0.00472002
Epoch [8/200], Train Loss: 0.004702
Validation Loss: 0.00471179
Epoch [9/200], Train Loss: 0.004700
Validation Loss: 0.00470820
Epoch [10/200], Train Loss: 0.004700
Validation Loss: 0.00471398
Epoch [11/200], Train Loss: 0.004697
Validation Loss: 0.00471169
Epoch [12/200], Train Loss: 0.004703
Validation Loss: 0.00470985
Epoch [13/200], Train Loss: 0.004694
Validation Loss: 0.00470287
Epoch [14/200], Train Loss: 0.004685
Validation Loss: 0.00467478
Epoch [15/200], Train Loss: 0.004636
Validation Loss: 0.00453832
Epoch [16/200], Train Loss: 0.004325
Validation Loss: 0.00355899
Epoch [17/200], Train Loss: 0.002559
Validation Loss: 0.00203348
Epoch [18/200], Train Loss: 0.001801
Validation Loss: 0.00163115
Epoch [19/200], Train Loss: 0.001523
Validation Loss: 0.00145055
Epoch [20/200], Train Loss: 0.001317
Validation Loss: 0.00123065
Epoch [21/200], Train Loss: 0.001171
Validation Loss: 0.00118957
Epoch [22/200], Train Loss: 0.001055
Validation Loss: 0.00112927
Epoch [23/200], Train Loss: 0.000977
Validation Loss: 0.00096744
Epoch [24/200], Train Loss: 0.000919
Validation Loss: 0.00093009
Epoch [25/200], Train Loss: 0.000875
Validation Loss: 0.00087844
Epoch [26/200], Train Loss: 0.000819
Validation Loss: 0.00085583
Epoch [27/200], Train Loss: 0.000811
Validation Loss: 0.00089746
Epoch [28/200], Train Loss: 0.000776
Validation Loss: 0.00080671
Epoch [29/200], Train Loss: 0.000727
Validation Loss: 0.00076185
Epoch [30/200], Train Loss: 0.000700
Validation Loss: 0.00075189
Epoch [31/200], Train Loss: 0.000729
Validation Loss: 0.00075309
Epoch [32/200], Train Loss: 0.000666
Validation Loss: 0.00067484
Epoch [33/200], Train Loss: 0.000639
Validation Loss: 0.00064341
Epoch [34/200], Train Loss: 0.000626
Validation Loss: 0.00065013
Epoch [35/200], Train Loss: 0.000611
Validation Loss: 0.00060455
Epoch [36/200], Train Loss: 0.000583
Validation Loss: 0.00058213
Epoch [37/200], Train Loss: 0.000613
Validation Loss: 0.00063023
Epoch [38/200], Train Loss: 0.000562
Validation Loss: 0.00056161
Epoch [39/200], Train Loss: 0.000541
Validation Loss: 0.00055207
Epoch [40/200], Train Loss: 0.000521
Validation Loss: 0.00054312
Epoch [41/200], Train Loss: 0.000517
Validation Loss: 0.00052486
Epoch [42/200], Train Loss: 0.000510
Validation Loss: 0.00050319
Epoch [43/200], Train Loss: 0.000497
Validation Loss: 0.00050375
Epoch [44/200], Train Loss: 0.000483
Validation Loss: 0.00049893
Epoch [45/200], Train Loss: 0.000605
Validation Loss: 0.00105606
Epoch [46/200], Train Loss: 0.000582
Validation Loss: 0.00052676
Epoch [47/200], Train Loss: 0.000470
Validation Loss: 0.00048266
Epoch [48/200], Train Loss: 0.000457
Validation Loss: 0.00047099
Epoch [49/200], Train Loss: 0.000446
Validation Loss: 0.00045870
Epoch [50/200], Train Loss: 0.000443
Validation Loss: 0.00045691
Epoch [51/200], Train Loss: 0.000433
Validation Loss: 0.00045264
Epoch [52/200], Train Loss: 0.000426
Validation Loss: 0.00044667
Epoch [53/200], Train Loss: 0.000422
Validation Loss: 0.00044119
Epoch [54/200], Train Loss: 0.000510
Validation Loss: 0.00047568
Epoch [55/200], Train Loss: 0.000431
Validation Loss: 0.00044226
Epoch [56/200], Train Loss: 0.000420
Validation Loss: 0.00041433
Epoch [57/200], Train Loss: 0.000507
Validation Loss: 0.00060399
Epoch [58/200], Train Loss: 0.000441
Validation Loss: 0.00043646
Epoch [59/200], Train Loss: 0.000400
Validation Loss: 0.00041249
Epoch [60/200], Train Loss: 0.000396
Validation Loss: 0.00039952
Epoch [61/200], Train Loss: 0.000391
Validation Loss: 0.00040447
Epoch [62/200], Train Loss: 0.000383
Validation Loss: 0.00039589
Epoch [63/200], Train Loss: 0.000383
Validation Loss: 0.00040082
Epoch [64/200], Train Loss: 0.000385
Validation Loss: 0.00041219
Epoch [65/200], Train Loss: 0.000381
Validation Loss: 0.00040039
Epoch [66/200], Train Loss: 0.000371
Validation Loss: 0.00039077
Epoch [67/200], Train Loss: 0.000375
Validation Loss: 0.00039178
Epoch [68/200], Train Loss: 0.000380
Validation Loss: 0.00037292
Epoch [69/200], Train Loss: 0.000361
Validation Loss: 0.00039297
Epoch [70/200], Train Loss: 0.000352
Validation Loss: 0.00036117
Epoch [71/200], Train Loss: 0.000365
Validation Loss: 0.00038331
Epoch [72/200], Train Loss: 0.000351
Validation Loss: 0.00036121
Epoch [73/200], Train Loss: 0.000358
Validation Loss: 0.00040438
Epoch [74/200], Train Loss: 0.000361
Validation Loss: 0.00035153
Epoch [75/200], Train Loss: 0.000333
Validation Loss: 0.00035388
Epoch [76/200], Train Loss: 0.000335
Validation Loss: 0.00035140
Epoch [77/200], Train Loss: 0.000330
Validation Loss: 0.00033943
Epoch [78/200], Train Loss: 0.000328
Validation Loss: 0.00032768
Epoch [79/200], Train Loss: 0.000328
Validation Loss: 0.00033237
Epoch [80/200], Train Loss: 0.000315
Validation Loss: 0.00033261
Epoch [81/200], Train Loss: 0.000305
Validation Loss: 0.00031016
Epoch [82/200], Train Loss: 0.000318
Validation Loss: 0.00032852
Epoch [83/200], Train Loss: 0.000310
Validation Loss: 0.00033506
Epoch [84/200], Train Loss: 0.000311
Validation Loss: 0.00030267
Epoch [85/200], Train Loss: 0.000300
Validation Loss: 0.00034161
Epoch [86/200], Train Loss: 0.000299
Validation Loss: 0.00029089
Epoch [87/200], Train Loss: 0.000300
Validation Loss: 0.00031791
Epoch [88/200], Train Loss: 0.000282
Validation Loss: 0.00028662
Epoch [89/200], Train Loss: 0.000288
Validation Loss: 0.00032777
Epoch [90/200], Train Loss: 0.000294
Validation Loss: 0.00029187
Epoch [91/200], Train Loss: 0.000286
Validation Loss: 0.00029575
Epoch [92/200], Train Loss: 0.000268
Validation Loss: 0.00026151
Epoch [93/200], Train Loss: 0.000259
Validation Loss: 0.00026377
Epoch [94/200], Train Loss: 0.000270
Validation Loss: 0.00026320
Epoch [95/200], Train Loss: 0.000264
Validation Loss: 0.00025551
Epoch [96/200], Train Loss: 0.000258
Validation Loss: 0.00025342
Epoch [97/200], Train Loss: 0.000248
Validation Loss: 0.00026344
Epoch [98/200], Train Loss: 0.000256
Validation Loss: 0.00024204
Epoch [99/200], Train Loss: 0.000240
Validation Loss: 0.00023986
Epoch [100/200], Train Loss: 0.000239
Validation Loss: 0.00027650
Epoch [101/200], Train Loss: 0.000391
Validation Loss: 0.00027538
Epoch [102/200], Train Loss: 0.000252
Validation Loss: 0.00025618
Epoch [103/200], Train Loss: 0.000244
Validation Loss: 0.00023545
Epoch [104/200], Train Loss: 0.000235
Validation Loss: 0.00023826
Epoch [105/200], Train Loss: 0.000237
Validation Loss: 0.00022936
Epoch [106/200], Train Loss: 0.000232
Validation Loss: 0.00023700
Epoch [107/200], Train Loss: 0.000227
Validation Loss: 0.00022444
Epoch [108/200], Train Loss: 0.000223
Validation Loss: 0.00022692
Epoch [109/200], Train Loss: 0.000221
Validation Loss: 0.00028988
Epoch [110/200], Train Loss: 0.000229
Validation Loss: 0.00022125
Epoch [111/200], Train Loss: 0.000225
Validation Loss: 0.00021726
Epoch [112/200], Train Loss: 0.000228
Validation Loss: 0.00022349
Epoch [113/200], Train Loss: 0.000223
Validation Loss: 0.00021223
Epoch [114/200], Train Loss: 0.000220
Validation Loss: 0.00021749
Epoch [115/200], Train Loss: 0.000220
Validation Loss: 0.00021743
Epoch [116/200], Train Loss: 0.000213
Validation Loss: 0.00020612
Epoch [117/200], Train Loss: 0.000220
Validation Loss: 0.00021036
Epoch [118/200], Train Loss: 0.000216
Validation Loss: 0.00022086
Epoch [119/200], Train Loss: 0.000212
Validation Loss: 0.00021179
Epoch [120/200], Train Loss: 0.000210
Validation Loss: 0.00020784
Epoch [121/200], Train Loss: 0.000215
Validation Loss: 0.00021284
Epoch [122/200], Train Loss: 0.000214
Validation Loss: 0.00025231
Epoch [123/200], Train Loss: 0.000210
Validation Loss: 0.00020423
Epoch [124/200], Train Loss: 0.000205
Validation Loss: 0.00019492
Epoch [125/200], Train Loss: 0.000199
Validation Loss: 0.00018887
Epoch [126/200], Train Loss: 0.000203
Validation Loss: 0.00019445
Epoch [127/200], Train Loss: 0.000200
Validation Loss: 0.00019456
Epoch [128/200], Train Loss: 0.000204
Validation Loss: 0.00020176
Epoch [129/200], Train Loss: 0.000225
Validation Loss: 0.00019231
Epoch [130/200], Train Loss: 0.000200
Validation Loss: 0.00021924
Epoch [131/200], Train Loss: 0.000208
Validation Loss: 0.00019260
Epoch [132/200], Train Loss: 0.000195
Validation Loss: 0.00019210
Epoch [133/200], Train Loss: 0.000212
Validation Loss: 0.00021523
Epoch [134/200], Train Loss: 0.000199
Validation Loss: 0.00018580
Epoch [135/200], Train Loss: 0.000196
Validation Loss: 0.00018458
Epoch [136/200], Train Loss: 0.000189
Validation Loss: 0.00018207
Epoch [137/200], Train Loss: 0.000189
Validation Loss: 0.00018490
Epoch [138/200], Train Loss: 0.000188
Validation Loss: 0.00020795
Epoch [139/200], Train Loss: 0.000207
Validation Loss: 0.00018753
Epoch [140/200], Train Loss: 0.000191
Validation Loss: 0.00017558
Epoch [141/200], Train Loss: 0.000185
Validation Loss: 0.00017700
Epoch [142/200], Train Loss: 0.000185
Validation Loss: 0.00017596
Epoch [143/200], Train Loss: 0.000180
Validation Loss: 0.00017872
Epoch [144/200], Train Loss: 0.000182
Validation Loss: 0.00017876
Epoch [145/200], Train Loss: 0.000189
Validation Loss: 0.00017354
Epoch [146/200], Train Loss: 0.000182
Validation Loss: 0.00017653
Epoch [147/200], Train Loss: 0.000181
Validation Loss: 0.00017446
Epoch [148/200], Train Loss: 0.000198
Validation Loss: 0.00017905
Epoch [149/200], Train Loss: 0.000182
Validation Loss: 0.00017120
Epoch [150/200], Train Loss: 0.000190
Validation Loss: 0.00018339
Epoch [151/200], Train Loss: 0.000177
Validation Loss: 0.00017001
Epoch [152/200], Train Loss: 0.000174
Validation Loss: 0.00016202
Epoch [153/200], Train Loss: 0.000174
Validation Loss: 0.00016819
Epoch [154/200], Train Loss: 0.000193
Validation Loss: 0.00016925
Epoch [155/200], Train Loss: 0.000188
Validation Loss: 0.00017499
Epoch [156/200], Train Loss: 0.000176
Validation Loss: 0.00016490
Epoch [157/200], Train Loss: 0.000173
Validation Loss: 0.00016928
Epoch [158/200], Train Loss: 0.000171
Validation Loss: 0.00016839
Epoch [159/200], Train Loss: 0.000172
Validation Loss: 0.00016601
Epoch [160/200], Train Loss: 0.000172
Validation Loss: 0.00016531
Epoch [161/200], Train Loss: 0.000167
Validation Loss: 0.00016012
Epoch [162/200], Train Loss: 0.000165
Validation Loss: 0.00016534
Epoch [163/200], Train Loss: 0.000173
Validation Loss: 0.00016407
Epoch [164/200], Train Loss: 0.000181
Validation Loss: 0.00016735
Epoch [165/200], Train Loss: 0.000165
Validation Loss: 0.00016629
Epoch [166/200], Train Loss: 0.000163
Validation Loss: 0.00015633
Epoch [167/200], Train Loss: 0.000163
Validation Loss: 0.00015817
Epoch [168/200], Train Loss: 0.000165
Validation Loss: 0.00015879
Epoch [169/200], Train Loss: 0.000163
Validation Loss: 0.00015706
Epoch [170/200], Train Loss: 0.000160
Validation Loss: 0.00017535
Epoch [171/200], Train Loss: 0.000169
Validation Loss: 0.00015066
Epoch [172/200], Train Loss: 0.000160
Validation Loss: 0.00015608
Epoch [173/200], Train Loss: 0.000165
Validation Loss: 0.00015978
Epoch [174/200], Train Loss: 0.000164
Validation Loss: 0.00015919
Epoch [175/200], Train Loss: 0.000160
Validation Loss: 0.00015389
Epoch [176/200], Train Loss: 0.000161
Validation Loss: 0.00015592
Epoch [177/200], Train Loss: 0.000166
Validation Loss: 0.00015708
Epoch [178/200], Train Loss: 0.000162
Validation Loss: 0.00015635
Epoch [179/200], Train Loss: 0.000163
Validation Loss: 0.00014714
Epoch [180/200], Train Loss: 0.000155
Validation Loss: 0.00014764
Epoch [181/200], Train Loss: 0.000156
Validation Loss: 0.00014730
Epoch [182/200], Train Loss: 0.000159
Validation Loss: 0.00015541
Epoch [183/200], Train Loss: 0.000154
Validation Loss: 0.00015349
Epoch [184/200], Train Loss: 0.000157
Validation Loss: 0.00015158
Epoch [185/200], Train Loss: 0.000153
Validation Loss: 0.00014991
Epoch [186/200], Train Loss: 0.000151
Validation Loss: 0.00014499
Epoch [187/200], Train Loss: 0.000155
Validation Loss: 0.00015017
Epoch [188/200], Train Loss: 0.000150
Validation Loss: 0.00014963
Epoch [189/200], Train Loss: 0.000158
Validation Loss: 0.00016472
Epoch [190/200], Train Loss: 0.000154
Validation Loss: 0.00014559
Epoch [191/200], Train Loss: 0.000153
Validation Loss: 0.00015549
Epoch [192/200], Train Loss: 0.000156
Validation Loss: 0.00014679
Epoch [193/200], Train Loss: 0.000149
Validation Loss: 0.00014497
Epoch [194/200], Train Loss: 0.000148
Validation Loss: 0.00014682
Epoch [195/200], Train Loss: 0.000149
Validation Loss: 0.00014365
Epoch [196/200], Train Loss: 0.000150
Validation Loss: 0.00015229
Epoch [197/200], Train Loss: 0.000145
Validation Loss: 0.00014139
Epoch [198/200], Train Loss: 0.000150
Validation Loss: 0.00014056
Epoch [199/200], Train Loss: 0.000145
Validation Loss: 0.00013843
Epoch [200/200], Train Loss: 0.000143
Validation Loss: 0.00014032

Evaluating model for: Lamp
Run 99/144 completed in 6516.26 seconds with: {'MAE': np.float32(0.224418), 'MSE': np.float32(6.8782897), 'RMSE': np.float32(2.6226494), 'SAE': np.float32(0.018008757), 'NDE': np.float32(0.20142876)}

Run 100/144: hidden=256, seq_len=120, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 28602 windows

Epoch [1/200], Train Loss: 0.004786
Validation Loss: 0.00474773
Epoch [2/200], Train Loss: 0.004726
Validation Loss: 0.00474956
Epoch [3/200], Train Loss: 0.004716
Validation Loss: 0.00471842
Epoch [4/200], Train Loss: 0.004713
Validation Loss: 0.00472069
Epoch [5/200], Train Loss: 0.004711
Validation Loss: 0.00472004
Epoch [6/200], Train Loss: 0.004708
Validation Loss: 0.00471425
Epoch [7/200], Train Loss: 0.004703
Validation Loss: 0.00470983
Epoch [8/200], Train Loss: 0.004706
Validation Loss: 0.00471121
Epoch [9/200], Train Loss: 0.004704
Validation Loss: 0.00471435
Epoch [10/200], Train Loss: 0.004707
Validation Loss: 0.00470991
Epoch [11/200], Train Loss: 0.004700
Validation Loss: 0.00472269
Epoch [12/200], Train Loss: 0.004698
Validation Loss: 0.00471682
Epoch [13/200], Train Loss: 0.004697
Validation Loss: 0.00470540
Epoch [14/200], Train Loss: 0.004696
Validation Loss: 0.00471557
Epoch [15/200], Train Loss: 0.004693
Validation Loss: 0.00469710
Epoch [16/200], Train Loss: 0.004693
Validation Loss: 0.00468771
Epoch [17/200], Train Loss: 0.004686
Validation Loss: 0.00469459
Epoch [18/200], Train Loss: 0.004667
Validation Loss: 0.00464480
Epoch [19/200], Train Loss: 0.004490
Validation Loss: 0.00400589
Epoch [20/200], Train Loss: 0.002876
Validation Loss: 0.00199198
Epoch [21/200], Train Loss: 0.001814
Validation Loss: 0.00156301
Epoch [22/200], Train Loss: 0.001465
Validation Loss: 0.00130482
Epoch [23/200], Train Loss: 0.001236
Validation Loss: 0.00119994
Epoch [24/200], Train Loss: 0.001093
Validation Loss: 0.00105517
Epoch [25/200], Train Loss: 0.001012
Validation Loss: 0.00105261
Epoch [26/200], Train Loss: 0.000925
Validation Loss: 0.00093264
Epoch [27/200], Train Loss: 0.000868
Validation Loss: 0.00087921
Epoch [28/200], Train Loss: 0.000842
Validation Loss: 0.00094534
Epoch [29/200], Train Loss: 0.000805
Validation Loss: 0.00081132
Epoch [30/200], Train Loss: 0.000738
Validation Loss: 0.00076841
Epoch [31/200], Train Loss: 0.000968
Validation Loss: 0.00081782
Epoch [32/200], Train Loss: 0.000729
Validation Loss: 0.00074175
Epoch [33/200], Train Loss: 0.000687
Validation Loss: 0.00070258
Epoch [34/200], Train Loss: 0.000662
Validation Loss: 0.00067096
Epoch [35/200], Train Loss: 0.000628
Validation Loss: 0.00066289
Epoch [36/200], Train Loss: 0.000610
Validation Loss: 0.00061985
Epoch [37/200], Train Loss: 0.000609
Validation Loss: 0.00062420
Epoch [38/200], Train Loss: 0.000662
Validation Loss: 0.00066337
Epoch [39/200], Train Loss: 0.000577
Validation Loss: 0.00058257
Epoch [40/200], Train Loss: 0.000577
Validation Loss: 0.00057076
Epoch [41/200], Train Loss: 0.000536
Validation Loss: 0.00053664
Epoch [42/200], Train Loss: 0.000519
Validation Loss: 0.00052167
Epoch [43/200], Train Loss: 0.000526
Validation Loss: 0.00051332
Epoch [44/200], Train Loss: 0.000503
Validation Loss: 0.00050013
Epoch [45/200], Train Loss: 0.000494
Validation Loss: 0.00048031
Epoch [46/200], Train Loss: 0.000470
Validation Loss: 0.00048423
Epoch [47/200], Train Loss: 0.000464
Validation Loss: 0.00047388
Epoch [48/200], Train Loss: 0.000456
Validation Loss: 0.00044650
Epoch [49/200], Train Loss: 0.000514
Validation Loss: 0.00046367
Epoch [50/200], Train Loss: 0.000441
Validation Loss: 0.00044331
Epoch [51/200], Train Loss: 0.000435
Validation Loss: 0.00050458
Epoch [52/200], Train Loss: 0.000432
Validation Loss: 0.00043623
Epoch [53/200], Train Loss: 0.000422
Validation Loss: 0.00044588
Epoch [54/200], Train Loss: 0.000408
Validation Loss: 0.00042394
Epoch [55/200], Train Loss: 0.000407
Validation Loss: 0.00041469
Epoch [56/200], Train Loss: 0.000410
Validation Loss: 0.00041304
Epoch [57/200], Train Loss: 0.000398
Validation Loss: 0.00040599
Epoch [58/200], Train Loss: 0.000400
Validation Loss: 0.00043896
Epoch [59/200], Train Loss: 0.000432
Validation Loss: 0.00040650
Epoch [60/200], Train Loss: 0.000386
Validation Loss: 0.00041023
Epoch [61/200], Train Loss: 0.000379
Validation Loss: 0.00039625
Epoch [62/200], Train Loss: 0.000369
Validation Loss: 0.00038887
Epoch [63/200], Train Loss: 0.000363
Validation Loss: 0.00037118
Epoch [64/200], Train Loss: 0.000539
Validation Loss: 0.00040164
Epoch [65/200], Train Loss: 0.000387
Validation Loss: 0.00038920
Epoch [66/200], Train Loss: 0.000363
Validation Loss: 0.00039500
Epoch [67/200], Train Loss: 0.000359
Validation Loss: 0.00036046
Epoch [68/200], Train Loss: 0.000355
Validation Loss: 0.00035347
Epoch [69/200], Train Loss: 0.000349
Validation Loss: 0.00036052
Epoch [70/200], Train Loss: 0.000348
Validation Loss: 0.00034306
Epoch [71/200], Train Loss: 0.000348
Validation Loss: 0.00034597
Epoch [72/200], Train Loss: 0.000336
Validation Loss: 0.00034596
Epoch [73/200], Train Loss: 0.000339
Validation Loss: 0.00033442
Epoch [74/200], Train Loss: 0.000347
Validation Loss: 0.00034689
Epoch [75/200], Train Loss: 0.000341
Validation Loss: 0.00033366
Epoch [76/200], Train Loss: 0.000324
Validation Loss: 0.00033421
Epoch [77/200], Train Loss: 0.000319
Validation Loss: 0.00031563
Epoch [78/200], Train Loss: 0.000319
Validation Loss: 0.00031342
Epoch [79/200], Train Loss: 0.000307
Validation Loss: 0.00032508
Epoch [80/200], Train Loss: 0.000316
Validation Loss: 0.00048602
Epoch [81/200], Train Loss: 0.000312
Validation Loss: 0.00030618
Epoch [82/200], Train Loss: 0.000347
Validation Loss: 0.00030672
Epoch [83/200], Train Loss: 0.000303
Validation Loss: 0.00031657
Epoch [84/200], Train Loss: 0.000301
Validation Loss: 0.00030096
Epoch [85/200], Train Loss: 0.000294
Validation Loss: 0.00029523
Epoch [86/200], Train Loss: 0.000288
Validation Loss: 0.00028454
Epoch [87/200], Train Loss: 0.000281
Validation Loss: 0.00028336
Epoch [88/200], Train Loss: 0.000291
Validation Loss: 0.00028363
Epoch [89/200], Train Loss: 0.000279
Validation Loss: 0.00028493
Epoch [90/200], Train Loss: 0.000272
Validation Loss: 0.00027025
Epoch [91/200], Train Loss: 0.000271
Validation Loss: 0.00026432
Epoch [92/200], Train Loss: 0.000255
Validation Loss: 0.00027567
Epoch [93/200], Train Loss: 0.000266
Validation Loss: 0.00027150
Epoch [94/200], Train Loss: 0.000266
Validation Loss: 0.00027192
Epoch [95/200], Train Loss: 0.000254
Validation Loss: 0.00026407
Epoch [96/200], Train Loss: 0.000247
Validation Loss: 0.00025555
Epoch [97/200], Train Loss: 0.000247
Validation Loss: 0.00026883
Epoch [98/200], Train Loss: 0.000252
Validation Loss: 0.00028802
Epoch [99/200], Train Loss: 0.000262
Validation Loss: 0.00025553
Epoch [100/200], Train Loss: 0.000249
Validation Loss: 0.00028703
Epoch [101/200], Train Loss: 0.000255
Validation Loss: 0.00023932
Epoch [102/200], Train Loss: 0.000237
Validation Loss: 0.00023302
Epoch [103/200], Train Loss: 0.000228
Validation Loss: 0.00022566
Epoch [104/200], Train Loss: 0.000238
Validation Loss: 0.00022740
Epoch [105/200], Train Loss: 0.000231
Validation Loss: 0.00022532
Epoch [106/200], Train Loss: 0.000225
Validation Loss: 0.00021524
Epoch [107/200], Train Loss: 0.000222
Validation Loss: 0.00021956
Epoch [108/200], Train Loss: 0.000225
Validation Loss: 0.00026776
Epoch [109/200], Train Loss: 0.000245
Validation Loss: 0.00024967
Epoch [110/200], Train Loss: 0.000234
Validation Loss: 0.00021826
Epoch [111/200], Train Loss: 0.000212
Validation Loss: 0.00021104
Epoch [112/200], Train Loss: 0.000218
Validation Loss: 0.00020590
Epoch [113/200], Train Loss: 0.000216
Validation Loss: 0.00020833
Epoch [114/200], Train Loss: 0.000209
Validation Loss: 0.00021135
Epoch [115/200], Train Loss: 0.000209
Validation Loss: 0.00020109
Epoch [116/200], Train Loss: 0.000219
Validation Loss: 0.00020221
Epoch [117/200], Train Loss: 0.000206
Validation Loss: 0.00020639
Epoch [118/200], Train Loss: 0.000202
Validation Loss: 0.00021122
Epoch [119/200], Train Loss: 0.000209
Validation Loss: 0.00022755
Epoch [120/200], Train Loss: 0.000211
Validation Loss: 0.00019339
Epoch [121/200], Train Loss: 0.000199
Validation Loss: 0.00019402
Epoch [122/200], Train Loss: 0.000204
Validation Loss: 0.00019637
Epoch [123/200], Train Loss: 0.000199
Validation Loss: 0.00019327
Epoch [124/200], Train Loss: 0.000195
Validation Loss: 0.00018837
Epoch [125/200], Train Loss: 0.000194
Validation Loss: 0.00018835
Epoch [126/200], Train Loss: 0.000197
Validation Loss: 0.00019467
Epoch [127/200], Train Loss: 0.000195
Validation Loss: 0.00018289
Epoch [128/200], Train Loss: 0.000198
Validation Loss: 0.00018656
Epoch [129/200], Train Loss: 0.000192
Validation Loss: 0.00017979
Epoch [130/200], Train Loss: 0.000204
Validation Loss: 0.00018159
Epoch [131/200], Train Loss: 0.000191
Validation Loss: 0.00018328
Epoch [132/200], Train Loss: 0.000186
Validation Loss: 0.00017464
Epoch [133/200], Train Loss: 0.000187
Validation Loss: 0.00018234
Epoch [134/200], Train Loss: 0.000184
Validation Loss: 0.00017694
Epoch [135/200], Train Loss: 0.000199
Validation Loss: 0.00018644
Epoch [136/200], Train Loss: 0.000183
Validation Loss: 0.00016689
Epoch [137/200], Train Loss: 0.000195
Validation Loss: 0.00017772
Epoch [138/200], Train Loss: 0.000185
Validation Loss: 0.00017259
Epoch [139/200], Train Loss: 0.000180
Validation Loss: 0.00018262
Epoch [140/200], Train Loss: 0.000179
Validation Loss: 0.00017087
Epoch [141/200], Train Loss: 0.000179
Validation Loss: 0.00016456
Epoch [142/200], Train Loss: 0.000177
Validation Loss: 0.00016537
Epoch [143/200], Train Loss: 0.000173
Validation Loss: 0.00017610
Epoch [144/200], Train Loss: 0.000183
Validation Loss: 0.00016208
Epoch [145/200], Train Loss: 0.000184
Validation Loss: 0.00017882
Epoch [146/200], Train Loss: 0.000176
Validation Loss: 0.00016348
Epoch [147/200], Train Loss: 0.000174
Validation Loss: 0.00015564
Epoch [148/200], Train Loss: 0.000174
Validation Loss: 0.00015877
Epoch [149/200], Train Loss: 0.000169
Validation Loss: 0.00016400
Epoch [150/200], Train Loss: 0.000169
Validation Loss: 0.00016245
Epoch [151/200], Train Loss: 0.000172
Validation Loss: 0.00015612
Epoch [152/200], Train Loss: 0.000175
Validation Loss: 0.00016743
Epoch [153/200], Train Loss: 0.000169
Validation Loss: 0.00015443
Epoch [154/200], Train Loss: 0.000172
Validation Loss: 0.00016020
Epoch [155/200], Train Loss: 0.000167
Validation Loss: 0.00015111
Epoch [156/200], Train Loss: 0.000165
Validation Loss: 0.00015849
Epoch [157/200], Train Loss: 0.000171
Validation Loss: 0.00014823
Epoch [158/200], Train Loss: 0.000163
Validation Loss: 0.00014766
Epoch [159/200], Train Loss: 0.000160
Validation Loss: 0.00015435
Epoch [160/200], Train Loss: 0.000163
Validation Loss: 0.00015116
Epoch [161/200], Train Loss: 0.000162
Validation Loss: 0.00015113
Epoch [162/200], Train Loss: 0.000159
Validation Loss: 0.00014804
Epoch [163/200], Train Loss: 0.000161
Validation Loss: 0.00015131
Epoch [164/200], Train Loss: 0.000163
Validation Loss: 0.00015407
Epoch [165/200], Train Loss: 0.000161
Validation Loss: 0.00014754
Epoch [166/200], Train Loss: 0.000154
Validation Loss: 0.00014725
Epoch [167/200], Train Loss: 0.000158
Validation Loss: 0.00015376
Epoch [168/200], Train Loss: 0.000158
Validation Loss: 0.00014518
Epoch [169/200], Train Loss: 0.000160
Validation Loss: 0.00014657
Epoch [170/200], Train Loss: 0.000156
Validation Loss: 0.00014513
Epoch [171/200], Train Loss: 0.000157
Validation Loss: 0.00014661
Epoch [172/200], Train Loss: 0.000155
Validation Loss: 0.00014215
Epoch [173/200], Train Loss: 0.000155
Validation Loss: 0.00014780
Epoch [174/200], Train Loss: 0.000151
Validation Loss: 0.00014300
Epoch [175/200], Train Loss: 0.000154
Validation Loss: 0.00015134
Epoch [176/200], Train Loss: 0.000152
Validation Loss: 0.00014043
Epoch [177/200], Train Loss: 0.000166
Validation Loss: 0.00015451
Epoch [178/200], Train Loss: 0.000157
Validation Loss: 0.00014662
Epoch [179/200], Train Loss: 0.000152
Validation Loss: 0.00014786
Epoch [180/200], Train Loss: 0.000157
Validation Loss: 0.00013936
Epoch [181/200], Train Loss: 0.000150
Validation Loss: 0.00014096
Epoch [182/200], Train Loss: 0.000151
Validation Loss: 0.00013702
Epoch [183/200], Train Loss: 0.000148
Validation Loss: 0.00013668
Epoch [184/200], Train Loss: 0.000149
Validation Loss: 0.00013748
Epoch [185/200], Train Loss: 0.000145
Validation Loss: 0.00013784
Epoch [186/200], Train Loss: 0.000145
Validation Loss: 0.00013546
Epoch [187/200], Train Loss: 0.000144
Validation Loss: 0.00013908
Epoch [188/200], Train Loss: 0.000146
Validation Loss: 0.00013657
Epoch [189/200], Train Loss: 0.000144
Validation Loss: 0.00013356
Epoch [190/200], Train Loss: 0.000144
Validation Loss: 0.00013651
Epoch [191/200], Train Loss: 0.000145
Validation Loss: 0.00013345
Epoch [192/200], Train Loss: 0.000144
Validation Loss: 0.00013495
Epoch [193/200], Train Loss: 0.000142
Validation Loss: 0.00013710
Epoch [194/200], Train Loss: 0.000139
Validation Loss: 0.00013315
Epoch [195/200], Train Loss: 0.000143
Validation Loss: 0.00013216
Epoch [196/200], Train Loss: 0.000145
Validation Loss: 0.00013354
Epoch [197/200], Train Loss: 0.000144
Validation Loss: 0.00013103
Epoch [198/200], Train Loss: 0.000142
Validation Loss: 0.00013262
Epoch [199/200], Train Loss: 0.000141
Validation Loss: 0.00013602
Epoch [200/200], Train Loss: 0.000140
Validation Loss: 0.00013128

Evaluating model for: Lamp
Run 100/144 completed in 7574.37 seconds with: {'MAE': np.float32(0.22010641), 'MSE': np.float32(6.543448), 'RMSE': np.float32(2.5580163), 'SAE': np.float32(0.010812333), 'NDE': np.float32(0.19646445)}

Run 101/144: hidden=256, seq_len=120, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.004891
Validation Loss: 0.00453777
Epoch [2/200], Train Loss: 0.004786
Validation Loss: 0.00451263
Epoch [3/200], Train Loss: 0.004761
Validation Loss: 0.00449824
Epoch [4/200], Train Loss: 0.004761
Validation Loss: 0.00449627
Epoch [5/200], Train Loss: 0.004749
Validation Loss: 0.00449077
Epoch [6/200], Train Loss: 0.004749
Validation Loss: 0.00448975
Epoch [7/200], Train Loss: 0.004731
Validation Loss: 0.00448269
Epoch [8/200], Train Loss: 0.004736
Validation Loss: 0.00448182
Epoch [9/200], Train Loss: 0.004717
Validation Loss: 0.00447032
Epoch [10/200], Train Loss: 0.004722
Validation Loss: 0.00446017
Epoch [11/200], Train Loss: 0.004705
Validation Loss: 0.00444025
Epoch [12/200], Train Loss: 0.004708
Validation Loss: 0.00443149
Epoch [13/200], Train Loss: 0.004663
Validation Loss: 0.00440345
Epoch [14/200], Train Loss: 0.004654
Validation Loss: 0.00437937
Epoch [15/200], Train Loss: 0.004648
Validation Loss: 0.00438311
Epoch [16/200], Train Loss: 0.004630
Validation Loss: 0.00433223
Epoch [17/200], Train Loss: 0.004549
Validation Loss: 0.00425590
Epoch [18/200], Train Loss: 0.004497
Validation Loss: 0.00415459
Epoch [19/200], Train Loss: 0.004390
Validation Loss: 0.00401388
Epoch [20/200], Train Loss: 0.004277
Validation Loss: 0.00394992
Epoch [21/200], Train Loss: 0.004147
Validation Loss: 0.00374080
Epoch [22/200], Train Loss: 0.003928
Validation Loss: 0.00350348
Epoch [23/200], Train Loss: 0.003740
Validation Loss: 0.00328756
Epoch [24/200], Train Loss: 0.003393
Validation Loss: 0.00291744
Epoch [25/200], Train Loss: 0.003057
Validation Loss: 0.00249814
Epoch [26/200], Train Loss: 0.002865
Validation Loss: 0.00234671
Epoch [27/200], Train Loss: 0.002642
Validation Loss: 0.00217146
Epoch [28/200], Train Loss: 0.002491
Validation Loss: 0.00205671
Epoch [29/200], Train Loss: 0.002384
Validation Loss: 0.00202104
Epoch [30/200], Train Loss: 0.002306
Validation Loss: 0.00193742
Epoch [31/200], Train Loss: 0.002179
Validation Loss: 0.00187914
Epoch [32/200], Train Loss: 0.002133
Validation Loss: 0.00179172
Epoch [33/200], Train Loss: 0.002089
Validation Loss: 0.00172306
Epoch [34/200], Train Loss: 0.002028
Validation Loss: 0.00170082
Epoch [35/200], Train Loss: 0.001998
Validation Loss: 0.00171813
Epoch [36/200], Train Loss: 0.001930
Validation Loss: 0.00163291
Epoch [37/200], Train Loss: 0.001886
Validation Loss: 0.00158333
Epoch [38/200], Train Loss: 0.001845
Validation Loss: 0.00157877
Epoch [39/200], Train Loss: 0.001829
Validation Loss: 0.00158375
Epoch [40/200], Train Loss: 0.001819
Validation Loss: 0.00150123
Epoch [41/200], Train Loss: 0.001733
Validation Loss: 0.00147723
Epoch [42/200], Train Loss: 0.001706
Validation Loss: 0.00143165
Epoch [43/200], Train Loss: 0.001671
Validation Loss: 0.00142359
Epoch [44/200], Train Loss: 0.001637
Validation Loss: 0.00137210
Epoch [45/200], Train Loss: 0.001631
Validation Loss: 0.00139092
Epoch [46/200], Train Loss: 0.001582
Validation Loss: 0.00131754
Epoch [47/200], Train Loss: 0.001544
Validation Loss: 0.00131598
Epoch [48/200], Train Loss: 0.001548
Validation Loss: 0.00130500
Epoch [49/200], Train Loss: 0.001507
Validation Loss: 0.00131545
Epoch [50/200], Train Loss: 0.001465
Validation Loss: 0.00125937
Epoch [51/200], Train Loss: 0.001450
Validation Loss: 0.00122750
Epoch [52/200], Train Loss: 0.001426
Validation Loss: 0.00121376
Epoch [53/200], Train Loss: 0.001398
Validation Loss: 0.00125317
Epoch [54/200], Train Loss: 0.001446
Validation Loss: 0.00124101
Epoch [55/200], Train Loss: 0.001364
Validation Loss: 0.00117259
Epoch [56/200], Train Loss: 0.001348
Validation Loss: 0.00115236
Epoch [57/200], Train Loss: 0.001313
Validation Loss: 0.00116239
Epoch [58/200], Train Loss: 0.001322
Validation Loss: 0.00120845
Epoch [59/200], Train Loss: 0.001308
Validation Loss: 0.00111959
Epoch [60/200], Train Loss: 0.001276
Validation Loss: 0.00112527
Epoch [61/200], Train Loss: 0.001264
Validation Loss: 0.00107201
Epoch [62/200], Train Loss: 0.001246
Validation Loss: 0.00114419
Epoch [63/200], Train Loss: 0.001233
Validation Loss: 0.00107014
Epoch [64/200], Train Loss: 0.001256
Validation Loss: 0.00104196
Epoch [65/200], Train Loss: 0.001197
Validation Loss: 0.00104681
Epoch [66/200], Train Loss: 0.001184
Validation Loss: 0.00102371
Epoch [67/200], Train Loss: 0.001224
Validation Loss: 0.00120398
Epoch [68/200], Train Loss: 0.001225
Validation Loss: 0.00102767
Epoch [69/200], Train Loss: 0.001145
Validation Loss: 0.00100062
Epoch [70/200], Train Loss: 0.001126
Validation Loss: 0.00100165
Epoch [71/200], Train Loss: 0.001133
Validation Loss: 0.00098093
Epoch [72/200], Train Loss: 0.001114
Validation Loss: 0.00098570
Epoch [73/200], Train Loss: 0.001101
Validation Loss: 0.00103246
Epoch [74/200], Train Loss: 0.001118
Validation Loss: 0.00099948
Epoch [75/200], Train Loss: 0.001084
Validation Loss: 0.00098051
Epoch [76/200], Train Loss: 0.001061
Validation Loss: 0.00096756
Epoch [77/200], Train Loss: 0.001050
Validation Loss: 0.00095748
Epoch [78/200], Train Loss: 0.001062
Validation Loss: 0.00093840
Epoch [79/200], Train Loss: 0.001058
Validation Loss: 0.00093017
Epoch [80/200], Train Loss: 0.001036
Validation Loss: 0.00092785
Epoch [81/200], Train Loss: 0.001055
Validation Loss: 0.00094817
Epoch [82/200], Train Loss: 0.001021
Validation Loss: 0.00091738
Epoch [83/200], Train Loss: 0.001006
Validation Loss: 0.00092035
Epoch [84/200], Train Loss: 0.001017
Validation Loss: 0.00094256
Epoch [85/200], Train Loss: 0.001005
Validation Loss: 0.00089367
Epoch [86/200], Train Loss: 0.000993
Validation Loss: 0.00091035
Epoch [87/200], Train Loss: 0.001003
Validation Loss: 0.00091109
Epoch [88/200], Train Loss: 0.000962
Validation Loss: 0.00089996
Epoch [89/200], Train Loss: 0.000988
Validation Loss: 0.00088888
Epoch [90/200], Train Loss: 0.000967
Validation Loss: 0.00086776
Epoch [91/200], Train Loss: 0.000953
Validation Loss: 0.00087206
Epoch [92/200], Train Loss: 0.000963
Validation Loss: 0.00084547
Epoch [93/200], Train Loss: 0.000950
Validation Loss: 0.00095454
Epoch [94/200], Train Loss: 0.000968
Validation Loss: 0.00084602
Epoch [95/200], Train Loss: 0.000946
Validation Loss: 0.00085795
Epoch [96/200], Train Loss: 0.000921
Validation Loss: 0.00081989
Epoch [97/200], Train Loss: 0.000924
Validation Loss: 0.00082012
Epoch [98/200], Train Loss: 0.000922
Validation Loss: 0.00084681
Epoch [99/200], Train Loss: 0.000913
Validation Loss: 0.00081029
Epoch [100/200], Train Loss: 0.000919
Validation Loss: 0.00083699
Epoch [101/200], Train Loss: 0.000896
Validation Loss: 0.00084587
Epoch [102/200], Train Loss: 0.000902
Validation Loss: 0.00079340
Epoch [103/200], Train Loss: 0.000888
Validation Loss: 0.00080045
Epoch [104/200], Train Loss: 0.000882
Validation Loss: 0.00081774
Epoch [105/200], Train Loss: 0.000873
Validation Loss: 0.00079230
Epoch [106/200], Train Loss: 0.000868
Validation Loss: 0.00078370
Epoch [107/200], Train Loss: 0.000870
Validation Loss: 0.00079498
Epoch [108/200], Train Loss: 0.000855
Validation Loss: 0.00076228
Epoch [109/200], Train Loss: 0.000886
Validation Loss: 0.00084026
Epoch [110/200], Train Loss: 0.000852
Validation Loss: 0.00075207
Epoch [111/200], Train Loss: 0.000839
Validation Loss: 0.00073829
Epoch [112/200], Train Loss: 0.000825
Validation Loss: 0.00073300
Epoch [113/200], Train Loss: 0.000832
Validation Loss: 0.00073846
Epoch [114/200], Train Loss: 0.000825
Validation Loss: 0.00075547
Epoch [115/200], Train Loss: 0.000822
Validation Loss: 0.00072577
Epoch [116/200], Train Loss: 0.000813
Validation Loss: 0.00070948
Epoch [117/200], Train Loss: 0.000804
Validation Loss: 0.00073218
Epoch [118/200], Train Loss: 0.000827
Validation Loss: 0.00078979
Epoch [119/200], Train Loss: 0.000825
Validation Loss: 0.00072132
Epoch [120/200], Train Loss: 0.000810
Validation Loss: 0.00070321
Epoch [121/200], Train Loss: 0.000780
Validation Loss: 0.00068558
Epoch [122/200], Train Loss: 0.000764
Validation Loss: 0.00067866
Epoch [123/200], Train Loss: 0.000768
Validation Loss: 0.00068963
Epoch [124/200], Train Loss: 0.000765
Validation Loss: 0.00068360
Epoch [125/200], Train Loss: 0.000755
Validation Loss: 0.00066323
Epoch [126/200], Train Loss: 0.000756
Validation Loss: 0.00065421
Epoch [127/200], Train Loss: 0.000760
Validation Loss: 0.00073149
Epoch [128/200], Train Loss: 0.000755
Validation Loss: 0.00066982
Epoch [129/200], Train Loss: 0.000743
Validation Loss: 0.00065419
Epoch [130/200], Train Loss: 0.000725
Validation Loss: 0.00063395
Epoch [131/200], Train Loss: 0.000726
Validation Loss: 0.00067409
Epoch [132/200], Train Loss: 0.000734
Validation Loss: 0.00066497
Epoch [133/200], Train Loss: 0.000750
Validation Loss: 0.00070500
Epoch [134/200], Train Loss: 0.000730
Validation Loss: 0.00064169
Epoch [135/200], Train Loss: 0.000712
Validation Loss: 0.00064228
Epoch [136/200], Train Loss: 0.000723
Validation Loss: 0.00063219
Epoch [137/200], Train Loss: 0.000709
Validation Loss: 0.00063556
Epoch [138/200], Train Loss: 0.000695
Validation Loss: 0.00062629
Epoch [139/200], Train Loss: 0.000691
Validation Loss: 0.00060743
Epoch [140/200], Train Loss: 0.000697
Validation Loss: 0.00062684
Epoch [141/200], Train Loss: 0.000710
Validation Loss: 0.00060254
Epoch [142/200], Train Loss: 0.000684
Validation Loss: 0.00062380
Epoch [143/200], Train Loss: 0.000679
Validation Loss: 0.00060174
Epoch [144/200], Train Loss: 0.000718
Validation Loss: 0.00062949
Epoch [145/200], Train Loss: 0.000796
Validation Loss: 0.00059327
Epoch [146/200], Train Loss: 0.000667
Validation Loss: 0.00058796
Epoch [147/200], Train Loss: 0.000667
Validation Loss: 0.00058928
Epoch [148/200], Train Loss: 0.000662
Validation Loss: 0.00058356
Epoch [149/200], Train Loss: 0.000655
Validation Loss: 0.00058664
Epoch [150/200], Train Loss: 0.000658
Validation Loss: 0.00059102
Epoch [151/200], Train Loss: 0.000654
Validation Loss: 0.00058669
Epoch [152/200], Train Loss: 0.000650
Validation Loss: 0.00058834
Epoch [153/200], Train Loss: 0.000652
Validation Loss: 0.00057014
Epoch [154/200], Train Loss: 0.000674
Validation Loss: 0.00060927
Epoch [155/200], Train Loss: 0.000652
Validation Loss: 0.00058833
Epoch [156/200], Train Loss: 0.000645
Validation Loss: 0.00056794
Epoch [157/200], Train Loss: 0.000641
Validation Loss: 0.00058293
Epoch [158/200], Train Loss: 0.000636
Validation Loss: 0.00056784
Epoch [159/200], Train Loss: 0.000632
Validation Loss: 0.00057141
Epoch [160/200], Train Loss: 0.000626
Validation Loss: 0.00055692
Epoch [161/200], Train Loss: 0.000627
Validation Loss: 0.00057894
Epoch [162/200], Train Loss: 0.000625
Validation Loss: 0.00057215
Epoch [163/200], Train Loss: 0.000632
Validation Loss: 0.00055169
Epoch [164/200], Train Loss: 0.000749
Validation Loss: 0.00057519
Epoch [165/200], Train Loss: 0.000644
Validation Loss: 0.00057168
Epoch [166/200], Train Loss: 0.000626
Validation Loss: 0.00054800
Epoch [167/200], Train Loss: 0.000639
Validation Loss: 0.00055098
Epoch [168/200], Train Loss: 0.000625
Validation Loss: 0.00055546
Epoch [169/200], Train Loss: 0.000618
Validation Loss: 0.00055670
Epoch [170/200], Train Loss: 0.000621
Validation Loss: 0.00054501
Epoch [171/200], Train Loss: 0.000633
Validation Loss: 0.00054552
Epoch [172/200], Train Loss: 0.000623
Validation Loss: 0.00054177
Epoch [173/200], Train Loss: 0.000608
Validation Loss: 0.00053520
Epoch [174/200], Train Loss: 0.000611
Validation Loss: 0.00053711
Epoch [175/200], Train Loss: 0.000599
Validation Loss: 0.00055273
Epoch [176/200], Train Loss: 0.000599
Validation Loss: 0.00053732
Epoch [177/200], Train Loss: 0.000597
Validation Loss: 0.00054333
Epoch [178/200], Train Loss: 0.000595
Validation Loss: 0.00054354
Epoch [179/200], Train Loss: 0.000906
Validation Loss: 0.00086260
Epoch [180/200], Train Loss: 0.000696
Validation Loss: 0.00058913
Epoch [181/200], Train Loss: 0.000623
Validation Loss: 0.00056910
Epoch [182/200], Train Loss: 0.000608
Validation Loss: 0.00055950
Epoch [183/200], Train Loss: 0.000600
Validation Loss: 0.00054840
Early stopping triggered

Evaluating model for: Lamp
Run 101/144 completed in 2005.06 seconds with: {'MAE': np.float32(0.6092371), 'MSE': np.float32(17.060852), 'RMSE': np.float32(4.1304784), 'SAE': np.float32(0.017961094), 'NDE': np.float32(0.31572202)}

Run 102/144: hidden=256, seq_len=120, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.004826
Validation Loss: 0.00450097
Epoch [2/200], Train Loss: 0.004737
Validation Loss: 0.00450046
Epoch [3/200], Train Loss: 0.004751
Validation Loss: 0.00448705
Epoch [4/200], Train Loss: 0.004743
Validation Loss: 0.00448435
Epoch [5/200], Train Loss: 0.004747
Validation Loss: 0.00448320
Epoch [6/200], Train Loss: 0.004746
Validation Loss: 0.00448790
Epoch [7/200], Train Loss: 0.004724
Validation Loss: 0.00448502
Epoch [8/200], Train Loss: 0.004746
Validation Loss: 0.00448111
Epoch [9/200], Train Loss: 0.004727
Validation Loss: 0.00447671
Epoch [10/200], Train Loss: 0.004733
Validation Loss: 0.00447495
Epoch [11/200], Train Loss: 0.004716
Validation Loss: 0.00447421
Epoch [12/200], Train Loss: 0.004728
Validation Loss: 0.00447684
Epoch [13/200], Train Loss: 0.004711
Validation Loss: 0.00447967
Epoch [14/200], Train Loss: 0.004722
Validation Loss: 0.00445618
Epoch [15/200], Train Loss: 0.004687
Validation Loss: 0.00445992
Epoch [16/200], Train Loss: 0.004695
Validation Loss: 0.00442873
Epoch [17/200], Train Loss: 0.004668
Validation Loss: 0.00444389
Epoch [18/200], Train Loss: 0.004655
Validation Loss: 0.00437454
Epoch [19/200], Train Loss: 0.004628
Validation Loss: 0.00445043
Epoch [20/200], Train Loss: 0.004612
Validation Loss: 0.00428457
Epoch [21/200], Train Loss: 0.004510
Validation Loss: 0.00422010
Epoch [22/200], Train Loss: 0.004411
Validation Loss: 0.00414071
Epoch [23/200], Train Loss: 0.004158
Validation Loss: 0.00368121
Epoch [24/200], Train Loss: 0.003728
Validation Loss: 0.00319572
Epoch [25/200], Train Loss: 0.003100
Validation Loss: 0.00234535
Epoch [26/200], Train Loss: 0.002631
Validation Loss: 0.00207283
Epoch [27/200], Train Loss: 0.002363
Validation Loss: 0.00192338
Epoch [28/200], Train Loss: 0.002220
Validation Loss: 0.00179929
Epoch [29/200], Train Loss: 0.002014
Validation Loss: 0.00166734
Epoch [30/200], Train Loss: 0.001938
Validation Loss: 0.00161818
Epoch [31/200], Train Loss: 0.001820
Validation Loss: 0.00153406
Epoch [32/200], Train Loss: 0.001737
Validation Loss: 0.00145755
Epoch [33/200], Train Loss: 0.001664
Validation Loss: 0.00142876
Epoch [34/200], Train Loss: 0.001614
Validation Loss: 0.00137229
Epoch [35/200], Train Loss: 0.001557
Validation Loss: 0.00136324
Epoch [36/200], Train Loss: 0.001483
Validation Loss: 0.00127912
Epoch [37/200], Train Loss: 0.001438
Validation Loss: 0.00122206
Epoch [38/200], Train Loss: 0.001382
Validation Loss: 0.00117302
Epoch [39/200], Train Loss: 0.001331
Validation Loss: 0.00116574
Epoch [40/200], Train Loss: 0.001327
Validation Loss: 0.00112795
Epoch [41/200], Train Loss: 0.001253
Validation Loss: 0.00110671
Epoch [42/200], Train Loss: 0.001240
Validation Loss: 0.00106935
Epoch [43/200], Train Loss: 0.001212
Validation Loss: 0.00105163
Epoch [44/200], Train Loss: 0.001195
Validation Loss: 0.00103033
Epoch [45/200], Train Loss: 0.001163
Validation Loss: 0.00100268
Epoch [46/200], Train Loss: 0.001101
Validation Loss: 0.00100060
Epoch [47/200], Train Loss: 0.001100
Validation Loss: 0.00097937
Epoch [48/200], Train Loss: 0.001087
Validation Loss: 0.00094188
Epoch [49/200], Train Loss: 0.001050
Validation Loss: 0.00091961
Epoch [50/200], Train Loss: 0.001018
Validation Loss: 0.00089268
Epoch [51/200], Train Loss: 0.001004
Validation Loss: 0.00086732
Epoch [52/200], Train Loss: 0.000965
Validation Loss: 0.00084650
Epoch [53/200], Train Loss: 0.000951
Validation Loss: 0.00085061
Epoch [54/200], Train Loss: 0.000919
Validation Loss: 0.00081390
Epoch [55/200], Train Loss: 0.000894
Validation Loss: 0.00080271
Epoch [56/200], Train Loss: 0.000958
Validation Loss: 0.00080527
Epoch [57/200], Train Loss: 0.000904
Validation Loss: 0.00078683
Epoch [58/200], Train Loss: 0.000912
Validation Loss: 0.00078889
Epoch [59/200], Train Loss: 0.000832
Validation Loss: 0.00074785
Epoch [60/200], Train Loss: 0.000814
Validation Loss: 0.00073124
Epoch [61/200], Train Loss: 0.000811
Validation Loss: 0.00069181
Epoch [62/200], Train Loss: 0.000788
Validation Loss: 0.00070516
Epoch [63/200], Train Loss: 0.000792
Validation Loss: 0.00078138
Epoch [64/200], Train Loss: 0.000803
Validation Loss: 0.00067469
Epoch [65/200], Train Loss: 0.000761
Validation Loss: 0.00069568
Epoch [66/200], Train Loss: 0.000760
Validation Loss: 0.00066327
Epoch [67/200], Train Loss: 0.000749
Validation Loss: 0.00065493
Epoch [68/200], Train Loss: 0.000746
Validation Loss: 0.00071957
Epoch [69/200], Train Loss: 0.000757
Validation Loss: 0.00063068
Epoch [70/200], Train Loss: 0.000761
Validation Loss: 0.00065624
Epoch [71/200], Train Loss: 0.000736
Validation Loss: 0.00065197
Epoch [72/200], Train Loss: 0.000728
Validation Loss: 0.00065910
Epoch [73/200], Train Loss: 0.000713
Validation Loss: 0.00060738
Epoch [74/200], Train Loss: 0.000699
Validation Loss: 0.00059656
Epoch [75/200], Train Loss: 0.000683
Validation Loss: 0.00059809
Epoch [76/200], Train Loss: 0.000674
Validation Loss: 0.00058202
Epoch [77/200], Train Loss: 0.000677
Validation Loss: 0.00060311
Epoch [78/200], Train Loss: 0.000689
Validation Loss: 0.00063207
Epoch [79/200], Train Loss: 0.000675
Validation Loss: 0.00059170
Epoch [80/200], Train Loss: 0.000659
Validation Loss: 0.00060631
Epoch [81/200], Train Loss: 0.000662
Validation Loss: 0.00057602
Epoch [82/200], Train Loss: 0.000647
Validation Loss: 0.00057080
Epoch [83/200], Train Loss: 0.000651
Validation Loss: 0.00056551
Epoch [84/200], Train Loss: 0.000633
Validation Loss: 0.00054720
Epoch [85/200], Train Loss: 0.000632
Validation Loss: 0.00055325
Epoch [86/200], Train Loss: 0.000635
Validation Loss: 0.00056593
Epoch [87/200], Train Loss: 0.000617
Validation Loss: 0.00054202
Epoch [88/200], Train Loss: 0.000625
Validation Loss: 0.00053991
Epoch [89/200], Train Loss: 0.000614
Validation Loss: 0.00053882
Epoch [90/200], Train Loss: 0.000656
Validation Loss: 0.00062331
Epoch [91/200], Train Loss: 0.000623
Validation Loss: 0.00052647
Epoch [92/200], Train Loss: 0.000594
Validation Loss: 0.00050980
Epoch [93/200], Train Loss: 0.000597
Validation Loss: 0.00052000
Epoch [94/200], Train Loss: 0.000593
Validation Loss: 0.00050627
Epoch [95/200], Train Loss: 0.000583
Validation Loss: 0.00052220
Epoch [96/200], Train Loss: 0.000579
Validation Loss: 0.00050565
Epoch [97/200], Train Loss: 0.000580
Validation Loss: 0.00050656
Epoch [98/200], Train Loss: 0.000575
Validation Loss: 0.00054155
Epoch [99/200], Train Loss: 0.000604
Validation Loss: 0.00060232
Epoch [100/200], Train Loss: 0.000598
Validation Loss: 0.00049978
Epoch [101/200], Train Loss: 0.000571
Validation Loss: 0.00048551
Epoch [102/200], Train Loss: 0.000578
Validation Loss: 0.00051514
Epoch [103/200], Train Loss: 0.000576
Validation Loss: 0.00048349
Epoch [104/200], Train Loss: 0.000551
Validation Loss: 0.00050828
Epoch [105/200], Train Loss: 0.000567
Validation Loss: 0.00052619
Epoch [106/200], Train Loss: 0.000562
Validation Loss: 0.00047332
Epoch [107/200], Train Loss: 0.000554
Validation Loss: 0.00050136
Epoch [108/200], Train Loss: 0.000556
Validation Loss: 0.00046644
Epoch [109/200], Train Loss: 0.000540
Validation Loss: 0.00047109
Epoch [110/200], Train Loss: 0.000528
Validation Loss: 0.00046792
Epoch [111/200], Train Loss: 0.000535
Validation Loss: 0.00047329
Epoch [112/200], Train Loss: 0.000523
Validation Loss: 0.00048424
Epoch [113/200], Train Loss: 0.000540
Validation Loss: 0.00046790
Epoch [114/200], Train Loss: 0.000524
Validation Loss: 0.00046138
Epoch [115/200], Train Loss: 0.000516
Validation Loss: 0.00045221
Epoch [116/200], Train Loss: 0.000515
Validation Loss: 0.00044959
Epoch [117/200], Train Loss: 0.000523
Validation Loss: 0.00048901
Epoch [118/200], Train Loss: 0.000594
Validation Loss: 0.00046028
Epoch [119/200], Train Loss: 0.000522
Validation Loss: 0.00045928
Epoch [120/200], Train Loss: 0.000506
Validation Loss: 0.00044841
Epoch [121/200], Train Loss: 0.000512
Validation Loss: 0.00044874
Epoch [122/200], Train Loss: 0.000511
Validation Loss: 0.00046206
Epoch [123/200], Train Loss: 0.000505
Validation Loss: 0.00044914
Epoch [124/200], Train Loss: 0.000498
Validation Loss: 0.00044320
Epoch [125/200], Train Loss: 0.000501
Validation Loss: 0.00044820
Epoch [126/200], Train Loss: 0.000487
Validation Loss: 0.00043720
Epoch [127/200], Train Loss: 0.000490
Validation Loss: 0.00044440
Epoch [128/200], Train Loss: 0.000489
Validation Loss: 0.00044585
Epoch [129/200], Train Loss: 0.000491
Validation Loss: 0.00042722
Epoch [130/200], Train Loss: 0.000482
Validation Loss: 0.00044913
Epoch [131/200], Train Loss: 0.000484
Validation Loss: 0.00042916
Epoch [132/200], Train Loss: 0.000486
Validation Loss: 0.00046154
Epoch [133/200], Train Loss: 0.000478
Validation Loss: 0.00042929
Epoch [134/200], Train Loss: 0.000475
Validation Loss: 0.00043613
Epoch [135/200], Train Loss: 0.000476
Validation Loss: 0.00041708
Epoch [136/200], Train Loss: 0.000477
Validation Loss: 0.00041905
Epoch [137/200], Train Loss: 0.000471
Validation Loss: 0.00044621
Epoch [138/200], Train Loss: 0.000465
Validation Loss: 0.00041987
Epoch [139/200], Train Loss: 0.000468
Validation Loss: 0.00041795
Epoch [140/200], Train Loss: 0.000465
Validation Loss: 0.00042164
Epoch [141/200], Train Loss: 0.000477
Validation Loss: 0.00041303
Epoch [142/200], Train Loss: 0.000465
Validation Loss: 0.00042337
Epoch [143/200], Train Loss: 0.000454
Validation Loss: 0.00043501
Epoch [144/200], Train Loss: 0.000458
Validation Loss: 0.00043159
Epoch [145/200], Train Loss: 0.000452
Validation Loss: 0.00040349
Epoch [146/200], Train Loss: 0.000463
Validation Loss: 0.00043282
Epoch [147/200], Train Loss: 0.000477
Validation Loss: 0.00040914
Epoch [148/200], Train Loss: 0.000464
Validation Loss: 0.00047284
Epoch [149/200], Train Loss: 0.000457
Validation Loss: 0.00039525
Epoch [150/200], Train Loss: 0.000451
Validation Loss: 0.00043040
Epoch [151/200], Train Loss: 0.000451
Validation Loss: 0.00040019
Epoch [152/200], Train Loss: 0.000448
Validation Loss: 0.00041634
Epoch [153/200], Train Loss: 0.000450
Validation Loss: 0.00039191
Epoch [154/200], Train Loss: 0.000444
Validation Loss: 0.00040375
Epoch [155/200], Train Loss: 0.000443
Validation Loss: 0.00040319
Epoch [156/200], Train Loss: 0.000438
Validation Loss: 0.00038806
Epoch [157/200], Train Loss: 0.000496
Validation Loss: 0.00054290
Epoch [158/200], Train Loss: 0.000509
Validation Loss: 0.00043939
Epoch [159/200], Train Loss: 0.000449
Validation Loss: 0.00039972
Epoch [160/200], Train Loss: 0.000441
Validation Loss: 0.00040034
Epoch [161/200], Train Loss: 0.000451
Validation Loss: 0.00038658
Epoch [162/200], Train Loss: 0.000439
Validation Loss: 0.00038896
Epoch [163/200], Train Loss: 0.000435
Validation Loss: 0.00038655
Epoch [164/200], Train Loss: 0.000432
Validation Loss: 0.00037868
Epoch [165/200], Train Loss: 0.000435
Validation Loss: 0.00038098
Epoch [166/200], Train Loss: 0.000436
Validation Loss: 0.00037881
Epoch [167/200], Train Loss: 0.000429
Validation Loss: 0.00037699
Epoch [168/200], Train Loss: 0.000424
Validation Loss: 0.00038216
Epoch [169/200], Train Loss: 0.000436
Validation Loss: 0.00041494
Epoch [170/200], Train Loss: 0.000429
Validation Loss: 0.00037961
Epoch [171/200], Train Loss: 0.000425
Validation Loss: 0.00037492
Epoch [172/200], Train Loss: 0.000431
Validation Loss: 0.00042934
Epoch [173/200], Train Loss: 0.000429
Validation Loss: 0.00037676
Epoch [174/200], Train Loss: 0.000420
Validation Loss: 0.00037575
Epoch [175/200], Train Loss: 0.000534
Validation Loss: 0.00039786
Epoch [176/200], Train Loss: 0.000432
Validation Loss: 0.00037581
Epoch [177/200], Train Loss: 0.000428
Validation Loss: 0.00037988
Epoch [178/200], Train Loss: 0.000420
Validation Loss: 0.00038075
Epoch [179/200], Train Loss: 0.000418
Validation Loss: 0.00039263
Epoch [180/200], Train Loss: 0.000426
Validation Loss: 0.00036590
Epoch [181/200], Train Loss: 0.000417
Validation Loss: 0.00037036
Epoch [182/200], Train Loss: 0.000412
Validation Loss: 0.00036844
Epoch [183/200], Train Loss: 0.000411
Validation Loss: 0.00035922
Epoch [184/200], Train Loss: 0.000416
Validation Loss: 0.00037522
Epoch [185/200], Train Loss: 0.000407
Validation Loss: 0.00035906
Epoch [186/200], Train Loss: 0.000407
Validation Loss: 0.00036503
Epoch [187/200], Train Loss: 0.000413
Validation Loss: 0.00037133
Epoch [188/200], Train Loss: 0.000414
Validation Loss: 0.00035599
Epoch [189/200], Train Loss: 0.000407
Validation Loss: 0.00034690
Epoch [190/200], Train Loss: 0.000405
Validation Loss: 0.00036136
Epoch [191/200], Train Loss: 0.000404
Validation Loss: 0.00035459
Epoch [192/200], Train Loss: 0.000403
Validation Loss: 0.00035002
Epoch [193/200], Train Loss: 0.000407
Validation Loss: 0.00035776
Epoch [194/200], Train Loss: 0.000405
Validation Loss: 0.00035597
Epoch [195/200], Train Loss: 0.000406
Validation Loss: 0.00035494
Epoch [196/200], Train Loss: 0.000404
Validation Loss: 0.00035777
Epoch [197/200], Train Loss: 0.000397
Validation Loss: 0.00034048
Epoch [198/200], Train Loss: 0.000402
Validation Loss: 0.00033443
Epoch [199/200], Train Loss: 0.000394
Validation Loss: 0.00035417
Epoch [200/200], Train Loss: 0.000394
Validation Loss: 0.00033793

Evaluating model for: Lamp
Run 102/144 completed in 2430.19 seconds with: {'MAE': np.float32(0.42149842), 'MSE': np.float32(10.754216), 'RMSE': np.float32(3.2793622), 'SAE': np.float32(0.041518304), 'NDE': np.float32(0.250665)}

Run 103/144: hidden=256, seq_len=120, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.004846
Validation Loss: 0.00456320
Epoch [2/200], Train Loss: 0.004767
Validation Loss: 0.00449672
Epoch [3/200], Train Loss: 0.004747
Validation Loss: 0.00449477
Epoch [4/200], Train Loss: 0.004766
Validation Loss: 0.00448956
Epoch [5/200], Train Loss: 0.004739
Validation Loss: 0.00448377
Epoch [6/200], Train Loss: 0.004740
Validation Loss: 0.00447962
Epoch [7/200], Train Loss: 0.004734
Validation Loss: 0.00447623
Epoch [8/200], Train Loss: 0.004761
Validation Loss: 0.00448397
Epoch [9/200], Train Loss: 0.004753
Validation Loss: 0.00447776
Epoch [10/200], Train Loss: 0.004751
Validation Loss: 0.00448411
Epoch [11/200], Train Loss: 0.004721
Validation Loss: 0.00447163
Epoch [12/200], Train Loss: 0.004717
Validation Loss: 0.00448077
Epoch [13/200], Train Loss: 0.004723
Validation Loss: 0.00446655
Epoch [14/200], Train Loss: 0.004733
Validation Loss: 0.00447630
Epoch [15/200], Train Loss: 0.004733
Validation Loss: 0.00446478
Epoch [16/200], Train Loss: 0.004732
Validation Loss: 0.00447162
Epoch [17/200], Train Loss: 0.004727
Validation Loss: 0.00446566
Epoch [18/200], Train Loss: 0.004739
Validation Loss: 0.00446893
Epoch [19/200], Train Loss: 0.004714
Validation Loss: 0.00446768
Epoch [20/200], Train Loss: 0.004727
Validation Loss: 0.00446477
Epoch [21/200], Train Loss: 0.004712
Validation Loss: 0.00445867
Epoch [22/200], Train Loss: 0.004709
Validation Loss: 0.00445995
Epoch [23/200], Train Loss: 0.004712
Validation Loss: 0.00445801
Epoch [24/200], Train Loss: 0.004727
Validation Loss: 0.00445584
Epoch [25/200], Train Loss: 0.004719
Validation Loss: 0.00445194
Epoch [26/200], Train Loss: 0.004718
Validation Loss: 0.00445040
Epoch [27/200], Train Loss: 0.004729
Validation Loss: 0.00445966
Epoch [28/200], Train Loss: 0.004716
Validation Loss: 0.00445529
Epoch [29/200], Train Loss: 0.004732
Validation Loss: 0.00445710
Epoch [30/200], Train Loss: 0.004734
Validation Loss: 0.00445176
Epoch [31/200], Train Loss: 0.004703
Validation Loss: 0.00444810
Epoch [32/200], Train Loss: 0.004730
Validation Loss: 0.00445306
Epoch [33/200], Train Loss: 0.004732
Validation Loss: 0.00444784
Epoch [34/200], Train Loss: 0.004711
Validation Loss: 0.00443867
Epoch [35/200], Train Loss: 0.004700
Validation Loss: 0.00443249
Epoch [36/200], Train Loss: 0.004681
Validation Loss: 0.00439703
Epoch [37/200], Train Loss: 0.004672
Validation Loss: 0.00439627
Epoch [38/200], Train Loss: 0.004684
Validation Loss: 0.00437724
Epoch [39/200], Train Loss: 0.004625
Validation Loss: 0.00427273
Epoch [40/200], Train Loss: 0.004509
Validation Loss: 0.00415439
Epoch [41/200], Train Loss: 0.004264
Validation Loss: 0.00386728
Epoch [42/200], Train Loss: 0.003784
Validation Loss: 0.00290875
Epoch [43/200], Train Loss: 0.002998
Validation Loss: 0.00225431
Epoch [44/200], Train Loss: 0.002494
Validation Loss: 0.00198878
Epoch [45/200], Train Loss: 0.002222
Validation Loss: 0.00174085
Epoch [46/200], Train Loss: 0.002000
Validation Loss: 0.00166922
Epoch [47/200], Train Loss: 0.001882
Validation Loss: 0.00152397
Epoch [48/200], Train Loss: 0.001761
Validation Loss: 0.00144604
Epoch [49/200], Train Loss: 0.001669
Validation Loss: 0.00144464
Epoch [50/200], Train Loss: 0.001569
Validation Loss: 0.00129572
Epoch [51/200], Train Loss: 0.001520
Validation Loss: 0.00130137
Epoch [52/200], Train Loss: 0.001459
Validation Loss: 0.00124270
Epoch [53/200], Train Loss: 0.001375
Validation Loss: 0.00120559
Epoch [54/200], Train Loss: 0.001351
Validation Loss: 0.00117020
Epoch [55/200], Train Loss: 0.001285
Validation Loss: 0.00110265
Epoch [56/200], Train Loss: 0.001270
Validation Loss: 0.00106619
Epoch [57/200], Train Loss: 0.001171
Validation Loss: 0.00094772
Epoch [58/200], Train Loss: 0.001166
Validation Loss: 0.00099416
Epoch [59/200], Train Loss: 0.001313
Validation Loss: 0.00097376
Epoch [60/200], Train Loss: 0.001101
Validation Loss: 0.00090015
Epoch [61/200], Train Loss: 0.001030
Validation Loss: 0.00087507
Epoch [62/200], Train Loss: 0.000995
Validation Loss: 0.00088046
Epoch [63/200], Train Loss: 0.000999
Validation Loss: 0.00089853
Epoch [64/200], Train Loss: 0.000976
Validation Loss: 0.00089376
Epoch [65/200], Train Loss: 0.000937
Validation Loss: 0.00080987
Epoch [66/200], Train Loss: 0.000913
Validation Loss: 0.00079107
Epoch [67/200], Train Loss: 0.000890
Validation Loss: 0.00078734
Epoch [68/200], Train Loss: 0.000883
Validation Loss: 0.00080921
Epoch [69/200], Train Loss: 0.000890
Validation Loss: 0.00076955
Epoch [70/200], Train Loss: 0.000847
Validation Loss: 0.00073641
Epoch [71/200], Train Loss: 0.000808
Validation Loss: 0.00070281
Epoch [72/200], Train Loss: 0.000817
Validation Loss: 0.00073936
Epoch [73/200], Train Loss: 0.000802
Validation Loss: 0.00074571
Epoch [74/200], Train Loss: 0.000787
Validation Loss: 0.00084656
Epoch [75/200], Train Loss: 0.000771
Validation Loss: 0.00066097
Epoch [76/200], Train Loss: 0.000736
Validation Loss: 0.00065177
Epoch [77/200], Train Loss: 0.000732
Validation Loss: 0.00067139
Epoch [78/200], Train Loss: 0.000721
Validation Loss: 0.00062931
Epoch [79/200], Train Loss: 0.000714
Validation Loss: 0.00062074
Epoch [80/200], Train Loss: 0.000700
Validation Loss: 0.00061501
Epoch [81/200], Train Loss: 0.000833
Validation Loss: 0.00070435
Epoch [82/200], Train Loss: 0.000705
Validation Loss: 0.00062844
Epoch [83/200], Train Loss: 0.000691
Validation Loss: 0.00062241
Epoch [84/200], Train Loss: 0.000693
Validation Loss: 0.00059280
Epoch [85/200], Train Loss: 0.000662
Validation Loss: 0.00058226
Epoch [86/200], Train Loss: 0.000654
Validation Loss: 0.00057221
Epoch [87/200], Train Loss: 0.000654
Validation Loss: 0.00057796
Epoch [88/200], Train Loss: 0.000679
Validation Loss: 0.00058544
Epoch [89/200], Train Loss: 0.000676
Validation Loss: 0.00067135
Epoch [90/200], Train Loss: 0.000658
Validation Loss: 0.00054767
Epoch [91/200], Train Loss: 0.000625
Validation Loss: 0.00055095
Epoch [92/200], Train Loss: 0.000613
Validation Loss: 0.00056675
Epoch [93/200], Train Loss: 0.000603
Validation Loss: 0.00054337
Epoch [94/200], Train Loss: 0.000610
Validation Loss: 0.00052634
Epoch [95/200], Train Loss: 0.000612
Validation Loss: 0.00053458
Epoch [96/200], Train Loss: 0.000596
Validation Loss: 0.00051926
Epoch [97/200], Train Loss: 0.000594
Validation Loss: 0.00051999
Epoch [98/200], Train Loss: 0.000596
Validation Loss: 0.00052404
Epoch [99/200], Train Loss: 0.000583
Validation Loss: 0.00057081
Epoch [100/200], Train Loss: 0.000592
Validation Loss: 0.00052917
Epoch [101/200], Train Loss: 0.000585
Validation Loss: 0.00051985
Epoch [102/200], Train Loss: 0.000578
Validation Loss: 0.00051242
Epoch [103/200], Train Loss: 0.000565
Validation Loss: 0.00051277
Epoch [104/200], Train Loss: 0.000559
Validation Loss: 0.00053094
Epoch [105/200], Train Loss: 0.000567
Validation Loss: 0.00051759
Epoch [106/200], Train Loss: 0.000554
Validation Loss: 0.00049547
Epoch [107/200], Train Loss: 0.000631
Validation Loss: 0.00056222
Epoch [108/200], Train Loss: 0.000572
Validation Loss: 0.00050895
Epoch [109/200], Train Loss: 0.000574
Validation Loss: 0.00051907
Epoch [110/200], Train Loss: 0.000556
Validation Loss: 0.00049209
Epoch [111/200], Train Loss: 0.000544
Validation Loss: 0.00048918
Epoch [112/200], Train Loss: 0.000536
Validation Loss: 0.00050356
Epoch [113/200], Train Loss: 0.000541
Validation Loss: 0.00048046
Epoch [114/200], Train Loss: 0.000528
Validation Loss: 0.00046740
Epoch [115/200], Train Loss: 0.000526
Validation Loss: 0.00048640
Epoch [116/200], Train Loss: 0.000544
Validation Loss: 0.00047274
Epoch [117/200], Train Loss: 0.000516
Validation Loss: 0.00046277
Epoch [118/200], Train Loss: 0.000569
Validation Loss: 0.00059326
Epoch [119/200], Train Loss: 0.000590
Validation Loss: 0.00047584
Epoch [120/200], Train Loss: 0.000520
Validation Loss: 0.00047254
Epoch [121/200], Train Loss: 0.000506
Validation Loss: 0.00047063
Epoch [122/200], Train Loss: 0.000503
Validation Loss: 0.00045041
Epoch [123/200], Train Loss: 0.000506
Validation Loss: 0.00046769
Epoch [124/200], Train Loss: 0.000528
Validation Loss: 0.00046240
Epoch [125/200], Train Loss: 0.000494
Validation Loss: 0.00044795
Epoch [126/200], Train Loss: 0.000494
Validation Loss: 0.00044323
Epoch [127/200], Train Loss: 0.000488
Validation Loss: 0.00044290
Epoch [128/200], Train Loss: 0.000487
Validation Loss: 0.00044241
Epoch [129/200], Train Loss: 0.000483
Validation Loss: 0.00044868
Epoch [130/200], Train Loss: 0.000483
Validation Loss: 0.00044396
Epoch [131/200], Train Loss: 0.000481
Validation Loss: 0.00044980
Epoch [132/200], Train Loss: 0.000481
Validation Loss: 0.00044774
Epoch [133/200], Train Loss: 0.000486
Validation Loss: 0.00044756
Epoch [134/200], Train Loss: 0.000482
Validation Loss: 0.00042967
Epoch [135/200], Train Loss: 0.000479
Validation Loss: 0.00045599
Epoch [136/200], Train Loss: 0.000474
Validation Loss: 0.00044347
Epoch [137/200], Train Loss: 0.000470
Validation Loss: 0.00043290
Epoch [138/200], Train Loss: 0.000481
Validation Loss: 0.00043916
Epoch [139/200], Train Loss: 0.000471
Validation Loss: 0.00042526
Epoch [140/200], Train Loss: 0.000464
Validation Loss: 0.00043631
Epoch [141/200], Train Loss: 0.000463
Validation Loss: 0.00042687
Epoch [142/200], Train Loss: 0.000462
Validation Loss: 0.00042778
Epoch [143/200], Train Loss: 0.000457
Validation Loss: 0.00042662
Epoch [144/200], Train Loss: 0.000557
Validation Loss: 0.00048806
Epoch [145/200], Train Loss: 0.000472
Validation Loss: 0.00043142
Epoch [146/200], Train Loss: 0.000457
Validation Loss: 0.00044787
Epoch [147/200], Train Loss: 0.000457
Validation Loss: 0.00043246
Epoch [148/200], Train Loss: 0.000460
Validation Loss: 0.00043167
Epoch [149/200], Train Loss: 0.000448
Validation Loss: 0.00040967
Epoch [150/200], Train Loss: 0.000453
Validation Loss: 0.00041782
Epoch [151/200], Train Loss: 0.000447
Validation Loss: 0.00041274
Epoch [152/200], Train Loss: 0.000447
Validation Loss: 0.00040976
Epoch [153/200], Train Loss: 0.000445
Validation Loss: 0.00041774
Epoch [154/200], Train Loss: 0.000439
Validation Loss: 0.00041221
Epoch [155/200], Train Loss: 0.000439
Validation Loss: 0.00040283
Epoch [156/200], Train Loss: 0.000437
Validation Loss: 0.00044124
Epoch [157/200], Train Loss: 0.000442
Validation Loss: 0.00040829
Epoch [158/200], Train Loss: 0.000438
Validation Loss: 0.00040514
Epoch [159/200], Train Loss: 0.000436
Validation Loss: 0.00039624
Epoch [160/200], Train Loss: 0.000437
Validation Loss: 0.00041646
Epoch [161/200], Train Loss: 0.000435
Validation Loss: 0.00040199
Epoch [162/200], Train Loss: 0.000433
Validation Loss: 0.00042244
Epoch [163/200], Train Loss: 0.000434
Validation Loss: 0.00043307
Epoch [164/200], Train Loss: 0.000432
Validation Loss: 0.00039194
Epoch [165/200], Train Loss: 0.000429
Validation Loss: 0.00042296
Epoch [166/200], Train Loss: 0.000437
Validation Loss: 0.00040302
Epoch [167/200], Train Loss: 0.000426
Validation Loss: 0.00040363
Epoch [168/200], Train Loss: 0.000423
Validation Loss: 0.00041421
Epoch [169/200], Train Loss: 0.000431
Validation Loss: 0.00039621
Epoch [170/200], Train Loss: 0.000434
Validation Loss: 0.00039482
Epoch [171/200], Train Loss: 0.000431
Validation Loss: 0.00039458
Epoch [172/200], Train Loss: 0.000449
Validation Loss: 0.00042413
Epoch [173/200], Train Loss: 0.000458
Validation Loss: 0.00039179
Epoch [174/200], Train Loss: 0.000419
Validation Loss: 0.00038683
Epoch [175/200], Train Loss: 0.000421
Validation Loss: 0.00037487
Epoch [176/200], Train Loss: 0.000416
Validation Loss: 0.00039585
Epoch [177/200], Train Loss: 0.000413
Validation Loss: 0.00037624
Epoch [178/200], Train Loss: 0.000414
Validation Loss: 0.00039112
Epoch [179/200], Train Loss: 0.000419
Validation Loss: 0.00038648
Epoch [180/200], Train Loss: 0.000409
Validation Loss: 0.00039830
Epoch [181/200], Train Loss: 0.000409
Validation Loss: 0.00039534
Epoch [182/200], Train Loss: 0.000407
Validation Loss: 0.00037533
Epoch [183/200], Train Loss: 0.000415
Validation Loss: 0.00040194
Epoch [184/200], Train Loss: 0.000420
Validation Loss: 0.00038276
Epoch [185/200], Train Loss: 0.000407
Validation Loss: 0.00040312
Early stopping triggered

Evaluating model for: Lamp
Run 103/144 completed in 2384.98 seconds with: {'MAE': np.float32(0.41307908), 'MSE': np.float32(11.609647), 'RMSE': np.float32(3.407293), 'SAE': np.float32(0.017422417), 'NDE': np.float32(0.2604432)}

Run 104/144: hidden=256, seq_len=120, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 11454 windows

Epoch [1/200], Train Loss: 0.004832
Validation Loss: 0.00457235
Epoch [2/200], Train Loss: 0.004784
Validation Loss: 0.00449864
Epoch [3/200], Train Loss: 0.004760
Validation Loss: 0.00449817
Epoch [4/200], Train Loss: 0.004760
Validation Loss: 0.00448795
Epoch [5/200], Train Loss: 0.004726
Validation Loss: 0.00448406
Epoch [6/200], Train Loss: 0.004729
Validation Loss: 0.00448935
Epoch [7/200], Train Loss: 0.004738
Validation Loss: 0.00448474
Epoch [8/200], Train Loss: 0.004723
Validation Loss: 0.00449104
Epoch [9/200], Train Loss: 0.004747
Validation Loss: 0.00448615
Epoch [10/200], Train Loss: 0.004752
Validation Loss: 0.00448397
Epoch [11/200], Train Loss: 0.004741
Validation Loss: 0.00448828
Epoch [12/200], Train Loss: 0.004756
Validation Loss: 0.00448480
Epoch [13/200], Train Loss: 0.004741
Validation Loss: 0.00448451
Epoch [14/200], Train Loss: 0.004734
Validation Loss: 0.00448102
Epoch [15/200], Train Loss: 0.004736
Validation Loss: 0.00448168
Epoch [16/200], Train Loss: 0.004722
Validation Loss: 0.00448709
Epoch [17/200], Train Loss: 0.004743
Validation Loss: 0.00448408
Epoch [18/200], Train Loss: 0.004714
Validation Loss: 0.00449643
Epoch [19/200], Train Loss: 0.004732
Validation Loss: 0.00448263
Epoch [20/200], Train Loss: 0.004714
Validation Loss: 0.00447956
Epoch [21/200], Train Loss: 0.004736
Validation Loss: 0.00447902
Epoch [22/200], Train Loss: 0.004731
Validation Loss: 0.00447877
Epoch [23/200], Train Loss: 0.004719
Validation Loss: 0.00447024
Epoch [24/200], Train Loss: 0.004714
Validation Loss: 0.00446527
Epoch [25/200], Train Loss: 0.004725
Validation Loss: 0.00446622
Epoch [26/200], Train Loss: 0.004705
Validation Loss: 0.00445927
Epoch [27/200], Train Loss: 0.004725
Validation Loss: 0.00445078
Epoch [28/200], Train Loss: 0.004683
Validation Loss: 0.00444229
Epoch [29/200], Train Loss: 0.004718
Validation Loss: 0.00442962
Epoch [30/200], Train Loss: 0.004683
Validation Loss: 0.00437696
Epoch [31/200], Train Loss: 0.004593
Validation Loss: 0.00424793
Epoch [32/200], Train Loss: 0.004267
Validation Loss: 0.00362869
Epoch [33/200], Train Loss: 0.003328
Validation Loss: 0.00241818
Epoch [34/200], Train Loss: 0.002490
Validation Loss: 0.00194468
Epoch [35/200], Train Loss: 0.002104
Validation Loss: 0.00166138
Epoch [36/200], Train Loss: 0.001810
Validation Loss: 0.00153801
Epoch [37/200], Train Loss: 0.001624
Validation Loss: 0.00138574
Epoch [38/200], Train Loss: 0.001498
Validation Loss: 0.00122830
Epoch [39/200], Train Loss: 0.001380
Validation Loss: 0.00118786
Epoch [40/200], Train Loss: 0.001304
Validation Loss: 0.00120627
Epoch [41/200], Train Loss: 0.001232
Validation Loss: 0.00114647
Epoch [42/200], Train Loss: 0.001198
Validation Loss: 0.00100096
Epoch [43/200], Train Loss: 0.001099
Validation Loss: 0.00091608
Epoch [44/200], Train Loss: 0.001060
Validation Loss: 0.00091530
Epoch [45/200], Train Loss: 0.001007
Validation Loss: 0.00084879
Epoch [46/200], Train Loss: 0.000953
Validation Loss: 0.00083114
Epoch [47/200], Train Loss: 0.000945
Validation Loss: 0.00083843
Epoch [48/200], Train Loss: 0.000913
Validation Loss: 0.00080400
Epoch [49/200], Train Loss: 0.000881
Validation Loss: 0.00076001
Epoch [50/200], Train Loss: 0.000892
Validation Loss: 0.00072365
Epoch [51/200], Train Loss: 0.000914
Validation Loss: 0.00070759
Epoch [52/200], Train Loss: 0.000819
Validation Loss: 0.00068680
Epoch [53/200], Train Loss: 0.000883
Validation Loss: 0.00070629
Epoch [54/200], Train Loss: 0.000800
Validation Loss: 0.00066029
Epoch [55/200], Train Loss: 0.000760
Validation Loss: 0.00064043
Epoch [56/200], Train Loss: 0.000742
Validation Loss: 0.00068026
Epoch [57/200], Train Loss: 0.000717
Validation Loss: 0.00062120
Epoch [58/200], Train Loss: 0.000724
Validation Loss: 0.00062309
Epoch [59/200], Train Loss: 0.000720
Validation Loss: 0.00061630
Epoch [60/200], Train Loss: 0.000700
Validation Loss: 0.00065247
Epoch [61/200], Train Loss: 0.000678
Validation Loss: 0.00059395
Epoch [62/200], Train Loss: 0.000669
Validation Loss: 0.00056971
Epoch [63/200], Train Loss: 0.000674
Validation Loss: 0.00057827
Epoch [64/200], Train Loss: 0.000646
Validation Loss: 0.00055811
Epoch [65/200], Train Loss: 0.000667
Validation Loss: 0.00057285
Epoch [66/200], Train Loss: 0.000638
Validation Loss: 0.00057090
Epoch [67/200], Train Loss: 0.000613
Validation Loss: 0.00054146
Epoch [68/200], Train Loss: 0.000617
Validation Loss: 0.00053767
Epoch [69/200], Train Loss: 0.000599
Validation Loss: 0.00053233
Epoch [70/200], Train Loss: 0.000609
Validation Loss: 0.00052537
Epoch [71/200], Train Loss: 0.000594
Validation Loss: 0.00051326
Epoch [72/200], Train Loss: 0.000603
Validation Loss: 0.00050800
Epoch [73/200], Train Loss: 0.000573
Validation Loss: 0.00052175
Epoch [74/200], Train Loss: 0.000591
Validation Loss: 0.00051680
Epoch [75/200], Train Loss: 0.000566
Validation Loss: 0.00052528
Epoch [76/200], Train Loss: 0.000566
Validation Loss: 0.00050294
Epoch [77/200], Train Loss: 0.000539
Validation Loss: 0.00047353
Epoch [78/200], Train Loss: 0.000538
Validation Loss: 0.00047927
Epoch [79/200], Train Loss: 0.000543
Validation Loss: 0.00047926
Epoch [80/200], Train Loss: 0.000535
Validation Loss: 0.00048005
Epoch [81/200], Train Loss: 0.000523
Validation Loss: 0.00048657
Epoch [82/200], Train Loss: 0.000539
Validation Loss: 0.00048014
Epoch [83/200], Train Loss: 0.000510
Validation Loss: 0.00046459
Epoch [84/200], Train Loss: 0.000504
Validation Loss: 0.00045067
Epoch [85/200], Train Loss: 0.000494
Validation Loss: 0.00045370
Epoch [86/200], Train Loss: 0.000494
Validation Loss: 0.00046016
Epoch [87/200], Train Loss: 0.000494
Validation Loss: 0.00047778
Epoch [88/200], Train Loss: 0.000494
Validation Loss: 0.00045462
Epoch [89/200], Train Loss: 0.000505
Validation Loss: 0.00045610
Epoch [90/200], Train Loss: 0.000487
Validation Loss: 0.00046251
Epoch [91/200], Train Loss: 0.000475
Validation Loss: 0.00044962
Epoch [92/200], Train Loss: 0.000468
Validation Loss: 0.00044234
Epoch [93/200], Train Loss: 0.000474
Validation Loss: 0.00044543
Epoch [94/200], Train Loss: 0.000469
Validation Loss: 0.00044738
Epoch [95/200], Train Loss: 0.000462
Validation Loss: 0.00043962
Epoch [96/200], Train Loss: 0.000459
Validation Loss: 0.00042711
Epoch [97/200], Train Loss: 0.000455
Validation Loss: 0.00042656
Epoch [98/200], Train Loss: 0.000521
Validation Loss: 0.00070209
Epoch [99/200], Train Loss: 0.000538
Validation Loss: 0.00044518
Epoch [100/200], Train Loss: 0.000456
Validation Loss: 0.00043371
Epoch [101/200], Train Loss: 0.000459
Validation Loss: 0.00043819
Epoch [102/200], Train Loss: 0.000453
Validation Loss: 0.00043043
Epoch [103/200], Train Loss: 0.000449
Validation Loss: 0.00043241
Epoch [104/200], Train Loss: 0.000447
Validation Loss: 0.00041067
Epoch [105/200], Train Loss: 0.000437
Validation Loss: 0.00042106
Epoch [106/200], Train Loss: 0.000436
Validation Loss: 0.00040980
Epoch [107/200], Train Loss: 0.000436
Validation Loss: 0.00040946
Epoch [108/200], Train Loss: 0.000430
Validation Loss: 0.00041170
Epoch [109/200], Train Loss: 0.000425
Validation Loss: 0.00041175
Epoch [110/200], Train Loss: 0.000433
Validation Loss: 0.00040803
Epoch [111/200], Train Loss: 0.000427
Validation Loss: 0.00041360
Epoch [112/200], Train Loss: 0.000428
Validation Loss: 0.00039430
Epoch [113/200], Train Loss: 0.000419
Validation Loss: 0.00040992
Epoch [114/200], Train Loss: 0.000421
Validation Loss: 0.00039551
Epoch [115/200], Train Loss: 0.000428
Validation Loss: 0.00040563
Epoch [116/200], Train Loss: 0.000424
Validation Loss: 0.00039647
Epoch [117/200], Train Loss: 0.000412
Validation Loss: 0.00039627
Epoch [118/200], Train Loss: 0.000410
Validation Loss: 0.00039029
Epoch [119/200], Train Loss: 0.000415
Validation Loss: 0.00039766
Epoch [120/200], Train Loss: 0.000404
Validation Loss: 0.00039751
Epoch [121/200], Train Loss: 0.000401
Validation Loss: 0.00037456
Epoch [122/200], Train Loss: 0.000403
Validation Loss: 0.00037658
Epoch [123/200], Train Loss: 0.000402
Validation Loss: 0.00037530
Epoch [124/200], Train Loss: 0.000403
Validation Loss: 0.00039378
Epoch [125/200], Train Loss: 0.000403
Validation Loss: 0.00039008
Epoch [126/200], Train Loss: 0.000404
Validation Loss: 0.00037218
Epoch [127/200], Train Loss: 0.000399
Validation Loss: 0.00041464
Epoch [128/200], Train Loss: 0.000394
Validation Loss: 0.00038567
Epoch [129/200], Train Loss: 0.000391
Validation Loss: 0.00037395
Epoch [130/200], Train Loss: 0.000392
Validation Loss: 0.00036883
Epoch [131/200], Train Loss: 0.000388
Validation Loss: 0.00040583
Epoch [132/200], Train Loss: 0.000388
Validation Loss: 0.00042493
Epoch [133/200], Train Loss: 0.000385
Validation Loss: 0.00035530
Epoch [134/200], Train Loss: 0.000386
Validation Loss: 0.00036634
Epoch [135/200], Train Loss: 0.000381
Validation Loss: 0.00036551
Epoch [136/200], Train Loss: 0.000381
Validation Loss: 0.00035509
Epoch [137/200], Train Loss: 0.000381
Validation Loss: 0.00037798
Epoch [138/200], Train Loss: 0.000378
Validation Loss: 0.00036505
Epoch [139/200], Train Loss: 0.000374
Validation Loss: 0.00039706
Epoch [140/200], Train Loss: 0.000380
Validation Loss: 0.00035337
Epoch [141/200], Train Loss: 0.000372
Validation Loss: 0.00035188
Epoch [142/200], Train Loss: 0.000366
Validation Loss: 0.00034425
Epoch [143/200], Train Loss: 0.000370
Validation Loss: 0.00035942
Epoch [144/200], Train Loss: 0.000368
Validation Loss: 0.00033507
Epoch [145/200], Train Loss: 0.000365
Validation Loss: 0.00033880
Epoch [146/200], Train Loss: 0.000387
Validation Loss: 0.00035803
Epoch [147/200], Train Loss: 0.000514
Validation Loss: 0.00039020
Epoch [148/200], Train Loss: 0.000384
Validation Loss: 0.00036529
Epoch [149/200], Train Loss: 0.000366
Validation Loss: 0.00035678
Epoch [150/200], Train Loss: 0.000353
Validation Loss: 0.00033114
Epoch [151/200], Train Loss: 0.000352
Validation Loss: 0.00033041
Epoch [152/200], Train Loss: 0.000348
Validation Loss: 0.00035989
Epoch [153/200], Train Loss: 0.000351
Validation Loss: 0.00032985
Epoch [154/200], Train Loss: 0.000348
Validation Loss: 0.00032921
Epoch [155/200], Train Loss: 0.000345
Validation Loss: 0.00033400
Epoch [156/200], Train Loss: 0.000341
Validation Loss: 0.00032657
Epoch [157/200], Train Loss: 0.000343
Validation Loss: 0.00031530
Epoch [158/200], Train Loss: 0.000348
Validation Loss: 0.00031377
Epoch [159/200], Train Loss: 0.000341
Validation Loss: 0.00031707
Epoch [160/200], Train Loss: 0.000339
Validation Loss: 0.00032557
Epoch [161/200], Train Loss: 0.000335
Validation Loss: 0.00031205
Epoch [162/200], Train Loss: 0.000333
Validation Loss: 0.00031903
Epoch [163/200], Train Loss: 0.000333
Validation Loss: 0.00030343
Epoch [164/200], Train Loss: 0.000324
Validation Loss: 0.00030779
Epoch [165/200], Train Loss: 0.000326
Validation Loss: 0.00030440
Epoch [166/200], Train Loss: 0.000325
Validation Loss: 0.00032623
Epoch [167/200], Train Loss: 0.000327
Validation Loss: 0.00035898
Epoch [168/200], Train Loss: 0.000331
Validation Loss: 0.00030555
Epoch [169/200], Train Loss: 0.000315
Validation Loss: 0.00031062
Epoch [170/200], Train Loss: 0.000328
Validation Loss: 0.00032101
Epoch [171/200], Train Loss: 0.000323
Validation Loss: 0.00028836
Epoch [172/200], Train Loss: 0.000313
Validation Loss: 0.00028237
Epoch [173/200], Train Loss: 0.000312
Validation Loss: 0.00029893
Epoch [174/200], Train Loss: 0.000307
Validation Loss: 0.00027795
Epoch [175/200], Train Loss: 0.000326
Validation Loss: 0.00030240
Epoch [176/200], Train Loss: 0.000314
Validation Loss: 0.00029138
Epoch [177/200], Train Loss: 0.000310
Validation Loss: 0.00029581
Epoch [178/200], Train Loss: 0.000304
Validation Loss: 0.00028201
Epoch [179/200], Train Loss: 0.000302
Validation Loss: 0.00028273
Epoch [180/200], Train Loss: 0.000308
Validation Loss: 0.00027613
Epoch [181/200], Train Loss: 0.000302
Validation Loss: 0.00027219
Epoch [182/200], Train Loss: 0.000312
Validation Loss: 0.00029442
Epoch [183/200], Train Loss: 0.000308
Validation Loss: 0.00027493
Epoch [184/200], Train Loss: 0.000304
Validation Loss: 0.00027108
Epoch [185/200], Train Loss: 0.000299
Validation Loss: 0.00027688
Epoch [186/200], Train Loss: 0.000299
Validation Loss: 0.00026066
Epoch [187/200], Train Loss: 0.000293
Validation Loss: 0.00026608
Epoch [188/200], Train Loss: 0.000290
Validation Loss: 0.00026390
Epoch [189/200], Train Loss: 0.000292
Validation Loss: 0.00026960
Epoch [190/200], Train Loss: 0.000304
Validation Loss: 0.00027361
Epoch [191/200], Train Loss: 0.000291
Validation Loss: 0.00026756
Epoch [192/200], Train Loss: 0.000293
Validation Loss: 0.00025940
Epoch [193/200], Train Loss: 0.000300
Validation Loss: 0.00027319
Epoch [194/200], Train Loss: 0.000289
Validation Loss: 0.00026590
Epoch [195/200], Train Loss: 0.000297
Validation Loss: 0.00026094
Epoch [196/200], Train Loss: 0.000279
Validation Loss: 0.00026190
Epoch [197/200], Train Loss: 0.000280
Validation Loss: 0.00025762
Epoch [198/200], Train Loss: 0.000286
Validation Loss: 0.00027806
Epoch [199/200], Train Loss: 0.000289
Validation Loss: 0.00025004
Epoch [200/200], Train Loss: 0.000280
Validation Loss: 0.00025159

Evaluating model for: Lamp
Run 104/144 completed in 3028.54 seconds with: {'MAE': np.float32(0.31042257), 'MSE': np.float32(8.546453), 'RMSE': np.float32(2.9234319), 'SAE': np.float32(0.0064515197), 'NDE': np.float32(0.22345848)}

Run 105/144: hidden=256, seq_len=120, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.004894
Validation Loss: 0.00662156
Epoch [2/200], Train Loss: 0.004724
Validation Loss: 0.00655014
Epoch [3/200], Train Loss: 0.004691
Validation Loss: 0.00653731
Epoch [4/200], Train Loss: 0.004692
Validation Loss: 0.00650304
Epoch [5/200], Train Loss: 0.004680
Validation Loss: 0.00649470
Epoch [6/200], Train Loss: 0.004683
Validation Loss: 0.00648885
Epoch [7/200], Train Loss: 0.004674
Validation Loss: 0.00649462
Epoch [8/200], Train Loss: 0.004676
Validation Loss: 0.00648346
Epoch [9/200], Train Loss: 0.004666
Validation Loss: 0.00651622
Epoch [10/200], Train Loss: 0.004674
Validation Loss: 0.00648699
Epoch [11/200], Train Loss: 0.004668
Validation Loss: 0.00647334
Epoch [12/200], Train Loss: 0.004661
Validation Loss: 0.00647403
Epoch [13/200], Train Loss: 0.004659
Validation Loss: 0.00647684
Epoch [14/200], Train Loss: 0.004655
Validation Loss: 0.00648376
Epoch [15/200], Train Loss: 0.004647
Validation Loss: 0.00647324
Epoch [16/200], Train Loss: 0.004648
Validation Loss: 0.00645821
Epoch [17/200], Train Loss: 0.004650
Validation Loss: 0.00646205
Epoch [18/200], Train Loss: 0.004654
Validation Loss: 0.00647690
Epoch [19/200], Train Loss: 0.004633
Validation Loss: 0.00644461
Epoch [20/200], Train Loss: 0.004636
Validation Loss: 0.00647346
Epoch [21/200], Train Loss: 0.004630
Validation Loss: 0.00644017
Epoch [22/200], Train Loss: 0.004626
Validation Loss: 0.00642310
Epoch [23/200], Train Loss: 0.004629
Validation Loss: 0.00642715
Epoch [24/200], Train Loss: 0.004616
Validation Loss: 0.00643227
Epoch [25/200], Train Loss: 0.004600
Validation Loss: 0.00641173
Epoch [26/200], Train Loss: 0.004597
Validation Loss: 0.00641724
Epoch [27/200], Train Loss: 0.004620
Validation Loss: 0.00638188
Epoch [28/200], Train Loss: 0.004586
Validation Loss: 0.00636484
Epoch [29/200], Train Loss: 0.004600
Validation Loss: 0.00640890
Epoch [30/200], Train Loss: 0.004577
Validation Loss: 0.00634789
Epoch [31/200], Train Loss: 0.004568
Validation Loss: 0.00636368
Epoch [32/200], Train Loss: 0.004564
Validation Loss: 0.00631095
Epoch [33/200], Train Loss: 0.004549
Validation Loss: 0.00628453
Epoch [34/200], Train Loss: 0.004528
Validation Loss: 0.00626333
Epoch [35/200], Train Loss: 0.004512
Validation Loss: 0.00623652
Epoch [36/200], Train Loss: 0.004509
Validation Loss: 0.00621043
Epoch [37/200], Train Loss: 0.004492
Validation Loss: 0.00618399
Epoch [38/200], Train Loss: 0.004452
Validation Loss: 0.00612788
Epoch [39/200], Train Loss: 0.004454
Validation Loss: 0.00616416
Epoch [40/200], Train Loss: 0.004398
Validation Loss: 0.00598674
Epoch [41/200], Train Loss: 0.004343
Validation Loss: 0.00595835
Epoch [42/200], Train Loss: 0.004331
Validation Loss: 0.00590302
Epoch [43/200], Train Loss: 0.004244
Validation Loss: 0.00575133
Epoch [44/200], Train Loss: 0.004254
Validation Loss: 0.00587583
Epoch [45/200], Train Loss: 0.004187
Validation Loss: 0.00560920
Epoch [46/200], Train Loss: 0.004104
Validation Loss: 0.00585635
Epoch [47/200], Train Loss: 0.004117
Validation Loss: 0.00547730
Epoch [48/200], Train Loss: 0.003981
Validation Loss: 0.00534256
Epoch [49/200], Train Loss: 0.003922
Validation Loss: 0.00525558
Epoch [50/200], Train Loss: 0.003836
Validation Loss: 0.00511017
Epoch [51/200], Train Loss: 0.003822
Validation Loss: 0.00500725
Epoch [52/200], Train Loss: 0.003669
Validation Loss: 0.00482414
Epoch [53/200], Train Loss: 0.003617
Validation Loss: 0.00481087
Epoch [54/200], Train Loss: 0.003499
Validation Loss: 0.00445190
Epoch [55/200], Train Loss: 0.003329
Validation Loss: 0.00423011
Epoch [56/200], Train Loss: 0.003258
Validation Loss: 0.00410634
Epoch [57/200], Train Loss: 0.003095
Validation Loss: 0.00398841
Epoch [58/200], Train Loss: 0.003022
Validation Loss: 0.00373478
Epoch [59/200], Train Loss: 0.002910
Validation Loss: 0.00361063
Epoch [60/200], Train Loss: 0.002817
Validation Loss: 0.00358071
Epoch [61/200], Train Loss: 0.002748
Validation Loss: 0.00347758
Epoch [62/200], Train Loss: 0.002660
Validation Loss: 0.00337968
Epoch [63/200], Train Loss: 0.002624
Validation Loss: 0.00329994
Epoch [64/200], Train Loss: 0.002566
Validation Loss: 0.00324126
Epoch [65/200], Train Loss: 0.002512
Validation Loss: 0.00317428
Epoch [66/200], Train Loss: 0.002481
Validation Loss: 0.00316075
Epoch [67/200], Train Loss: 0.002415
Validation Loss: 0.00308107
Epoch [68/200], Train Loss: 0.002400
Validation Loss: 0.00309856
Epoch [69/200], Train Loss: 0.002353
Validation Loss: 0.00316340
Epoch [70/200], Train Loss: 0.002312
Validation Loss: 0.00299424
Epoch [71/200], Train Loss: 0.002268
Validation Loss: 0.00300254
Epoch [72/200], Train Loss: 0.002229
Validation Loss: 0.00297308
Epoch [73/200], Train Loss: 0.002193
Validation Loss: 0.00287643
Epoch [74/200], Train Loss: 0.002183
Validation Loss: 0.00292251
Epoch [75/200], Train Loss: 0.002190
Validation Loss: 0.00282651
Epoch [76/200], Train Loss: 0.002117
Validation Loss: 0.00282431
Epoch [77/200], Train Loss: 0.002108
Validation Loss: 0.00284500
Epoch [78/200], Train Loss: 0.002064
Validation Loss: 0.00278574
Epoch [79/200], Train Loss: 0.002059
Validation Loss: 0.00277264
Epoch [80/200], Train Loss: 0.002028
Validation Loss: 0.00270186
Epoch [81/200], Train Loss: 0.002004
Validation Loss: 0.00265976
Epoch [82/200], Train Loss: 0.001994
Validation Loss: 0.00268450
Epoch [83/200], Train Loss: 0.001959
Validation Loss: 0.00262725
Epoch [84/200], Train Loss: 0.001955
Validation Loss: 0.00261327
Epoch [85/200], Train Loss: 0.001924
Validation Loss: 0.00257379
Epoch [86/200], Train Loss: 0.001915
Validation Loss: 0.00255082
Epoch [87/200], Train Loss: 0.001897
Validation Loss: 0.00254244
Epoch [88/200], Train Loss: 0.001902
Validation Loss: 0.00257161
Epoch [89/200], Train Loss: 0.001852
Validation Loss: 0.00254997
Epoch [90/200], Train Loss: 0.001849
Validation Loss: 0.00252890
Epoch [91/200], Train Loss: 0.001830
Validation Loss: 0.00249468
Epoch [92/200], Train Loss: 0.001827
Validation Loss: 0.00247444
Epoch [93/200], Train Loss: 0.001796
Validation Loss: 0.00243322
Epoch [94/200], Train Loss: 0.001773
Validation Loss: 0.00240312
Epoch [95/200], Train Loss: 0.001754
Validation Loss: 0.00246181
Epoch [96/200], Train Loss: 0.001754
Validation Loss: 0.00236966
Epoch [97/200], Train Loss: 0.001742
Validation Loss: 0.00236700
Epoch [98/200], Train Loss: 0.001721
Validation Loss: 0.00235543
Epoch [99/200], Train Loss: 0.001709
Validation Loss: 0.00233131
Epoch [100/200], Train Loss: 0.001695
Validation Loss: 0.00234359
Epoch [101/200], Train Loss: 0.001700
Validation Loss: 0.00230842
Epoch [102/200], Train Loss: 0.001673
Validation Loss: 0.00228422
Epoch [103/200], Train Loss: 0.001666
Validation Loss: 0.00225344
Epoch [104/200], Train Loss: 0.001642
Validation Loss: 0.00228799
Epoch [105/200], Train Loss: 0.001629
Validation Loss: 0.00222862
Epoch [106/200], Train Loss: 0.001618
Validation Loss: 0.00225834
Epoch [107/200], Train Loss: 0.001601
Validation Loss: 0.00224036
Epoch [108/200], Train Loss: 0.001590
Validation Loss: 0.00221920
Epoch [109/200], Train Loss: 0.001589
Validation Loss: 0.00219466
Epoch [110/200], Train Loss: 0.001566
Validation Loss: 0.00216918
Epoch [111/200], Train Loss: 0.001574
Validation Loss: 0.00216454
Epoch [112/200], Train Loss: 0.001555
Validation Loss: 0.00213508
Epoch [113/200], Train Loss: 0.001546
Validation Loss: 0.00210523
Epoch [114/200], Train Loss: 0.001545
Validation Loss: 0.00213307
Epoch [115/200], Train Loss: 0.001525
Validation Loss: 0.00209144
Epoch [116/200], Train Loss: 0.001522
Validation Loss: 0.00213445
Epoch [117/200], Train Loss: 0.001520
Validation Loss: 0.00211490
Epoch [118/200], Train Loss: 0.001506
Validation Loss: 0.00207428
Epoch [119/200], Train Loss: 0.001497
Validation Loss: 0.00207729
Epoch [120/200], Train Loss: 0.001496
Validation Loss: 0.00205062
Epoch [121/200], Train Loss: 0.001474
Validation Loss: 0.00202397
Epoch [122/200], Train Loss: 0.001468
Validation Loss: 0.00201710
Epoch [123/200], Train Loss: 0.001455
Validation Loss: 0.00203931
Epoch [124/200], Train Loss: 0.001451
Validation Loss: 0.00202810
Epoch [125/200], Train Loss: 0.001463
Validation Loss: 0.00198129
Epoch [126/200], Train Loss: 0.001454
Validation Loss: 0.00207910
Epoch [127/200], Train Loss: 0.001443
Validation Loss: 0.00198146
Epoch [128/200], Train Loss: 0.001417
Validation Loss: 0.00194695
Epoch [129/200], Train Loss: 0.001402
Validation Loss: 0.00196025
Epoch [130/200], Train Loss: 0.001419
Validation Loss: 0.00196965
Epoch [131/200], Train Loss: 0.001402
Validation Loss: 0.00193593
Epoch [132/200], Train Loss: 0.001389
Validation Loss: 0.00190138
Epoch [133/200], Train Loss: 0.001389
Validation Loss: 0.00192831
Epoch [134/200], Train Loss: 0.001388
Validation Loss: 0.00189810
Epoch [135/200], Train Loss: 0.001373
Validation Loss: 0.00190513
Epoch [136/200], Train Loss: 0.001372
Validation Loss: 0.00190859
Epoch [137/200], Train Loss: 0.001362
Validation Loss: 0.00189445
Epoch [138/200], Train Loss: 0.001353
Validation Loss: 0.00187952
Epoch [139/200], Train Loss: 0.001358
Validation Loss: 0.00188352
Epoch [140/200], Train Loss: 0.001344
Validation Loss: 0.00187094
Epoch [141/200], Train Loss: 0.001338
Validation Loss: 0.00188775
Epoch [142/200], Train Loss: 0.001329
Validation Loss: 0.00186683
Epoch [143/200], Train Loss: 0.001326
Validation Loss: 0.00185905
Epoch [144/200], Train Loss: 0.001312
Validation Loss: 0.00181165
Epoch [145/200], Train Loss: 0.001307
Validation Loss: 0.00180191
Epoch [146/200], Train Loss: 0.001297
Validation Loss: 0.00184561
Epoch [147/200], Train Loss: 0.001310
Validation Loss: 0.00176485
Epoch [148/200], Train Loss: 0.001291
Validation Loss: 0.00178317
Epoch [149/200], Train Loss: 0.001290
Validation Loss: 0.00182796
Epoch [150/200], Train Loss: 0.001286
Validation Loss: 0.00175233
Epoch [151/200], Train Loss: 0.001277
Validation Loss: 0.00180644
Epoch [152/200], Train Loss: 0.001282
Validation Loss: 0.00176970
Epoch [153/200], Train Loss: 0.001280
Validation Loss: 0.00179910
Epoch [154/200], Train Loss: 0.001286
Validation Loss: 0.00172638
Epoch [155/200], Train Loss: 0.001251
Validation Loss: 0.00176110
Epoch [156/200], Train Loss: 0.001260
Validation Loss: 0.00173824
Epoch [157/200], Train Loss: 0.001254
Validation Loss: 0.00174178
Epoch [158/200], Train Loss: 0.001241
Validation Loss: 0.00172079
Epoch [159/200], Train Loss: 0.001234
Validation Loss: 0.00171807
Epoch [160/200], Train Loss: 0.001231
Validation Loss: 0.00170424
Epoch [161/200], Train Loss: 0.001240
Validation Loss: 0.00169647
Epoch [162/200], Train Loss: 0.001221
Validation Loss: 0.00169516
Epoch [163/200], Train Loss: 0.001211
Validation Loss: 0.00166376
Epoch [164/200], Train Loss: 0.001218
Validation Loss: 0.00167824
Epoch [165/200], Train Loss: 0.001241
Validation Loss: 0.00165937
Epoch [166/200], Train Loss: 0.001210
Validation Loss: 0.00164904
Epoch [167/200], Train Loss: 0.001210
Validation Loss: 0.00163610
Epoch [168/200], Train Loss: 0.001196
Validation Loss: 0.00163892
Epoch [169/200], Train Loss: 0.001192
Validation Loss: 0.00162186
Epoch [170/200], Train Loss: 0.001194
Validation Loss: 0.00161138
Epoch [171/200], Train Loss: 0.001185
Validation Loss: 0.00162955
Epoch [172/200], Train Loss: 0.001180
Validation Loss: 0.00161683
Epoch [173/200], Train Loss: 0.001180
Validation Loss: 0.00163113
Epoch [174/200], Train Loss: 0.001177
Validation Loss: 0.00162928
Epoch [175/200], Train Loss: 0.001177
Validation Loss: 0.00159369
Epoch [176/200], Train Loss: 0.001161
Validation Loss: 0.00159734
Epoch [177/200], Train Loss: 0.001176
Validation Loss: 0.00159798
Epoch [178/200], Train Loss: 0.001157
Validation Loss: 0.00161481
Epoch [179/200], Train Loss: 0.001157
Validation Loss: 0.00159558
Epoch [180/200], Train Loss: 0.001162
Validation Loss: 0.00161620
Epoch [181/200], Train Loss: 0.001163
Validation Loss: 0.00157406
Epoch [182/200], Train Loss: 0.001151
Validation Loss: 0.00157382
Epoch [183/200], Train Loss: 0.001147
Validation Loss: 0.00157034
Epoch [184/200], Train Loss: 0.001146
Validation Loss: 0.00155802
Epoch [185/200], Train Loss: 0.001131
Validation Loss: 0.00158409
Epoch [186/200], Train Loss: 0.001144
Validation Loss: 0.00155393
Epoch [187/200], Train Loss: 0.001139
Validation Loss: 0.00161488
Epoch [188/200], Train Loss: 0.001132
Validation Loss: 0.00154475
Epoch [189/200], Train Loss: 0.001131
Validation Loss: 0.00155470
Epoch [190/200], Train Loss: 0.001122
Validation Loss: 0.00159340
Epoch [191/200], Train Loss: 0.001126
Validation Loss: 0.00152205
Epoch [192/200], Train Loss: 0.001128
Validation Loss: 0.00152805
Epoch [193/200], Train Loss: 0.001114
Validation Loss: 0.00153967
Epoch [194/200], Train Loss: 0.001110
Validation Loss: 0.00154607
Epoch [195/200], Train Loss: 0.001108
Validation Loss: 0.00152972
Epoch [196/200], Train Loss: 0.001112
Validation Loss: 0.00156167
Epoch [197/200], Train Loss: 0.001125
Validation Loss: 0.00155136
Epoch [198/200], Train Loss: 0.001139
Validation Loss: 0.00152065
Epoch [199/200], Train Loss: 0.001109
Validation Loss: 0.00152269
Epoch [200/200], Train Loss: 0.001091
Validation Loss: 0.00150649

Evaluating model for: Lamp
Run 105/144 completed in 1122.44 seconds with: {'MAE': np.float32(0.82828915), 'MSE': np.float32(19.825571), 'RMSE': np.float32(4.4525914), 'SAE': np.float32(0.1627384), 'NDE': np.float32(0.39608723)}

Run 106/144: hidden=256, seq_len=120, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.004739
Validation Loss: 0.00657568
Epoch [2/200], Train Loss: 0.004692
Validation Loss: 0.00651083
Epoch [3/200], Train Loss: 0.004682
Validation Loss: 0.00650395
Epoch [4/200], Train Loss: 0.004666
Validation Loss: 0.00649143
Epoch [5/200], Train Loss: 0.004667
Validation Loss: 0.00648728
Epoch [6/200], Train Loss: 0.004666
Validation Loss: 0.00649065
Epoch [7/200], Train Loss: 0.004659
Validation Loss: 0.00647040
Epoch [8/200], Train Loss: 0.004654
Validation Loss: 0.00648738
Epoch [9/200], Train Loss: 0.004661
Validation Loss: 0.00648449
Epoch [10/200], Train Loss: 0.004648
Validation Loss: 0.00653789
Epoch [11/200], Train Loss: 0.004646
Validation Loss: 0.00648524
Epoch [12/200], Train Loss: 0.004640
Validation Loss: 0.00653511
Epoch [13/200], Train Loss: 0.004667
Validation Loss: 0.00648286
Epoch [14/200], Train Loss: 0.004658
Validation Loss: 0.00648529
Epoch [15/200], Train Loss: 0.004643
Validation Loss: 0.00647893
Epoch [16/200], Train Loss: 0.004656
Validation Loss: 0.00646953
Epoch [17/200], Train Loss: 0.004663
Validation Loss: 0.00648451
Epoch [18/200], Train Loss: 0.004655
Validation Loss: 0.00648661
Epoch [19/200], Train Loss: 0.004639
Validation Loss: 0.00647681
Epoch [20/200], Train Loss: 0.004643
Validation Loss: 0.00646828
Epoch [21/200], Train Loss: 0.004652
Validation Loss: 0.00647303
Epoch [22/200], Train Loss: 0.004637
Validation Loss: 0.00647284
Epoch [23/200], Train Loss: 0.004640
Validation Loss: 0.00647382
Epoch [24/200], Train Loss: 0.004638
Validation Loss: 0.00647976
Epoch [25/200], Train Loss: 0.004641
Validation Loss: 0.00646985
Epoch [26/200], Train Loss: 0.004642
Validation Loss: 0.00650256
Epoch [27/200], Train Loss: 0.004652
Validation Loss: 0.00647387
Epoch [28/200], Train Loss: 0.004639
Validation Loss: 0.00648197
Epoch [29/200], Train Loss: 0.004649
Validation Loss: 0.00649124
Epoch [30/200], Train Loss: 0.004638
Validation Loss: 0.00647888
Early stopping triggered

Evaluating model for: Lamp
Run 106/144 completed in 183.21 seconds with: {'MAE': np.float32(2.3633044), 'MSE': np.float32(122.97422), 'RMSE': np.float32(11.089375), 'SAE': np.float32(0.28246048), 'NDE': np.float32(0.9864723)}

Run 107/144: hidden=256, seq_len=120, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.004789
Validation Loss: 0.00660448
Epoch [2/200], Train Loss: 0.004743
Validation Loss: 0.00661621
Epoch [3/200], Train Loss: 0.004730
Validation Loss: 0.00654272
Epoch [4/200], Train Loss: 0.004675
Validation Loss: 0.00649612
Epoch [5/200], Train Loss: 0.004672
Validation Loss: 0.00651882
Epoch [6/200], Train Loss: 0.004670
Validation Loss: 0.00651733
Epoch [7/200], Train Loss: 0.004662
Validation Loss: 0.00651468
Epoch [8/200], Train Loss: 0.004662
Validation Loss: 0.00648135
Epoch [9/200], Train Loss: 0.004660
Validation Loss: 0.00648995
Epoch [10/200], Train Loss: 0.004666
Validation Loss: 0.00650003
Epoch [11/200], Train Loss: 0.004649
Validation Loss: 0.00653852
Epoch [12/200], Train Loss: 0.004668
Validation Loss: 0.00650506
Epoch [13/200], Train Loss: 0.004656
Validation Loss: 0.00648149
Epoch [14/200], Train Loss: 0.004644
Validation Loss: 0.00648669
Epoch [15/200], Train Loss: 0.004653
Validation Loss: 0.00647513
Epoch [16/200], Train Loss: 0.004653
Validation Loss: 0.00649316
Epoch [17/200], Train Loss: 0.004650
Validation Loss: 0.00650139
Epoch [18/200], Train Loss: 0.004644
Validation Loss: 0.00648924
Epoch [19/200], Train Loss: 0.004636
Validation Loss: 0.00647657
Epoch [20/200], Train Loss: 0.004635
Validation Loss: 0.00649889
Epoch [21/200], Train Loss: 0.004640
Validation Loss: 0.00647629
Epoch [22/200], Train Loss: 0.004660
Validation Loss: 0.00648203
Epoch [23/200], Train Loss: 0.004644
Validation Loss: 0.00649464
Epoch [24/200], Train Loss: 0.004651
Validation Loss: 0.00647304
Epoch [25/200], Train Loss: 0.004636
Validation Loss: 0.00648469
Epoch [26/200], Train Loss: 0.004626
Validation Loss: 0.00651604
Epoch [27/200], Train Loss: 0.004656
Validation Loss: 0.00646764
Epoch [28/200], Train Loss: 0.004635
Validation Loss: 0.00647439
Epoch [29/200], Train Loss: 0.004636
Validation Loss: 0.00647259
Epoch [30/200], Train Loss: 0.004645
Validation Loss: 0.00648427
Epoch [31/200], Train Loss: 0.004642
Validation Loss: 0.00651973
Epoch [32/200], Train Loss: 0.004642
Validation Loss: 0.00648753
Epoch [33/200], Train Loss: 0.004632
Validation Loss: 0.00649423
Epoch [34/200], Train Loss: 0.004643
Validation Loss: 0.00647221
Epoch [35/200], Train Loss: 0.004640
Validation Loss: 0.00649387
Epoch [36/200], Train Loss: 0.004630
Validation Loss: 0.00648193
Epoch [37/200], Train Loss: 0.004632
Validation Loss: 0.00647350
Early stopping triggered

Evaluating model for: Lamp
Run 107/144 completed in 239.54 seconds with: {'MAE': np.float32(2.6023238), 'MSE': np.float32(123.265175), 'RMSE': np.float32(11.102485), 'SAE': np.float32(0.4769635), 'NDE': np.float32(0.9876391)}

Run 108/144: hidden=256, seq_len=120, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 5738 windows

Epoch [1/200], Train Loss: 0.004766
Validation Loss: 0.00661180
Epoch [2/200], Train Loss: 0.004746
Validation Loss: 0.00662096
Epoch [3/200], Train Loss: 0.004737
Validation Loss: 0.00657963
Epoch [4/200], Train Loss: 0.004680
Validation Loss: 0.00654063
Epoch [5/200], Train Loss: 0.004684
Validation Loss: 0.00651945
Epoch [6/200], Train Loss: 0.004678
Validation Loss: 0.00647914
Epoch [7/200], Train Loss: 0.004663
Validation Loss: 0.00647680
Epoch [8/200], Train Loss: 0.004660
Validation Loss: 0.00647629
Epoch [9/200], Train Loss: 0.004667
Validation Loss: 0.00648229
Epoch [10/200], Train Loss: 0.004661
Validation Loss: 0.00650232
Epoch [11/200], Train Loss: 0.004663
Validation Loss: 0.00650017
Epoch [12/200], Train Loss: 0.004652
Validation Loss: 0.00650059
Epoch [13/200], Train Loss: 0.004643
Validation Loss: 0.00650455
Epoch [14/200], Train Loss: 0.004656
Validation Loss: 0.00650770
Epoch [15/200], Train Loss: 0.004651
Validation Loss: 0.00648148
Epoch [16/200], Train Loss: 0.004647
Validation Loss: 0.00647613
Epoch [17/200], Train Loss: 0.004647
Validation Loss: 0.00648740
Epoch [18/200], Train Loss: 0.004646
Validation Loss: 0.00650041
Epoch [19/200], Train Loss: 0.004639
Validation Loss: 0.00648671
Epoch [20/200], Train Loss: 0.004647
Validation Loss: 0.00649203
Epoch [21/200], Train Loss: 0.004645
Validation Loss: 0.00649059
Epoch [22/200], Train Loss: 0.004649
Validation Loss: 0.00648743
Epoch [23/200], Train Loss: 0.004643
Validation Loss: 0.00647899
Epoch [24/200], Train Loss: 0.004655
Validation Loss: 0.00649291
Epoch [25/200], Train Loss: 0.004634
Validation Loss: 0.00652287
Epoch [26/200], Train Loss: 0.004643
Validation Loss: 0.00651585
Early stopping triggered

Evaluating model for: Lamp
Run 108/144 completed in 196.72 seconds with: {'MAE': np.float32(1.958932), 'MSE': np.float32(122.83458), 'RMSE': np.float32(11.0830765), 'SAE': np.float32(0.05317889), 'NDE': np.float32(0.98591256)}

Run 109/144: hidden=256, seq_len=360, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.004885
Validation Loss: 0.00471975
Epoch [2/200], Train Loss: 0.004736
Validation Loss: 0.00468766
Epoch [3/200], Train Loss: 0.004716
Validation Loss: 0.00468045
Epoch [4/200], Train Loss: 0.004713
Validation Loss: 0.00467765
Epoch [5/200], Train Loss: 0.004724
Validation Loss: 0.00467506
Epoch [6/200], Train Loss: 0.004706
Validation Loss: 0.00467167
Epoch [7/200], Train Loss: 0.004721
Validation Loss: 0.00466875
Epoch [8/200], Train Loss: 0.004692
Validation Loss: 0.00466700
Epoch [9/200], Train Loss: 0.004701
Validation Loss: 0.00467696
Epoch [10/200], Train Loss: 0.004695
Validation Loss: 0.00465976
Epoch [11/200], Train Loss: 0.004683
Validation Loss: 0.00465336
Epoch [12/200], Train Loss: 0.004672
Validation Loss: 0.00464418
Epoch [13/200], Train Loss: 0.004669
Validation Loss: 0.00463577
Epoch [14/200], Train Loss: 0.004641
Validation Loss: 0.00462288
Epoch [15/200], Train Loss: 0.004638
Validation Loss: 0.00458749
Epoch [16/200], Train Loss: 0.004604
Validation Loss: 0.00455862
Epoch [17/200], Train Loss: 0.004578
Validation Loss: 0.00455048
Epoch [18/200], Train Loss: 0.004538
Validation Loss: 0.00450172
Epoch [19/200], Train Loss: 0.004477
Validation Loss: 0.00446248
Epoch [20/200], Train Loss: 0.004403
Validation Loss: 0.00430371
Epoch [21/200], Train Loss: 0.004301
Validation Loss: 0.00417206
Epoch [22/200], Train Loss: 0.004199
Validation Loss: 0.00408157
Epoch [23/200], Train Loss: 0.004055
Validation Loss: 0.00398445
Epoch [24/200], Train Loss: 0.003889
Validation Loss: 0.00375274
Epoch [25/200], Train Loss: 0.003758
Validation Loss: 0.00350201
Epoch [26/200], Train Loss: 0.003456
Validation Loss: 0.00322506
Epoch [27/200], Train Loss: 0.003186
Validation Loss: 0.00301885
Epoch [28/200], Train Loss: 0.002980
Validation Loss: 0.00276848
Epoch [29/200], Train Loss: 0.002736
Validation Loss: 0.00256364
Epoch [30/200], Train Loss: 0.002610
Validation Loss: 0.00250557
Epoch [31/200], Train Loss: 0.002461
Validation Loss: 0.00236078
Epoch [32/200], Train Loss: 0.002353
Validation Loss: 0.00228218
Epoch [33/200], Train Loss: 0.002267
Validation Loss: 0.00217851
Epoch [34/200], Train Loss: 0.002155
Validation Loss: 0.00211491
Epoch [35/200], Train Loss: 0.002085
Validation Loss: 0.00207350
Epoch [36/200], Train Loss: 0.002025
Validation Loss: 0.00197714
Epoch [37/200], Train Loss: 0.001958
Validation Loss: 0.00190145
Epoch [38/200], Train Loss: 0.001893
Validation Loss: 0.00189008
Epoch [39/200], Train Loss: 0.001856
Validation Loss: 0.00183235
Epoch [40/200], Train Loss: 0.001823
Validation Loss: 0.00177802
Epoch [41/200], Train Loss: 0.001754
Validation Loss: 0.00174820
Epoch [42/200], Train Loss: 0.001723
Validation Loss: 0.00169792
Epoch [43/200], Train Loss: 0.001669
Validation Loss: 0.00164131
Epoch [44/200], Train Loss: 0.001634
Validation Loss: 0.00161499
Epoch [45/200], Train Loss: 0.001604
Validation Loss: 0.00159946
Epoch [46/200], Train Loss: 0.001567
Validation Loss: 0.00155057
Epoch [47/200], Train Loss: 0.001516
Validation Loss: 0.00152196
Epoch [48/200], Train Loss: 0.001498
Validation Loss: 0.00146358
Epoch [49/200], Train Loss: 0.001479
Validation Loss: 0.00147343
Epoch [50/200], Train Loss: 0.001444
Validation Loss: 0.00141561
Epoch [51/200], Train Loss: 0.001411
Validation Loss: 0.00139866
Epoch [52/200], Train Loss: 0.001390
Validation Loss: 0.00141036
Epoch [53/200], Train Loss: 0.001392
Validation Loss: 0.00157682
Epoch [54/200], Train Loss: 0.001418
Validation Loss: 0.00133771
Epoch [55/200], Train Loss: 0.001338
Validation Loss: 0.00131021
Epoch [56/200], Train Loss: 0.001306
Validation Loss: 0.00130587
Epoch [57/200], Train Loss: 0.001281
Validation Loss: 0.00129754
Epoch [58/200], Train Loss: 0.001273
Validation Loss: 0.00124357
Epoch [59/200], Train Loss: 0.001305
Validation Loss: 0.00125063
Epoch [60/200], Train Loss: 0.001226
Validation Loss: 0.00120934
Epoch [61/200], Train Loss: 0.001209
Validation Loss: 0.00120241
Epoch [62/200], Train Loss: 0.001184
Validation Loss: 0.00119326
Epoch [63/200], Train Loss: 0.001188
Validation Loss: 0.00118298
Epoch [64/200], Train Loss: 0.001166
Validation Loss: 0.00116063
Epoch [65/200], Train Loss: 0.001154
Validation Loss: 0.00114600
Epoch [66/200], Train Loss: 0.001137
Validation Loss: 0.00113789
Epoch [67/200], Train Loss: 0.001127
Validation Loss: 0.00111682
Epoch [68/200], Train Loss: 0.001112
Validation Loss: 0.00109612
Epoch [69/200], Train Loss: 0.001122
Validation Loss: 0.00111800
Epoch [70/200], Train Loss: 0.001120
Validation Loss: 0.00112203
Epoch [71/200], Train Loss: 0.001080
Validation Loss: 0.00108663
Epoch [72/200], Train Loss: 0.001070
Validation Loss: 0.00109634
Epoch [73/200], Train Loss: 0.001078
Validation Loss: 0.00106133
Epoch [74/200], Train Loss: 0.001039
Validation Loss: 0.00106114
Epoch [75/200], Train Loss: 0.001038
Validation Loss: 0.00104272
Epoch [76/200], Train Loss: 0.001030
Validation Loss: 0.00102135
Epoch [77/200], Train Loss: 0.001007
Validation Loss: 0.00100604
Epoch [78/200], Train Loss: 0.001004
Validation Loss: 0.00100527
Epoch [79/200], Train Loss: 0.000995
Validation Loss: 0.00100319
Epoch [80/200], Train Loss: 0.001010
Validation Loss: 0.00130454
Epoch [81/200], Train Loss: 0.001059
Validation Loss: 0.00101670
Epoch [82/200], Train Loss: 0.000980
Validation Loss: 0.00100043
Epoch [83/200], Train Loss: 0.000965
Validation Loss: 0.00099676
Epoch [84/200], Train Loss: 0.000960
Validation Loss: 0.00098010
Epoch [85/200], Train Loss: 0.001033
Validation Loss: 0.00097748
Epoch [86/200], Train Loss: 0.000948
Validation Loss: 0.00099082
Epoch [87/200], Train Loss: 0.000936
Validation Loss: 0.00094693
Epoch [88/200], Train Loss: 0.000934
Validation Loss: 0.00096679
Epoch [89/200], Train Loss: 0.000919
Validation Loss: 0.00092702
Epoch [90/200], Train Loss: 0.000903
Validation Loss: 0.00093362
Epoch [91/200], Train Loss: 0.000907
Validation Loss: 0.00092651
Epoch [92/200], Train Loss: 0.000895
Validation Loss: 0.00091386
Epoch [93/200], Train Loss: 0.000885
Validation Loss: 0.00090419
Epoch [94/200], Train Loss: 0.000878
Validation Loss: 0.00089934
Epoch [95/200], Train Loss: 0.000873
Validation Loss: 0.00089246
Epoch [96/200], Train Loss: 0.000883
Validation Loss: 0.00089712
Epoch [97/200], Train Loss: 0.000875
Validation Loss: 0.00089984
Epoch [98/200], Train Loss: 0.000862
Validation Loss: 0.00088201
Epoch [99/200], Train Loss: 0.000844
Validation Loss: 0.00085773
Epoch [100/200], Train Loss: 0.000853
Validation Loss: 0.00086886
Epoch [101/200], Train Loss: 0.001031
Validation Loss: 0.00104824
Epoch [102/200], Train Loss: 0.000989
Validation Loss: 0.00096625
Epoch [103/200], Train Loss: 0.000916
Validation Loss: 0.00093101
Epoch [104/200], Train Loss: 0.000896
Validation Loss: 0.00089616
Epoch [105/200], Train Loss: 0.000879
Validation Loss: 0.00090680
Epoch [106/200], Train Loss: 0.000907
Validation Loss: 0.00096825
Epoch [107/200], Train Loss: 0.000890
Validation Loss: 0.00089452
Epoch [108/200], Train Loss: 0.000848
Validation Loss: 0.00086402
Epoch [109/200], Train Loss: 0.000835
Validation Loss: 0.00086152
Early stopping triggered

Evaluating model for: Lamp
Run 109/144 completed in 1357.86 seconds with: {'MAE': np.float32(0.8852085), 'MSE': np.float32(32.692474), 'RMSE': np.float32(5.7177334), 'SAE': np.float32(0.04570221), 'NDE': np.float32(0.42054653)}

Run 110/144: hidden=256, seq_len=360, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.004764
Validation Loss: 0.00470677
Epoch [2/200], Train Loss: 0.004715
Validation Loss: 0.00467597
Epoch [3/200], Train Loss: 0.004707
Validation Loss: 0.00467274
Epoch [4/200], Train Loss: 0.004715
Validation Loss: 0.00467752
Epoch [5/200], Train Loss: 0.004701
Validation Loss: 0.00466873
Epoch [6/200], Train Loss: 0.004697
Validation Loss: 0.00466692
Epoch [7/200], Train Loss: 0.004689
Validation Loss: 0.00466909
Epoch [8/200], Train Loss: 0.004693
Validation Loss: 0.00466653
Epoch [9/200], Train Loss: 0.004701
Validation Loss: 0.00467339
Epoch [10/200], Train Loss: 0.004691
Validation Loss: 0.00466741
Epoch [11/200], Train Loss: 0.004688
Validation Loss: 0.00466387
Epoch [12/200], Train Loss: 0.004686
Validation Loss: 0.00467170
Epoch [13/200], Train Loss: 0.004682
Validation Loss: 0.00467885
Epoch [14/200], Train Loss: 0.004681
Validation Loss: 0.00465682
Epoch [15/200], Train Loss: 0.004681
Validation Loss: 0.00465978
Epoch [16/200], Train Loss: 0.004690
Validation Loss: 0.00465449
Epoch [17/200], Train Loss: 0.004665
Validation Loss: 0.00464866
Epoch [18/200], Train Loss: 0.004666
Validation Loss: 0.00464319
Epoch [19/200], Train Loss: 0.004651
Validation Loss: 0.00461763
Epoch [20/200], Train Loss: 0.004634
Validation Loss: 0.00459584
Epoch [21/200], Train Loss: 0.004584
Validation Loss: 0.00454170
Epoch [22/200], Train Loss: 0.004516
Validation Loss: 0.00442962
Epoch [23/200], Train Loss: 0.004416
Validation Loss: 0.00433304
Epoch [24/200], Train Loss: 0.004252
Validation Loss: 0.00418071
Epoch [25/200], Train Loss: 0.004097
Validation Loss: 0.00393593
Epoch [26/200], Train Loss: 0.003815
Validation Loss: 0.00369009
Epoch [27/200], Train Loss: 0.003523
Validation Loss: 0.00328609
Epoch [28/200], Train Loss: 0.002986
Validation Loss: 0.00271432
Epoch [29/200], Train Loss: 0.002598
Validation Loss: 0.00240604
Epoch [30/200], Train Loss: 0.002322
Validation Loss: 0.00219279
Epoch [31/200], Train Loss: 0.002142
Validation Loss: 0.00205124
Epoch [32/200], Train Loss: 0.002008
Validation Loss: 0.00196751
Epoch [33/200], Train Loss: 0.001929
Validation Loss: 0.00185209
Epoch [34/200], Train Loss: 0.001808
Validation Loss: 0.00178274
Epoch [35/200], Train Loss: 0.001736
Validation Loss: 0.00169936
Epoch [36/200], Train Loss: 0.001664
Validation Loss: 0.00168196
Epoch [37/200], Train Loss: 0.001613
Validation Loss: 0.00160704
Epoch [38/200], Train Loss: 0.001543
Validation Loss: 0.00152066
Epoch [39/200], Train Loss: 0.001479
Validation Loss: 0.00145920
Epoch [40/200], Train Loss: 0.001430
Validation Loss: 0.00142378
Epoch [41/200], Train Loss: 0.001381
Validation Loss: 0.00137122
Epoch [42/200], Train Loss: 0.001331
Validation Loss: 0.00132771
Epoch [43/200], Train Loss: 0.001316
Validation Loss: 0.00130297
Epoch [44/200], Train Loss: 0.001267
Validation Loss: 0.00125595
Epoch [45/200], Train Loss: 0.001234
Validation Loss: 0.00123378
Epoch [46/200], Train Loss: 0.001194
Validation Loss: 0.00120662
Epoch [47/200], Train Loss: 0.001169
Validation Loss: 0.00117252
Epoch [48/200], Train Loss: 0.001182
Validation Loss: 0.00121556
Epoch [49/200], Train Loss: 0.001146
Validation Loss: 0.00114999
Epoch [50/200], Train Loss: 0.001110
Validation Loss: 0.00117001
Epoch [51/200], Train Loss: 0.001071
Validation Loss: 0.00111030
Epoch [52/200], Train Loss: 0.001048
Validation Loss: 0.00108232
Epoch [53/200], Train Loss: 0.001026
Validation Loss: 0.00104404
Epoch [54/200], Train Loss: 0.001006
Validation Loss: 0.00104393
Epoch [55/200], Train Loss: 0.001029
Validation Loss: 0.00106512
Epoch [56/200], Train Loss: 0.000974
Validation Loss: 0.00099723
Epoch [57/200], Train Loss: 0.000956
Validation Loss: 0.00096744
Epoch [58/200], Train Loss: 0.000934
Validation Loss: 0.00094985
Epoch [59/200], Train Loss: 0.000922
Validation Loss: 0.00093208
Epoch [60/200], Train Loss: 0.000914
Validation Loss: 0.00093122
Epoch [61/200], Train Loss: 0.000905
Validation Loss: 0.00093824
Epoch [62/200], Train Loss: 0.000894
Validation Loss: 0.00091079
Epoch [63/200], Train Loss: 0.000862
Validation Loss: 0.00083913
Epoch [64/200], Train Loss: 0.000826
Validation Loss: 0.00082178
Epoch [65/200], Train Loss: 0.000851
Validation Loss: 0.00080921
Epoch [66/200], Train Loss: 0.000794
Validation Loss: 0.00081041
Epoch [67/200], Train Loss: 0.000788
Validation Loss: 0.00079211
Epoch [68/200], Train Loss: 0.000787
Validation Loss: 0.00074578
Epoch [69/200], Train Loss: 0.000763
Validation Loss: 0.00091394
Epoch [70/200], Train Loss: 0.000808
Validation Loss: 0.00073971
Epoch [71/200], Train Loss: 0.000747
Validation Loss: 0.00071360
Epoch [72/200], Train Loss: 0.000729
Validation Loss: 0.00070422
Epoch [73/200], Train Loss: 0.000734
Validation Loss: 0.00078157
Epoch [74/200], Train Loss: 0.000752
Validation Loss: 0.00072527
Epoch [75/200], Train Loss: 0.000729
Validation Loss: 0.00072098
Epoch [76/200], Train Loss: 0.000715
Validation Loss: 0.00069027
Epoch [77/200], Train Loss: 0.000690
Validation Loss: 0.00066179
Epoch [78/200], Train Loss: 0.000753
Validation Loss: 0.00069394
Epoch [79/200], Train Loss: 0.000674
Validation Loss: 0.00064756
Epoch [80/200], Train Loss: 0.000671
Validation Loss: 0.00064952
Epoch [81/200], Train Loss: 0.000654
Validation Loss: 0.00067121
Epoch [82/200], Train Loss: 0.000643
Validation Loss: 0.00061807
Epoch [83/200], Train Loss: 0.000631
Validation Loss: 0.00061193
Epoch [84/200], Train Loss: 0.000622
Validation Loss: 0.00061291
Epoch [85/200], Train Loss: 0.000627
Validation Loss: 0.00063725
Epoch [86/200], Train Loss: 0.000609
Validation Loss: 0.00057541
Epoch [87/200], Train Loss: 0.000608
Validation Loss: 0.00059006
Epoch [88/200], Train Loss: 0.000599
Validation Loss: 0.00056344
Epoch [89/200], Train Loss: 0.000585
Validation Loss: 0.00060295
Epoch [90/200], Train Loss: 0.000631
Validation Loss: 0.00057687
Epoch [91/200], Train Loss: 0.000580
Validation Loss: 0.00054266
Epoch [92/200], Train Loss: 0.000579
Validation Loss: 0.00053909
Epoch [93/200], Train Loss: 0.000574
Validation Loss: 0.00052563
Epoch [94/200], Train Loss: 0.000557
Validation Loss: 0.00054793
Epoch [95/200], Train Loss: 0.000559
Validation Loss: 0.00060122
Epoch [96/200], Train Loss: 0.000559
Validation Loss: 0.00051274
Epoch [97/200], Train Loss: 0.000543
Validation Loss: 0.00052249
Epoch [98/200], Train Loss: 0.000538
Validation Loss: 0.00050128
Epoch [99/200], Train Loss: 0.000545
Validation Loss: 0.00050706
Epoch [100/200], Train Loss: 0.000536
Validation Loss: 0.00052595
Epoch [101/200], Train Loss: 0.000529
Validation Loss: 0.00050048
Epoch [102/200], Train Loss: 0.000525
Validation Loss: 0.00048379
Epoch [103/200], Train Loss: 0.000516
Validation Loss: 0.00051495
Epoch [104/200], Train Loss: 0.000553
Validation Loss: 0.00050679
Epoch [105/200], Train Loss: 0.000512
Validation Loss: 0.00048896
Epoch [106/200], Train Loss: 0.000519
Validation Loss: 0.00049311
Epoch [107/200], Train Loss: 0.000506
Validation Loss: 0.00046846
Epoch [108/200], Train Loss: 0.000501
Validation Loss: 0.00048242
Epoch [109/200], Train Loss: 0.000491
Validation Loss: 0.00045990
Epoch [110/200], Train Loss: 0.000494
Validation Loss: 0.00049651
Epoch [111/200], Train Loss: 0.000493
Validation Loss: 0.00045295
Epoch [112/200], Train Loss: 0.000514
Validation Loss: 0.00045497
Epoch [113/200], Train Loss: 0.000486
Validation Loss: 0.00045510
Epoch [114/200], Train Loss: 0.000485
Validation Loss: 0.00045906
Epoch [115/200], Train Loss: 0.000483
Validation Loss: 0.00044816
Epoch [116/200], Train Loss: 0.000473
Validation Loss: 0.00049384
Epoch [117/200], Train Loss: 0.000566
Validation Loss: 0.00049023
Epoch [118/200], Train Loss: 0.000495
Validation Loss: 0.00044403
Epoch [119/200], Train Loss: 0.000464
Validation Loss: 0.00043008
Epoch [120/200], Train Loss: 0.000461
Validation Loss: 0.00044858
Epoch [121/200], Train Loss: 0.000453
Validation Loss: 0.00042186
Epoch [122/200], Train Loss: 0.000454
Validation Loss: 0.00041609
Epoch [123/200], Train Loss: 0.000450
Validation Loss: 0.00042519
Epoch [124/200], Train Loss: 0.000451
Validation Loss: 0.00042121
Epoch [125/200], Train Loss: 0.000440
Validation Loss: 0.00041229
Epoch [126/200], Train Loss: 0.000446
Validation Loss: 0.00046668
Epoch [127/200], Train Loss: 0.000457
Validation Loss: 0.00042353
Epoch [128/200], Train Loss: 0.000457
Validation Loss: 0.00040438
Epoch [129/200], Train Loss: 0.000433
Validation Loss: 0.00040255
Epoch [130/200], Train Loss: 0.000438
Validation Loss: 0.00043091
Epoch [131/200], Train Loss: 0.000476
Validation Loss: 0.00045564
Epoch [132/200], Train Loss: 0.000448
Validation Loss: 0.00039959
Epoch [133/200], Train Loss: 0.000422
Validation Loss: 0.00038955
Epoch [134/200], Train Loss: 0.000420
Validation Loss: 0.00038935
Epoch [135/200], Train Loss: 0.000420
Validation Loss: 0.00038787
Epoch [136/200], Train Loss: 0.000416
Validation Loss: 0.00038298
Epoch [137/200], Train Loss: 0.000413
Validation Loss: 0.00038608
Epoch [138/200], Train Loss: 0.000410
Validation Loss: 0.00037495
Epoch [139/200], Train Loss: 0.000404
Validation Loss: 0.00037099
Epoch [140/200], Train Loss: 0.000410
Validation Loss: 0.00038431
Epoch [141/200], Train Loss: 0.000404
Validation Loss: 0.00036617
Epoch [142/200], Train Loss: 0.000430
Validation Loss: 0.00055339
Epoch [143/200], Train Loss: 0.000425
Validation Loss: 0.00037200
Epoch [144/200], Train Loss: 0.000399
Validation Loss: 0.00036286
Epoch [145/200], Train Loss: 0.000393
Validation Loss: 0.00036187
Epoch [146/200], Train Loss: 0.000393
Validation Loss: 0.00037620
Epoch [147/200], Train Loss: 0.000400
Validation Loss: 0.00035493
Epoch [148/200], Train Loss: 0.000394
Validation Loss: 0.00036186
Epoch [149/200], Train Loss: 0.000395
Validation Loss: 0.00037759
Epoch [150/200], Train Loss: 0.000401
Validation Loss: 0.00035715
Epoch [151/200], Train Loss: 0.000395
Validation Loss: 0.00035296
Epoch [152/200], Train Loss: 0.000386
Validation Loss: 0.00036492
Epoch [153/200], Train Loss: 0.000384
Validation Loss: 0.00034604
Epoch [154/200], Train Loss: 0.000378
Validation Loss: 0.00035003
Epoch [155/200], Train Loss: 0.000375
Validation Loss: 0.00034192
Epoch [156/200], Train Loss: 0.000370
Validation Loss: 0.00033355
Epoch [157/200], Train Loss: 0.000367
Validation Loss: 0.00036690
Epoch [158/200], Train Loss: 0.000570
Validation Loss: 0.00042756
Epoch [159/200], Train Loss: 0.000394
Validation Loss: 0.00034149
Epoch [160/200], Train Loss: 0.000369
Validation Loss: 0.00034884
Epoch [161/200], Train Loss: 0.000369
Validation Loss: 0.00033426
Epoch [162/200], Train Loss: 0.000371
Validation Loss: 0.00033679
Epoch [163/200], Train Loss: 0.000365
Validation Loss: 0.00035847
Epoch [164/200], Train Loss: 0.000366
Validation Loss: 0.00032925
Epoch [165/200], Train Loss: 0.000357
Validation Loss: 0.00033622
Epoch [166/200], Train Loss: 0.000354
Validation Loss: 0.00032396
Epoch [167/200], Train Loss: 0.000360
Validation Loss: 0.00032064
Epoch [168/200], Train Loss: 0.000351
Validation Loss: 0.00031958
Epoch [169/200], Train Loss: 0.000351
Validation Loss: 0.00031689
Epoch [170/200], Train Loss: 0.000352
Validation Loss: 0.00031932
Epoch [171/200], Train Loss: 0.000353
Validation Loss: 0.00032085
Epoch [172/200], Train Loss: 0.000346
Validation Loss: 0.00032540
Epoch [173/200], Train Loss: 0.000345
Validation Loss: 0.00031328
Epoch [174/200], Train Loss: 0.000341
Validation Loss: 0.00031126
Epoch [175/200], Train Loss: 0.000341
Validation Loss: 0.00031123
Epoch [176/200], Train Loss: 0.000345
Validation Loss: 0.00030651
Epoch [177/200], Train Loss: 0.000339
Validation Loss: 0.00030788
Epoch [178/200], Train Loss: 0.000342
Validation Loss: 0.00032672
Epoch [179/200], Train Loss: 0.000349
Validation Loss: 0.00030471
Epoch [180/200], Train Loss: 0.000337
Validation Loss: 0.00035503
Epoch [181/200], Train Loss: 0.000351
Validation Loss: 0.00031192
Epoch [182/200], Train Loss: 0.000338
Validation Loss: 0.00030327
Epoch [183/200], Train Loss: 0.000339
Validation Loss: 0.00030460
Epoch [184/200], Train Loss: 0.000330
Validation Loss: 0.00029508
Epoch [185/200], Train Loss: 0.000326
Validation Loss: 0.00029645
Epoch [186/200], Train Loss: 0.000326
Validation Loss: 0.00029076
Epoch [187/200], Train Loss: 0.000324
Validation Loss: 0.00029563
Epoch [188/200], Train Loss: 0.000326
Validation Loss: 0.00029512
Epoch [189/200], Train Loss: 0.000324
Validation Loss: 0.00029040
Epoch [190/200], Train Loss: 0.000321
Validation Loss: 0.00029087
Epoch [191/200], Train Loss: 0.000329
Validation Loss: 0.00028976
Epoch [192/200], Train Loss: 0.000328
Validation Loss: 0.00030458
Epoch [193/200], Train Loss: 0.000328
Validation Loss: 0.00030735
Epoch [194/200], Train Loss: 0.000324
Validation Loss: 0.00028963
Epoch [195/200], Train Loss: 0.000318
Validation Loss: 0.00028581
Epoch [196/200], Train Loss: 0.000321
Validation Loss: 0.00028738
Epoch [197/200], Train Loss: 0.000314
Validation Loss: 0.00029056
Epoch [198/200], Train Loss: 0.000313
Validation Loss: 0.00029338
Epoch [199/200], Train Loss: 0.000313
Validation Loss: 0.00027941
Epoch [200/200], Train Loss: 0.000312
Validation Loss: 0.00028065

Evaluating model for: Lamp
Run 110/144 completed in 2866.42 seconds with: {'MAE': np.float32(0.39835992), 'MSE': np.float32(12.059862), 'RMSE': np.float32(3.472731), 'SAE': np.float32(0.009524565), 'NDE': np.float32(0.2554237)}

Run 111/144: hidden=256, seq_len=360, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.004813
Validation Loss: 0.00474584
Epoch [2/200], Train Loss: 0.004757
Validation Loss: 0.00468776
Epoch [3/200], Train Loss: 0.004723
Validation Loss: 0.00467615
Epoch [4/200], Train Loss: 0.004714
Validation Loss: 0.00467630
Epoch [5/200], Train Loss: 0.004705
Validation Loss: 0.00466931
Epoch [6/200], Train Loss: 0.004701
Validation Loss: 0.00467749
Epoch [7/200], Train Loss: 0.004691
Validation Loss: 0.00466677
Epoch [8/200], Train Loss: 0.004700
Validation Loss: 0.00466622
Epoch [9/200], Train Loss: 0.004687
Validation Loss: 0.00466484
Epoch [10/200], Train Loss: 0.004686
Validation Loss: 0.00466170
Epoch [11/200], Train Loss: 0.004694
Validation Loss: 0.00466076
Epoch [12/200], Train Loss: 0.004682
Validation Loss: 0.00466296
Epoch [13/200], Train Loss: 0.004687
Validation Loss: 0.00466285
Epoch [14/200], Train Loss: 0.004682
Validation Loss: 0.00465757
Epoch [15/200], Train Loss: 0.004691
Validation Loss: 0.00466627
Epoch [16/200], Train Loss: 0.004682
Validation Loss: 0.00466104
Epoch [17/200], Train Loss: 0.004691
Validation Loss: 0.00466185
Epoch [18/200], Train Loss: 0.004680
Validation Loss: 0.00465813
Epoch [19/200], Train Loss: 0.004678
Validation Loss: 0.00465754
Epoch [20/200], Train Loss: 0.004692
Validation Loss: 0.00466833
Epoch [21/200], Train Loss: 0.004690
Validation Loss: 0.00467223
Epoch [22/200], Train Loss: 0.004670
Validation Loss: 0.00465832
Epoch [23/200], Train Loss: 0.004681
Validation Loss: 0.00465816
Epoch [24/200], Train Loss: 0.004684
Validation Loss: 0.00466058
Epoch [25/200], Train Loss: 0.004689
Validation Loss: 0.00465673
Epoch [26/200], Train Loss: 0.004686
Validation Loss: 0.00466016
Epoch [27/200], Train Loss: 0.004678
Validation Loss: 0.00465663
Epoch [28/200], Train Loss: 0.004675
Validation Loss: 0.00465556
Epoch [29/200], Train Loss: 0.004676
Validation Loss: 0.00465907
Epoch [30/200], Train Loss: 0.004671
Validation Loss: 0.00465504
Epoch [31/200], Train Loss: 0.004676
Validation Loss: 0.00465257
Epoch [32/200], Train Loss: 0.004677
Validation Loss: 0.00465001
Epoch [33/200], Train Loss: 0.004678
Validation Loss: 0.00465279
Epoch [34/200], Train Loss: 0.004670
Validation Loss: 0.00464624
Epoch [35/200], Train Loss: 0.004656
Validation Loss: 0.00464254
Epoch [36/200], Train Loss: 0.004666
Validation Loss: 0.00464205
Epoch [37/200], Train Loss: 0.004652
Validation Loss: 0.00460447
Epoch [38/200], Train Loss: 0.004618
Validation Loss: 0.00459240
Epoch [39/200], Train Loss: 0.004588
Validation Loss: 0.00454038
Epoch [40/200], Train Loss: 0.004513
Validation Loss: 0.00443994
Epoch [41/200], Train Loss: 0.004387
Validation Loss: 0.00439806
Epoch [42/200], Train Loss: 0.004203
Validation Loss: 0.00427185
Epoch [43/200], Train Loss: 0.003881
Validation Loss: 0.00354552
Epoch [44/200], Train Loss: 0.003396
Validation Loss: 0.00288094
Epoch [45/200], Train Loss: 0.002694
Validation Loss: 0.00252045
Epoch [46/200], Train Loss: 0.002326
Validation Loss: 0.00211425
Epoch [47/200], Train Loss: 0.002099
Validation Loss: 0.00197673
Epoch [48/200], Train Loss: 0.001904
Validation Loss: 0.00178786
Epoch [49/200], Train Loss: 0.001743
Validation Loss: 0.00174932
Epoch [50/200], Train Loss: 0.001639
Validation Loss: 0.00161058
Epoch [51/200], Train Loss: 0.001551
Validation Loss: 0.00155693
Epoch [52/200], Train Loss: 0.001465
Validation Loss: 0.00145586
Epoch [53/200], Train Loss: 0.001368
Validation Loss: 0.00141404
Epoch [54/200], Train Loss: 0.001307
Validation Loss: 0.00138715
Epoch [55/200], Train Loss: 0.001253
Validation Loss: 0.00125555
Epoch [56/200], Train Loss: 0.001226
Validation Loss: 0.00124634
Epoch [57/200], Train Loss: 0.001164
Validation Loss: 0.00116982
Epoch [58/200], Train Loss: 0.001091
Validation Loss: 0.00106766
Epoch [59/200], Train Loss: 0.001037
Validation Loss: 0.00099409
Epoch [60/200], Train Loss: 0.001033
Validation Loss: 0.00095235
Epoch [61/200], Train Loss: 0.000955
Validation Loss: 0.00088908
Epoch [62/200], Train Loss: 0.001079
Validation Loss: 0.00105901
Epoch [63/200], Train Loss: 0.000923
Validation Loss: 0.00085474
Epoch [64/200], Train Loss: 0.000874
Validation Loss: 0.00084088
Epoch [65/200], Train Loss: 0.000841
Validation Loss: 0.00079904
Epoch [66/200], Train Loss: 0.000928
Validation Loss: 0.00082636
Epoch [67/200], Train Loss: 0.000826
Validation Loss: 0.00076922
Epoch [68/200], Train Loss: 0.000796
Validation Loss: 0.00078130
Epoch [69/200], Train Loss: 0.000769
Validation Loss: 0.00073877
Epoch [70/200], Train Loss: 0.000744
Validation Loss: 0.00073309
Epoch [71/200], Train Loss: 0.000736
Validation Loss: 0.00070338
Epoch [72/200], Train Loss: 0.000735
Validation Loss: 0.00071516
Epoch [73/200], Train Loss: 0.000720
Validation Loss: 0.00069124
Epoch [74/200], Train Loss: 0.000699
Validation Loss: 0.00068516
Epoch [75/200], Train Loss: 0.000691
Validation Loss: 0.00067993
Epoch [76/200], Train Loss: 0.000686
Validation Loss: 0.00067852
Epoch [77/200], Train Loss: 0.000693
Validation Loss: 0.00064183
Epoch [78/200], Train Loss: 0.000667
Validation Loss: 0.00063157
Epoch [79/200], Train Loss: 0.000651
Validation Loss: 0.00061736
Epoch [80/200], Train Loss: 0.000648
Validation Loss: 0.00064463
Epoch [81/200], Train Loss: 0.000640
Validation Loss: 0.00062320
Epoch [82/200], Train Loss: 0.000632
Validation Loss: 0.00060400
Epoch [83/200], Train Loss: 0.000628
Validation Loss: 0.00059508
Epoch [84/200], Train Loss: 0.000608
Validation Loss: 0.00058129
Epoch [85/200], Train Loss: 0.000611
Validation Loss: 0.00060485
Epoch [86/200], Train Loss: 0.000612
Validation Loss: 0.00058604
Epoch [87/200], Train Loss: 0.000576
Validation Loss: 0.00055756
Epoch [88/200], Train Loss: 0.000575
Validation Loss: 0.00053135
Epoch [89/200], Train Loss: 0.000558
Validation Loss: 0.00052527
Epoch [90/200], Train Loss: 0.000552
Validation Loss: 0.00051316
Epoch [91/200], Train Loss: 0.000558
Validation Loss: 0.00053132
Epoch [92/200], Train Loss: 0.000534
Validation Loss: 0.00051313
Epoch [93/200], Train Loss: 0.000521
Validation Loss: 0.00049458
Epoch [94/200], Train Loss: 0.000514
Validation Loss: 0.00048926
Epoch [95/200], Train Loss: 0.000506
Validation Loss: 0.00048550
Epoch [96/200], Train Loss: 0.000505
Validation Loss: 0.00047591
Epoch [97/200], Train Loss: 0.000516
Validation Loss: 0.00047503
Epoch [98/200], Train Loss: 0.000502
Validation Loss: 0.00047122
Epoch [99/200], Train Loss: 0.000488
Validation Loss: 0.00046137
Epoch [100/200], Train Loss: 0.000481
Validation Loss: 0.00044100
Epoch [101/200], Train Loss: 0.000486
Validation Loss: 0.00045688
Epoch [102/200], Train Loss: 0.000471
Validation Loss: 0.00049878
Epoch [103/200], Train Loss: 0.000471
Validation Loss: 0.00045348
Epoch [104/200], Train Loss: 0.000467
Validation Loss: 0.00042299
Epoch [105/200], Train Loss: 0.000466
Validation Loss: 0.00043412
Epoch [106/200], Train Loss: 0.000446
Validation Loss: 0.00041177
Epoch [107/200], Train Loss: 0.000445
Validation Loss: 0.00040620
Epoch [108/200], Train Loss: 0.000437
Validation Loss: 0.00040855
Epoch [109/200], Train Loss: 0.000437
Validation Loss: 0.00039896
Epoch [110/200], Train Loss: 0.000438
Validation Loss: 0.00038284
Epoch [111/200], Train Loss: 0.000421
Validation Loss: 0.00039771
Epoch [112/200], Train Loss: 0.000421
Validation Loss: 0.00038754
Epoch [113/200], Train Loss: 0.000408
Validation Loss: 0.00037701
Epoch [114/200], Train Loss: 0.000423
Validation Loss: 0.00037649
Epoch [115/200], Train Loss: 0.000408
Validation Loss: 0.00038973
Epoch [116/200], Train Loss: 0.000420
Validation Loss: 0.00038999
Epoch [117/200], Train Loss: 0.000398
Validation Loss: 0.00035668
Epoch [118/200], Train Loss: 0.000398
Validation Loss: 0.00036297
Epoch [119/200], Train Loss: 0.000393
Validation Loss: 0.00034921
Epoch [120/200], Train Loss: 0.000394
Validation Loss: 0.00037603
Epoch [121/200], Train Loss: 0.000401
Validation Loss: 0.00034651
Epoch [122/200], Train Loss: 0.000384
Validation Loss: 0.00033461
Epoch [123/200], Train Loss: 0.000449
Validation Loss: 0.00037688
Epoch [124/200], Train Loss: 0.000392
Validation Loss: 0.00033770
Epoch [125/200], Train Loss: 0.000376
Validation Loss: 0.00033515
Epoch [126/200], Train Loss: 0.000373
Validation Loss: 0.00033628
Epoch [127/200], Train Loss: 0.000378
Validation Loss: 0.00033278
Epoch [128/200], Train Loss: 0.000372
Validation Loss: 0.00033234
Epoch [129/200], Train Loss: 0.000380
Validation Loss: 0.00034649
Epoch [130/200], Train Loss: 0.000374
Validation Loss: 0.00032481
Epoch [131/200], Train Loss: 0.000368
Validation Loss: 0.00034176
Epoch [132/200], Train Loss: 0.000370
Validation Loss: 0.00031788
Epoch [133/200], Train Loss: 0.000362
Validation Loss: 0.00031067
Epoch [134/200], Train Loss: 0.000366
Validation Loss: 0.00033163
Epoch [135/200], Train Loss: 0.000370
Validation Loss: 0.00033239
Epoch [136/200], Train Loss: 0.000352
Validation Loss: 0.00030942
Epoch [137/200], Train Loss: 0.000365
Validation Loss: 0.00032261
Epoch [138/200], Train Loss: 0.000356
Validation Loss: 0.00032249
Epoch [139/200], Train Loss: 0.000364
Validation Loss: 0.00031530
Epoch [140/200], Train Loss: 0.000343
Validation Loss: 0.00029348
Epoch [141/200], Train Loss: 0.000344
Validation Loss: 0.00031674
Epoch [142/200], Train Loss: 0.000357
Validation Loss: 0.00030253
Epoch [143/200], Train Loss: 0.000341
Validation Loss: 0.00030034
Epoch [144/200], Train Loss: 0.000336
Validation Loss: 0.00028782
Epoch [145/200], Train Loss: 0.000329
Validation Loss: 0.00028760
Epoch [146/200], Train Loss: 0.000352
Validation Loss: 0.00033715
Epoch [147/200], Train Loss: 0.000339
Validation Loss: 0.00028134
Epoch [148/200], Train Loss: 0.000332
Validation Loss: 0.00028438
Epoch [149/200], Train Loss: 0.000329
Validation Loss: 0.00028975
Epoch [150/200], Train Loss: 0.000344
Validation Loss: 0.00028505
Epoch [151/200], Train Loss: 0.000328
Validation Loss: 0.00027188
Epoch [152/200], Train Loss: 0.000320
Validation Loss: 0.00026979
Epoch [153/200], Train Loss: 0.000322
Validation Loss: 0.00027002
Epoch [154/200], Train Loss: 0.000318
Validation Loss: 0.00027967
Epoch [155/200], Train Loss: 0.000331
Validation Loss: 0.00028788
Epoch [156/200], Train Loss: 0.000315
Validation Loss: 0.00027541
Epoch [157/200], Train Loss: 0.000320
Validation Loss: 0.00031730
Epoch [158/200], Train Loss: 0.000315
Validation Loss: 0.00028639
Epoch [159/200], Train Loss: 0.000312
Validation Loss: 0.00025647
Epoch [160/200], Train Loss: 0.000310
Validation Loss: 0.00029453
Epoch [161/200], Train Loss: 0.000541
Validation Loss: 0.00090409
Epoch [162/200], Train Loss: 0.000526
Validation Loss: 0.00039264
Epoch [163/200], Train Loss: 0.000381
Validation Loss: 0.00035006
Epoch [164/200], Train Loss: 0.000362
Validation Loss: 0.00032365
Epoch [165/200], Train Loss: 0.000346
Validation Loss: 0.00030787
Epoch [166/200], Train Loss: 0.000335
Validation Loss: 0.00029302
Epoch [167/200], Train Loss: 0.000328
Validation Loss: 0.00028585
Epoch [168/200], Train Loss: 0.000319
Validation Loss: 0.00029359
Epoch [169/200], Train Loss: 0.000328
Validation Loss: 0.00028140
Early stopping triggered

Evaluating model for: Lamp
Run 111/144 completed in 2744.59 seconds with: {'MAE': np.float32(0.36384526), 'MSE': np.float32(11.260821), 'RMSE': np.float32(3.3557148), 'SAE': np.float32(0.028955529), 'NDE': np.float32(0.2468172)}

Run 112/144: hidden=256, seq_len=360, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 9402 windows

Epoch [1/200], Train Loss: 0.004810
Validation Loss: 0.00475240
Epoch [2/200], Train Loss: 0.004775
Validation Loss: 0.00469667
Epoch [3/200], Train Loss: 0.004711
Validation Loss: 0.00467533
Epoch [4/200], Train Loss: 0.004705
Validation Loss: 0.00467202
Epoch [5/200], Train Loss: 0.004694
Validation Loss: 0.00467067
Epoch [6/200], Train Loss: 0.004693
Validation Loss: 0.00467160
Epoch [7/200], Train Loss: 0.004694
Validation Loss: 0.00467126
Epoch [8/200], Train Loss: 0.004687
Validation Loss: 0.00466512
Epoch [9/200], Train Loss: 0.004702
Validation Loss: 0.00467083
Epoch [10/200], Train Loss: 0.004688
Validation Loss: 0.00466492
Epoch [11/200], Train Loss: 0.004691
Validation Loss: 0.00466295
Epoch [12/200], Train Loss: 0.004680
Validation Loss: 0.00466236
Epoch [13/200], Train Loss: 0.004688
Validation Loss: 0.00466604
Epoch [14/200], Train Loss: 0.004682
Validation Loss: 0.00467216
Epoch [15/200], Train Loss: 0.004688
Validation Loss: 0.00465909
Epoch [16/200], Train Loss: 0.004688
Validation Loss: 0.00466412
Epoch [17/200], Train Loss: 0.004687
Validation Loss: 0.00465988
Epoch [18/200], Train Loss: 0.004677
Validation Loss: 0.00465848
Epoch [19/200], Train Loss: 0.004685
Validation Loss: 0.00466446
Epoch [20/200], Train Loss: 0.004679
Validation Loss: 0.00466012
Epoch [21/200], Train Loss: 0.004680
Validation Loss: 0.00466124
Epoch [22/200], Train Loss: 0.004684
Validation Loss: 0.00465915
Epoch [23/200], Train Loss: 0.004676
Validation Loss: 0.00466165
Epoch [24/200], Train Loss: 0.004676
Validation Loss: 0.00466163
Epoch [25/200], Train Loss: 0.004683
Validation Loss: 0.00465843
Epoch [26/200], Train Loss: 0.004681
Validation Loss: 0.00466942
Epoch [27/200], Train Loss: 0.004678
Validation Loss: 0.00466640
Epoch [28/200], Train Loss: 0.004684
Validation Loss: 0.00465819
Epoch [29/200], Train Loss: 0.004686
Validation Loss: 0.00465661
Epoch [30/200], Train Loss: 0.004682
Validation Loss: 0.00465668
Epoch [31/200], Train Loss: 0.004683
Validation Loss: 0.00466223
Epoch [32/200], Train Loss: 0.004690
Validation Loss: 0.00466611
Epoch [33/200], Train Loss: 0.004696
Validation Loss: 0.00465587
Epoch [34/200], Train Loss: 0.004673
Validation Loss: 0.00467198
Epoch [35/200], Train Loss: 0.004681
Validation Loss: 0.00465687
Epoch [36/200], Train Loss: 0.004677
Validation Loss: 0.00465867
Epoch [37/200], Train Loss: 0.004680
Validation Loss: 0.00465521
Epoch [38/200], Train Loss: 0.004680
Validation Loss: 0.00465607
Epoch [39/200], Train Loss: 0.004681
Validation Loss: 0.00465358
Epoch [40/200], Train Loss: 0.004671
Validation Loss: 0.00465132
Epoch [41/200], Train Loss: 0.004668
Validation Loss: 0.00464544
Epoch [42/200], Train Loss: 0.004664
Validation Loss: 0.00464764
Epoch [43/200], Train Loss: 0.004670
Validation Loss: 0.00465864
Epoch [44/200], Train Loss: 0.004673
Validation Loss: 0.00464062
Epoch [45/200], Train Loss: 0.004647
Validation Loss: 0.00462590
Epoch [46/200], Train Loss: 0.004642
Validation Loss: 0.00461073
Epoch [47/200], Train Loss: 0.004626
Validation Loss: 0.00461025
Epoch [48/200], Train Loss: 0.004613
Validation Loss: 0.00454840
Epoch [49/200], Train Loss: 0.004536
Validation Loss: 0.00445951
Epoch [50/200], Train Loss: 0.004415
Validation Loss: 0.00439687
Epoch [51/200], Train Loss: 0.004187
Validation Loss: 0.00391609
Epoch [52/200], Train Loss: 0.004185
Validation Loss: 0.00357010
Epoch [53/200], Train Loss: 0.002998
Validation Loss: 0.00258284
Epoch [54/200], Train Loss: 0.002338
Validation Loss: 0.00214847
Epoch [55/200], Train Loss: 0.002022
Validation Loss: 0.00194533
Epoch [56/200], Train Loss: 0.001819
Validation Loss: 0.00178182
Epoch [57/200], Train Loss: 0.001657
Validation Loss: 0.00162054
Epoch [58/200], Train Loss: 0.001517
Validation Loss: 0.00149079
Epoch [59/200], Train Loss: 0.001407
Validation Loss: 0.00146739
Epoch [60/200], Train Loss: 0.001328
Validation Loss: 0.00133046
Epoch [61/200], Train Loss: 0.001237
Validation Loss: 0.00120987
Epoch [62/200], Train Loss: 0.001157
Validation Loss: 0.00115647
Epoch [63/200], Train Loss: 0.001147
Validation Loss: 0.00134435
Epoch [64/200], Train Loss: 0.001262
Validation Loss: 0.00121037
Epoch [65/200], Train Loss: 0.001100
Validation Loss: 0.00110844
Epoch [66/200], Train Loss: 0.001026
Validation Loss: 0.00102456
Epoch [67/200], Train Loss: 0.000961
Validation Loss: 0.00092195
Epoch [68/200], Train Loss: 0.000918
Validation Loss: 0.00088859
Epoch [69/200], Train Loss: 0.000889
Validation Loss: 0.00086700
Epoch [70/200], Train Loss: 0.000857
Validation Loss: 0.00082469
Epoch [71/200], Train Loss: 0.000864
Validation Loss: 0.00081067
Epoch [72/200], Train Loss: 0.000817
Validation Loss: 0.00078873
Epoch [73/200], Train Loss: 0.000801
Validation Loss: 0.00076422
Epoch [74/200], Train Loss: 0.000780
Validation Loss: 0.00073384
Epoch [75/200], Train Loss: 0.000770
Validation Loss: 0.00076303
Epoch [76/200], Train Loss: 0.000764
Validation Loss: 0.00072762
Epoch [77/200], Train Loss: 0.000729
Validation Loss: 0.00069565
Epoch [78/200], Train Loss: 0.000713
Validation Loss: 0.00071581
Epoch [79/200], Train Loss: 0.000702
Validation Loss: 0.00067785
Epoch [80/200], Train Loss: 0.000692
Validation Loss: 0.00067197
Epoch [81/200], Train Loss: 0.000697
Validation Loss: 0.00066607
Epoch [82/200], Train Loss: 0.000673
Validation Loss: 0.00067751
Epoch [83/200], Train Loss: 0.000665
Validation Loss: 0.00064631
Epoch [84/200], Train Loss: 0.000665
Validation Loss: 0.00062880
Epoch [85/200], Train Loss: 0.000647
Validation Loss: 0.00064535
Epoch [86/200], Train Loss: 0.000739
Validation Loss: 0.00085037
Epoch [87/200], Train Loss: 0.000711
Validation Loss: 0.00062696
Epoch [88/200], Train Loss: 0.000630
Validation Loss: 0.00062549
Epoch [89/200], Train Loss: 0.000621
Validation Loss: 0.00060325
Epoch [90/200], Train Loss: 0.000662
Validation Loss: 0.00063980
Epoch [91/200], Train Loss: 0.000600
Validation Loss: 0.00058148
Epoch [92/200], Train Loss: 0.000581
Validation Loss: 0.00056936
Epoch [93/200], Train Loss: 0.000580
Validation Loss: 0.00056742
Epoch [94/200], Train Loss: 0.000569
Validation Loss: 0.00054660
Epoch [95/200], Train Loss: 0.000558
Validation Loss: 0.00056649
Epoch [96/200], Train Loss: 0.000560
Validation Loss: 0.00053349
Epoch [97/200], Train Loss: 0.000557
Validation Loss: 0.00052863
Epoch [98/200], Train Loss: 0.000544
Validation Loss: 0.00055794
Epoch [99/200], Train Loss: 0.000538
Validation Loss: 0.00050818
Epoch [100/200], Train Loss: 0.000540
Validation Loss: 0.00055763
Epoch [101/200], Train Loss: 0.000544
Validation Loss: 0.00051549
Epoch [102/200], Train Loss: 0.000528
Validation Loss: 0.00051103
Epoch [103/200], Train Loss: 0.000521
Validation Loss: 0.00050433
Epoch [104/200], Train Loss: 0.000516
Validation Loss: 0.00049142
Epoch [105/200], Train Loss: 0.000508
Validation Loss: 0.00048772
Epoch [106/200], Train Loss: 0.000524
Validation Loss: 0.00048040
Epoch [107/200], Train Loss: 0.000501
Validation Loss: 0.00047770
Epoch [108/200], Train Loss: 0.000503
Validation Loss: 0.00048745
Epoch [109/200], Train Loss: 0.000500
Validation Loss: 0.00051856
Epoch [110/200], Train Loss: 0.000512
Validation Loss: 0.00045747
Epoch [111/200], Train Loss: 0.000486
Validation Loss: 0.00045907
Epoch [112/200], Train Loss: 0.000480
Validation Loss: 0.00045964
Epoch [113/200], Train Loss: 0.000474
Validation Loss: 0.00046669
Epoch [114/200], Train Loss: 0.000479
Validation Loss: 0.00050842
Epoch [115/200], Train Loss: 0.000490
Validation Loss: 0.00044321
Epoch [116/200], Train Loss: 0.000467
Validation Loss: 0.00048049
Epoch [117/200], Train Loss: 0.000489
Validation Loss: 0.00044890
Epoch [118/200], Train Loss: 0.000468
Validation Loss: 0.00043759
Epoch [119/200], Train Loss: 0.000457
Validation Loss: 0.00044771
Epoch [120/200], Train Loss: 0.000508
Validation Loss: 0.00046095
Epoch [121/200], Train Loss: 0.000471
Validation Loss: 0.00043513
Epoch [122/200], Train Loss: 0.000457
Validation Loss: 0.00043886
Epoch [123/200], Train Loss: 0.000463
Validation Loss: 0.00042812
Epoch [124/200], Train Loss: 0.000456
Validation Loss: 0.00047439
Epoch [125/200], Train Loss: 0.000473
Validation Loss: 0.00044608
Epoch [126/200], Train Loss: 0.000462
Validation Loss: 0.00042249
Epoch [127/200], Train Loss: 0.000453
Validation Loss: 0.00042304
Epoch [128/200], Train Loss: 0.000446
Validation Loss: 0.00041459
Epoch [129/200], Train Loss: 0.000439
Validation Loss: 0.00040405
Epoch [130/200], Train Loss: 0.000427
Validation Loss: 0.00040184
Epoch [131/200], Train Loss: 0.000422
Validation Loss: 0.00039632
Epoch [132/200], Train Loss: 0.000424
Validation Loss: 0.00042165
Epoch [133/200], Train Loss: 0.000429
Validation Loss: 0.00039172
Epoch [134/200], Train Loss: 0.000424
Validation Loss: 0.00038955
Epoch [135/200], Train Loss: 0.000450
Validation Loss: 0.00041954
Epoch [136/200], Train Loss: 0.000436
Validation Loss: 0.00039968
Epoch [137/200], Train Loss: 0.000418
Validation Loss: 0.00038670
Epoch [138/200], Train Loss: 0.000415
Validation Loss: 0.00039167
Epoch [139/200], Train Loss: 0.000416
Validation Loss: 0.00040189
Epoch [140/200], Train Loss: 0.000405
Validation Loss: 0.00037801
Epoch [141/200], Train Loss: 0.000400
Validation Loss: 0.00037234
Epoch [142/200], Train Loss: 0.000398
Validation Loss: 0.00036989
Epoch [143/200], Train Loss: 0.000400
Validation Loss: 0.00037856
Epoch [144/200], Train Loss: 0.000399
Validation Loss: 0.00037427
Epoch [145/200], Train Loss: 0.000397
Validation Loss: 0.00037314
Epoch [146/200], Train Loss: 0.000390
Validation Loss: 0.00036969
Epoch [147/200], Train Loss: 0.000394
Validation Loss: 0.00035616
Epoch [148/200], Train Loss: 0.000394
Validation Loss: 0.00036101
Epoch [149/200], Train Loss: 0.000388
Validation Loss: 0.00036849
Epoch [150/200], Train Loss: 0.000389
Validation Loss: 0.00037706
Epoch [151/200], Train Loss: 0.000383
Validation Loss: 0.00036076
Epoch [152/200], Train Loss: 0.000385
Validation Loss: 0.00035444
Epoch [153/200], Train Loss: 0.000382
Validation Loss: 0.00035564
Epoch [154/200], Train Loss: 0.000384
Validation Loss: 0.00035059
Epoch [155/200], Train Loss: 0.000370
Validation Loss: 0.00034076
Epoch [156/200], Train Loss: 0.000369
Validation Loss: 0.00035300
Epoch [157/200], Train Loss: 0.000369
Validation Loss: 0.00033870
Epoch [158/200], Train Loss: 0.000367
Validation Loss: 0.00033688
Epoch [159/200], Train Loss: 0.000361
Validation Loss: 0.00033160
Epoch [160/200], Train Loss: 0.000363
Validation Loss: 0.00032810
Epoch [161/200], Train Loss: 0.000364
Validation Loss: 0.00033100
Epoch [162/200], Train Loss: 0.000418
Validation Loss: 0.00041150
Epoch [163/200], Train Loss: 0.000389
Validation Loss: 0.00033305
Epoch [164/200], Train Loss: 0.000363
Validation Loss: 0.00031967
Epoch [165/200], Train Loss: 0.000359
Validation Loss: 0.00031834
Epoch [166/200], Train Loss: 0.000353
Validation Loss: 0.00034212
Epoch [167/200], Train Loss: 0.000351
Validation Loss: 0.00031750
Epoch [168/200], Train Loss: 0.000405
Validation Loss: 0.00033436
Epoch [169/200], Train Loss: 0.000351
Validation Loss: 0.00030961
Epoch [170/200], Train Loss: 0.000347
Validation Loss: 0.00031653
Epoch [171/200], Train Loss: 0.000354
Validation Loss: 0.00031776
Epoch [172/200], Train Loss: 0.000347
Validation Loss: 0.00031607
Epoch [173/200], Train Loss: 0.000339
Validation Loss: 0.00029770
Epoch [174/200], Train Loss: 0.000350
Validation Loss: 0.00042434
Epoch [175/200], Train Loss: 0.000419
Validation Loss: 0.00034703
Epoch [176/200], Train Loss: 0.000348
Validation Loss: 0.00031802
Epoch [177/200], Train Loss: 0.000339
Validation Loss: 0.00029982
Epoch [178/200], Train Loss: 0.000334
Validation Loss: 0.00028995
Epoch [179/200], Train Loss: 0.000334
Validation Loss: 0.00029775
Epoch [180/200], Train Loss: 0.000344
Validation Loss: 0.00029755
Epoch [181/200], Train Loss: 0.000334
Validation Loss: 0.00029886
Epoch [182/200], Train Loss: 0.000332
Validation Loss: 0.00029384
Epoch [183/200], Train Loss: 0.000328
Validation Loss: 0.00032080
Epoch [184/200], Train Loss: 0.000356
Validation Loss: 0.00028670
Epoch [185/200], Train Loss: 0.000327
Validation Loss: 0.00028245
Epoch [186/200], Train Loss: 0.000319
Validation Loss: 0.00028386
Epoch [187/200], Train Loss: 0.000321
Validation Loss: 0.00027712
Epoch [188/200], Train Loss: 0.000321
Validation Loss: 0.00027916
Epoch [189/200], Train Loss: 0.000331
Validation Loss: 0.00030904
Epoch [190/200], Train Loss: 0.000402
Validation Loss: 0.00044289
Epoch [191/200], Train Loss: 0.000440
Validation Loss: 0.00042295
Epoch [192/200], Train Loss: 0.000433
Validation Loss: 0.00041281
Epoch [193/200], Train Loss: 0.000420
Validation Loss: 0.00040620
Epoch [194/200], Train Loss: 0.000412
Validation Loss: 0.00038057
Epoch [195/200], Train Loss: 0.000386
Validation Loss: 0.00035790
Epoch [196/200], Train Loss: 0.000368
Validation Loss: 0.00034587
Epoch [197/200], Train Loss: 0.000357
Validation Loss: 0.00032541
Early stopping triggered

Evaluating model for: Lamp
Run 112/144 completed in 4085.34 seconds with: {'MAE': np.float32(0.4057035), 'MSE': np.float32(14.96866), 'RMSE': np.float32(3.8689353), 'SAE': np.float32(0.014984132), 'NDE': np.float32(0.2845649)}

Run 113/144: hidden=256, seq_len=360, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.005197
Validation Loss: 0.00385394
Epoch [2/200], Train Loss: 0.004999
Validation Loss: 0.00382022
Epoch [3/200], Train Loss: 0.004970
Validation Loss: 0.00379839
Epoch [4/200], Train Loss: 0.004868
Validation Loss: 0.00379167
Epoch [5/200], Train Loss: 0.004902
Validation Loss: 0.00378161
Epoch [6/200], Train Loss: 0.004909
Validation Loss: 0.00378212
Epoch [7/200], Train Loss: 0.004893
Validation Loss: 0.00378375
Epoch [8/200], Train Loss: 0.004871
Validation Loss: 0.00377868
Epoch [9/200], Train Loss: 0.004929
Validation Loss: 0.00377725
Epoch [10/200], Train Loss: 0.004939
Validation Loss: 0.00377603
Epoch [11/200], Train Loss: 0.004942
Validation Loss: 0.00378051
Epoch [12/200], Train Loss: 0.004871
Validation Loss: 0.00377388
Epoch [13/200], Train Loss: 0.004891
Validation Loss: 0.00377247
Epoch [14/200], Train Loss: 0.004865
Validation Loss: 0.00377004
Epoch [15/200], Train Loss: 0.004867
Validation Loss: 0.00376864
Epoch [16/200], Train Loss: 0.004865
Validation Loss: 0.00377107
Epoch [17/200], Train Loss: 0.004885
Validation Loss: 0.00376965
Epoch [18/200], Train Loss: 0.004883
Validation Loss: 0.00376542
Epoch [19/200], Train Loss: 0.004897
Validation Loss: 0.00376628
Epoch [20/200], Train Loss: 0.004872
Validation Loss: 0.00376290
Epoch [21/200], Train Loss: 0.004922
Validation Loss: 0.00376068
Epoch [22/200], Train Loss: 0.004899
Validation Loss: 0.00376190
Epoch [23/200], Train Loss: 0.004843
Validation Loss: 0.00376705
Epoch [24/200], Train Loss: 0.004914
Validation Loss: 0.00375499
Epoch [25/200], Train Loss: 0.004830
Validation Loss: 0.00375570
Epoch [26/200], Train Loss: 0.004866
Validation Loss: 0.00375039
Epoch [27/200], Train Loss: 0.004838
Validation Loss: 0.00374556
Epoch [28/200], Train Loss: 0.004844
Validation Loss: 0.00374481
Epoch [29/200], Train Loss: 0.004860
Validation Loss: 0.00374087
Epoch [30/200], Train Loss: 0.004840
Validation Loss: 0.00373539
Epoch [31/200], Train Loss: 0.004863
Validation Loss: 0.00374084
Epoch [32/200], Train Loss: 0.004818
Validation Loss: 0.00373085
Epoch [33/200], Train Loss: 0.004859
Validation Loss: 0.00372903
Epoch [34/200], Train Loss: 0.004799
Validation Loss: 0.00371947
Epoch [35/200], Train Loss: 0.004794
Validation Loss: 0.00371277
Epoch [36/200], Train Loss: 0.004801
Validation Loss: 0.00370985
Epoch [37/200], Train Loss: 0.004822
Validation Loss: 0.00371308
Epoch [38/200], Train Loss: 0.004784
Validation Loss: 0.00369448
Epoch [39/200], Train Loss: 0.004840
Validation Loss: 0.00369586
Epoch [40/200], Train Loss: 0.004743
Validation Loss: 0.00370743
Epoch [41/200], Train Loss: 0.004741
Validation Loss: 0.00366912
Epoch [42/200], Train Loss: 0.004770
Validation Loss: 0.00374815
Epoch [43/200], Train Loss: 0.004729
Validation Loss: 0.00365968
Epoch [44/200], Train Loss: 0.004731
Validation Loss: 0.00365300
Epoch [45/200], Train Loss: 0.004753
Validation Loss: 0.00363397
Epoch [46/200], Train Loss: 0.004736
Validation Loss: 0.00371993
Epoch [47/200], Train Loss: 0.004710
Validation Loss: 0.00361241
Epoch [48/200], Train Loss: 0.004674
Validation Loss: 0.00361450
Epoch [49/200], Train Loss: 0.004675
Validation Loss: 0.00357434
Epoch [50/200], Train Loss: 0.004616
Validation Loss: 0.00360777
Epoch [51/200], Train Loss: 0.004602
Validation Loss: 0.00353787
Epoch [52/200], Train Loss: 0.004568
Validation Loss: 0.00349973
Epoch [53/200], Train Loss: 0.004524
Validation Loss: 0.00347224
Epoch [54/200], Train Loss: 0.004480
Validation Loss: 0.00351085
Epoch [55/200], Train Loss: 0.004413
Validation Loss: 0.00341362
Epoch [56/200], Train Loss: 0.004386
Validation Loss: 0.00339286
Epoch [57/200], Train Loss: 0.004397
Validation Loss: 0.00340813
Epoch [58/200], Train Loss: 0.004329
Validation Loss: 0.00333176
Epoch [59/200], Train Loss: 0.004256
Validation Loss: 0.00342997
Epoch [60/200], Train Loss: 0.004256
Validation Loss: 0.00325408
Epoch [61/200], Train Loss: 0.004176
Validation Loss: 0.00321622
Epoch [62/200], Train Loss: 0.004115
Validation Loss: 0.00325340
Epoch [63/200], Train Loss: 0.004047
Validation Loss: 0.00314691
Epoch [64/200], Train Loss: 0.004013
Validation Loss: 0.00317303
Epoch [65/200], Train Loss: 0.003981
Validation Loss: 0.00306659
Epoch [66/200], Train Loss: 0.003920
Validation Loss: 0.00300715
Epoch [67/200], Train Loss: 0.003887
Validation Loss: 0.00293757
Epoch [68/200], Train Loss: 0.003814
Validation Loss: 0.00286989
Epoch [69/200], Train Loss: 0.003669
Validation Loss: 0.00280015
Epoch [70/200], Train Loss: 0.003555
Validation Loss: 0.00275064
Epoch [71/200], Train Loss: 0.003503
Validation Loss: 0.00266285
Epoch [72/200], Train Loss: 0.003409
Validation Loss: 0.00256363
Epoch [73/200], Train Loss: 0.003317
Validation Loss: 0.00254704
Epoch [74/200], Train Loss: 0.003284
Validation Loss: 0.00243391
Epoch [75/200], Train Loss: 0.003133
Validation Loss: 0.00240556
Epoch [76/200], Train Loss: 0.003058
Validation Loss: 0.00234257
Epoch [77/200], Train Loss: 0.002981
Validation Loss: 0.00228534
Epoch [78/200], Train Loss: 0.002911
Validation Loss: 0.00223076
Epoch [79/200], Train Loss: 0.002832
Validation Loss: 0.00220141
Epoch [80/200], Train Loss: 0.002760
Validation Loss: 0.00216492
Epoch [81/200], Train Loss: 0.002765
Validation Loss: 0.00215476
Epoch [82/200], Train Loss: 0.002674
Validation Loss: 0.00209170
Epoch [83/200], Train Loss: 0.002629
Validation Loss: 0.00205218
Epoch [84/200], Train Loss: 0.002602
Validation Loss: 0.00204703
Epoch [85/200], Train Loss: 0.002584
Validation Loss: 0.00201499
Epoch [86/200], Train Loss: 0.002505
Validation Loss: 0.00199780
Epoch [87/200], Train Loss: 0.002524
Validation Loss: 0.00197245
Epoch [88/200], Train Loss: 0.002425
Validation Loss: 0.00192430
Epoch [89/200], Train Loss: 0.002396
Validation Loss: 0.00190585
Epoch [90/200], Train Loss: 0.002411
Validation Loss: 0.00190764
Epoch [91/200], Train Loss: 0.002348
Validation Loss: 0.00186295
Epoch [92/200], Train Loss: 0.002319
Validation Loss: 0.00184595
Epoch [93/200], Train Loss: 0.002303
Validation Loss: 0.00184523
Epoch [94/200], Train Loss: 0.002264
Validation Loss: 0.00181437
Epoch [95/200], Train Loss: 0.002249
Validation Loss: 0.00178691
Epoch [96/200], Train Loss: 0.002252
Validation Loss: 0.00177939
Epoch [97/200], Train Loss: 0.002195
Validation Loss: 0.00176163
Epoch [98/200], Train Loss: 0.002192
Validation Loss: 0.00173722
Epoch [99/200], Train Loss: 0.002176
Validation Loss: 0.00172179
Epoch [100/200], Train Loss: 0.002147
Validation Loss: 0.00170102
Epoch [101/200], Train Loss: 0.002130
Validation Loss: 0.00168584
Epoch [102/200], Train Loss: 0.002130
Validation Loss: 0.00168463
Epoch [103/200], Train Loss: 0.002111
Validation Loss: 0.00166883
Epoch [104/200], Train Loss: 0.002105
Validation Loss: 0.00164985
Epoch [105/200], Train Loss: 0.002079
Validation Loss: 0.00162825
Epoch [106/200], Train Loss: 0.002058
Validation Loss: 0.00166832
Epoch [107/200], Train Loss: 0.002056
Validation Loss: 0.00163792
Epoch [108/200], Train Loss: 0.002033
Validation Loss: 0.00160166
Epoch [109/200], Train Loss: 0.002000
Validation Loss: 0.00159501
Epoch [110/200], Train Loss: 0.002002
Validation Loss: 0.00158511
Epoch [111/200], Train Loss: 0.001980
Validation Loss: 0.00156112
Epoch [112/200], Train Loss: 0.001972
Validation Loss: 0.00154931
Epoch [113/200], Train Loss: 0.001958
Validation Loss: 0.00156136
Epoch [114/200], Train Loss: 0.001940
Validation Loss: 0.00154610
Epoch [115/200], Train Loss: 0.001937
Validation Loss: 0.00152156
Epoch [116/200], Train Loss: 0.001902
Validation Loss: 0.00153653
Epoch [117/200], Train Loss: 0.001886
Validation Loss: 0.00151369
Epoch [118/200], Train Loss: 0.001879
Validation Loss: 0.00152642
Epoch [119/200], Train Loss: 0.001890
Validation Loss: 0.00150667
Epoch [120/200], Train Loss: 0.001880
Validation Loss: 0.00148530
Epoch [121/200], Train Loss: 0.001877
Validation Loss: 0.00150933
Epoch [122/200], Train Loss: 0.001853
Validation Loss: 0.00148392
Epoch [123/200], Train Loss: 0.001824
Validation Loss: 0.00146816
Epoch [124/200], Train Loss: 0.001823
Validation Loss: 0.00145699
Epoch [125/200], Train Loss: 0.001831
Validation Loss: 0.00144611
Epoch [126/200], Train Loss: 0.001801
Validation Loss: 0.00144337
Epoch [127/200], Train Loss: 0.001787
Validation Loss: 0.00143612
Epoch [128/200], Train Loss: 0.001796
Validation Loss: 0.00142377
Epoch [129/200], Train Loss: 0.001773
Validation Loss: 0.00142931
Epoch [130/200], Train Loss: 0.001803
Validation Loss: 0.00142353
Epoch [131/200], Train Loss: 0.001786
Validation Loss: 0.00142474
Epoch [132/200], Train Loss: 0.001775
Validation Loss: 0.00141750
Epoch [133/200], Train Loss: 0.001745
Validation Loss: 0.00142138
Epoch [134/200], Train Loss: 0.001735
Validation Loss: 0.00138408
Epoch [135/200], Train Loss: 0.001708
Validation Loss: 0.00139023
Epoch [136/200], Train Loss: 0.001706
Validation Loss: 0.00138419
Epoch [137/200], Train Loss: 0.001765
Validation Loss: 0.00145760
Epoch [138/200], Train Loss: 0.001727
Validation Loss: 0.00138701
Epoch [139/200], Train Loss: 0.001698
Validation Loss: 0.00135941
Epoch [140/200], Train Loss: 0.001693
Validation Loss: 0.00135844
Epoch [141/200], Train Loss: 0.001751
Validation Loss: 0.00137510
Epoch [142/200], Train Loss: 0.001678
Validation Loss: 0.00134660
Epoch [143/200], Train Loss: 0.001680
Validation Loss: 0.00134130
Epoch [144/200], Train Loss: 0.001666
Validation Loss: 0.00133431
Epoch [145/200], Train Loss: 0.001650
Validation Loss: 0.00132830
Epoch [146/200], Train Loss: 0.001661
Validation Loss: 0.00132534
Epoch [147/200], Train Loss: 0.001642
Validation Loss: 0.00131897
Epoch [148/200], Train Loss: 0.001622
Validation Loss: 0.00131004
Epoch [149/200], Train Loss: 0.001618
Validation Loss: 0.00130179
Epoch [150/200], Train Loss: 0.001633
Validation Loss: 0.00131400
Epoch [151/200], Train Loss: 0.001605
Validation Loss: 0.00128806
Epoch [152/200], Train Loss: 0.001589
Validation Loss: 0.00128683
Epoch [153/200], Train Loss: 0.001592
Validation Loss: 0.00128317
Epoch [154/200], Train Loss: 0.001574
Validation Loss: 0.00127892
Epoch [155/200], Train Loss: 0.001584
Validation Loss: 0.00127656
Epoch [156/200], Train Loss: 0.001565
Validation Loss: 0.00127936
Epoch [157/200], Train Loss: 0.001567
Validation Loss: 0.00126200
Epoch [158/200], Train Loss: 0.001562
Validation Loss: 0.00126777
Epoch [159/200], Train Loss: 0.001558
Validation Loss: 0.00125799
Epoch [160/200], Train Loss: 0.001548
Validation Loss: 0.00125226
Epoch [161/200], Train Loss: 0.001548
Validation Loss: 0.00124582
Epoch [162/200], Train Loss: 0.001531
Validation Loss: 0.00124530
Epoch [163/200], Train Loss: 0.001526
Validation Loss: 0.00124757
Epoch [164/200], Train Loss: 0.001523
Validation Loss: 0.00124539
Epoch [165/200], Train Loss: 0.001499
Validation Loss: 0.00122766
Epoch [166/200], Train Loss: 0.001514
Validation Loss: 0.00122729
Epoch [167/200], Train Loss: 0.001487
Validation Loss: 0.00121356
Epoch [168/200], Train Loss: 0.001482
Validation Loss: 0.00121384
Epoch [169/200], Train Loss: 0.001491
Validation Loss: 0.00122753
Epoch [170/200], Train Loss: 0.001485
Validation Loss: 0.00123353
Epoch [171/200], Train Loss: 0.001478
Validation Loss: 0.00120778
Epoch [172/200], Train Loss: 0.001465
Validation Loss: 0.00123124
Epoch [173/200], Train Loss: 0.001481
Validation Loss: 0.00119302
Epoch [174/200], Train Loss: 0.001451
Validation Loss: 0.00118772
Epoch [175/200], Train Loss: 0.001466
Validation Loss: 0.00118939
Epoch [176/200], Train Loss: 0.001456
Validation Loss: 0.00118639
Epoch [177/200], Train Loss: 0.001437
Validation Loss: 0.00119818
Epoch [178/200], Train Loss: 0.001447
Validation Loss: 0.00119271
Epoch [179/200], Train Loss: 0.001461
Validation Loss: 0.00120160
Epoch [180/200], Train Loss: 0.001435
Validation Loss: 0.00118285
Epoch [181/200], Train Loss: 0.001428
Validation Loss: 0.00119018
Epoch [182/200], Train Loss: 0.001425
Validation Loss: 0.00118343
Epoch [183/200], Train Loss: 0.001418
Validation Loss: 0.00117608
Epoch [184/200], Train Loss: 0.001407
Validation Loss: 0.00116509
Epoch [185/200], Train Loss: 0.001403
Validation Loss: 0.00117188
Epoch [186/200], Train Loss: 0.001403
Validation Loss: 0.00115836
Epoch [187/200], Train Loss: 0.001397
Validation Loss: 0.00115752
Epoch [188/200], Train Loss: 0.001399
Validation Loss: 0.00116668
Epoch [189/200], Train Loss: 0.001405
Validation Loss: 0.00114602
Epoch [190/200], Train Loss: 0.001378
Validation Loss: 0.00116634
Epoch [191/200], Train Loss: 0.001394
Validation Loss: 0.00115014
Epoch [192/200], Train Loss: 0.001382
Validation Loss: 0.00114349
Epoch [193/200], Train Loss: 0.001370
Validation Loss: 0.00114964
Epoch [194/200], Train Loss: 0.001362
Validation Loss: 0.00115305
Epoch [195/200], Train Loss: 0.001383
Validation Loss: 0.00113310
Epoch [196/200], Train Loss: 0.001369
Validation Loss: 0.00113790
Epoch [197/200], Train Loss: 0.001372
Validation Loss: 0.00113728
Epoch [198/200], Train Loss: 0.001352
Validation Loss: 0.00112550
Epoch [199/200], Train Loss: 0.001358
Validation Loss: 0.00112535
Epoch [200/200], Train Loss: 0.001355
Validation Loss: 0.00112376

Evaluating model for: Lamp
Run 113/144 completed in 994.77 seconds with: {'MAE': np.float32(1.2705001), 'MSE': np.float32(41.70166), 'RMSE': np.float32(6.457682), 'SAE': np.float32(0.036920942), 'NDE': np.float32(0.49971223)}

Run 114/144: hidden=256, seq_len=360, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.004932
Validation Loss: 0.00381901
Epoch [2/200], Train Loss: 0.004934
Validation Loss: 0.00380515
Epoch [3/200], Train Loss: 0.004903
Validation Loss: 0.00378717
Epoch [4/200], Train Loss: 0.004889
Validation Loss: 0.00377500
Epoch [5/200], Train Loss: 0.004871
Validation Loss: 0.00377371
Epoch [6/200], Train Loss: 0.004835
Validation Loss: 0.00377538
Epoch [7/200], Train Loss: 0.004916
Validation Loss: 0.00377847
Epoch [8/200], Train Loss: 0.004853
Validation Loss: 0.00377074
Epoch [9/200], Train Loss: 0.004911
Validation Loss: 0.00377759
Epoch [10/200], Train Loss: 0.004896
Validation Loss: 0.00376755
Epoch [11/200], Train Loss: 0.004865
Validation Loss: 0.00378074
Epoch [12/200], Train Loss: 0.004824
Validation Loss: 0.00376673
Epoch [13/200], Train Loss: 0.004857
Validation Loss: 0.00376518
Epoch [14/200], Train Loss: 0.004880
Validation Loss: 0.00376324
Epoch [15/200], Train Loss: 0.004864
Validation Loss: 0.00376319
Epoch [16/200], Train Loss: 0.004876
Validation Loss: 0.00376233
Epoch [17/200], Train Loss: 0.004845
Validation Loss: 0.00377842
Epoch [18/200], Train Loss: 0.004910
Validation Loss: 0.00376370
Epoch [19/200], Train Loss: 0.004876
Validation Loss: 0.00376455
Epoch [20/200], Train Loss: 0.004853
Validation Loss: 0.00376273
Epoch [21/200], Train Loss: 0.004870
Validation Loss: 0.00376194
Epoch [22/200], Train Loss: 0.004850
Validation Loss: 0.00377661
Epoch [23/200], Train Loss: 0.004895
Validation Loss: 0.00376268
Epoch [24/200], Train Loss: 0.004825
Validation Loss: 0.00375798
Epoch [25/200], Train Loss: 0.004839
Validation Loss: 0.00376125
Epoch [26/200], Train Loss: 0.004867
Validation Loss: 0.00376167
Epoch [27/200], Train Loss: 0.004823
Validation Loss: 0.00375618
Epoch [28/200], Train Loss: 0.004900
Validation Loss: 0.00376791
Epoch [29/200], Train Loss: 0.004867
Validation Loss: 0.00375688
Epoch [30/200], Train Loss: 0.004871
Validation Loss: 0.00375419
Epoch [31/200], Train Loss: 0.004903
Validation Loss: 0.00375894
Epoch [32/200], Train Loss: 0.004854
Validation Loss: 0.00375644
Epoch [33/200], Train Loss: 0.004858
Validation Loss: 0.00375146
Epoch [34/200], Train Loss: 0.004851
Validation Loss: 0.00375066
Epoch [35/200], Train Loss: 0.004904
Validation Loss: 0.00375552
Epoch [36/200], Train Loss: 0.004899
Validation Loss: 0.00374985
Epoch [37/200], Train Loss: 0.004906
Validation Loss: 0.00375000
Epoch [38/200], Train Loss: 0.004880
Validation Loss: 0.00374743
Epoch [39/200], Train Loss: 0.004853
Validation Loss: 0.00377865
Epoch [40/200], Train Loss: 0.004869
Validation Loss: 0.00375191
Epoch [41/200], Train Loss: 0.004829
Validation Loss: 0.00374713
Epoch [42/200], Train Loss: 0.004860
Validation Loss: 0.00374884
Epoch [43/200], Train Loss: 0.004859
Validation Loss: 0.00374228
Epoch [44/200], Train Loss: 0.004858
Validation Loss: 0.00374466
Epoch [45/200], Train Loss: 0.004862
Validation Loss: 0.00374032
Epoch [46/200], Train Loss: 0.004865
Validation Loss: 0.00374062
Epoch [47/200], Train Loss: 0.004825
Validation Loss: 0.00373214
Epoch [48/200], Train Loss: 0.004833
Validation Loss: 0.00372606
Epoch [49/200], Train Loss: 0.004840
Validation Loss: 0.00372511
Epoch [50/200], Train Loss: 0.004845
Validation Loss: 0.00370245
Epoch [51/200], Train Loss: 0.004816
Validation Loss: 0.00370793
Epoch [52/200], Train Loss: 0.004806
Validation Loss: 0.00371663
Epoch [53/200], Train Loss: 0.004793
Validation Loss: 0.00370095
Epoch [54/200], Train Loss: 0.004823
Validation Loss: 0.00369069
Epoch [55/200], Train Loss: 0.004798
Validation Loss: 0.00370796
Epoch [56/200], Train Loss: 0.004764
Validation Loss: 0.00365009
Epoch [57/200], Train Loss: 0.004706
Validation Loss: 0.00366734
Epoch [58/200], Train Loss: 0.004712
Validation Loss: 0.00370922
Epoch [59/200], Train Loss: 0.004717
Validation Loss: 0.00364853
Epoch [60/200], Train Loss: 0.004671
Validation Loss: 0.00363339
Epoch [61/200], Train Loss: 0.004665
Validation Loss: 0.00355872
Epoch [62/200], Train Loss: 0.004541
Validation Loss: 0.00355230
Epoch [63/200], Train Loss: 0.004558
Validation Loss: 0.00352867
Epoch [64/200], Train Loss: 0.004522
Validation Loss: 0.00358999
Epoch [65/200], Train Loss: 0.004492
Validation Loss: 0.00347078
Epoch [66/200], Train Loss: 0.004441
Validation Loss: 0.00342630
Epoch [67/200], Train Loss: 0.004365
Validation Loss: 0.00343783
Epoch [68/200], Train Loss: 0.004432
Validation Loss: 0.00340692
Epoch [69/200], Train Loss: 0.004282
Validation Loss: 0.00334291
Epoch [70/200], Train Loss: 0.004255
Validation Loss: 0.00332890
Epoch [71/200], Train Loss: 0.004245
Validation Loss: 0.00324368
Epoch [72/200], Train Loss: 0.004113
Validation Loss: 0.00320290
Epoch [73/200], Train Loss: 0.004088
Validation Loss: 0.00315317
Epoch [74/200], Train Loss: 0.004000
Validation Loss: 0.00306879
Epoch [75/200], Train Loss: 0.003885
Validation Loss: 0.00300244
Epoch [76/200], Train Loss: 0.003799
Validation Loss: 0.00290129
Epoch [77/200], Train Loss: 0.003728
Validation Loss: 0.00285708
Epoch [78/200], Train Loss: 0.003623
Validation Loss: 0.00281780
Epoch [79/200], Train Loss: 0.003517
Validation Loss: 0.00263579
Epoch [80/200], Train Loss: 0.003389
Validation Loss: 0.00253976
Epoch [81/200], Train Loss: 0.003247
Validation Loss: 0.00245401
Epoch [82/200], Train Loss: 0.003187
Validation Loss: 0.00235492
Epoch [83/200], Train Loss: 0.003014
Validation Loss: 0.00227612
Epoch [84/200], Train Loss: 0.002935
Validation Loss: 0.00219767
Epoch [85/200], Train Loss: 0.002815
Validation Loss: 0.00212049
Epoch [86/200], Train Loss: 0.002734
Validation Loss: 0.00212715
Epoch [87/200], Train Loss: 0.002663
Validation Loss: 0.00202373
Epoch [88/200], Train Loss: 0.002646
Validation Loss: 0.00210594
Epoch [89/200], Train Loss: 0.002578
Validation Loss: 0.00194126
Epoch [90/200], Train Loss: 0.002441
Validation Loss: 0.00188780
Epoch [91/200], Train Loss: 0.002441
Validation Loss: 0.00188306
Epoch [92/200], Train Loss: 0.002354
Validation Loss: 0.00183194
Epoch [93/200], Train Loss: 0.002306
Validation Loss: 0.00179880
Epoch [94/200], Train Loss: 0.002289
Validation Loss: 0.00179131
Epoch [95/200], Train Loss: 0.002234
Validation Loss: 0.00176651
Epoch [96/200], Train Loss: 0.002208
Validation Loss: 0.00172142
Epoch [97/200], Train Loss: 0.002188
Validation Loss: 0.00168002
Epoch [98/200], Train Loss: 0.002141
Validation Loss: 0.00166221
Epoch [99/200], Train Loss: 0.002105
Validation Loss: 0.00164958
Epoch [100/200], Train Loss: 0.002120
Validation Loss: 0.00163270
Epoch [101/200], Train Loss: 0.002050
Validation Loss: 0.00160260
Epoch [102/200], Train Loss: 0.002045
Validation Loss: 0.00158576
Epoch [103/200], Train Loss: 0.002013
Validation Loss: 0.00156108
Epoch [104/200], Train Loss: 0.001985
Validation Loss: 0.00154483
Epoch [105/200], Train Loss: 0.001940
Validation Loss: 0.00150691
Epoch [106/200], Train Loss: 0.001952
Validation Loss: 0.00152859
Epoch [107/200], Train Loss: 0.001902
Validation Loss: 0.00148394
Epoch [108/200], Train Loss: 0.001868
Validation Loss: 0.00147532
Epoch [109/200], Train Loss: 0.001856
Validation Loss: 0.00145087
Epoch [110/200], Train Loss: 0.001840
Validation Loss: 0.00145493
Epoch [111/200], Train Loss: 0.001851
Validation Loss: 0.00141993
Epoch [112/200], Train Loss: 0.001834
Validation Loss: 0.00141601
Epoch [113/200], Train Loss: 0.001801
Validation Loss: 0.00140233
Epoch [114/200], Train Loss: 0.001773
Validation Loss: 0.00139653
Epoch [115/200], Train Loss: 0.001752
Validation Loss: 0.00137576
Epoch [116/200], Train Loss: 0.001737
Validation Loss: 0.00136696
Epoch [117/200], Train Loss: 0.001721
Validation Loss: 0.00138872
Epoch [118/200], Train Loss: 0.001720
Validation Loss: 0.00135986
Epoch [119/200], Train Loss: 0.001702
Validation Loss: 0.00134067
Epoch [120/200], Train Loss: 0.001678
Validation Loss: 0.00132273
Epoch [121/200], Train Loss: 0.001646
Validation Loss: 0.00131005
Epoch [122/200], Train Loss: 0.001640
Validation Loss: 0.00129849
Epoch [123/200], Train Loss: 0.001621
Validation Loss: 0.00129754
Epoch [124/200], Train Loss: 0.001623
Validation Loss: 0.00129337
Epoch [125/200], Train Loss: 0.001603
Validation Loss: 0.00128463
Epoch [126/200], Train Loss: 0.001581
Validation Loss: 0.00127771
Epoch [127/200], Train Loss: 0.001572
Validation Loss: 0.00127356
Epoch [128/200], Train Loss: 0.001599
Validation Loss: 0.00125241
Epoch [129/200], Train Loss: 0.001573
Validation Loss: 0.00125248
Epoch [130/200], Train Loss: 0.001543
Validation Loss: 0.00124294
Epoch [131/200], Train Loss: 0.001527
Validation Loss: 0.00123598
Epoch [132/200], Train Loss: 0.001515
Validation Loss: 0.00123133
Epoch [133/200], Train Loss: 0.001535
Validation Loss: 0.00124188
Epoch [134/200], Train Loss: 0.001495
Validation Loss: 0.00120113
Epoch [135/200], Train Loss: 0.001498
Validation Loss: 0.00119706
Epoch [136/200], Train Loss: 0.001497
Validation Loss: 0.00119529
Epoch [137/200], Train Loss: 0.001473
Validation Loss: 0.00119286
Epoch [138/200], Train Loss: 0.001446
Validation Loss: 0.00117427
Epoch [139/200], Train Loss: 0.001534
Validation Loss: 0.00130902
Epoch [140/200], Train Loss: 0.001559
Validation Loss: 0.00119649
Epoch [141/200], Train Loss: 0.001439
Validation Loss: 0.00117423
Epoch [142/200], Train Loss: 0.001410
Validation Loss: 0.00116098
Epoch [143/200], Train Loss: 0.001409
Validation Loss: 0.00114110
Epoch [144/200], Train Loss: 0.001399
Validation Loss: 0.00113460
Epoch [145/200], Train Loss: 0.001382
Validation Loss: 0.00114707
Epoch [146/200], Train Loss: 0.001370
Validation Loss: 0.00113604
Epoch [147/200], Train Loss: 0.001422
Validation Loss: 0.00112018
Epoch [148/200], Train Loss: 0.001354
Validation Loss: 0.00111602
Epoch [149/200], Train Loss: 0.001343
Validation Loss: 0.00111035
Epoch [150/200], Train Loss: 0.001369
Validation Loss: 0.00111107
Epoch [151/200], Train Loss: 0.001339
Validation Loss: 0.00109811
Epoch [152/200], Train Loss: 0.001313
Validation Loss: 0.00109067
Epoch [153/200], Train Loss: 0.001304
Validation Loss: 0.00108703
Epoch [154/200], Train Loss: 0.001295
Validation Loss: 0.00108068
Epoch [155/200], Train Loss: 0.001313
Validation Loss: 0.00108036
Epoch [156/200], Train Loss: 0.001312
Validation Loss: 0.00107462
Epoch [157/200], Train Loss: 0.001289
Validation Loss: 0.00105570
Epoch [158/200], Train Loss: 0.001286
Validation Loss: 0.00106342
Epoch [159/200], Train Loss: 0.001273
Validation Loss: 0.00105743
Epoch [160/200], Train Loss: 0.001275
Validation Loss: 0.00105103
Epoch [161/200], Train Loss: 0.001265
Validation Loss: 0.00105095
Epoch [162/200], Train Loss: 0.001258
Validation Loss: 0.00103838
Epoch [163/200], Train Loss: 0.001248
Validation Loss: 0.00102066
Epoch [164/200], Train Loss: 0.001234
Validation Loss: 0.00103343
Epoch [165/200], Train Loss: 0.001224
Validation Loss: 0.00103218
Epoch [166/200], Train Loss: 0.001221
Validation Loss: 0.00101109
Epoch [167/200], Train Loss: 0.001216
Validation Loss: 0.00101776
Epoch [168/200], Train Loss: 0.001249
Validation Loss: 0.00107973
Epoch [169/200], Train Loss: 0.001239
Validation Loss: 0.00100962
Epoch [170/200], Train Loss: 0.001191
Validation Loss: 0.00100397
Epoch [171/200], Train Loss: 0.001202
Validation Loss: 0.00100516
Epoch [172/200], Train Loss: 0.001189
Validation Loss: 0.00099771
Epoch [173/200], Train Loss: 0.001207
Validation Loss: 0.00104549
Epoch [174/200], Train Loss: 0.001237
Validation Loss: 0.00101924
Epoch [175/200], Train Loss: 0.001184
Validation Loss: 0.00097803
Epoch [176/200], Train Loss: 0.001165
Validation Loss: 0.00098079
Epoch [177/200], Train Loss: 0.001162
Validation Loss: 0.00097632
Epoch [178/200], Train Loss: 0.001154
Validation Loss: 0.00096403
Epoch [179/200], Train Loss: 0.001152
Validation Loss: 0.00096718
Epoch [180/200], Train Loss: 0.001141
Validation Loss: 0.00096471
Epoch [181/200], Train Loss: 0.001139
Validation Loss: 0.00095618
Epoch [182/200], Train Loss: 0.001123
Validation Loss: 0.00095483
Epoch [183/200], Train Loss: 0.001122
Validation Loss: 0.00095462
Epoch [184/200], Train Loss: 0.001424
Validation Loss: 0.00133440
Epoch [185/200], Train Loss: 0.001350
Validation Loss: 0.00108309
Epoch [186/200], Train Loss: 0.001226
Validation Loss: 0.00102269
Epoch [187/200], Train Loss: 0.001176
Validation Loss: 0.00101270
Epoch [188/200], Train Loss: 0.001148
Validation Loss: 0.00098005
Epoch [189/200], Train Loss: 0.001152
Validation Loss: 0.00096843
Epoch [190/200], Train Loss: 0.001117
Validation Loss: 0.00097348
Epoch [191/200], Train Loss: 0.001120
Validation Loss: 0.00095991
Epoch [192/200], Train Loss: 0.001136
Validation Loss: 0.00096322
Epoch [193/200], Train Loss: 0.001104
Validation Loss: 0.00095110
Epoch [194/200], Train Loss: 0.001092
Validation Loss: 0.00094720
Epoch [195/200], Train Loss: 0.001109
Validation Loss: 0.00093618
Epoch [196/200], Train Loss: 0.001098
Validation Loss: 0.00093797
Epoch [197/200], Train Loss: 0.001087
Validation Loss: 0.00094720
Epoch [198/200], Train Loss: 0.001089
Validation Loss: 0.00092877
Epoch [199/200], Train Loss: 0.001078
Validation Loss: 0.00092198
Epoch [200/200], Train Loss: 0.001104
Validation Loss: 0.00094403

Evaluating model for: Lamp
Run 114/144 completed in 1161.87 seconds with: {'MAE': np.float32(1.1547999), 'MSE': np.float32(34.52102), 'RMSE': np.float32(5.875459), 'SAE': np.float32(0.11721612), 'NDE': np.float32(0.45465827)}

Run 115/144: hidden=256, seq_len=360, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.005064
Validation Loss: 0.00384473
Epoch [2/200], Train Loss: 0.004967
Validation Loss: 0.00383264
Epoch [3/200], Train Loss: 0.004997
Validation Loss: 0.00382827
Epoch [4/200], Train Loss: 0.004972
Validation Loss: 0.00379487
Epoch [5/200], Train Loss: 0.004876
Validation Loss: 0.00378644
Epoch [6/200], Train Loss: 0.004892
Validation Loss: 0.00377512
Epoch [7/200], Train Loss: 0.004874
Validation Loss: 0.00377676
Epoch [8/200], Train Loss: 0.004871
Validation Loss: 0.00377536
Epoch [9/200], Train Loss: 0.004912
Validation Loss: 0.00377697
Epoch [10/200], Train Loss: 0.004898
Validation Loss: 0.00378846
Epoch [11/200], Train Loss: 0.004872
Validation Loss: 0.00376809
Epoch [12/200], Train Loss: 0.004849
Validation Loss: 0.00376611
Epoch [13/200], Train Loss: 0.004908
Validation Loss: 0.00377178
Epoch [14/200], Train Loss: 0.004863
Validation Loss: 0.00376653
Epoch [15/200], Train Loss: 0.004933
Validation Loss: 0.00376359
Epoch [16/200], Train Loss: 0.004865
Validation Loss: 0.00375963
Epoch [17/200], Train Loss: 0.004837
Validation Loss: 0.00375848
Epoch [18/200], Train Loss: 0.004850
Validation Loss: 0.00376808
Epoch [19/200], Train Loss: 0.004881
Validation Loss: 0.00375945
Epoch [20/200], Train Loss: 0.004858
Validation Loss: 0.00376976
Epoch [21/200], Train Loss: 0.004851
Validation Loss: 0.00375793
Epoch [22/200], Train Loss: 0.004894
Validation Loss: 0.00376848
Epoch [23/200], Train Loss: 0.004872
Validation Loss: 0.00375423
Epoch [24/200], Train Loss: 0.004863
Validation Loss: 0.00375644
Epoch [25/200], Train Loss: 0.004852
Validation Loss: 0.00375252
Epoch [26/200], Train Loss: 0.004855
Validation Loss: 0.00375395
Epoch [27/200], Train Loss: 0.004864
Validation Loss: 0.00376114
Epoch [28/200], Train Loss: 0.004830
Validation Loss: 0.00375213
Epoch [29/200], Train Loss: 0.004865
Validation Loss: 0.00375651
Epoch [30/200], Train Loss: 0.004830
Validation Loss: 0.00374780
Epoch [31/200], Train Loss: 0.004868
Validation Loss: 0.00375160
Epoch [32/200], Train Loss: 0.004855
Validation Loss: 0.00374931
Epoch [33/200], Train Loss: 0.004856
Validation Loss: 0.00374760
Epoch [34/200], Train Loss: 0.004860
Validation Loss: 0.00374935
Epoch [35/200], Train Loss: 0.004854
Validation Loss: 0.00376424
Epoch [36/200], Train Loss: 0.004869
Validation Loss: 0.00375118
Epoch [37/200], Train Loss: 0.004954
Validation Loss: 0.00375192
Epoch [38/200], Train Loss: 0.004886
Validation Loss: 0.00375874
Epoch [39/200], Train Loss: 0.004884
Validation Loss: 0.00374536
Epoch [40/200], Train Loss: 0.004842
Validation Loss: 0.00374957
Epoch [41/200], Train Loss: 0.004874
Validation Loss: 0.00374710
Epoch [42/200], Train Loss: 0.004875
Validation Loss: 0.00374809
Epoch [43/200], Train Loss: 0.004903
Validation Loss: 0.00374693
Epoch [44/200], Train Loss: 0.004878
Validation Loss: 0.00374956
Epoch [45/200], Train Loss: 0.004853
Validation Loss: 0.00375248
Epoch [46/200], Train Loss: 0.004872
Validation Loss: 0.00374572
Epoch [47/200], Train Loss: 0.004867
Validation Loss: 0.00374931
Epoch [48/200], Train Loss: 0.004863
Validation Loss: 0.00374680
Epoch [49/200], Train Loss: 0.004871
Validation Loss: 0.00374950
Early stopping triggered

Evaluating model for: Lamp
Run 115/144 completed in 321.67 seconds with: {'MAE': np.float32(3.0166502), 'MSE': np.float32(161.03366), 'RMSE': np.float32(12.689904), 'SAE': np.float32(0.22343764), 'NDE': np.float32(0.9819793)}

Run 116/144: hidden=256, seq_len=360, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 3774 windows

Epoch [1/200], Train Loss: 0.004991
Validation Loss: 0.00383554
Epoch [2/200], Train Loss: 0.004944
Validation Loss: 0.00384118
Epoch [3/200], Train Loss: 0.004968
Validation Loss: 0.00383480
Epoch [4/200], Train Loss: 0.004960
Validation Loss: 0.00379212
Epoch [5/200], Train Loss: 0.004899
Validation Loss: 0.00377753
Epoch [6/200], Train Loss: 0.004908
Validation Loss: 0.00377783
Epoch [7/200], Train Loss: 0.004900
Validation Loss: 0.00377296
Epoch [8/200], Train Loss: 0.004893
Validation Loss: 0.00377782
Epoch [9/200], Train Loss: 0.004907
Validation Loss: 0.00379393
Epoch [10/200], Train Loss: 0.004884
Validation Loss: 0.00377784
Epoch [11/200], Train Loss: 0.004877
Validation Loss: 0.00376774
Epoch [12/200], Train Loss: 0.004871
Validation Loss: 0.00376382
Epoch [13/200], Train Loss: 0.004880
Validation Loss: 0.00376606
Epoch [14/200], Train Loss: 0.004851
Validation Loss: 0.00378448
Epoch [15/200], Train Loss: 0.004868
Validation Loss: 0.00376739
Epoch [16/200], Train Loss: 0.004866
Validation Loss: 0.00376030
Epoch [17/200], Train Loss: 0.004869
Validation Loss: 0.00375999
Epoch [18/200], Train Loss: 0.004881
Validation Loss: 0.00376501
Epoch [19/200], Train Loss: 0.004890
Validation Loss: 0.00377908
Epoch [20/200], Train Loss: 0.004885
Validation Loss: 0.00376046
Epoch [21/200], Train Loss: 0.004874
Validation Loss: 0.00375830
Epoch [22/200], Train Loss: 0.004815
Validation Loss: 0.00375609
Epoch [23/200], Train Loss: 0.004901
Validation Loss: 0.00375523
Epoch [24/200], Train Loss: 0.004877
Validation Loss: 0.00376270
Epoch [25/200], Train Loss: 0.004880
Validation Loss: 0.00375621
Epoch [26/200], Train Loss: 0.004903
Validation Loss: 0.00376178
Epoch [27/200], Train Loss: 0.004881
Validation Loss: 0.00375519
Epoch [28/200], Train Loss: 0.004876
Validation Loss: 0.00375281
Epoch [29/200], Train Loss: 0.004862
Validation Loss: 0.00375582
Epoch [30/200], Train Loss: 0.004867
Validation Loss: 0.00375143
Epoch [31/200], Train Loss: 0.004839
Validation Loss: 0.00375436
Epoch [32/200], Train Loss: 0.004961
Validation Loss: 0.00375397
Epoch [33/200], Train Loss: 0.004891
Validation Loss: 0.00375649
Epoch [34/200], Train Loss: 0.004865
Validation Loss: 0.00374841
Epoch [35/200], Train Loss: 0.004849
Validation Loss: 0.00375213
Epoch [36/200], Train Loss: 0.004865
Validation Loss: 0.00375027
Epoch [37/200], Train Loss: 0.004897
Validation Loss: 0.00374826
Epoch [38/200], Train Loss: 0.004853
Validation Loss: 0.00374942
Epoch [39/200], Train Loss: 0.004882
Validation Loss: 0.00375535
Epoch [40/200], Train Loss: 0.004812
Validation Loss: 0.00374899
Epoch [41/200], Train Loss: 0.004838
Validation Loss: 0.00375270
Epoch [42/200], Train Loss: 0.004873
Validation Loss: 0.00374759
Epoch [43/200], Train Loss: 0.004851
Validation Loss: 0.00375558
Epoch [44/200], Train Loss: 0.004855
Validation Loss: 0.00374942
Epoch [45/200], Train Loss: 0.004888
Validation Loss: 0.00375037
Epoch [46/200], Train Loss: 0.004844
Validation Loss: 0.00374687
Epoch [47/200], Train Loss: 0.004894
Validation Loss: 0.00376204
Epoch [48/200], Train Loss: 0.004851
Validation Loss: 0.00375663
Epoch [49/200], Train Loss: 0.004878
Validation Loss: 0.00374763
Epoch [50/200], Train Loss: 0.004834
Validation Loss: 0.00375509
Epoch [51/200], Train Loss: 0.004879
Validation Loss: 0.00374726
Epoch [52/200], Train Loss: 0.004886
Validation Loss: 0.00375326
Epoch [53/200], Train Loss: 0.004844
Validation Loss: 0.00375344
Epoch [54/200], Train Loss: 0.004861
Validation Loss: 0.00375219
Epoch [55/200], Train Loss: 0.004824
Validation Loss: 0.00374702
Epoch [56/200], Train Loss: 0.004844
Validation Loss: 0.00374626
Epoch [57/200], Train Loss: 0.004838
Validation Loss: 0.00374839
Epoch [58/200], Train Loss: 0.004852
Validation Loss: 0.00376210
Epoch [59/200], Train Loss: 0.004838
Validation Loss: 0.00374811
Epoch [60/200], Train Loss: 0.004911
Validation Loss: 0.00375064
Epoch [61/200], Train Loss: 0.004831
Validation Loss: 0.00374695
Epoch [62/200], Train Loss: 0.004889
Validation Loss: 0.00375013
Epoch [63/200], Train Loss: 0.004857
Validation Loss: 0.00374751
Epoch [64/200], Train Loss: 0.004848
Validation Loss: 0.00375232
Epoch [65/200], Train Loss: 0.004869
Validation Loss: 0.00375202
Epoch [66/200], Train Loss: 0.004838
Validation Loss: 0.00375103
Early stopping triggered

Evaluating model for: Lamp
Run 116/144 completed in 546.12 seconds with: {'MAE': np.float32(2.9842024), 'MSE': np.float32(161.02661), 'RMSE': np.float32(12.689626), 'SAE': np.float32(0.20718934), 'NDE': np.float32(0.98195475)}

Run 117/144: hidden=256, seq_len=360, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.005070
Validation Loss: 0.00537149
Epoch [2/200], Train Loss: 0.004760
Validation Loss: 0.00535748
Epoch [3/200], Train Loss: 0.004702
Validation Loss: 0.00536772
Epoch [4/200], Train Loss: 0.004705
Validation Loss: 0.00533373
Epoch [5/200], Train Loss: 0.004705
Validation Loss: 0.00532545
Epoch [6/200], Train Loss: 0.004691
Validation Loss: 0.00531394
Epoch [7/200], Train Loss: 0.004673
Validation Loss: 0.00530248
Epoch [8/200], Train Loss: 0.004648
Validation Loss: 0.00529248
Epoch [9/200], Train Loss: 0.004641
Validation Loss: 0.00528345
Epoch [10/200], Train Loss: 0.004633
Validation Loss: 0.00528003
Epoch [11/200], Train Loss: 0.004631
Validation Loss: 0.00527295
Epoch [12/200], Train Loss: 0.004657
Validation Loss: 0.00527137
Epoch [13/200], Train Loss: 0.004645
Validation Loss: 0.00527199
Epoch [14/200], Train Loss: 0.004631
Validation Loss: 0.00527331
Epoch [15/200], Train Loss: 0.004649
Validation Loss: 0.00527041
Epoch [16/200], Train Loss: 0.004622
Validation Loss: 0.00526728
Epoch [17/200], Train Loss: 0.004642
Validation Loss: 0.00526981
Epoch [18/200], Train Loss: 0.004635
Validation Loss: 0.00526693
Epoch [19/200], Train Loss: 0.004630
Validation Loss: 0.00527714
Epoch [20/200], Train Loss: 0.004618
Validation Loss: 0.00526543
Epoch [21/200], Train Loss: 0.004626
Validation Loss: 0.00526495
Epoch [22/200], Train Loss: 0.004648
Validation Loss: 0.00526972
Epoch [23/200], Train Loss: 0.004637
Validation Loss: 0.00526409
Epoch [24/200], Train Loss: 0.004677
Validation Loss: 0.00526188
Epoch [25/200], Train Loss: 0.004634
Validation Loss: 0.00526101
Epoch [26/200], Train Loss: 0.004622
Validation Loss: 0.00525996
Epoch [27/200], Train Loss: 0.004620
Validation Loss: 0.00526212
Epoch [28/200], Train Loss: 0.004634
Validation Loss: 0.00526128
Epoch [29/200], Train Loss: 0.004628
Validation Loss: 0.00525729
Epoch [30/200], Train Loss: 0.004623
Validation Loss: 0.00525748
Epoch [31/200], Train Loss: 0.004627
Validation Loss: 0.00525792
Epoch [32/200], Train Loss: 0.004620
Validation Loss: 0.00525504
Epoch [33/200], Train Loss: 0.004621
Validation Loss: 0.00525388
Epoch [34/200], Train Loss: 0.004618
Validation Loss: 0.00525409
Epoch [35/200], Train Loss: 0.004626
Validation Loss: 0.00525905
Epoch [36/200], Train Loss: 0.004610
Validation Loss: 0.00525275
Epoch [37/200], Train Loss: 0.004606
Validation Loss: 0.00525149
Epoch [38/200], Train Loss: 0.004620
Validation Loss: 0.00525894
Epoch [39/200], Train Loss: 0.004617
Validation Loss: 0.00525189
Epoch [40/200], Train Loss: 0.004638
Validation Loss: 0.00524952
Epoch [41/200], Train Loss: 0.004613
Validation Loss: 0.00524771
Epoch [42/200], Train Loss: 0.004621
Validation Loss: 0.00525098
Epoch [43/200], Train Loss: 0.004647
Validation Loss: 0.00524625
Epoch [44/200], Train Loss: 0.004592
Validation Loss: 0.00524578
Epoch [45/200], Train Loss: 0.004603
Validation Loss: 0.00524714
Epoch [46/200], Train Loss: 0.004639
Validation Loss: 0.00524514
Epoch [47/200], Train Loss: 0.004619
Validation Loss: 0.00525694
Epoch [48/200], Train Loss: 0.004631
Validation Loss: 0.00524511
Epoch [49/200], Train Loss: 0.004615
Validation Loss: 0.00524317
Epoch [50/200], Train Loss: 0.004622
Validation Loss: 0.00524043
Epoch [51/200], Train Loss: 0.004594
Validation Loss: 0.00524185
Epoch [52/200], Train Loss: 0.004617
Validation Loss: 0.00524220
Epoch [53/200], Train Loss: 0.004625
Validation Loss: 0.00523652
Epoch [54/200], Train Loss: 0.004596
Validation Loss: 0.00523534
Epoch [55/200], Train Loss: 0.004589
Validation Loss: 0.00523947
Epoch [56/200], Train Loss: 0.004620
Validation Loss: 0.00523224
Epoch [57/200], Train Loss: 0.004604
Validation Loss: 0.00523059
Epoch [58/200], Train Loss: 0.004581
Validation Loss: 0.00523334
Epoch [59/200], Train Loss: 0.004585
Validation Loss: 0.00523044
Epoch [60/200], Train Loss: 0.004605
Validation Loss: 0.00524152
Epoch [61/200], Train Loss: 0.004581
Validation Loss: 0.00523426
Epoch [62/200], Train Loss: 0.004602
Validation Loss: 0.00522529
Epoch [63/200], Train Loss: 0.004604
Validation Loss: 0.00521596
Epoch [64/200], Train Loss: 0.004582
Validation Loss: 0.00521476
Epoch [65/200], Train Loss: 0.004579
Validation Loss: 0.00521007
Epoch [66/200], Train Loss: 0.004569
Validation Loss: 0.00520794
Epoch [67/200], Train Loss: 0.004575
Validation Loss: 0.00520577
Epoch [68/200], Train Loss: 0.004591
Validation Loss: 0.00520009
Epoch [69/200], Train Loss: 0.004566
Validation Loss: 0.00519707
Epoch [70/200], Train Loss: 0.004555
Validation Loss: 0.00519959
Epoch [71/200], Train Loss: 0.004585
Validation Loss: 0.00519970
Epoch [72/200], Train Loss: 0.004562
Validation Loss: 0.00519055
Epoch [73/200], Train Loss: 0.004563
Validation Loss: 0.00517887
Epoch [74/200], Train Loss: 0.004558
Validation Loss: 0.00517566
Epoch [75/200], Train Loss: 0.004578
Validation Loss: 0.00518368
Epoch [76/200], Train Loss: 0.004562
Validation Loss: 0.00519129
Epoch [77/200], Train Loss: 0.004581
Validation Loss: 0.00516616
Epoch [78/200], Train Loss: 0.004579
Validation Loss: 0.00515913
Epoch [79/200], Train Loss: 0.004548
Validation Loss: 0.00515328
Epoch [80/200], Train Loss: 0.004545
Validation Loss: 0.00514766
Epoch [81/200], Train Loss: 0.004523
Validation Loss: 0.00514990
Epoch [82/200], Train Loss: 0.004543
Validation Loss: 0.00514043
Epoch [83/200], Train Loss: 0.004526
Validation Loss: 0.00516281
Epoch [84/200], Train Loss: 0.004551
Validation Loss: 0.00512934
Epoch [85/200], Train Loss: 0.004554
Validation Loss: 0.00514773
Epoch [86/200], Train Loss: 0.004542
Validation Loss: 0.00512777
Epoch [87/200], Train Loss: 0.004535
Validation Loss: 0.00511969
Epoch [88/200], Train Loss: 0.004499
Validation Loss: 0.00514094
Epoch [89/200], Train Loss: 0.004518
Validation Loss: 0.00515338
Epoch [90/200], Train Loss: 0.004527
Validation Loss: 0.00510390
Epoch [91/200], Train Loss: 0.004524
Validation Loss: 0.00510257
Epoch [92/200], Train Loss: 0.004524
Validation Loss: 0.00509899
Epoch [93/200], Train Loss: 0.004495
Validation Loss: 0.00508538
Epoch [94/200], Train Loss: 0.004486
Validation Loss: 0.00510285
Epoch [95/200], Train Loss: 0.004492
Validation Loss: 0.00507496
Epoch [96/200], Train Loss: 0.004494
Validation Loss: 0.00506838
Epoch [97/200], Train Loss: 0.004485
Validation Loss: 0.00506746
Epoch [98/200], Train Loss: 0.004469
Validation Loss: 0.00505356
Epoch [99/200], Train Loss: 0.004471
Validation Loss: 0.00504706
Epoch [100/200], Train Loss: 0.004497
Validation Loss: 0.00508311
Epoch [101/200], Train Loss: 0.004482
Validation Loss: 0.00505103
Epoch [102/200], Train Loss: 0.004464
Validation Loss: 0.00502603
Epoch [103/200], Train Loss: 0.004448
Validation Loss: 0.00503088
Epoch [104/200], Train Loss: 0.004480
Validation Loss: 0.00502851
Epoch [105/200], Train Loss: 0.004440
Validation Loss: 0.00499873
Epoch [106/200], Train Loss: 0.004415
Validation Loss: 0.00503723
Epoch [107/200], Train Loss: 0.004438
Validation Loss: 0.00498512
Epoch [108/200], Train Loss: 0.004432
Validation Loss: 0.00497422
Epoch [109/200], Train Loss: 0.004402
Validation Loss: 0.00495494
Epoch [110/200], Train Loss: 0.004413
Validation Loss: 0.00494972
Epoch [111/200], Train Loss: 0.004389
Validation Loss: 0.00493578
Epoch [112/200], Train Loss: 0.004399
Validation Loss: 0.00491480
Epoch [113/200], Train Loss: 0.004381
Validation Loss: 0.00495588
Epoch [114/200], Train Loss: 0.004386
Validation Loss: 0.00491201
Epoch [115/200], Train Loss: 0.004367
Validation Loss: 0.00496599
Epoch [116/200], Train Loss: 0.004370
Validation Loss: 0.00497126
Epoch [117/200], Train Loss: 0.004372
Validation Loss: 0.00495478
Epoch [118/200], Train Loss: 0.004342
Validation Loss: 0.00489695
Epoch [119/200], Train Loss: 0.004330
Validation Loss: 0.00484814
Epoch [120/200], Train Loss: 0.004352
Validation Loss: 0.00483666
Epoch [121/200], Train Loss: 0.004337
Validation Loss: 0.00483787
Epoch [122/200], Train Loss: 0.004309
Validation Loss: 0.00491031
Epoch [123/200], Train Loss: 0.004299
Validation Loss: 0.00487051
Epoch [124/200], Train Loss: 0.004321
Validation Loss: 0.00481790
Epoch [125/200], Train Loss: 0.004310
Validation Loss: 0.00481224
Epoch [126/200], Train Loss: 0.004298
Validation Loss: 0.00478499
Epoch [127/200], Train Loss: 0.004295
Validation Loss: 0.00479885
Epoch [128/200], Train Loss: 0.004283
Validation Loss: 0.00481505
Epoch [129/200], Train Loss: 0.004278
Validation Loss: 0.00487888
Epoch [130/200], Train Loss: 0.004281
Validation Loss: 0.00475336
Epoch [131/200], Train Loss: 0.004262
Validation Loss: 0.00476592
Epoch [132/200], Train Loss: 0.004288
Validation Loss: 0.00475356
Epoch [133/200], Train Loss: 0.004265
Validation Loss: 0.00473962
Epoch [134/200], Train Loss: 0.004237
Validation Loss: 0.00472909
Epoch [135/200], Train Loss: 0.004245
Validation Loss: 0.00470290
Epoch [136/200], Train Loss: 0.004232
Validation Loss: 0.00469519
Epoch [137/200], Train Loss: 0.004216
Validation Loss: 0.00472269
Epoch [138/200], Train Loss: 0.004181
Validation Loss: 0.00467969
Epoch [139/200], Train Loss: 0.004189
Validation Loss: 0.00469980
Epoch [140/200], Train Loss: 0.004179
Validation Loss: 0.00464668
Epoch [141/200], Train Loss: 0.004171
Validation Loss: 0.00462774
Epoch [142/200], Train Loss: 0.004162
Validation Loss: 0.00464769
Epoch [143/200], Train Loss: 0.004198
Validation Loss: 0.00464106
Epoch [144/200], Train Loss: 0.004201
Validation Loss: 0.00465095
Epoch [145/200], Train Loss: 0.004165
Validation Loss: 0.00460211
Epoch [146/200], Train Loss: 0.004177
Validation Loss: 0.00458028
Epoch [147/200], Train Loss: 0.004146
Validation Loss: 0.00457010
Epoch [148/200], Train Loss: 0.004137
Validation Loss: 0.00455594
Epoch [149/200], Train Loss: 0.004114
Validation Loss: 0.00454325
Epoch [150/200], Train Loss: 0.004121
Validation Loss: 0.00453116
Epoch [151/200], Train Loss: 0.004090
Validation Loss: 0.00451668
Epoch [152/200], Train Loss: 0.004091
Validation Loss: 0.00450242
Epoch [153/200], Train Loss: 0.004060
Validation Loss: 0.00454430
Epoch [154/200], Train Loss: 0.004085
Validation Loss: 0.00451265
Epoch [155/200], Train Loss: 0.004076
Validation Loss: 0.00446656
Epoch [156/200], Train Loss: 0.004051
Validation Loss: 0.00445618
Epoch [157/200], Train Loss: 0.004079
Validation Loss: 0.00444960
Epoch [158/200], Train Loss: 0.004050
Validation Loss: 0.00443670
Epoch [159/200], Train Loss: 0.004056
Validation Loss: 0.00442379
Epoch [160/200], Train Loss: 0.004026
Validation Loss: 0.00442065
Epoch [161/200], Train Loss: 0.004010
Validation Loss: 0.00439141
Epoch [162/200], Train Loss: 0.003999
Validation Loss: 0.00437167
Epoch [163/200], Train Loss: 0.004009
Validation Loss: 0.00436884
Epoch [164/200], Train Loss: 0.003982
Validation Loss: 0.00436175
Epoch [165/200], Train Loss: 0.004006
Validation Loss: 0.00433921
Epoch [166/200], Train Loss: 0.003978
Validation Loss: 0.00432545
Epoch [167/200], Train Loss: 0.003976
Validation Loss: 0.00431065
Epoch [168/200], Train Loss: 0.003978
Validation Loss: 0.00430514
Epoch [169/200], Train Loss: 0.003962
Validation Loss: 0.00429582
Epoch [170/200], Train Loss: 0.003960
Validation Loss: 0.00427018
Epoch [171/200], Train Loss: 0.003936
Validation Loss: 0.00425998
Epoch [172/200], Train Loss: 0.003955
Validation Loss: 0.00426438
Epoch [173/200], Train Loss: 0.003919
Validation Loss: 0.00423355
Epoch [174/200], Train Loss: 0.003909
Validation Loss: 0.00421580
Epoch [175/200], Train Loss: 0.003889
Validation Loss: 0.00422389
Epoch [176/200], Train Loss: 0.003885
Validation Loss: 0.00418827
Epoch [177/200], Train Loss: 0.003873
Validation Loss: 0.00419885
Epoch [178/200], Train Loss: 0.003885
Validation Loss: 0.00418740
Epoch [179/200], Train Loss: 0.003865
Validation Loss: 0.00417315
Epoch [180/200], Train Loss: 0.003860
Validation Loss: 0.00415565
Epoch [181/200], Train Loss: 0.003840
Validation Loss: 0.00410802
Epoch [182/200], Train Loss: 0.003844
Validation Loss: 0.00408966
Epoch [183/200], Train Loss: 0.003834
Validation Loss: 0.00409633
Epoch [184/200], Train Loss: 0.003827
Validation Loss: 0.00407357
Epoch [185/200], Train Loss: 0.003807
Validation Loss: 0.00407530
Epoch [186/200], Train Loss: 0.003805
Validation Loss: 0.00405998
Epoch [187/200], Train Loss: 0.003771
Validation Loss: 0.00403140
Epoch [188/200], Train Loss: 0.003792
Validation Loss: 0.00402237
Epoch [189/200], Train Loss: 0.003752
Validation Loss: 0.00404151
Epoch [190/200], Train Loss: 0.003800
Validation Loss: 0.00404259
Epoch [191/200], Train Loss: 0.003778
Validation Loss: 0.00397803
Epoch [192/200], Train Loss: 0.003734
Validation Loss: 0.00394265
Epoch [193/200], Train Loss: 0.003723
Validation Loss: 0.00394449
Epoch [194/200], Train Loss: 0.003741
Validation Loss: 0.00391491
Epoch [195/200], Train Loss: 0.003695
Validation Loss: 0.00387814
Epoch [196/200], Train Loss: 0.003676
Validation Loss: 0.00388345
Epoch [197/200], Train Loss: 0.003671
Validation Loss: 0.00384592
Epoch [198/200], Train Loss: 0.003680
Validation Loss: 0.00382672
Epoch [199/200], Train Loss: 0.003687
Validation Loss: 0.00381356
Epoch [200/200], Train Loss: 0.003684
Validation Loss: 0.00384873

Evaluating model for: Lamp
Run 117/144 completed in 515.18 seconds with: {'MAE': np.float32(2.6728501), 'MSE': np.float32(145.34978), 'RMSE': np.float32(12.056109), 'SAE': np.float32(0.4301392), 'NDE': np.float32(0.84237236)}

Run 118/144: hidden=256, seq_len=360, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.004708
Validation Loss: 0.00536302
Epoch [2/200], Train Loss: 0.004734
Validation Loss: 0.00535157
Epoch [3/200], Train Loss: 0.004690
Validation Loss: 0.00534243
Epoch [4/200], Train Loss: 0.004671
Validation Loss: 0.00531646
Epoch [5/200], Train Loss: 0.004689
Validation Loss: 0.00528409
Epoch [6/200], Train Loss: 0.004666
Validation Loss: 0.00526824
Epoch [7/200], Train Loss: 0.004627
Validation Loss: 0.00526994
Epoch [8/200], Train Loss: 0.004636
Validation Loss: 0.00527482
Epoch [9/200], Train Loss: 0.004653
Validation Loss: 0.00527588
Epoch [10/200], Train Loss: 0.004612
Validation Loss: 0.00526634
Epoch [11/200], Train Loss: 0.004623
Validation Loss: 0.00526418
Epoch [12/200], Train Loss: 0.004614
Validation Loss: 0.00526405
Epoch [13/200], Train Loss: 0.004645
Validation Loss: 0.00526503
Epoch [14/200], Train Loss: 0.004627
Validation Loss: 0.00525986
Epoch [15/200], Train Loss: 0.004622
Validation Loss: 0.00526243
Epoch [16/200], Train Loss: 0.004656
Validation Loss: 0.00525950
Epoch [17/200], Train Loss: 0.004614
Validation Loss: 0.00525783
Epoch [18/200], Train Loss: 0.004622
Validation Loss: 0.00525752
Epoch [19/200], Train Loss: 0.004634
Validation Loss: 0.00525837
Epoch [20/200], Train Loss: 0.004625
Validation Loss: 0.00527026
Epoch [21/200], Train Loss: 0.004627
Validation Loss: 0.00526062
Epoch [22/200], Train Loss: 0.004608
Validation Loss: 0.00525534
Epoch [23/200], Train Loss: 0.004625
Validation Loss: 0.00525467
Epoch [24/200], Train Loss: 0.004622
Validation Loss: 0.00525272
Epoch [25/200], Train Loss: 0.004600
Validation Loss: 0.00525267
Epoch [26/200], Train Loss: 0.004630
Validation Loss: 0.00525630
Epoch [27/200], Train Loss: 0.004605
Validation Loss: 0.00525168
Epoch [28/200], Train Loss: 0.004617
Validation Loss: 0.00525094
Epoch [29/200], Train Loss: 0.004604
Validation Loss: 0.00527295
Epoch [30/200], Train Loss: 0.004615
Validation Loss: 0.00525404
Epoch [31/200], Train Loss: 0.004609
Validation Loss: 0.00525645
Epoch [32/200], Train Loss: 0.004625
Validation Loss: 0.00525963
Epoch [33/200], Train Loss: 0.004590
Validation Loss: 0.00525056
Epoch [34/200], Train Loss: 0.004616
Validation Loss: 0.00525195
Epoch [35/200], Train Loss: 0.004613
Validation Loss: 0.00525067
Epoch [36/200], Train Loss: 0.004616
Validation Loss: 0.00524865
Epoch [37/200], Train Loss: 0.004610
Validation Loss: 0.00525278
Epoch [38/200], Train Loss: 0.004627
Validation Loss: 0.00524909
Epoch [39/200], Train Loss: 0.004608
Validation Loss: 0.00527624
Epoch [40/200], Train Loss: 0.004622
Validation Loss: 0.00525275
Epoch [41/200], Train Loss: 0.004596
Validation Loss: 0.00524835
Epoch [42/200], Train Loss: 0.004619
Validation Loss: 0.00524893
Epoch [43/200], Train Loss: 0.004590
Validation Loss: 0.00525238
Epoch [44/200], Train Loss: 0.004607
Validation Loss: 0.00525144
Epoch [45/200], Train Loss: 0.004601
Validation Loss: 0.00524887
Epoch [46/200], Train Loss: 0.004606
Validation Loss: 0.00525085
Epoch [47/200], Train Loss: 0.004611
Validation Loss: 0.00525536
Epoch [48/200], Train Loss: 0.004604
Validation Loss: 0.00524921
Epoch [49/200], Train Loss: 0.004635
Validation Loss: 0.00524978
Epoch [50/200], Train Loss: 0.004608
Validation Loss: 0.00525285
Epoch [51/200], Train Loss: 0.004627
Validation Loss: 0.00525151
Early stopping triggered

Evaluating model for: Lamp
Run 118/144 completed in 150.61 seconds with: {'MAE': np.float32(2.9246802), 'MSE': np.float32(196.34616), 'RMSE': np.float32(14.012358), 'SAE': np.float32(0.17362064), 'NDE': np.float32(0.9790567)}

Run 119/144: hidden=256, seq_len=360, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.004801
Validation Loss: 0.00539303
Epoch [2/200], Train Loss: 0.004719
Validation Loss: 0.00538669
Epoch [3/200], Train Loss: 0.004711
Validation Loss: 0.00536862
Epoch [4/200], Train Loss: 0.004703
Validation Loss: 0.00536903
Epoch [5/200], Train Loss: 0.004701
Validation Loss: 0.00536109
Epoch [6/200], Train Loss: 0.004700
Validation Loss: 0.00535603
Epoch [7/200], Train Loss: 0.004700
Validation Loss: 0.00533191
Epoch [8/200], Train Loss: 0.004687
Validation Loss: 0.00529275
Epoch [9/200], Train Loss: 0.004629
Validation Loss: 0.00527063
Epoch [10/200], Train Loss: 0.004639
Validation Loss: 0.00529325
Epoch [11/200], Train Loss: 0.004632
Validation Loss: 0.00528931
Epoch [12/200], Train Loss: 0.004636
Validation Loss: 0.00526912
Epoch [13/200], Train Loss: 0.004631
Validation Loss: 0.00526439
Epoch [14/200], Train Loss: 0.004626
Validation Loss: 0.00526341
Epoch [15/200], Train Loss: 0.004655
Validation Loss: 0.00526316
Epoch [16/200], Train Loss: 0.004619
Validation Loss: 0.00526146
Epoch [17/200], Train Loss: 0.004609
Validation Loss: 0.00526580
Epoch [18/200], Train Loss: 0.004615
Validation Loss: 0.00526542
Epoch [19/200], Train Loss: 0.004626
Validation Loss: 0.00526116
Epoch [20/200], Train Loss: 0.004639
Validation Loss: 0.00526400
Epoch [21/200], Train Loss: 0.004646
Validation Loss: 0.00526120
Epoch [22/200], Train Loss: 0.004623
Validation Loss: 0.00526720
Epoch [23/200], Train Loss: 0.004629
Validation Loss: 0.00525784
Epoch [24/200], Train Loss: 0.004643
Validation Loss: 0.00526674
Epoch [25/200], Train Loss: 0.004620
Validation Loss: 0.00527058
Epoch [26/200], Train Loss: 0.004635
Validation Loss: 0.00525541
Epoch [27/200], Train Loss: 0.004631
Validation Loss: 0.00525421
Epoch [28/200], Train Loss: 0.004607
Validation Loss: 0.00525482
Epoch [29/200], Train Loss: 0.004624
Validation Loss: 0.00525406
Epoch [30/200], Train Loss: 0.004598
Validation Loss: 0.00525262
Epoch [31/200], Train Loss: 0.004604
Validation Loss: 0.00525223
Epoch [32/200], Train Loss: 0.004591
Validation Loss: 0.00527217
Epoch [33/200], Train Loss: 0.004599
Validation Loss: 0.00525272
Epoch [34/200], Train Loss: 0.004632
Validation Loss: 0.00525212
Epoch [35/200], Train Loss: 0.004641
Validation Loss: 0.00525829
Epoch [36/200], Train Loss: 0.004621
Validation Loss: 0.00526328
Epoch [37/200], Train Loss: 0.004653
Validation Loss: 0.00526404
Epoch [38/200], Train Loss: 0.004614
Validation Loss: 0.00525386
Epoch [39/200], Train Loss: 0.004605
Validation Loss: 0.00526054
Epoch [40/200], Train Loss: 0.004621
Validation Loss: 0.00526209
Epoch [41/200], Train Loss: 0.004619
Validation Loss: 0.00525827
Epoch [42/200], Train Loss: 0.004616
Validation Loss: 0.00525597
Epoch [43/200], Train Loss: 0.004602
Validation Loss: 0.00525444
Epoch [44/200], Train Loss: 0.004619
Validation Loss: 0.00525707
Early stopping triggered

Evaluating model for: Lamp
Run 119/144 completed in 148.47 seconds with: {'MAE': np.float32(3.4313853), 'MSE': np.float32(196.11168), 'RMSE': np.float32(14.003988), 'SAE': np.float32(0.101626016), 'NDE': np.float32(0.9784729)}

Run 120/144: hidden=256, seq_len=360, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1898 windows

Epoch [1/200], Train Loss: 0.004746
Validation Loss: 0.00537796
Epoch [2/200], Train Loss: 0.004738
Validation Loss: 0.00538016
Epoch [3/200], Train Loss: 0.004698
Validation Loss: 0.00537216
Epoch [4/200], Train Loss: 0.004717
Validation Loss: 0.00537529
Epoch [5/200], Train Loss: 0.004730
Validation Loss: 0.00537022
Epoch [6/200], Train Loss: 0.004723
Validation Loss: 0.00536671
Epoch [7/200], Train Loss: 0.004698
Validation Loss: 0.00535905
Epoch [8/200], Train Loss: 0.004703
Validation Loss: 0.00533724
Epoch [9/200], Train Loss: 0.004660
Validation Loss: 0.00528753
Epoch [10/200], Train Loss: 0.004632
Validation Loss: 0.00527338
Epoch [11/200], Train Loss: 0.004630
Validation Loss: 0.00528570
Epoch [12/200], Train Loss: 0.004628
Validation Loss: 0.00527306
Epoch [13/200], Train Loss: 0.004626
Validation Loss: 0.00526518
Epoch [14/200], Train Loss: 0.004652
Validation Loss: 0.00526569
Epoch [15/200], Train Loss: 0.004624
Validation Loss: 0.00527247
Epoch [16/200], Train Loss: 0.004635
Validation Loss: 0.00526691
Epoch [17/200], Train Loss: 0.004638
Validation Loss: 0.00525857
Epoch [18/200], Train Loss: 0.004626
Validation Loss: 0.00525954
Epoch [19/200], Train Loss: 0.004648
Validation Loss: 0.00525641
Epoch [20/200], Train Loss: 0.004623
Validation Loss: 0.00527658
Epoch [21/200], Train Loss: 0.004623
Validation Loss: 0.00525902
Epoch [22/200], Train Loss: 0.004629
Validation Loss: 0.00525462
Epoch [23/200], Train Loss: 0.004639
Validation Loss: 0.00525363
Epoch [24/200], Train Loss: 0.004626
Validation Loss: 0.00525177
Epoch [25/200], Train Loss: 0.004610
Validation Loss: 0.00525583
Epoch [26/200], Train Loss: 0.004627
Validation Loss: 0.00526363
Epoch [27/200], Train Loss: 0.004624
Validation Loss: 0.00525281
Epoch [28/200], Train Loss: 0.004611
Validation Loss: 0.00525200
Epoch [29/200], Train Loss: 0.004611
Validation Loss: 0.00525111
Epoch [30/200], Train Loss: 0.004595
Validation Loss: 0.00525645
Epoch [31/200], Train Loss: 0.004643
Validation Loss: 0.00525355
Epoch [32/200], Train Loss: 0.004626
Validation Loss: 0.00525139
Epoch [33/200], Train Loss: 0.004619
Validation Loss: 0.00525795
Epoch [34/200], Train Loss: 0.004600
Validation Loss: 0.00526188
Epoch [35/200], Train Loss: 0.004612
Validation Loss: 0.00525224
Epoch [36/200], Train Loss: 0.004623
Validation Loss: 0.00525214
Epoch [37/200], Train Loss: 0.004612
Validation Loss: 0.00525677
Epoch [38/200], Train Loss: 0.004615
Validation Loss: 0.00525442
Epoch [39/200], Train Loss: 0.004602
Validation Loss: 0.00526479
Early stopping triggered

Evaluating model for: Lamp
Run 120/144 completed in 165.57 seconds with: {'MAE': np.float32(2.7391067), 'MSE': np.float32(196.79097), 'RMSE': np.float32(14.02822), 'SAE': np.float32(0.3692033), 'NDE': np.float32(0.9801661)}

Run 121/144: hidden=256, seq_len=720, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.005059
Validation Loss: 0.00497182
Epoch [2/200], Train Loss: 0.004845
Validation Loss: 0.00494747
Epoch [3/200], Train Loss: 0.004830
Validation Loss: 0.00492681
Epoch [4/200], Train Loss: 0.004825
Validation Loss: 0.00491073
Epoch [5/200], Train Loss: 0.004794
Validation Loss: 0.00490736
Epoch [6/200], Train Loss: 0.004803
Validation Loss: 0.00490342
Epoch [7/200], Train Loss: 0.004809
Validation Loss: 0.00490144
Epoch [8/200], Train Loss: 0.004795
Validation Loss: 0.00489965
Epoch [9/200], Train Loss: 0.004786
Validation Loss: 0.00489884
Epoch [10/200], Train Loss: 0.004778
Validation Loss: 0.00490628
Epoch [11/200], Train Loss: 0.004793
Validation Loss: 0.00489532
Epoch [12/200], Train Loss: 0.004792
Validation Loss: 0.00489225
Epoch [13/200], Train Loss: 0.004773
Validation Loss: 0.00489734
Epoch [14/200], Train Loss: 0.004791
Validation Loss: 0.00488902
Epoch [15/200], Train Loss: 0.004779
Validation Loss: 0.00488668
Epoch [16/200], Train Loss: 0.004773
Validation Loss: 0.00488522
Epoch [17/200], Train Loss: 0.004777
Validation Loss: 0.00488286
Epoch [18/200], Train Loss: 0.004789
Validation Loss: 0.00488086
Epoch [19/200], Train Loss: 0.004765
Validation Loss: 0.00488078
Epoch [20/200], Train Loss: 0.004753
Validation Loss: 0.00487452
Epoch [21/200], Train Loss: 0.004775
Validation Loss: 0.00486871
Epoch [22/200], Train Loss: 0.004750
Validation Loss: 0.00486095
Epoch [23/200], Train Loss: 0.004753
Validation Loss: 0.00485375
Epoch [24/200], Train Loss: 0.004736
Validation Loss: 0.00483935
Epoch [25/200], Train Loss: 0.004723
Validation Loss: 0.00482402
Epoch [26/200], Train Loss: 0.004712
Validation Loss: 0.00481338
Epoch [27/200], Train Loss: 0.004694
Validation Loss: 0.00479892
Epoch [28/200], Train Loss: 0.004711
Validation Loss: 0.00479518
Epoch [29/200], Train Loss: 0.004676
Validation Loss: 0.00477022
Epoch [30/200], Train Loss: 0.004683
Validation Loss: 0.00475354
Epoch [31/200], Train Loss: 0.004644
Validation Loss: 0.00474698
Epoch [32/200], Train Loss: 0.004629
Validation Loss: 0.00471012
Epoch [33/200], Train Loss: 0.004615
Validation Loss: 0.00469029
Epoch [34/200], Train Loss: 0.004574
Validation Loss: 0.00470066
Epoch [35/200], Train Loss: 0.004560
Validation Loss: 0.00458309
Epoch [36/200], Train Loss: 0.004537
Validation Loss: 0.00468050
Epoch [37/200], Train Loss: 0.004490
Validation Loss: 0.00454605
Epoch [38/200], Train Loss: 0.004444
Validation Loss: 0.00450891
Epoch [39/200], Train Loss: 0.004443
Validation Loss: 0.00443303
Epoch [40/200], Train Loss: 0.004356
Validation Loss: 0.00442854
Epoch [41/200], Train Loss: 0.004351
Validation Loss: 0.00432931
Epoch [42/200], Train Loss: 0.004235
Validation Loss: 0.00424139
Epoch [43/200], Train Loss: 0.004202
Validation Loss: 0.00417463
Epoch [44/200], Train Loss: 0.004125
Validation Loss: 0.00442816
Epoch [45/200], Train Loss: 0.004158
Validation Loss: 0.00408127
Epoch [46/200], Train Loss: 0.004063
Validation Loss: 0.00401491
Epoch [47/200], Train Loss: 0.003991
Validation Loss: 0.00392422
Epoch [48/200], Train Loss: 0.003886
Validation Loss: 0.00379437
Epoch [49/200], Train Loss: 0.003792
Validation Loss: 0.00370102
Epoch [50/200], Train Loss: 0.003782
Validation Loss: 0.00378966
Epoch [51/200], Train Loss: 0.003656
Validation Loss: 0.00357120
Epoch [52/200], Train Loss: 0.003576
Validation Loss: 0.00342072
Epoch [53/200], Train Loss: 0.003408
Validation Loss: 0.00322435
Epoch [54/200], Train Loss: 0.003307
Validation Loss: 0.00311418
Epoch [55/200], Train Loss: 0.003202
Validation Loss: 0.00300133
Epoch [56/200], Train Loss: 0.003105
Validation Loss: 0.00287083
Epoch [57/200], Train Loss: 0.003016
Validation Loss: 0.00279581
Epoch [58/200], Train Loss: 0.002949
Validation Loss: 0.00271941
Epoch [59/200], Train Loss: 0.002823
Validation Loss: 0.00264368
Epoch [60/200], Train Loss: 0.002736
Validation Loss: 0.00257122
Epoch [61/200], Train Loss: 0.002676
Validation Loss: 0.00250859
Epoch [62/200], Train Loss: 0.002610
Validation Loss: 0.00248057
Epoch [63/200], Train Loss: 0.002579
Validation Loss: 0.00240791
Epoch [64/200], Train Loss: 0.002528
Validation Loss: 0.00236524
Epoch [65/200], Train Loss: 0.002450
Validation Loss: 0.00231775
Epoch [66/200], Train Loss: 0.002401
Validation Loss: 0.00230730
Epoch [67/200], Train Loss: 0.002360
Validation Loss: 0.00222828
Epoch [68/200], Train Loss: 0.002320
Validation Loss: 0.00222740
Epoch [69/200], Train Loss: 0.002300
Validation Loss: 0.00220878
Epoch [70/200], Train Loss: 0.002248
Validation Loss: 0.00216293
Epoch [71/200], Train Loss: 0.002215
Validation Loss: 0.00212690
Epoch [72/200], Train Loss: 0.002179
Validation Loss: 0.00207396
Epoch [73/200], Train Loss: 0.002138
Validation Loss: 0.00205804
Epoch [74/200], Train Loss: 0.002097
Validation Loss: 0.00203073
Epoch [75/200], Train Loss: 0.002076
Validation Loss: 0.00199487
Epoch [76/200], Train Loss: 0.002041
Validation Loss: 0.00198160
Epoch [77/200], Train Loss: 0.002013
Validation Loss: 0.00197743
Epoch [78/200], Train Loss: 0.001999
Validation Loss: 0.00194509
Epoch [79/200], Train Loss: 0.001984
Validation Loss: 0.00191397
Epoch [80/200], Train Loss: 0.001949
Validation Loss: 0.00190713
Epoch [81/200], Train Loss: 0.001937
Validation Loss: 0.00189398
Epoch [82/200], Train Loss: 0.001905
Validation Loss: 0.00187404
Epoch [83/200], Train Loss: 0.001895
Validation Loss: 0.00187328
Epoch [84/200], Train Loss: 0.001871
Validation Loss: 0.00182283
Epoch [85/200], Train Loss: 0.001843
Validation Loss: 0.00182434
Epoch [86/200], Train Loss: 0.001824
Validation Loss: 0.00179541
Epoch [87/200], Train Loss: 0.001830
Validation Loss: 0.00180719
Epoch [88/200], Train Loss: 0.001798
Validation Loss: 0.00177002
Epoch [89/200], Train Loss: 0.001763
Validation Loss: 0.00174123
Epoch [90/200], Train Loss: 0.001756
Validation Loss: 0.00173888
Epoch [91/200], Train Loss: 0.001735
Validation Loss: 0.00170738
Epoch [92/200], Train Loss: 0.001734
Validation Loss: 0.00170095
Epoch [93/200], Train Loss: 0.001704
Validation Loss: 0.00167884
Epoch [94/200], Train Loss: 0.001689
Validation Loss: 0.00170487
Epoch [95/200], Train Loss: 0.001680
Validation Loss: 0.00165387
Epoch [96/200], Train Loss: 0.001663
Validation Loss: 0.00164869
Epoch [97/200], Train Loss: 0.001642
Validation Loss: 0.00162876
Epoch [98/200], Train Loss: 0.001638
Validation Loss: 0.00161924
Epoch [99/200], Train Loss: 0.001621
Validation Loss: 0.00159918
Epoch [100/200], Train Loss: 0.001607
Validation Loss: 0.00158037
Epoch [101/200], Train Loss: 0.001597
Validation Loss: 0.00165235
Epoch [102/200], Train Loss: 0.001583
Validation Loss: 0.00159390
Epoch [103/200], Train Loss: 0.001559
Validation Loss: 0.00158758
Epoch [104/200], Train Loss: 0.001547
Validation Loss: 0.00156709
Epoch [105/200], Train Loss: 0.001634
Validation Loss: 0.00177448
Epoch [106/200], Train Loss: 0.001623
Validation Loss: 0.00160550
Epoch [107/200], Train Loss: 0.001540
Validation Loss: 0.00153723
Epoch [108/200], Train Loss: 0.001517
Validation Loss: 0.00153674
Epoch [109/200], Train Loss: 0.001498
Validation Loss: 0.00151642
Epoch [110/200], Train Loss: 0.001488
Validation Loss: 0.00150902
Epoch [111/200], Train Loss: 0.001472
Validation Loss: 0.00150755
Epoch [112/200], Train Loss: 0.001469
Validation Loss: 0.00152328
Epoch [113/200], Train Loss: 0.001471
Validation Loss: 0.00151131
Epoch [114/200], Train Loss: 0.001482
Validation Loss: 0.00149414
Epoch [115/200], Train Loss: 0.001458
Validation Loss: 0.00148729
Epoch [116/200], Train Loss: 0.001440
Validation Loss: 0.00145938
Epoch [117/200], Train Loss: 0.001425
Validation Loss: 0.00145028
Epoch [118/200], Train Loss: 0.001425
Validation Loss: 0.00145729
Epoch [119/200], Train Loss: 0.001407
Validation Loss: 0.00143186
Epoch [120/200], Train Loss: 0.001409
Validation Loss: 0.00147075
Epoch [121/200], Train Loss: 0.001397
Validation Loss: 0.00142422
Epoch [122/200], Train Loss: 0.001380
Validation Loss: 0.00142273
Epoch [123/200], Train Loss: 0.001378
Validation Loss: 0.00149577
Epoch [124/200], Train Loss: 0.001395
Validation Loss: 0.00142490
Epoch [125/200], Train Loss: 0.001365
Validation Loss: 0.00138873
Epoch [126/200], Train Loss: 0.001363
Validation Loss: 0.00142386
Epoch [127/200], Train Loss: 0.001381
Validation Loss: 0.00142596
Epoch [128/200], Train Loss: 0.001356
Validation Loss: 0.00138229
Epoch [129/200], Train Loss: 0.001336
Validation Loss: 0.00139154
Epoch [130/200], Train Loss: 0.001332
Validation Loss: 0.00136930
Epoch [131/200], Train Loss: 0.001320
Validation Loss: 0.00135889
Epoch [132/200], Train Loss: 0.001308
Validation Loss: 0.00134400
Epoch [133/200], Train Loss: 0.001296
Validation Loss: 0.00133441
Epoch [134/200], Train Loss: 0.001299
Validation Loss: 0.00133579
Epoch [135/200], Train Loss: 0.001292
Validation Loss: 0.00132404
Epoch [136/200], Train Loss: 0.001280
Validation Loss: 0.00131928
Epoch [137/200], Train Loss: 0.001309
Validation Loss: 0.00131802
Epoch [138/200], Train Loss: 0.001274
Validation Loss: 0.00131608
Epoch [139/200], Train Loss: 0.001272
Validation Loss: 0.00131004
Epoch [140/200], Train Loss: 0.001270
Validation Loss: 0.00132172
Epoch [141/200], Train Loss: 0.001274
Validation Loss: 0.00130388
Epoch [142/200], Train Loss: 0.001253
Validation Loss: 0.00129684
Epoch [143/200], Train Loss: 0.001249
Validation Loss: 0.00128059
Epoch [144/200], Train Loss: 0.001234
Validation Loss: 0.00128272
Epoch [145/200], Train Loss: 0.001239
Validation Loss: 0.00127379
Epoch [146/200], Train Loss: 0.001223
Validation Loss: 0.00127111
Epoch [147/200], Train Loss: 0.001214
Validation Loss: 0.00127885
Epoch [148/200], Train Loss: 0.001227
Validation Loss: 0.00129262
Epoch [149/200], Train Loss: 0.001222
Validation Loss: 0.00129260
Epoch [150/200], Train Loss: 0.001247
Validation Loss: 0.00130539
Epoch [151/200], Train Loss: 0.001227
Validation Loss: 0.00125310
Epoch [152/200], Train Loss: 0.001196
Validation Loss: 0.00124824
Epoch [153/200], Train Loss: 0.001193
Validation Loss: 0.00124465
Epoch [154/200], Train Loss: 0.001184
Validation Loss: 0.00123890
Epoch [155/200], Train Loss: 0.001181
Validation Loss: 0.00122221
Epoch [156/200], Train Loss: 0.001176
Validation Loss: 0.00121948
Epoch [157/200], Train Loss: 0.001169
Validation Loss: 0.00121202
Epoch [158/200], Train Loss: 0.001179
Validation Loss: 0.00121897
Epoch [159/200], Train Loss: 0.001160
Validation Loss: 0.00122069
Epoch [160/200], Train Loss: 0.001171
Validation Loss: 0.00122278
Epoch [161/200], Train Loss: 0.001157
Validation Loss: 0.00120105
Epoch [162/200], Train Loss: 0.001147
Validation Loss: 0.00120202
Epoch [163/200], Train Loss: 0.001154
Validation Loss: 0.00122003
Epoch [164/200], Train Loss: 0.001148
Validation Loss: 0.00119628
Epoch [165/200], Train Loss: 0.001144
Validation Loss: 0.00121457
Epoch [166/200], Train Loss: 0.001159
Validation Loss: 0.00119141
Epoch [167/200], Train Loss: 0.001145
Validation Loss: 0.00118210
Epoch [168/200], Train Loss: 0.001122
Validation Loss: 0.00117934
Epoch [169/200], Train Loss: 0.001124
Validation Loss: 0.00120411
Epoch [170/200], Train Loss: 0.001127
Validation Loss: 0.00117453
Epoch [171/200], Train Loss: 0.001118
Validation Loss: 0.00121312
Epoch [172/200], Train Loss: 0.001129
Validation Loss: 0.00116306
Epoch [173/200], Train Loss: 0.001105
Validation Loss: 0.00115949
Epoch [174/200], Train Loss: 0.001111
Validation Loss: 0.00117053
Epoch [175/200], Train Loss: 0.001099
Validation Loss: 0.00115584
Epoch [176/200], Train Loss: 0.001118
Validation Loss: 0.00117254
Epoch [177/200], Train Loss: 0.001115
Validation Loss: 0.00118106
Epoch [178/200], Train Loss: 0.001118
Validation Loss: 0.00116043
Epoch [179/200], Train Loss: 0.001090
Validation Loss: 0.00114165
Epoch [180/200], Train Loss: 0.001092
Validation Loss: 0.00115084
Epoch [181/200], Train Loss: 0.001089
Validation Loss: 0.00114156
Epoch [182/200], Train Loss: 0.001080
Validation Loss: 0.00113485
Epoch [183/200], Train Loss: 0.001080
Validation Loss: 0.00113145
Epoch [184/200], Train Loss: 0.001071
Validation Loss: 0.00113160
Epoch [185/200], Train Loss: 0.001074
Validation Loss: 0.00112768
Epoch [186/200], Train Loss: 0.001069
Validation Loss: 0.00112519
Epoch [187/200], Train Loss: 0.001074
Validation Loss: 0.00112641
Epoch [188/200], Train Loss: 0.001062
Validation Loss: 0.00112329
Epoch [189/200], Train Loss: 0.001065
Validation Loss: 0.00112342
Epoch [190/200], Train Loss: 0.001065
Validation Loss: 0.00111626
Epoch [191/200], Train Loss: 0.001053
Validation Loss: 0.00110901
Epoch [192/200], Train Loss: 0.001044
Validation Loss: 0.00109963
Epoch [193/200], Train Loss: 0.001051
Validation Loss: 0.00113258
Epoch [194/200], Train Loss: 0.001045
Validation Loss: 0.00110508
Epoch [195/200], Train Loss: 0.001043
Validation Loss: 0.00109850
Epoch [196/200], Train Loss: 0.001035
Validation Loss: 0.00110870
Epoch [197/200], Train Loss: 0.001038
Validation Loss: 0.00111737
Epoch [198/200], Train Loss: 0.001049
Validation Loss: 0.00111358
Epoch [199/200], Train Loss: 0.001035
Validation Loss: 0.00108940
Epoch [200/200], Train Loss: 0.001024
Validation Loss: 0.00108488

Evaluating model for: Lamp
Run 121/144 completed in 1587.11 seconds with: {'MAE': np.float32(1.0221746), 'MSE': np.float32(32.90505), 'RMSE': np.float32(5.7362924), 'SAE': np.float32(0.11202546), 'NDE': np.float32(0.44652295)}

Run 122/144: hidden=256, seq_len=720, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.004883
Validation Loss: 0.00496577
Epoch [2/200], Train Loss: 0.004835
Validation Loss: 0.00492066
Epoch [3/200], Train Loss: 0.004795
Validation Loss: 0.00490617
Epoch [4/200], Train Loss: 0.004792
Validation Loss: 0.00490007
Epoch [5/200], Train Loss: 0.004783
Validation Loss: 0.00489959
Epoch [6/200], Train Loss: 0.004792
Validation Loss: 0.00489598
Epoch [7/200], Train Loss: 0.004778
Validation Loss: 0.00489641
Epoch [8/200], Train Loss: 0.004771
Validation Loss: 0.00489182
Epoch [9/200], Train Loss: 0.004775
Validation Loss: 0.00488995
Epoch [10/200], Train Loss: 0.004770
Validation Loss: 0.00489712
Epoch [11/200], Train Loss: 0.004777
Validation Loss: 0.00489038
Epoch [12/200], Train Loss: 0.004776
Validation Loss: 0.00488627
Epoch [13/200], Train Loss: 0.004762
Validation Loss: 0.00489024
Epoch [14/200], Train Loss: 0.004765
Validation Loss: 0.00488439
Epoch [15/200], Train Loss: 0.004770
Validation Loss: 0.00488377
Epoch [16/200], Train Loss: 0.004762
Validation Loss: 0.00489134
Epoch [17/200], Train Loss: 0.004758
Validation Loss: 0.00489026
Epoch [18/200], Train Loss: 0.004748
Validation Loss: 0.00488253
Epoch [19/200], Train Loss: 0.004776
Validation Loss: 0.00489205
Epoch [20/200], Train Loss: 0.004769
Validation Loss: 0.00487946
Epoch [21/200], Train Loss: 0.004757
Validation Loss: 0.00488089
Epoch [22/200], Train Loss: 0.004761
Validation Loss: 0.00487779
Epoch [23/200], Train Loss: 0.004765
Validation Loss: 0.00489288
Epoch [24/200], Train Loss: 0.004780
Validation Loss: 0.00487625
Epoch [25/200], Train Loss: 0.004771
Validation Loss: 0.00487473
Epoch [26/200], Train Loss: 0.004756
Validation Loss: 0.00487812
Epoch [27/200], Train Loss: 0.004759
Validation Loss: 0.00487985
Epoch [28/200], Train Loss: 0.004761
Validation Loss: 0.00487296
Epoch [29/200], Train Loss: 0.004742
Validation Loss: 0.00487351
Epoch [30/200], Train Loss: 0.004735
Validation Loss: 0.00486777
Epoch [31/200], Train Loss: 0.004754
Validation Loss: 0.00486466
Epoch [32/200], Train Loss: 0.004755
Validation Loss: 0.00486689
Epoch [33/200], Train Loss: 0.004754
Validation Loss: 0.00486769
Epoch [34/200], Train Loss: 0.004761
Validation Loss: 0.00485020
Epoch [35/200], Train Loss: 0.004733
Validation Loss: 0.00484573
Epoch [36/200], Train Loss: 0.004733
Validation Loss: 0.00482253
Epoch [37/200], Train Loss: 0.004724
Validation Loss: 0.00480350
Epoch [38/200], Train Loss: 0.004702
Validation Loss: 0.00482248
Epoch [39/200], Train Loss: 0.004692
Validation Loss: 0.00477596
Epoch [40/200], Train Loss: 0.004646
Validation Loss: 0.00472394
Epoch [41/200], Train Loss: 0.004608
Validation Loss: 0.00467196
Epoch [42/200], Train Loss: 0.004607
Validation Loss: 0.00463731
Epoch [43/200], Train Loss: 0.004532
Validation Loss: 0.00458327
Epoch [44/200], Train Loss: 0.004479
Validation Loss: 0.00458028
Epoch [45/200], Train Loss: 0.004456
Validation Loss: 0.00446859
Epoch [46/200], Train Loss: 0.004398
Validation Loss: 0.00439122
Epoch [47/200], Train Loss: 0.004295
Validation Loss: 0.00431740
Epoch [48/200], Train Loss: 0.004243
Validation Loss: 0.00421588
Epoch [49/200], Train Loss: 0.004129
Validation Loss: 0.00412891
Epoch [50/200], Train Loss: 0.004127
Validation Loss: 0.00404424
Epoch [51/200], Train Loss: 0.003988
Validation Loss: 0.00393620
Epoch [52/200], Train Loss: 0.003872
Validation Loss: 0.00383960
Epoch [53/200], Train Loss: 0.003732
Validation Loss: 0.00358820
Epoch [54/200], Train Loss: 0.003587
Validation Loss: 0.00340974
Epoch [55/200], Train Loss: 0.003476
Validation Loss: 0.00317435
Epoch [56/200], Train Loss: 0.003210
Validation Loss: 0.00292096
Epoch [57/200], Train Loss: 0.003042
Validation Loss: 0.00283182
Epoch [58/200], Train Loss: 0.003078
Validation Loss: 0.00289746
Epoch [59/200], Train Loss: 0.002835
Validation Loss: 0.00256078
Epoch [60/200], Train Loss: 0.002663
Validation Loss: 0.00243392
Epoch [61/200], Train Loss: 0.002553
Validation Loss: 0.00238612
Epoch [62/200], Train Loss: 0.002462
Validation Loss: 0.00230057
Epoch [63/200], Train Loss: 0.002378
Validation Loss: 0.00223004
Epoch [64/200], Train Loss: 0.002322
Validation Loss: 0.00224942
Epoch [65/200], Train Loss: 0.002267
Validation Loss: 0.00214192
Epoch [66/200], Train Loss: 0.002198
Validation Loss: 0.00208313
Epoch [67/200], Train Loss: 0.002146
Validation Loss: 0.00204705
Epoch [68/200], Train Loss: 0.002088
Validation Loss: 0.00201964
Epoch [69/200], Train Loss: 0.002039
Validation Loss: 0.00196314
Epoch [70/200], Train Loss: 0.001991
Validation Loss: 0.00194649
Epoch [71/200], Train Loss: 0.001967
Validation Loss: 0.00191157
Epoch [72/200], Train Loss: 0.001916
Validation Loss: 0.00188250
Epoch [73/200], Train Loss: 0.001876
Validation Loss: 0.00184309
Epoch [74/200], Train Loss: 0.001851
Validation Loss: 0.00182058
Epoch [75/200], Train Loss: 0.001808
Validation Loss: 0.00178960
Epoch [76/200], Train Loss: 0.001777
Validation Loss: 0.00177351
Epoch [77/200], Train Loss: 0.001762
Validation Loss: 0.00175954
Epoch [78/200], Train Loss: 0.001722
Validation Loss: 0.00173520
Epoch [79/200], Train Loss: 0.001709
Validation Loss: 0.00174218
Epoch [80/200], Train Loss: 0.001687
Validation Loss: 0.00171334
Epoch [81/200], Train Loss: 0.001665
Validation Loss: 0.00165933
Epoch [82/200], Train Loss: 0.001632
Validation Loss: 0.00166039
Epoch [83/200], Train Loss: 0.001605
Validation Loss: 0.00161362
Epoch [84/200], Train Loss: 0.001587
Validation Loss: 0.00159680
Epoch [85/200], Train Loss: 0.001559
Validation Loss: 0.00159644
Epoch [86/200], Train Loss: 0.001534
Validation Loss: 0.00157044
Epoch [87/200], Train Loss: 0.001518
Validation Loss: 0.00154538
Epoch [88/200], Train Loss: 0.001489
Validation Loss: 0.00153980
Epoch [89/200], Train Loss: 0.001479
Validation Loss: 0.00153610
Epoch [90/200], Train Loss: 0.001460
Validation Loss: 0.00149050
Epoch [91/200], Train Loss: 0.001439
Validation Loss: 0.00147207
Epoch [92/200], Train Loss: 0.001423
Validation Loss: 0.00144706
Epoch [93/200], Train Loss: 0.001397
Validation Loss: 0.00143057
Epoch [94/200], Train Loss: 0.001383
Validation Loss: 0.00143246
Epoch [95/200], Train Loss: 0.001361
Validation Loss: 0.00142356
Epoch [96/200], Train Loss: 0.001351
Validation Loss: 0.00140409
Epoch [97/200], Train Loss: 0.001333
Validation Loss: 0.00139251
Epoch [98/200], Train Loss: 0.001341
Validation Loss: 0.00138531
Epoch [99/200], Train Loss: 0.001307
Validation Loss: 0.00135373
Epoch [100/200], Train Loss: 0.001288
Validation Loss: 0.00135018
Epoch [101/200], Train Loss: 0.001270
Validation Loss: 0.00134181
Epoch [102/200], Train Loss: 0.001265
Validation Loss: 0.00131186
Epoch [103/200], Train Loss: 0.001239
Validation Loss: 0.00129591
Epoch [104/200], Train Loss: 0.001219
Validation Loss: 0.00128061
Epoch [105/200], Train Loss: 0.001221
Validation Loss: 0.00127681
Epoch [106/200], Train Loss: 0.001197
Validation Loss: 0.00125398
Epoch [107/200], Train Loss: 0.001182
Validation Loss: 0.00124797
Epoch [108/200], Train Loss: 0.001172
Validation Loss: 0.00122263
Epoch [109/200], Train Loss: 0.001153
Validation Loss: 0.00121410
Epoch [110/200], Train Loss: 0.001155
Validation Loss: 0.00120833
Epoch [111/200], Train Loss: 0.001160
Validation Loss: 0.00121475
Epoch [112/200], Train Loss: 0.001133
Validation Loss: 0.00117807
Epoch [113/200], Train Loss: 0.001118
Validation Loss: 0.00116805
Epoch [114/200], Train Loss: 0.001109
Validation Loss: 0.00118553
Epoch [115/200], Train Loss: 0.001132
Validation Loss: 0.00119657
Epoch [116/200], Train Loss: 0.001109
Validation Loss: 0.00114910
Epoch [117/200], Train Loss: 0.001090
Validation Loss: 0.00115468
Epoch [118/200], Train Loss: 0.001073
Validation Loss: 0.00113638
Epoch [119/200], Train Loss: 0.001159
Validation Loss: 0.00122970
Epoch [120/200], Train Loss: 0.001109
Validation Loss: 0.00115961
Epoch [121/200], Train Loss: 0.001061
Validation Loss: 0.00112726
Epoch [122/200], Train Loss: 0.001038
Validation Loss: 0.00110693
Epoch [123/200], Train Loss: 0.001061
Validation Loss: 0.00113443
Epoch [124/200], Train Loss: 0.001033
Validation Loss: 0.00109316
Epoch [125/200], Train Loss: 0.001028
Validation Loss: 0.00108267
Epoch [126/200], Train Loss: 0.001014
Validation Loss: 0.00106901
Epoch [127/200], Train Loss: 0.001011
Validation Loss: 0.00106976
Epoch [128/200], Train Loss: 0.001007
Validation Loss: 0.00106713
Epoch [129/200], Train Loss: 0.001011
Validation Loss: 0.00124904
Epoch [130/200], Train Loss: 0.001045
Validation Loss: 0.00107547
Epoch [131/200], Train Loss: 0.000986
Validation Loss: 0.00104778
Epoch [132/200], Train Loss: 0.000986
Validation Loss: 0.00106567
Epoch [133/200], Train Loss: 0.000972
Validation Loss: 0.00103140
Epoch [134/200], Train Loss: 0.000961
Validation Loss: 0.00102868
Epoch [135/200], Train Loss: 0.000959
Validation Loss: 0.00101868
Epoch [136/200], Train Loss: 0.000962
Validation Loss: 0.00103512
Epoch [137/200], Train Loss: 0.000958
Validation Loss: 0.00102043
Epoch [138/200], Train Loss: 0.000947
Validation Loss: 0.00100240
Epoch [139/200], Train Loss: 0.000936
Validation Loss: 0.00101246
Epoch [140/200], Train Loss: 0.001009
Validation Loss: 0.00106363
Epoch [141/200], Train Loss: 0.000958
Validation Loss: 0.00099582
Epoch [142/200], Train Loss: 0.000948
Validation Loss: 0.00132728
Epoch [143/200], Train Loss: 0.001109
Validation Loss: 0.00113332
Epoch [144/200], Train Loss: 0.000998
Validation Loss: 0.00099753
Epoch [145/200], Train Loss: 0.000924
Validation Loss: 0.00098431
Epoch [146/200], Train Loss: 0.000908
Validation Loss: 0.00096748
Epoch [147/200], Train Loss: 0.000932
Validation Loss: 0.00101476
Epoch [148/200], Train Loss: 0.000910
Validation Loss: 0.00097751
Epoch [149/200], Train Loss: 0.000914
Validation Loss: 0.00096763
Epoch [150/200], Train Loss: 0.000904
Validation Loss: 0.00095982
Epoch [151/200], Train Loss: 0.000889
Validation Loss: 0.00095276
Epoch [152/200], Train Loss: 0.000908
Validation Loss: 0.00104666
Epoch [153/200], Train Loss: 0.000947
Validation Loss: 0.00098529
Epoch [154/200], Train Loss: 0.000896
Validation Loss: 0.00095432
Epoch [155/200], Train Loss: 0.000886
Validation Loss: 0.00094552
Epoch [156/200], Train Loss: 0.000874
Validation Loss: 0.00093610
Epoch [157/200], Train Loss: 0.000888
Validation Loss: 0.00094667
Epoch [158/200], Train Loss: 0.000877
Validation Loss: 0.00095585
Epoch [159/200], Train Loss: 0.000872
Validation Loss: 0.00092973
Epoch [160/200], Train Loss: 0.000857
Validation Loss: 0.00093284
Epoch [161/200], Train Loss: 0.000849
Validation Loss: 0.00091259
Epoch [162/200], Train Loss: 0.000856
Validation Loss: 0.00093285
Epoch [163/200], Train Loss: 0.000853
Validation Loss: 0.00095271
Epoch [164/200], Train Loss: 0.000848
Validation Loss: 0.00090924
Epoch [165/200], Train Loss: 0.000833
Validation Loss: 0.00090612
Epoch [166/200], Train Loss: 0.000833
Validation Loss: 0.00089509
Epoch [167/200], Train Loss: 0.000852
Validation Loss: 0.00104159
Epoch [168/200], Train Loss: 0.000969
Validation Loss: 0.00113351
Epoch [169/200], Train Loss: 0.000971
Validation Loss: 0.00100507
Epoch [170/200], Train Loss: 0.000872
Validation Loss: 0.00090805
Epoch [171/200], Train Loss: 0.000832
Validation Loss: 0.00088619
Epoch [172/200], Train Loss: 0.000820
Validation Loss: 0.00088631
Epoch [173/200], Train Loss: 0.000813
Validation Loss: 0.00087882
Epoch [174/200], Train Loss: 0.000808
Validation Loss: 0.00086971
Epoch [175/200], Train Loss: 0.000806
Validation Loss: 0.00086351
Epoch [176/200], Train Loss: 0.000803
Validation Loss: 0.00086116
Epoch [177/200], Train Loss: 0.000794
Validation Loss: 0.00087311
Epoch [178/200], Train Loss: 0.000852
Validation Loss: 0.00089952
Epoch [179/200], Train Loss: 0.000819
Validation Loss: 0.00085223
Epoch [180/200], Train Loss: 0.000794
Validation Loss: 0.00084622
Epoch [181/200], Train Loss: 0.000786
Validation Loss: 0.00083844
Epoch [182/200], Train Loss: 0.000780
Validation Loss: 0.00083677
Epoch [183/200], Train Loss: 0.000778
Validation Loss: 0.00082970
Epoch [184/200], Train Loss: 0.000778
Validation Loss: 0.00082786
Epoch [185/200], Train Loss: 0.000767
Validation Loss: 0.00082019
Epoch [186/200], Train Loss: 0.000768
Validation Loss: 0.00081638
Epoch [187/200], Train Loss: 0.000774
Validation Loss: 0.00084098
Epoch [188/200], Train Loss: 0.000763
Validation Loss: 0.00081306
Epoch [189/200], Train Loss: 0.000754
Validation Loss: 0.00082642
Epoch [190/200], Train Loss: 0.000756
Validation Loss: 0.00080219
Epoch [191/200], Train Loss: 0.000751
Validation Loss: 0.00079964
Epoch [192/200], Train Loss: 0.000756
Validation Loss: 0.00080504
Epoch [193/200], Train Loss: 0.000753
Validation Loss: 0.00080739
Epoch [194/200], Train Loss: 0.000747
Validation Loss: 0.00079751
Epoch [195/200], Train Loss: 0.000795
Validation Loss: 0.00096793
Epoch [196/200], Train Loss: 0.000818
Validation Loss: 0.00079440
Epoch [197/200], Train Loss: 0.000737
Validation Loss: 0.00078465
Epoch [198/200], Train Loss: 0.000732
Validation Loss: 0.00077799
Epoch [199/200], Train Loss: 0.000728
Validation Loss: 0.00076926
Epoch [200/200], Train Loss: 0.000719
Validation Loss: 0.00077050

Evaluating model for: Lamp
Run 122/144 completed in 2054.64 seconds with: {'MAE': np.float32(0.6851122), 'MSE': np.float32(23.966608), 'RMSE': np.float32(4.8955703), 'SAE': np.float32(0.11035118), 'NDE': np.float32(0.3810792)}

Run 123/144: hidden=256, seq_len=720, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.004920
Validation Loss: 0.00498618
Epoch [2/200], Train Loss: 0.004866
Validation Loss: 0.00497965
Epoch [3/200], Train Loss: 0.004856
Validation Loss: 0.00494889
Epoch [4/200], Train Loss: 0.004818
Validation Loss: 0.00491016
Epoch [5/200], Train Loss: 0.004788
Validation Loss: 0.00491383
Epoch [6/200], Train Loss: 0.004791
Validation Loss: 0.00490501
Epoch [7/200], Train Loss: 0.004798
Validation Loss: 0.00489674
Epoch [8/200], Train Loss: 0.004782
Validation Loss: 0.00489460
Epoch [9/200], Train Loss: 0.004783
Validation Loss: 0.00489228
Epoch [10/200], Train Loss: 0.004783
Validation Loss: 0.00489849
Epoch [11/200], Train Loss: 0.004757
Validation Loss: 0.00491964
Epoch [12/200], Train Loss: 0.004785
Validation Loss: 0.00488878
Epoch [13/200], Train Loss: 0.004784
Validation Loss: 0.00488683
Epoch [14/200], Train Loss: 0.004779
Validation Loss: 0.00489197
Epoch [15/200], Train Loss: 0.004786
Validation Loss: 0.00488588
Epoch [16/200], Train Loss: 0.004775
Validation Loss: 0.00488776
Epoch [17/200], Train Loss: 0.004755
Validation Loss: 0.00488601
Epoch [18/200], Train Loss: 0.004768
Validation Loss: 0.00488249
Epoch [19/200], Train Loss: 0.004775
Validation Loss: 0.00488167
Epoch [20/200], Train Loss: 0.004783
Validation Loss: 0.00488417
Epoch [21/200], Train Loss: 0.004752
Validation Loss: 0.00489347
Epoch [22/200], Train Loss: 0.004762
Validation Loss: 0.00487989
Epoch [23/200], Train Loss: 0.004747
Validation Loss: 0.00488003
Epoch [24/200], Train Loss: 0.004776
Validation Loss: 0.00487869
Epoch [25/200], Train Loss: 0.004768
Validation Loss: 0.00488457
Epoch [26/200], Train Loss: 0.004775
Validation Loss: 0.00487906
Epoch [27/200], Train Loss: 0.004754
Validation Loss: 0.00488071
Epoch [28/200], Train Loss: 0.004781
Validation Loss: 0.00488487
Epoch [29/200], Train Loss: 0.004766
Validation Loss: 0.00487710
Epoch [30/200], Train Loss: 0.004758
Validation Loss: 0.00487754
Epoch [31/200], Train Loss: 0.004753
Validation Loss: 0.00487716
Epoch [32/200], Train Loss: 0.004766
Validation Loss: 0.00487968
Epoch [33/200], Train Loss: 0.004769
Validation Loss: 0.00487743
Epoch [34/200], Train Loss: 0.004764
Validation Loss: 0.00487803
Epoch [35/200], Train Loss: 0.004758
Validation Loss: 0.00487645
Epoch [36/200], Train Loss: 0.004757
Validation Loss: 0.00490056
Epoch [37/200], Train Loss: 0.004769
Validation Loss: 0.00487611
Epoch [38/200], Train Loss: 0.004752
Validation Loss: 0.00487592
Epoch [39/200], Train Loss: 0.004751
Validation Loss: 0.00487706
Epoch [40/200], Train Loss: 0.004759
Validation Loss: 0.00487711
Epoch [41/200], Train Loss: 0.004765
Validation Loss: 0.00487621
Epoch [42/200], Train Loss: 0.004762
Validation Loss: 0.00487781
Epoch [43/200], Train Loss: 0.004767
Validation Loss: 0.00487613
Epoch [44/200], Train Loss: 0.004762
Validation Loss: 0.00487619
Epoch [45/200], Train Loss: 0.004768
Validation Loss: 0.00487457
Epoch [46/200], Train Loss: 0.004778
Validation Loss: 0.00487521
Epoch [47/200], Train Loss: 0.004762
Validation Loss: 0.00487387
Epoch [48/200], Train Loss: 0.004745
Validation Loss: 0.00487420
Epoch [49/200], Train Loss: 0.004761
Validation Loss: 0.00487525
Epoch [50/200], Train Loss: 0.004748
Validation Loss: 0.00487574
Epoch [51/200], Train Loss: 0.004758
Validation Loss: 0.00487364
Epoch [52/200], Train Loss: 0.004753
Validation Loss: 0.00487743
Epoch [53/200], Train Loss: 0.004755
Validation Loss: 0.00487815
Epoch [54/200], Train Loss: 0.004760
Validation Loss: 0.00487388
Epoch [55/200], Train Loss: 0.004744
Validation Loss: 0.00487421
Epoch [56/200], Train Loss: 0.004760
Validation Loss: 0.00487176
Epoch [57/200], Train Loss: 0.004757
Validation Loss: 0.00487155
Epoch [58/200], Train Loss: 0.004745
Validation Loss: 0.00487372
Epoch [59/200], Train Loss: 0.004763
Validation Loss: 0.00487561
Epoch [60/200], Train Loss: 0.004761
Validation Loss: 0.00487043
Epoch [61/200], Train Loss: 0.004748
Validation Loss: 0.00486592
Epoch [62/200], Train Loss: 0.004747
Validation Loss: 0.00486735
Epoch [63/200], Train Loss: 0.004745
Validation Loss: 0.00486096
Epoch [64/200], Train Loss: 0.004759
Validation Loss: 0.00486254
Epoch [65/200], Train Loss: 0.004750
Validation Loss: 0.00485958
Epoch [66/200], Train Loss: 0.004725
Validation Loss: 0.00486316
Epoch [67/200], Train Loss: 0.004761
Validation Loss: 0.00485775
Epoch [68/200], Train Loss: 0.004745
Validation Loss: 0.00485504
Epoch [69/200], Train Loss: 0.004740
Validation Loss: 0.00484720
Epoch [70/200], Train Loss: 0.004733
Validation Loss: 0.00484057
Epoch [71/200], Train Loss: 0.004727
Validation Loss: 0.00482222
Epoch [72/200], Train Loss: 0.004703
Validation Loss: 0.00478962
Epoch [73/200], Train Loss: 0.004705
Validation Loss: 0.00482429
Epoch [74/200], Train Loss: 0.004682
Validation Loss: 0.00489614
Epoch [75/200], Train Loss: 0.004680
Validation Loss: 0.00474951
Epoch [76/200], Train Loss: 0.004634
Validation Loss: 0.00474752
Epoch [77/200], Train Loss: 0.004596
Validation Loss: 0.00465411
Epoch [78/200], Train Loss: 0.004576
Validation Loss: 0.00464041
Epoch [79/200], Train Loss: 0.004547
Validation Loss: 0.00463994
Epoch [80/200], Train Loss: 0.004492
Validation Loss: 0.00460301
Epoch [81/200], Train Loss: 0.004467
Validation Loss: 0.00451187
Epoch [82/200], Train Loss: 0.004437
Validation Loss: 0.00449029
Epoch [83/200], Train Loss: 0.004343
Validation Loss: 0.00434808
Epoch [84/200], Train Loss: 0.004331
Validation Loss: 0.00448521
Epoch [85/200], Train Loss: 0.004311
Validation Loss: 0.00418710
Epoch [86/200], Train Loss: 0.004102
Validation Loss: 0.00405179
Epoch [87/200], Train Loss: 0.004034
Validation Loss: 0.00386098
Epoch [88/200], Train Loss: 0.003764
Validation Loss: 0.00353875
Epoch [89/200], Train Loss: 0.003421
Validation Loss: 0.00327971
Epoch [90/200], Train Loss: 0.003090
Validation Loss: 0.00279203
Epoch [91/200], Train Loss: 0.002775
Validation Loss: 0.00277033
Epoch [92/200], Train Loss: 0.002619
Validation Loss: 0.00246174
Epoch [93/200], Train Loss: 0.002453
Validation Loss: 0.00227392
Epoch [94/200], Train Loss: 0.002344
Validation Loss: 0.00222249
Epoch [95/200], Train Loss: 0.002273
Validation Loss: 0.00210980
Epoch [96/200], Train Loss: 0.002182
Validation Loss: 0.00219815
Epoch [97/200], Train Loss: 0.002098
Validation Loss: 0.00205637
Epoch [98/200], Train Loss: 0.002026
Validation Loss: 0.00196910
Epoch [99/200], Train Loss: 0.001976
Validation Loss: 0.00192275
Epoch [100/200], Train Loss: 0.001898
Validation Loss: 0.00192026
Epoch [101/200], Train Loss: 0.001852
Validation Loss: 0.00181010
Epoch [102/200], Train Loss: 0.001805
Validation Loss: 0.00178493
Epoch [103/200], Train Loss: 0.001756
Validation Loss: 0.00178577
Epoch [104/200], Train Loss: 0.001717
Validation Loss: 0.00168733
Epoch [105/200], Train Loss: 0.001673
Validation Loss: 0.00168304
Epoch [106/200], Train Loss: 0.001640
Validation Loss: 0.00163237
Epoch [107/200], Train Loss: 0.001620
Validation Loss: 0.00163724
Epoch [108/200], Train Loss: 0.001561
Validation Loss: 0.00156535
Epoch [109/200], Train Loss: 0.001535
Validation Loss: 0.00154454
Epoch [110/200], Train Loss: 0.001509
Validation Loss: 0.00151436
Epoch [111/200], Train Loss: 0.001480
Validation Loss: 0.00149913
Epoch [112/200], Train Loss: 0.001442
Validation Loss: 0.00147044
Epoch [113/200], Train Loss: 0.001414
Validation Loss: 0.00142857
Epoch [114/200], Train Loss: 0.001374
Validation Loss: 0.00139219
Epoch [115/200], Train Loss: 0.001338
Validation Loss: 0.00138948
Epoch [116/200], Train Loss: 0.001306
Validation Loss: 0.00135090
Epoch [117/200], Train Loss: 0.001288
Validation Loss: 0.00132011
Epoch [118/200], Train Loss: 0.001259
Validation Loss: 0.00130777
Epoch [119/200], Train Loss: 0.001244
Validation Loss: 0.00127827
Epoch [120/200], Train Loss: 0.001216
Validation Loss: 0.00125408
Epoch [121/200], Train Loss: 0.001211
Validation Loss: 0.00125622
Epoch [122/200], Train Loss: 0.001185
Validation Loss: 0.00123868
Epoch [123/200], Train Loss: 0.001179
Validation Loss: 0.00126144
Epoch [124/200], Train Loss: 0.001149
Validation Loss: 0.00120443
Epoch [125/200], Train Loss: 0.001137
Validation Loss: 0.00120181
Epoch [126/200], Train Loss: 0.001121
Validation Loss: 0.00115879
Epoch [127/200], Train Loss: 0.001096
Validation Loss: 0.00114423
Epoch [128/200], Train Loss: 0.001082
Validation Loss: 0.00113233
Epoch [129/200], Train Loss: 0.001070
Validation Loss: 0.00110460
Epoch [130/200], Train Loss: 0.001038
Validation Loss: 0.00109528
Epoch [131/200], Train Loss: 0.001030
Validation Loss: 0.00108010
Epoch [132/200], Train Loss: 0.001012
Validation Loss: 0.00103844
Epoch [133/200], Train Loss: 0.000995
Validation Loss: 0.00102710
Epoch [134/200], Train Loss: 0.000998
Validation Loss: 0.00121652
Epoch [135/200], Train Loss: 0.001143
Validation Loss: 0.00114042
Epoch [136/200], Train Loss: 0.001040
Validation Loss: 0.00105433
Epoch [137/200], Train Loss: 0.000975
Validation Loss: 0.00102234
Epoch [138/200], Train Loss: 0.000947
Validation Loss: 0.00098228
Epoch [139/200], Train Loss: 0.000930
Validation Loss: 0.00097137
Epoch [140/200], Train Loss: 0.000915
Validation Loss: 0.00094715
Epoch [141/200], Train Loss: 0.000914
Validation Loss: 0.00097336
Epoch [142/200], Train Loss: 0.000909
Validation Loss: 0.00094365
Epoch [143/200], Train Loss: 0.000885
Validation Loss: 0.00091985
Epoch [144/200], Train Loss: 0.000876
Validation Loss: 0.00090850
Epoch [145/200], Train Loss: 0.000854
Validation Loss: 0.00088772
Epoch [146/200], Train Loss: 0.000857
Validation Loss: 0.00087298
Epoch [147/200], Train Loss: 0.000832
Validation Loss: 0.00086946
Epoch [148/200], Train Loss: 0.000833
Validation Loss: 0.00085404
Epoch [149/200], Train Loss: 0.000825
Validation Loss: 0.00086517
Epoch [150/200], Train Loss: 0.000822
Validation Loss: 0.00085545
Epoch [151/200], Train Loss: 0.000802
Validation Loss: 0.00084801
Epoch [152/200], Train Loss: 0.000818
Validation Loss: 0.00083687
Epoch [153/200], Train Loss: 0.000791
Validation Loss: 0.00083628
Epoch [154/200], Train Loss: 0.000792
Validation Loss: 0.00084650
Epoch [155/200], Train Loss: 0.000839
Validation Loss: 0.00100567
Epoch [156/200], Train Loss: 0.000801
Validation Loss: 0.00080842
Epoch [157/200], Train Loss: 0.000769
Validation Loss: 0.00083126
Epoch [158/200], Train Loss: 0.000770
Validation Loss: 0.00090587
Epoch [159/200], Train Loss: 0.000778
Validation Loss: 0.00079796
Epoch [160/200], Train Loss: 0.000748
Validation Loss: 0.00078238
Epoch [161/200], Train Loss: 0.000741
Validation Loss: 0.00078692
Epoch [162/200], Train Loss: 0.000732
Validation Loss: 0.00076896
Epoch [163/200], Train Loss: 0.000731
Validation Loss: 0.00078039
Epoch [164/200], Train Loss: 0.000730
Validation Loss: 0.00075225
Epoch [165/200], Train Loss: 0.000722
Validation Loss: 0.00075545
Epoch [166/200], Train Loss: 0.000704
Validation Loss: 0.00075243
Epoch [167/200], Train Loss: 0.000696
Validation Loss: 0.00074257
Epoch [168/200], Train Loss: 0.000707
Validation Loss: 0.00074591
Epoch [169/200], Train Loss: 0.000687
Validation Loss: 0.00073032
Epoch [170/200], Train Loss: 0.000681
Validation Loss: 0.00072569
Epoch [171/200], Train Loss: 0.000678
Validation Loss: 0.00072136
Epoch [172/200], Train Loss: 0.000688
Validation Loss: 0.00073153
Epoch [173/200], Train Loss: 0.000673
Validation Loss: 0.00073289
Epoch [174/200], Train Loss: 0.000664
Validation Loss: 0.00070789
Epoch [175/200], Train Loss: 0.000664
Validation Loss: 0.00070917
Epoch [176/200], Train Loss: 0.000658
Validation Loss: 0.00072050
Epoch [177/200], Train Loss: 0.000664
Validation Loss: 0.00069448
Epoch [178/200], Train Loss: 0.000651
Validation Loss: 0.00068482
Epoch [179/200], Train Loss: 0.000640
Validation Loss: 0.00069042
Epoch [180/200], Train Loss: 0.000643
Validation Loss: 0.00067866
Epoch [181/200], Train Loss: 0.000632
Validation Loss: 0.00067693
Epoch [182/200], Train Loss: 0.000622
Validation Loss: 0.00067236
Epoch [183/200], Train Loss: 0.000628
Validation Loss: 0.00068540
Epoch [184/200], Train Loss: 0.000626
Validation Loss: 0.00069343
Epoch [185/200], Train Loss: 0.000622
Validation Loss: 0.00066373
Epoch [186/200], Train Loss: 0.000620
Validation Loss: 0.00065730
Epoch [187/200], Train Loss: 0.000610
Validation Loss: 0.00066521
Epoch [188/200], Train Loss: 0.000611
Validation Loss: 0.00066201
Epoch [189/200], Train Loss: 0.000608
Validation Loss: 0.00065101
Epoch [190/200], Train Loss: 0.000605
Validation Loss: 0.00065777
Epoch [191/200], Train Loss: 0.000603
Validation Loss: 0.00065661
Epoch [192/200], Train Loss: 0.000614
Validation Loss: 0.00065837
Epoch [193/200], Train Loss: 0.000615
Validation Loss: 0.00064967
Epoch [194/200], Train Loss: 0.000597
Validation Loss: 0.00064933
Epoch [195/200], Train Loss: 0.000609
Validation Loss: 0.00065873
Epoch [196/200], Train Loss: 0.000602
Validation Loss: 0.00064380
Epoch [197/200], Train Loss: 0.000587
Validation Loss: 0.00062976
Epoch [198/200], Train Loss: 0.000584
Validation Loss: 0.00062840
Epoch [199/200], Train Loss: 0.000584
Validation Loss: 0.00062874
Epoch [200/200], Train Loss: 0.000589
Validation Loss: 0.00066296

Evaluating model for: Lamp
Run 123/144 completed in 2465.81 seconds with: {'MAE': np.float32(0.5162355), 'MSE': np.float32(19.686249), 'RMSE': np.float32(4.4369187), 'SAE': np.float32(0.002051987), 'NDE': np.float32(0.34537727)}

Run 124/144: hidden=256, seq_len=720, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 4602 windows

Epoch [1/200], Train Loss: 0.004915
Validation Loss: 0.00498752
Epoch [2/200], Train Loss: 0.004878
Validation Loss: 0.00498522
Epoch [3/200], Train Loss: 0.004865
Validation Loss: 0.00497311
Epoch [4/200], Train Loss: 0.004829
Validation Loss: 0.00491098
Epoch [5/200], Train Loss: 0.004792
Validation Loss: 0.00490478
Epoch [6/200], Train Loss: 0.004780
Validation Loss: 0.00489894
Epoch [7/200], Train Loss: 0.004779
Validation Loss: 0.00489425
Epoch [8/200], Train Loss: 0.004791
Validation Loss: 0.00489829
Epoch [9/200], Train Loss: 0.004782
Validation Loss: 0.00489312
Epoch [10/200], Train Loss: 0.004771
Validation Loss: 0.00489668
Epoch [11/200], Train Loss: 0.004777
Validation Loss: 0.00488799
Epoch [12/200], Train Loss: 0.004777
Validation Loss: 0.00489043
Epoch [13/200], Train Loss: 0.004772
Validation Loss: 0.00488755
Epoch [14/200], Train Loss: 0.004771
Validation Loss: 0.00488465
Epoch [15/200], Train Loss: 0.004772
Validation Loss: 0.00488410
Epoch [16/200], Train Loss: 0.004764
Validation Loss: 0.00488383
Epoch [17/200], Train Loss: 0.004760
Validation Loss: 0.00489063
Epoch [18/200], Train Loss: 0.004775
Validation Loss: 0.00488463
Epoch [19/200], Train Loss: 0.004761
Validation Loss: 0.00488727
Epoch [20/200], Train Loss: 0.004773
Validation Loss: 0.00488129
Epoch [21/200], Train Loss: 0.004766
Validation Loss: 0.00488095
Epoch [22/200], Train Loss: 0.004758
Validation Loss: 0.00488315
Epoch [23/200], Train Loss: 0.004759
Validation Loss: 0.00488037
Epoch [24/200], Train Loss: 0.004774
Validation Loss: 0.00488276
Epoch [25/200], Train Loss: 0.004766
Validation Loss: 0.00487926
Epoch [26/200], Train Loss: 0.004752
Validation Loss: 0.00488217
Epoch [27/200], Train Loss: 0.004757
Validation Loss: 0.00488141
Epoch [28/200], Train Loss: 0.004755
Validation Loss: 0.00487833
Epoch [29/200], Train Loss: 0.004760
Validation Loss: 0.00487860
Epoch [30/200], Train Loss: 0.004758
Validation Loss: 0.00487995
Epoch [31/200], Train Loss: 0.004758
Validation Loss: 0.00487826
Epoch [32/200], Train Loss: 0.004767
Validation Loss: 0.00487717
Epoch [33/200], Train Loss: 0.004761
Validation Loss: 0.00487824
Epoch [34/200], Train Loss: 0.004744
Validation Loss: 0.00487878
Epoch [35/200], Train Loss: 0.004762
Validation Loss: 0.00487984
Epoch [36/200], Train Loss: 0.004771
Validation Loss: 0.00488128
Epoch [37/200], Train Loss: 0.004771
Validation Loss: 0.00487737
Epoch [38/200], Train Loss: 0.004774
Validation Loss: 0.00487844
Epoch [39/200], Train Loss: 0.004762
Validation Loss: 0.00487656
Epoch [40/200], Train Loss: 0.004755
Validation Loss: 0.00488345
Epoch [41/200], Train Loss: 0.004756
Validation Loss: 0.00487684
Epoch [42/200], Train Loss: 0.004769
Validation Loss: 0.00488095
Epoch [43/200], Train Loss: 0.004765
Validation Loss: 0.00488000
Epoch [44/200], Train Loss: 0.004763
Validation Loss: 0.00488387
Epoch [45/200], Train Loss: 0.004758
Validation Loss: 0.00487679
Epoch [46/200], Train Loss: 0.004758
Validation Loss: 0.00487637
Epoch [47/200], Train Loss: 0.004768
Validation Loss: 0.00488573
Epoch [48/200], Train Loss: 0.004766
Validation Loss: 0.00487545
Epoch [49/200], Train Loss: 0.004753
Validation Loss: 0.00487615
Epoch [50/200], Train Loss: 0.004752
Validation Loss: 0.00487739
Epoch [51/200], Train Loss: 0.004767
Validation Loss: 0.00487603
Epoch [52/200], Train Loss: 0.004753
Validation Loss: 0.00487759
Epoch [53/200], Train Loss: 0.004748
Validation Loss: 0.00487715
Epoch [54/200], Train Loss: 0.004784
Validation Loss: 0.00487682
Epoch [55/200], Train Loss: 0.004744
Validation Loss: 0.00487780
Epoch [56/200], Train Loss: 0.004757
Validation Loss: 0.00487515
Epoch [57/200], Train Loss: 0.004749
Validation Loss: 0.00487461
Epoch [58/200], Train Loss: 0.004758
Validation Loss: 0.00488467
Epoch [59/200], Train Loss: 0.004754
Validation Loss: 0.00487661
Epoch [60/200], Train Loss: 0.004753
Validation Loss: 0.00487600
Epoch [61/200], Train Loss: 0.004758
Validation Loss: 0.00487618
Epoch [62/200], Train Loss: 0.004765
Validation Loss: 0.00487422
Epoch [63/200], Train Loss: 0.004753
Validation Loss: 0.00487762
Epoch [64/200], Train Loss: 0.004750
Validation Loss: 0.00487472
Epoch [65/200], Train Loss: 0.004752
Validation Loss: 0.00487401
Epoch [66/200], Train Loss: 0.004769
Validation Loss: 0.00487400
Epoch [67/200], Train Loss: 0.004763
Validation Loss: 0.00487290
Epoch [68/200], Train Loss: 0.004761
Validation Loss: 0.00487391
Epoch [69/200], Train Loss: 0.004746
Validation Loss: 0.00487475
Epoch [70/200], Train Loss: 0.004763
Validation Loss: 0.00487229
Epoch [71/200], Train Loss: 0.004753
Validation Loss: 0.00487553
Epoch [72/200], Train Loss: 0.004773
Validation Loss: 0.00486917
Epoch [73/200], Train Loss: 0.004763
Validation Loss: 0.00487005
Epoch [74/200], Train Loss: 0.004753
Validation Loss: 0.00486919
Epoch [75/200], Train Loss: 0.004737
Validation Loss: 0.00487247
Epoch [76/200], Train Loss: 0.004752
Validation Loss: 0.00486891
Epoch [77/200], Train Loss: 0.004753
Validation Loss: 0.00486346
Epoch [78/200], Train Loss: 0.004736
Validation Loss: 0.00486446
Epoch [79/200], Train Loss: 0.004748
Validation Loss: 0.00486298
Epoch [80/200], Train Loss: 0.004744
Validation Loss: 0.00486037
Epoch [81/200], Train Loss: 0.004742
Validation Loss: 0.00485901
Epoch [82/200], Train Loss: 0.004742
Validation Loss: 0.00485865
Epoch [83/200], Train Loss: 0.004739
Validation Loss: 0.00485701
Epoch [84/200], Train Loss: 0.004743
Validation Loss: 0.00485534
Epoch [85/200], Train Loss: 0.004737
Validation Loss: 0.00485427
Epoch [86/200], Train Loss: 0.004740
Validation Loss: 0.00484788
Epoch [87/200], Train Loss: 0.004733
Validation Loss: 0.00484358
Epoch [88/200], Train Loss: 0.004740
Validation Loss: 0.00485909
Epoch [89/200], Train Loss: 0.004738
Validation Loss: 0.00484805
Epoch [90/200], Train Loss: 0.004727
Validation Loss: 0.00483668
Epoch [91/200], Train Loss: 0.004731
Validation Loss: 0.00482830
Epoch [92/200], Train Loss: 0.004706
Validation Loss: 0.00482801
Epoch [93/200], Train Loss: 0.004718
Validation Loss: 0.00482653
Epoch [94/200], Train Loss: 0.004732
Validation Loss: 0.00482373
Epoch [95/200], Train Loss: 0.004704
Validation Loss: 0.00479641
Epoch [96/200], Train Loss: 0.004690
Validation Loss: 0.00478035
Epoch [97/200], Train Loss: 0.004675
Validation Loss: 0.00481294
Epoch [98/200], Train Loss: 0.004665
Validation Loss: 0.00476212
Epoch [99/200], Train Loss: 0.004654
Validation Loss: 0.00473052
Epoch [100/200], Train Loss: 0.004630
Validation Loss: 0.00475457
Epoch [101/200], Train Loss: 0.004629
Validation Loss: 0.00473817
Epoch [102/200], Train Loss: 0.004597
Validation Loss: 0.00465817
Epoch [103/200], Train Loss: 0.004559
Validation Loss: 0.00464469
Epoch [104/200], Train Loss: 0.004536
Validation Loss: 0.00458057
Epoch [105/200], Train Loss: 0.004509
Validation Loss: 0.00454567
Epoch [106/200], Train Loss: 0.004452
Validation Loss: 0.00447053
Epoch [107/200], Train Loss: 0.004438
Validation Loss: 0.00448963
Epoch [108/200], Train Loss: 0.004349
Validation Loss: 0.00432515
Epoch [109/200], Train Loss: 0.004258
Validation Loss: 0.00430785
Epoch [110/200], Train Loss: 0.004140
Validation Loss: 0.00403638
Epoch [111/200], Train Loss: 0.003955
Validation Loss: 0.00364952
Epoch [112/200], Train Loss: 0.003635
Validation Loss: 0.00343828
Epoch [113/200], Train Loss: 0.003208
Validation Loss: 0.00289603
Epoch [114/200], Train Loss: 0.003028
Validation Loss: 0.00279004
Epoch [115/200], Train Loss: 0.002797
Validation Loss: 0.00254862
Epoch [116/200], Train Loss: 0.002565
Validation Loss: 0.00242274
Epoch [117/200], Train Loss: 0.002457
Validation Loss: 0.00234197
Epoch [118/200], Train Loss: 0.002361
Validation Loss: 0.00228346
Epoch [119/200], Train Loss: 0.002253
Validation Loss: 0.00218015
Epoch [120/200], Train Loss: 0.002167
Validation Loss: 0.00209266
Epoch [121/200], Train Loss: 0.002105
Validation Loss: 0.00204694
Epoch [122/200], Train Loss: 0.002041
Validation Loss: 0.00199434
Epoch [123/200], Train Loss: 0.001977
Validation Loss: 0.00197330
Epoch [124/200], Train Loss: 0.001933
Validation Loss: 0.00196279
Epoch [125/200], Train Loss: 0.001906
Validation Loss: 0.00182451
Epoch [126/200], Train Loss: 0.001855
Validation Loss: 0.00183065
Epoch [127/200], Train Loss: 0.001805
Validation Loss: 0.00182970
Epoch [128/200], Train Loss: 0.001761
Validation Loss: 0.00179755
Epoch [129/200], Train Loss: 0.001725
Validation Loss: 0.00168676
Epoch [130/200], Train Loss: 0.001694
Validation Loss: 0.00165303
Epoch [131/200], Train Loss: 0.001660
Validation Loss: 0.00162628
Epoch [132/200], Train Loss: 0.001613
Validation Loss: 0.00159766
Epoch [133/200], Train Loss: 0.001581
Validation Loss: 0.00157091
Epoch [134/200], Train Loss: 0.001545
Validation Loss: 0.00154736
Epoch [135/200], Train Loss: 0.001509
Validation Loss: 0.00151398
Epoch [136/200], Train Loss: 0.001475
Validation Loss: 0.00149113
Epoch [137/200], Train Loss: 0.001454
Validation Loss: 0.00145611
Epoch [138/200], Train Loss: 0.001427
Validation Loss: 0.00145462
Epoch [139/200], Train Loss: 0.001406
Validation Loss: 0.00143132
Epoch [140/200], Train Loss: 0.001388
Validation Loss: 0.00142809
Epoch [141/200], Train Loss: 0.001360
Validation Loss: 0.00141146
Epoch [142/200], Train Loss: 0.001342
Validation Loss: 0.00135641
Epoch [143/200], Train Loss: 0.001311
Validation Loss: 0.00135822
Epoch [144/200], Train Loss: 0.001298
Validation Loss: 0.00134495
Epoch [145/200], Train Loss: 0.001277
Validation Loss: 0.00132345
Epoch [146/200], Train Loss: 0.001270
Validation Loss: 0.00131038
Epoch [147/200], Train Loss: 0.001239
Validation Loss: 0.00129751
Epoch [148/200], Train Loss: 0.001231
Validation Loss: 0.00129700
Epoch [149/200], Train Loss: 0.001212
Validation Loss: 0.00126343
Epoch [150/200], Train Loss: 0.001189
Validation Loss: 0.00125664
Epoch [151/200], Train Loss: 0.001174
Validation Loss: 0.00126222
Epoch [152/200], Train Loss: 0.001159
Validation Loss: 0.00123742
Epoch [153/200], Train Loss: 0.001153
Validation Loss: 0.00120622
Epoch [154/200], Train Loss: 0.001135
Validation Loss: 0.00122087
Epoch [155/200], Train Loss: 0.001124
Validation Loss: 0.00118794
Epoch [156/200], Train Loss: 0.001104
Validation Loss: 0.00118193
Epoch [157/200], Train Loss: 0.001103
Validation Loss: 0.00115909
Epoch [158/200], Train Loss: 0.001083
Validation Loss: 0.00113049
Epoch [159/200], Train Loss: 0.001087
Validation Loss: 0.00114663
Epoch [160/200], Train Loss: 0.001050
Validation Loss: 0.00109655
Epoch [161/200], Train Loss: 0.001036
Validation Loss: 0.00110712
Epoch [162/200], Train Loss: 0.001023
Validation Loss: 0.00109510
Epoch [163/200], Train Loss: 0.001006
Validation Loss: 0.00109686
Epoch [164/200], Train Loss: 0.000990
Validation Loss: 0.00101168
Epoch [165/200], Train Loss: 0.000988
Validation Loss: 0.00100618
Epoch [166/200], Train Loss: 0.000971
Validation Loss: 0.00097552
Epoch [167/200], Train Loss: 0.000939
Validation Loss: 0.00096021
Epoch [168/200], Train Loss: 0.000930
Validation Loss: 0.00094340
Epoch [169/200], Train Loss: 0.000913
Validation Loss: 0.00093148
Epoch [170/200], Train Loss: 0.000904
Validation Loss: 0.00092651
Epoch [171/200], Train Loss: 0.000895
Validation Loss: 0.00094430
Epoch [172/200], Train Loss: 0.000890
Validation Loss: 0.00091518
Epoch [173/200], Train Loss: 0.000907
Validation Loss: 0.00089568
Epoch [174/200], Train Loss: 0.000861
Validation Loss: 0.00087883
Epoch [175/200], Train Loss: 0.000846
Validation Loss: 0.00090299
Epoch [176/200], Train Loss: 0.000854
Validation Loss: 0.00087971
Epoch [177/200], Train Loss: 0.000828
Validation Loss: 0.00085901
Epoch [178/200], Train Loss: 0.000826
Validation Loss: 0.00085201
Epoch [179/200], Train Loss: 0.000817
Validation Loss: 0.00083770
Epoch [180/200], Train Loss: 0.000807
Validation Loss: 0.00083755
Epoch [181/200], Train Loss: 0.000790
Validation Loss: 0.00085004
Epoch [182/200], Train Loss: 0.000789
Validation Loss: 0.00081890
Epoch [183/200], Train Loss: 0.000776
Validation Loss: 0.00082019
Epoch [184/200], Train Loss: 0.000766
Validation Loss: 0.00080200
Epoch [185/200], Train Loss: 0.000764
Validation Loss: 0.00080814
Epoch [186/200], Train Loss: 0.000758
Validation Loss: 0.00079059
Epoch [187/200], Train Loss: 0.000753
Validation Loss: 0.00079005
Epoch [188/200], Train Loss: 0.000740
Validation Loss: 0.00077542
Epoch [189/200], Train Loss: 0.000738
Validation Loss: 0.00076590
Epoch [190/200], Train Loss: 0.000734
Validation Loss: 0.00077192
Epoch [191/200], Train Loss: 0.000732
Validation Loss: 0.00076520
Epoch [192/200], Train Loss: 0.000724
Validation Loss: 0.00076501
Epoch [193/200], Train Loss: 0.000729
Validation Loss: 0.00077157
Epoch [194/200], Train Loss: 0.000709
Validation Loss: 0.00074786
Epoch [195/200], Train Loss: 0.000703
Validation Loss: 0.00074662
Epoch [196/200], Train Loss: 0.000697
Validation Loss: 0.00073372
Epoch [197/200], Train Loss: 0.000701
Validation Loss: 0.00073815
Epoch [198/200], Train Loss: 0.000694
Validation Loss: 0.00075394
Epoch [199/200], Train Loss: 0.000706
Validation Loss: 0.00074410
Epoch [200/200], Train Loss: 0.000697
Validation Loss: 0.00073978

Evaluating model for: Lamp
Run 124/144 completed in 3396.62 seconds with: {'MAE': np.float32(0.5345643), 'MSE': np.float32(23.15451), 'RMSE': np.float32(4.8119135), 'SAE': np.float32(0.034613438), 'NDE': np.float32(0.37456733)}

Run 125/144: hidden=256, seq_len=720, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.005312
Validation Loss: 0.00512341
Epoch [2/200], Train Loss: 0.005022
Validation Loss: 0.00511282
Epoch [3/200], Train Loss: 0.004861
Validation Loss: 0.00510652
Epoch [4/200], Train Loss: 0.004884
Validation Loss: 0.00508550
Epoch [5/200], Train Loss: 0.004865
Validation Loss: 0.00507467
Epoch [6/200], Train Loss: 0.004898
Validation Loss: 0.00506530
Epoch [7/200], Train Loss: 0.004858
Validation Loss: 0.00505416
Epoch [8/200], Train Loss: 0.004870
Validation Loss: 0.00504681
Epoch [9/200], Train Loss: 0.004797
Validation Loss: 0.00503750
Epoch [10/200], Train Loss: 0.004826
Validation Loss: 0.00503486
Epoch [11/200], Train Loss: 0.004859
Validation Loss: 0.00503353
Epoch [12/200], Train Loss: 0.004697
Validation Loss: 0.00503595
Epoch [13/200], Train Loss: 0.004780
Validation Loss: 0.00503026
Epoch [14/200], Train Loss: 0.004795
Validation Loss: 0.00502953
Epoch [15/200], Train Loss: 0.004794
Validation Loss: 0.00502757
Epoch [16/200], Train Loss: 0.004794
Validation Loss: 0.00503018
Epoch [17/200], Train Loss: 0.004917
Validation Loss: 0.00502538
Epoch [18/200], Train Loss: 0.004800
Validation Loss: 0.00503125
Epoch [19/200], Train Loss: 0.004819
Validation Loss: 0.00502397
Epoch [20/200], Train Loss: 0.004773
Validation Loss: 0.00502293
Epoch [21/200], Train Loss: 0.004828
Validation Loss: 0.00502339
Epoch [22/200], Train Loss: 0.004761
Validation Loss: 0.00502201
Epoch [23/200], Train Loss: 0.004771
Validation Loss: 0.00502079
Epoch [24/200], Train Loss: 0.004818
Validation Loss: 0.00502140
Epoch [25/200], Train Loss: 0.004851
Validation Loss: 0.00502081
Epoch [26/200], Train Loss: 0.004805
Validation Loss: 0.00502006
Epoch [27/200], Train Loss: 0.004778
Validation Loss: 0.00502185
Epoch [28/200], Train Loss: 0.004906
Validation Loss: 0.00501945
Epoch [29/200], Train Loss: 0.004828
Validation Loss: 0.00501950
Epoch [30/200], Train Loss: 0.004819
Validation Loss: 0.00502378
Epoch [31/200], Train Loss: 0.004865
Validation Loss: 0.00501605
Epoch [32/200], Train Loss: 0.004744
Validation Loss: 0.00501336
Epoch [33/200], Train Loss: 0.004815
Validation Loss: 0.00501954
Epoch [34/200], Train Loss: 0.004803
Validation Loss: 0.00501170
Epoch [35/200], Train Loss: 0.004818
Validation Loss: 0.00501156
Epoch [36/200], Train Loss: 0.004808
Validation Loss: 0.00501209
Epoch [37/200], Train Loss: 0.004775
Validation Loss: 0.00501357
Epoch [38/200], Train Loss: 0.004730
Validation Loss: 0.00500921
Epoch [39/200], Train Loss: 0.004776
Validation Loss: 0.00501083
Epoch [40/200], Train Loss: 0.004812
Validation Loss: 0.00501211
Epoch [41/200], Train Loss: 0.004728
Validation Loss: 0.00500646
Epoch [42/200], Train Loss: 0.004804
Validation Loss: 0.00500842
Epoch [43/200], Train Loss: 0.004745
Validation Loss: 0.00500590
Epoch [44/200], Train Loss: 0.004787
Validation Loss: 0.00500487
Epoch [45/200], Train Loss: 0.004868
Validation Loss: 0.00500730
Epoch [46/200], Train Loss: 0.004803
Validation Loss: 0.00500771
Epoch [47/200], Train Loss: 0.004785
Validation Loss: 0.00500780
Epoch [48/200], Train Loss: 0.004824
Validation Loss: 0.00500076
Epoch [49/200], Train Loss: 0.004836
Validation Loss: 0.00500638
Epoch [50/200], Train Loss: 0.004816
Validation Loss: 0.00500079
Epoch [51/200], Train Loss: 0.004813
Validation Loss: 0.00500028
Epoch [52/200], Train Loss: 0.004757
Validation Loss: 0.00499622
Epoch [53/200], Train Loss: 0.004717
Validation Loss: 0.00499081
Epoch [54/200], Train Loss: 0.004765
Validation Loss: 0.00498983
Epoch [55/200], Train Loss: 0.004808
Validation Loss: 0.00498738
Epoch [56/200], Train Loss: 0.004770
Validation Loss: 0.00498985
Epoch [57/200], Train Loss: 0.004788
Validation Loss: 0.00498231
Epoch [58/200], Train Loss: 0.004797
Validation Loss: 0.00497533
Epoch [59/200], Train Loss: 0.004808
Validation Loss: 0.00497153
Epoch [60/200], Train Loss: 0.004751
Validation Loss: 0.00496631
Epoch [61/200], Train Loss: 0.004737
Validation Loss: 0.00496420
Epoch [62/200], Train Loss: 0.004703
Validation Loss: 0.00495973
Epoch [63/200], Train Loss: 0.004783
Validation Loss: 0.00495383
Epoch [64/200], Train Loss: 0.004676
Validation Loss: 0.00498213
Epoch [65/200], Train Loss: 0.004689
Validation Loss: 0.00495598
Epoch [66/200], Train Loss: 0.004709
Validation Loss: 0.00495014
Epoch [67/200], Train Loss: 0.004778
Validation Loss: 0.00493928
Epoch [68/200], Train Loss: 0.004736
Validation Loss: 0.00493988
Epoch [69/200], Train Loss: 0.004736
Validation Loss: 0.00494241
Epoch [70/200], Train Loss: 0.004738
Validation Loss: 0.00492868
Epoch [71/200], Train Loss: 0.004707
Validation Loss: 0.00492538
Epoch [72/200], Train Loss: 0.004722
Validation Loss: 0.00492056
Epoch [73/200], Train Loss: 0.004672
Validation Loss: 0.00491662
Epoch [74/200], Train Loss: 0.004776
Validation Loss: 0.00492981
Epoch [75/200], Train Loss: 0.004722
Validation Loss: 0.00491229
Epoch [76/200], Train Loss: 0.004685
Validation Loss: 0.00491920
Epoch [77/200], Train Loss: 0.004689
Validation Loss: 0.00490614
Epoch [78/200], Train Loss: 0.004655
Validation Loss: 0.00489361
Epoch [79/200], Train Loss: 0.004755
Validation Loss: 0.00488703
Epoch [80/200], Train Loss: 0.004648
Validation Loss: 0.00488113
Epoch [81/200], Train Loss: 0.004728
Validation Loss: 0.00487731
Epoch [82/200], Train Loss: 0.004626
Validation Loss: 0.00486933
Epoch [83/200], Train Loss: 0.004656
Validation Loss: 0.00486510
Epoch [84/200], Train Loss: 0.004631
Validation Loss: 0.00484936
Epoch [85/200], Train Loss: 0.004632
Validation Loss: 0.00484191
Epoch [86/200], Train Loss: 0.004615
Validation Loss: 0.00482888
Epoch [87/200], Train Loss: 0.004615
Validation Loss: 0.00481669
Epoch [88/200], Train Loss: 0.004599
Validation Loss: 0.00480420
Epoch [89/200], Train Loss: 0.004597
Validation Loss: 0.00481767
Epoch [90/200], Train Loss: 0.004559
Validation Loss: 0.00478290
Epoch [91/200], Train Loss: 0.004606
Validation Loss: 0.00476456
Epoch [92/200], Train Loss: 0.004560
Validation Loss: 0.00474774
Epoch [93/200], Train Loss: 0.004716
Validation Loss: 0.00478114
Epoch [94/200], Train Loss: 0.004570
Validation Loss: 0.00480648
Epoch [95/200], Train Loss: 0.004544
Validation Loss: 0.00475671
Epoch [96/200], Train Loss: 0.004512
Validation Loss: 0.00473421
Epoch [97/200], Train Loss: 0.004530
Validation Loss: 0.00470070
Epoch [98/200], Train Loss: 0.004564
Validation Loss: 0.00469556
Epoch [99/200], Train Loss: 0.004525
Validation Loss: 0.00468361
Epoch [100/200], Train Loss: 0.004540
Validation Loss: 0.00467047
Epoch [101/200], Train Loss: 0.004475
Validation Loss: 0.00466183
Epoch [102/200], Train Loss: 0.004451
Validation Loss: 0.00473752
Epoch [103/200], Train Loss: 0.004462
Validation Loss: 0.00467365
Epoch [104/200], Train Loss: 0.004459
Validation Loss: 0.00463716
Epoch [105/200], Train Loss: 0.004472
Validation Loss: 0.00465763
Epoch [106/200], Train Loss: 0.004437
Validation Loss: 0.00463307
Epoch [107/200], Train Loss: 0.004426
Validation Loss: 0.00462117
Epoch [108/200], Train Loss: 0.004429
Validation Loss: 0.00460022
Epoch [109/200], Train Loss: 0.004373
Validation Loss: 0.00458081
Epoch [110/200], Train Loss: 0.004405
Validation Loss: 0.00457405
Epoch [111/200], Train Loss: 0.004388
Validation Loss: 0.00456033
Epoch [112/200], Train Loss: 0.004396
Validation Loss: 0.00455368
Epoch [113/200], Train Loss: 0.004367
Validation Loss: 0.00454156
Epoch [114/200], Train Loss: 0.004356
Validation Loss: 0.00467663
Epoch [115/200], Train Loss: 0.004352
Validation Loss: 0.00454027
Epoch [116/200], Train Loss: 0.004330
Validation Loss: 0.00454092
Epoch [117/200], Train Loss: 0.004290
Validation Loss: 0.00454675
Epoch [118/200], Train Loss: 0.004292
Validation Loss: 0.00453778
Epoch [119/200], Train Loss: 0.004335
Validation Loss: 0.00453520
Epoch [120/200], Train Loss: 0.004301
Validation Loss: 0.00450079
Epoch [121/200], Train Loss: 0.004328
Validation Loss: 0.00454277
Epoch [122/200], Train Loss: 0.004281
Validation Loss: 0.00449133
Epoch [123/200], Train Loss: 0.004201
Validation Loss: 0.00448164
Epoch [124/200], Train Loss: 0.004221
Validation Loss: 0.00447271
Epoch [125/200], Train Loss: 0.004224
Validation Loss: 0.00445485
Epoch [126/200], Train Loss: 0.004226
Validation Loss: 0.00444690
Epoch [127/200], Train Loss: 0.004197
Validation Loss: 0.00443069
Epoch [128/200], Train Loss: 0.004226
Validation Loss: 0.00442640
Epoch [129/200], Train Loss: 0.004164
Validation Loss: 0.00441502
Epoch [130/200], Train Loss: 0.004173
Validation Loss: 0.00440746
Epoch [131/200], Train Loss: 0.004160
Validation Loss: 0.00443217
Epoch [132/200], Train Loss: 0.004194
Validation Loss: 0.00439198
Epoch [133/200], Train Loss: 0.004173
Validation Loss: 0.00438157
Epoch [134/200], Train Loss: 0.004120
Validation Loss: 0.00436714
Epoch [135/200], Train Loss: 0.004107
Validation Loss: 0.00436489
Epoch [136/200], Train Loss: 0.004038
Validation Loss: 0.00436891
Epoch [137/200], Train Loss: 0.004100
Validation Loss: 0.00434853
Epoch [138/200], Train Loss: 0.004044
Validation Loss: 0.00435118
Epoch [139/200], Train Loss: 0.004047
Validation Loss: 0.00434637
Epoch [140/200], Train Loss: 0.004096
Validation Loss: 0.00439834
Epoch [141/200], Train Loss: 0.004116
Validation Loss: 0.00429535
Epoch [142/200], Train Loss: 0.004045
Validation Loss: 0.00427952
Epoch [143/200], Train Loss: 0.004051
Validation Loss: 0.00429961
Epoch [144/200], Train Loss: 0.004007
Validation Loss: 0.00426471
Epoch [145/200], Train Loss: 0.003962
Validation Loss: 0.00424551
Epoch [146/200], Train Loss: 0.003930
Validation Loss: 0.00422596
Epoch [147/200], Train Loss: 0.004043
Validation Loss: 0.00420585
Epoch [148/200], Train Loss: 0.003934
Validation Loss: 0.00419169
Epoch [149/200], Train Loss: 0.003972
Validation Loss: 0.00417881
Epoch [150/200], Train Loss: 0.003961
Validation Loss: 0.00417503
Epoch [151/200], Train Loss: 0.003963
Validation Loss: 0.00413856
Epoch [152/200], Train Loss: 0.003892
Validation Loss: 0.00412756
Epoch [153/200], Train Loss: 0.003821
Validation Loss: 0.00411598
Epoch [154/200], Train Loss: 0.003916
Validation Loss: 0.00409189
Epoch [155/200], Train Loss: 0.003839
Validation Loss: 0.00407550
Epoch [156/200], Train Loss: 0.003853
Validation Loss: 0.00406635
Epoch [157/200], Train Loss: 0.003849
Validation Loss: 0.00403319
Epoch [158/200], Train Loss: 0.003853
Validation Loss: 0.00402375
Epoch [159/200], Train Loss: 0.003854
Validation Loss: 0.00407945
Epoch [160/200], Train Loss: 0.003798
Validation Loss: 0.00398765
Epoch [161/200], Train Loss: 0.003771
Validation Loss: 0.00398040
Epoch [162/200], Train Loss: 0.003762
Validation Loss: 0.00394915
Epoch [163/200], Train Loss: 0.003759
Validation Loss: 0.00392661
Epoch [164/200], Train Loss: 0.003700
Validation Loss: 0.00391508
Epoch [165/200], Train Loss: 0.003682
Validation Loss: 0.00389497
Epoch [166/200], Train Loss: 0.003659
Validation Loss: 0.00391011
Epoch [167/200], Train Loss: 0.003672
Validation Loss: 0.00388473
Epoch [168/200], Train Loss: 0.003696
Validation Loss: 0.00384032
Epoch [169/200], Train Loss: 0.003659
Validation Loss: 0.00381704
Epoch [170/200], Train Loss: 0.003601
Validation Loss: 0.00378315
Epoch [171/200], Train Loss: 0.003589
Validation Loss: 0.00377051
Epoch [172/200], Train Loss: 0.003544
Validation Loss: 0.00376140
Epoch [173/200], Train Loss: 0.003548
Validation Loss: 0.00373803
Epoch [174/200], Train Loss: 0.003527
Validation Loss: 0.00370879
Epoch [175/200], Train Loss: 0.003515
Validation Loss: 0.00369761
Epoch [176/200], Train Loss: 0.003462
Validation Loss: 0.00368774
Epoch [177/200], Train Loss: 0.003445
Validation Loss: 0.00365817
Epoch [178/200], Train Loss: 0.003398
Validation Loss: 0.00363469
Epoch [179/200], Train Loss: 0.003451
Validation Loss: 0.00362375
Epoch [180/200], Train Loss: 0.003413
Validation Loss: 0.00360279
Epoch [181/200], Train Loss: 0.003409
Validation Loss: 0.00359072
Epoch [182/200], Train Loss: 0.003368
Validation Loss: 0.00357200
Epoch [183/200], Train Loss: 0.003347
Validation Loss: 0.00359243
Epoch [184/200], Train Loss: 0.003353
Validation Loss: 0.00359757
Epoch [185/200], Train Loss: 0.003345
Validation Loss: 0.00362849
Epoch [186/200], Train Loss: 0.003349
Validation Loss: 0.00352463
Epoch [187/200], Train Loss: 0.003288
Validation Loss: 0.00349624
Epoch [188/200], Train Loss: 0.003277
Validation Loss: 0.00346356
Epoch [189/200], Train Loss: 0.003284
Validation Loss: 0.00344570
Epoch [190/200], Train Loss: 0.003254
Validation Loss: 0.00344554
Epoch [191/200], Train Loss: 0.003213
Validation Loss: 0.00342019
Epoch [192/200], Train Loss: 0.003187
Validation Loss: 0.00339618
Epoch [193/200], Train Loss: 0.003219
Validation Loss: 0.00338142
Epoch [194/200], Train Loss: 0.003185
Validation Loss: 0.00340047
Epoch [195/200], Train Loss: 0.003175
Validation Loss: 0.00333544
Epoch [196/200], Train Loss: 0.003125
Validation Loss: 0.00334452
Epoch [197/200], Train Loss: 0.003136
Validation Loss: 0.00330467
Epoch [198/200], Train Loss: 0.003133
Validation Loss: 0.00329868
Epoch [199/200], Train Loss: 0.003086
Validation Loss: 0.00328337
Epoch [200/200], Train Loss: 0.003087
Validation Loss: 0.00326696

Evaluating model for: Lamp
Run 125/144 completed in 643.44 seconds with: {'MAE': np.float32(1.9794444), 'MSE': np.float32(87.32804), 'RMSE': np.float32(9.344948), 'SAE': np.float32(0.2248455), 'NDE': np.float32(0.74665344)}

Run 126/144: hidden=256, seq_len=720, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.004885
Validation Loss: 0.00511088
Epoch [2/200], Train Loss: 0.004910
Validation Loss: 0.00510149
Epoch [3/200], Train Loss: 0.004812
Validation Loss: 0.00508649
Epoch [4/200], Train Loss: 0.004810
Validation Loss: 0.00506238
Epoch [5/200], Train Loss: 0.004837
Validation Loss: 0.00503990
Epoch [6/200], Train Loss: 0.004781
Validation Loss: 0.00502821
Epoch [7/200], Train Loss: 0.004808
Validation Loss: 0.00503047
Epoch [8/200], Train Loss: 0.004769
Validation Loss: 0.00502707
Epoch [9/200], Train Loss: 0.004836
Validation Loss: 0.00502622
Epoch [10/200], Train Loss: 0.004789
Validation Loss: 0.00502514
Epoch [11/200], Train Loss: 0.004846
Validation Loss: 0.00502349
Epoch [12/200], Train Loss: 0.004811
Validation Loss: 0.00502463
Epoch [13/200], Train Loss: 0.004834
Validation Loss: 0.00502462
Epoch [14/200], Train Loss: 0.004746
Validation Loss: 0.00503098
Epoch [15/200], Train Loss: 0.004826
Validation Loss: 0.00502742
Epoch [16/200], Train Loss: 0.004878
Validation Loss: 0.00502808
Epoch [17/200], Train Loss: 0.004875
Validation Loss: 0.00501976
Epoch [18/200], Train Loss: 0.004863
Validation Loss: 0.00502024
Epoch [19/200], Train Loss: 0.004842
Validation Loss: 0.00501775
Epoch [20/200], Train Loss: 0.004811
Validation Loss: 0.00502972
Epoch [21/200], Train Loss: 0.004811
Validation Loss: 0.00501777
Epoch [22/200], Train Loss: 0.004745
Validation Loss: 0.00501336
Epoch [23/200], Train Loss: 0.004838
Validation Loss: 0.00501199
Epoch [24/200], Train Loss: 0.004818
Validation Loss: 0.00501432
Epoch [25/200], Train Loss: 0.004789
Validation Loss: 0.00501178
Epoch [26/200], Train Loss: 0.004861
Validation Loss: 0.00501470
Epoch [27/200], Train Loss: 0.004790
Validation Loss: 0.00501522
Epoch [28/200], Train Loss: 0.004774
Validation Loss: 0.00500908
Epoch [29/200], Train Loss: 0.004837
Validation Loss: 0.00500881
Epoch [30/200], Train Loss: 0.004766
Validation Loss: 0.00500936
Epoch [31/200], Train Loss: 0.004827
Validation Loss: 0.00500757
Epoch [32/200], Train Loss: 0.004821
Validation Loss: 0.00500697
Epoch [33/200], Train Loss: 0.004827
Validation Loss: 0.00501294
Epoch [34/200], Train Loss: 0.004750
Validation Loss: 0.00502080
Epoch [35/200], Train Loss: 0.004705
Validation Loss: 0.00501888
Epoch [36/200], Train Loss: 0.004835
Validation Loss: 0.00500787
Epoch [37/200], Train Loss: 0.004810
Validation Loss: 0.00500494
Epoch [38/200], Train Loss: 0.004854
Validation Loss: 0.00500358
Epoch [39/200], Train Loss: 0.004795
Validation Loss: 0.00500286
Epoch [40/200], Train Loss: 0.004838
Validation Loss: 0.00500511
Epoch [41/200], Train Loss: 0.004807
Validation Loss: 0.00501169
Epoch [42/200], Train Loss: 0.004805
Validation Loss: 0.00501445
Epoch [43/200], Train Loss: 0.004760
Validation Loss: 0.00500185
Epoch [44/200], Train Loss: 0.004774
Validation Loss: 0.00500349
Epoch [45/200], Train Loss: 0.004772
Validation Loss: 0.00500088
Epoch [46/200], Train Loss: 0.004728
Validation Loss: 0.00500017
Epoch [47/200], Train Loss: 0.004757
Validation Loss: 0.00499945
Epoch [48/200], Train Loss: 0.004730
Validation Loss: 0.00500186
Epoch [49/200], Train Loss: 0.004781
Validation Loss: 0.00499889
Epoch [50/200], Train Loss: 0.004735
Validation Loss: 0.00499950
Epoch [51/200], Train Loss: 0.004886
Validation Loss: 0.00499663
Epoch [52/200], Train Loss: 0.004798
Validation Loss: 0.00499974
Epoch [53/200], Train Loss: 0.004764
Validation Loss: 0.00499497
Epoch [54/200], Train Loss: 0.004841
Validation Loss: 0.00499521
Epoch [55/200], Train Loss: 0.004750
Validation Loss: 0.00499595
Epoch [56/200], Train Loss: 0.004828
Validation Loss: 0.00500294
Epoch [57/200], Train Loss: 0.004822
Validation Loss: 0.00501381
Epoch [58/200], Train Loss: 0.004746
Validation Loss: 0.00499414
Epoch [59/200], Train Loss: 0.004762
Validation Loss: 0.00499562
Epoch [60/200], Train Loss: 0.004791
Validation Loss: 0.00499294
Epoch [61/200], Train Loss: 0.004842
Validation Loss: 0.00499246
Epoch [62/200], Train Loss: 0.004796
Validation Loss: 0.00499699
Epoch [63/200], Train Loss: 0.004791
Validation Loss: 0.00499155
Epoch [64/200], Train Loss: 0.004776
Validation Loss: 0.00499745
Epoch [65/200], Train Loss: 0.004841
Validation Loss: 0.00499465
Epoch [66/200], Train Loss: 0.004744
Validation Loss: 0.00499047
Epoch [67/200], Train Loss: 0.004837
Validation Loss: 0.00498827
Epoch [68/200], Train Loss: 0.004731
Validation Loss: 0.00499159
Epoch [69/200], Train Loss: 0.004848
Validation Loss: 0.00498970
Epoch [70/200], Train Loss: 0.004744
Validation Loss: 0.00499846
Epoch [71/200], Train Loss: 0.004692
Validation Loss: 0.00499012
Epoch [72/200], Train Loss: 0.004820
Validation Loss: 0.00498930
Epoch [73/200], Train Loss: 0.004737
Validation Loss: 0.00499379
Epoch [74/200], Train Loss: 0.004786
Validation Loss: 0.00498716
Epoch [75/200], Train Loss: 0.004778
Validation Loss: 0.00498682
Epoch [76/200], Train Loss: 0.004798
Validation Loss: 0.00498757
Epoch [77/200], Train Loss: 0.004760
Validation Loss: 0.00498698
Epoch [78/200], Train Loss: 0.004792
Validation Loss: 0.00498619
Epoch [79/200], Train Loss: 0.004772
Validation Loss: 0.00499093
Epoch [80/200], Train Loss: 0.004754
Validation Loss: 0.00498764
Epoch [81/200], Train Loss: 0.004802
Validation Loss: 0.00498371
Epoch [82/200], Train Loss: 0.004880
Validation Loss: 0.00498401
Epoch [83/200], Train Loss: 0.004713
Validation Loss: 0.00498529
Epoch [84/200], Train Loss: 0.004768
Validation Loss: 0.00498254
Epoch [85/200], Train Loss: 0.004810
Validation Loss: 0.00498301
Epoch [86/200], Train Loss: 0.004743
Validation Loss: 0.00498128
Epoch [87/200], Train Loss: 0.004743
Validation Loss: 0.00498321
Epoch [88/200], Train Loss: 0.004750
Validation Loss: 0.00497780
Epoch [89/200], Train Loss: 0.004788
Validation Loss: 0.00497857
Epoch [90/200], Train Loss: 0.004729
Validation Loss: 0.00497997
Epoch [91/200], Train Loss: 0.004750
Validation Loss: 0.00497733
Epoch [92/200], Train Loss: 0.004714
Validation Loss: 0.00498019
Epoch [93/200], Train Loss: 0.004760
Validation Loss: 0.00498033
Epoch [94/200], Train Loss: 0.004724
Validation Loss: 0.00497280
Epoch [95/200], Train Loss: 0.004753
Validation Loss: 0.00497419
Epoch [96/200], Train Loss: 0.004815
Validation Loss: 0.00497744
Epoch [97/200], Train Loss: 0.004756
Validation Loss: 0.00497147
Epoch [98/200], Train Loss: 0.004799
Validation Loss: 0.00496932
Epoch [99/200], Train Loss: 0.004751
Validation Loss: 0.00496379
Epoch [100/200], Train Loss: 0.004855
Validation Loss: 0.00496085
Epoch [101/200], Train Loss: 0.004794
Validation Loss: 0.00495970
Epoch [102/200], Train Loss: 0.004788
Validation Loss: 0.00495977
Epoch [103/200], Train Loss: 0.004760
Validation Loss: 0.00495088
Epoch [104/200], Train Loss: 0.004735
Validation Loss: 0.00495034
Epoch [105/200], Train Loss: 0.004688
Validation Loss: 0.00494112
Epoch [106/200], Train Loss: 0.004710
Validation Loss: 0.00494005
Epoch [107/200], Train Loss: 0.004661
Validation Loss: 0.00492899
Epoch [108/200], Train Loss: 0.004709
Validation Loss: 0.00492494
Epoch [109/200], Train Loss: 0.004710
Validation Loss: 0.00491743
Epoch [110/200], Train Loss: 0.004715
Validation Loss: 0.00490728
Epoch [111/200], Train Loss: 0.004599
Validation Loss: 0.00489642
Epoch [112/200], Train Loss: 0.004642
Validation Loss: 0.00488187
Epoch [113/200], Train Loss: 0.004718
Validation Loss: 0.00488062
Epoch [114/200], Train Loss: 0.004658
Validation Loss: 0.00485708
Epoch [115/200], Train Loss: 0.004668
Validation Loss: 0.00493376
Epoch [116/200], Train Loss: 0.004686
Validation Loss: 0.00491210
Epoch [117/200], Train Loss: 0.004688
Validation Loss: 0.00490298
Epoch [118/200], Train Loss: 0.004688
Validation Loss: 0.00487831
Epoch [119/200], Train Loss: 0.004647
Validation Loss: 0.00483960
Epoch [120/200], Train Loss: 0.004597
Validation Loss: 0.00482699
Epoch [121/200], Train Loss: 0.004582
Validation Loss: 0.00485347
Epoch [122/200], Train Loss: 0.004625
Validation Loss: 0.00480046
Epoch [123/200], Train Loss: 0.004547
Validation Loss: 0.00479562
Epoch [124/200], Train Loss: 0.004604
Validation Loss: 0.00477791
Epoch [125/200], Train Loss: 0.004568
Validation Loss: 0.00475471
Epoch [126/200], Train Loss: 0.004602
Validation Loss: 0.00474257
Epoch [127/200], Train Loss: 0.004548
Validation Loss: 0.00472288
Epoch [128/200], Train Loss: 0.004526
Validation Loss: 0.00471710
Epoch [129/200], Train Loss: 0.004514
Validation Loss: 0.00471885
Epoch [130/200], Train Loss: 0.004543
Validation Loss: 0.00474452
Epoch [131/200], Train Loss: 0.004461
Validation Loss: 0.00469721
Epoch [132/200], Train Loss: 0.004460
Validation Loss: 0.00472287
Epoch [133/200], Train Loss: 0.004459
Validation Loss: 0.00467600
Epoch [134/200], Train Loss: 0.004482
Validation Loss: 0.00469535
Epoch [135/200], Train Loss: 0.004526
Validation Loss: 0.00472796
Epoch [136/200], Train Loss: 0.004505
Validation Loss: 0.00467714
Epoch [137/200], Train Loss: 0.004503
Validation Loss: 0.00471297
Epoch [138/200], Train Loss: 0.004483
Validation Loss: 0.00465661
Epoch [139/200], Train Loss: 0.004427
Validation Loss: 0.00463884
Epoch [140/200], Train Loss: 0.004394
Validation Loss: 0.00463877
Epoch [141/200], Train Loss: 0.004466
Validation Loss: 0.00463077
Epoch [142/200], Train Loss: 0.004368
Validation Loss: 0.00467676
Epoch [143/200], Train Loss: 0.004452
Validation Loss: 0.00473067
Epoch [144/200], Train Loss: 0.004461
Validation Loss: 0.00463665
Epoch [145/200], Train Loss: 0.004450
Validation Loss: 0.00463269
Epoch [146/200], Train Loss: 0.004444
Validation Loss: 0.00461691
Epoch [147/200], Train Loss: 0.004421
Validation Loss: 0.00459457
Epoch [148/200], Train Loss: 0.004371
Validation Loss: 0.00460203
Epoch [149/200], Train Loss: 0.004339
Validation Loss: 0.00457081
Epoch [150/200], Train Loss: 0.004283
Validation Loss: 0.00457062
Epoch [151/200], Train Loss: 0.004317
Validation Loss: 0.00455973
Epoch [152/200], Train Loss: 0.004349
Validation Loss: 0.00454969
Epoch [153/200], Train Loss: 0.004362
Validation Loss: 0.00453895
Epoch [154/200], Train Loss: 0.004280
Validation Loss: 0.00454164
Epoch [155/200], Train Loss: 0.004286
Validation Loss: 0.00452614
Epoch [156/200], Train Loss: 0.004347
Validation Loss: 0.00452373
Epoch [157/200], Train Loss: 0.004274
Validation Loss: 0.00450850
Epoch [158/200], Train Loss: 0.004292
Validation Loss: 0.00450643
Epoch [159/200], Train Loss: 0.004285
Validation Loss: 0.00449437
Epoch [160/200], Train Loss: 0.004298
Validation Loss: 0.00448306
Epoch [161/200], Train Loss: 0.004298
Validation Loss: 0.00447716
Epoch [162/200], Train Loss: 0.004211
Validation Loss: 0.00449635
Epoch [163/200], Train Loss: 0.004258
Validation Loss: 0.00446838
Epoch [164/200], Train Loss: 0.004158
Validation Loss: 0.00444244
Epoch [165/200], Train Loss: 0.004159
Validation Loss: 0.00443631
Epoch [166/200], Train Loss: 0.004231
Validation Loss: 0.00442891
Epoch [167/200], Train Loss: 0.004176
Validation Loss: 0.00441146
Epoch [168/200], Train Loss: 0.004181
Validation Loss: 0.00440351
Epoch [169/200], Train Loss: 0.004277
Validation Loss: 0.00440402
Epoch [170/200], Train Loss: 0.004124
Validation Loss: 0.00438601
Epoch [171/200], Train Loss: 0.004189
Validation Loss: 0.00437730
Epoch [172/200], Train Loss: 0.004170
Validation Loss: 0.00437258
Epoch [173/200], Train Loss: 0.004165
Validation Loss: 0.00437158
Epoch [174/200], Train Loss: 0.004036
Validation Loss: 0.00433733
Epoch [175/200], Train Loss: 0.004155
Validation Loss: 0.00431890
Epoch [176/200], Train Loss: 0.004135
Validation Loss: 0.00430266
Epoch [177/200], Train Loss: 0.004068
Validation Loss: 0.00430679
Epoch [178/200], Train Loss: 0.004066
Validation Loss: 0.00427964
Epoch [179/200], Train Loss: 0.003997
Validation Loss: 0.00425678
Epoch [180/200], Train Loss: 0.004014
Validation Loss: 0.00424932
Epoch [181/200], Train Loss: 0.004001
Validation Loss: 0.00422941
Epoch [182/200], Train Loss: 0.003998
Validation Loss: 0.00421879
Epoch [183/200], Train Loss: 0.003945
Validation Loss: 0.00420392
Epoch [184/200], Train Loss: 0.004030
Validation Loss: 0.00418640
Epoch [185/200], Train Loss: 0.003949
Validation Loss: 0.00422195
Epoch [186/200], Train Loss: 0.003983
Validation Loss: 0.00415358
Epoch [187/200], Train Loss: 0.003945
Validation Loss: 0.00414006
Epoch [188/200], Train Loss: 0.003860
Validation Loss: 0.00412779
Epoch [189/200], Train Loss: 0.003873
Validation Loss: 0.00409789
Epoch [190/200], Train Loss: 0.003871
Validation Loss: 0.00409864
Epoch [191/200], Train Loss: 0.003815
Validation Loss: 0.00408568
Epoch [192/200], Train Loss: 0.003872
Validation Loss: 0.00404107
Epoch [193/200], Train Loss: 0.003789
Validation Loss: 0.00403799
Epoch [194/200], Train Loss: 0.003892
Validation Loss: 0.00401366
Epoch [195/200], Train Loss: 0.003815
Validation Loss: 0.00398161
Epoch [196/200], Train Loss: 0.003784
Validation Loss: 0.00395164
Epoch [197/200], Train Loss: 0.003755
Validation Loss: 0.00394109
Epoch [198/200], Train Loss: 0.003683
Validation Loss: 0.00390830
Epoch [199/200], Train Loss: 0.003755
Validation Loss: 0.00391537
Epoch [200/200], Train Loss: 0.003716
Validation Loss: 0.00387072

Evaluating model for: Lamp
Run 126/144 completed in 830.16 seconds with: {'MAE': np.float32(2.0499218), 'MSE': np.float32(109.04603), 'RMSE': np.float32(10.442511), 'SAE': np.float32(0.23224558), 'NDE': np.float32(0.8343475)}

Run 127/144: hidden=256, seq_len=720, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.005009
Validation Loss: 0.00515135
Epoch [2/200], Train Loss: 0.004897
Validation Loss: 0.00512953
Epoch [3/200], Train Loss: 0.004937
Validation Loss: 0.00511720
Epoch [4/200], Train Loss: 0.004921
Validation Loss: 0.00511442
Epoch [5/200], Train Loss: 0.004799
Validation Loss: 0.00510912
Epoch [6/200], Train Loss: 0.004844
Validation Loss: 0.00510039
Epoch [7/200], Train Loss: 0.004891
Validation Loss: 0.00508104
Epoch [8/200], Train Loss: 0.004899
Validation Loss: 0.00504215
Epoch [9/200], Train Loss: 0.004816
Validation Loss: 0.00502934
Epoch [10/200], Train Loss: 0.004797
Validation Loss: 0.00503248
Epoch [11/200], Train Loss: 0.004856
Validation Loss: 0.00503964
Epoch [12/200], Train Loss: 0.004859
Validation Loss: 0.00503444
Epoch [13/200], Train Loss: 0.004872
Validation Loss: 0.00502405
Epoch [14/200], Train Loss: 0.004869
Validation Loss: 0.00502317
Epoch [15/200], Train Loss: 0.004881
Validation Loss: 0.00502229
Epoch [16/200], Train Loss: 0.004729
Validation Loss: 0.00503256
Epoch [17/200], Train Loss: 0.004834
Validation Loss: 0.00502351
Epoch [18/200], Train Loss: 0.004888
Validation Loss: 0.00501824
Epoch [19/200], Train Loss: 0.004867
Validation Loss: 0.00501631
Epoch [20/200], Train Loss: 0.004780
Validation Loss: 0.00502075
Epoch [21/200], Train Loss: 0.004898
Validation Loss: 0.00501544
Epoch [22/200], Train Loss: 0.004814
Validation Loss: 0.00501307
Epoch [23/200], Train Loss: 0.004761
Validation Loss: 0.00502734
Epoch [24/200], Train Loss: 0.004848
Validation Loss: 0.00502084
Epoch [25/200], Train Loss: 0.004704
Validation Loss: 0.00501130
Epoch [26/200], Train Loss: 0.004778
Validation Loss: 0.00501180
Epoch [27/200], Train Loss: 0.004802
Validation Loss: 0.00500875
Epoch [28/200], Train Loss: 0.004727
Validation Loss: 0.00501925
Epoch [29/200], Train Loss: 0.004766
Validation Loss: 0.00500883
Epoch [30/200], Train Loss: 0.004760
Validation Loss: 0.00500602
Epoch [31/200], Train Loss: 0.004774
Validation Loss: 0.00500403
Epoch [32/200], Train Loss: 0.004774
Validation Loss: 0.00500703
Epoch [33/200], Train Loss: 0.004729
Validation Loss: 0.00502946
Epoch [34/200], Train Loss: 0.004771
Validation Loss: 0.00500332
Epoch [35/200], Train Loss: 0.004800
Validation Loss: 0.00500133
Epoch [36/200], Train Loss: 0.004746
Validation Loss: 0.00501527
Epoch [37/200], Train Loss: 0.004794
Validation Loss: 0.00500157
Epoch [38/200], Train Loss: 0.004820
Validation Loss: 0.00499867
Epoch [39/200], Train Loss: 0.004782
Validation Loss: 0.00499870
Epoch [40/200], Train Loss: 0.004780
Validation Loss: 0.00499640
Epoch [41/200], Train Loss: 0.004749
Validation Loss: 0.00501670
Epoch [42/200], Train Loss: 0.004798
Validation Loss: 0.00499704
Epoch [43/200], Train Loss: 0.004772
Validation Loss: 0.00499545
Epoch [44/200], Train Loss: 0.004750
Validation Loss: 0.00499440
Epoch [45/200], Train Loss: 0.004847
Validation Loss: 0.00499582
Epoch [46/200], Train Loss: 0.004835
Validation Loss: 0.00499386
Epoch [47/200], Train Loss: 0.004829
Validation Loss: 0.00499463
Epoch [48/200], Train Loss: 0.004774
Validation Loss: 0.00499247
Epoch [49/200], Train Loss: 0.004800
Validation Loss: 0.00499393
Epoch [50/200], Train Loss: 0.004831
Validation Loss: 0.00499244
Epoch [51/200], Train Loss: 0.004889
Validation Loss: 0.00498956
Epoch [52/200], Train Loss: 0.004765
Validation Loss: 0.00500579
Epoch [53/200], Train Loss: 0.004804
Validation Loss: 0.00499284
Epoch [54/200], Train Loss: 0.004777
Validation Loss: 0.00499090
Epoch [55/200], Train Loss: 0.004753
Validation Loss: 0.00498972
Epoch [56/200], Train Loss: 0.004830
Validation Loss: 0.00499003
Epoch [57/200], Train Loss: 0.004783
Validation Loss: 0.00499112
Epoch [58/200], Train Loss: 0.004739
Validation Loss: 0.00498830
Epoch [59/200], Train Loss: 0.004729
Validation Loss: 0.00498840
Epoch [60/200], Train Loss: 0.004758
Validation Loss: 0.00499622
Epoch [61/200], Train Loss: 0.004786
Validation Loss: 0.00498909
Epoch [62/200], Train Loss: 0.004770
Validation Loss: 0.00498907
Epoch [63/200], Train Loss: 0.004740
Validation Loss: 0.00498714
Epoch [64/200], Train Loss: 0.004802
Validation Loss: 0.00498748
Epoch [65/200], Train Loss: 0.004751
Validation Loss: 0.00498649
Epoch [66/200], Train Loss: 0.004763
Validation Loss: 0.00498675
Epoch [67/200], Train Loss: 0.004827
Validation Loss: 0.00498852
Epoch [68/200], Train Loss: 0.004836
Validation Loss: 0.00498318
Epoch [69/200], Train Loss: 0.004824
Validation Loss: 0.00499437
Epoch [70/200], Train Loss: 0.004777
Validation Loss: 0.00498568
Epoch [71/200], Train Loss: 0.004749
Validation Loss: 0.00498553
Epoch [72/200], Train Loss: 0.004861
Validation Loss: 0.00498637
Epoch [73/200], Train Loss: 0.004790
Validation Loss: 0.00498823
Epoch [74/200], Train Loss: 0.004744
Validation Loss: 0.00498450
Epoch [75/200], Train Loss: 0.004859
Validation Loss: 0.00498485
Epoch [76/200], Train Loss: 0.004738
Validation Loss: 0.00498928
Epoch [77/200], Train Loss: 0.004808
Validation Loss: 0.00498395
Epoch [78/200], Train Loss: 0.004788
Validation Loss: 0.00498780
Early stopping triggered

Evaluating model for: Lamp
Run 127/144 completed in 390.71 seconds with: {'MAE': np.float32(2.3231547), 'MSE': np.float32(150.8115), 'RMSE': np.float32(12.280533), 'SAE': np.float32(0.177505), 'NDE': np.float32(0.9812036)}

Run 128/144: hidden=256, seq_len=720, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1854 windows

Epoch [1/200], Train Loss: 0.004981
Validation Loss: 0.00513024
Epoch [2/200], Train Loss: 0.004915
Validation Loss: 0.00512237
Epoch [3/200], Train Loss: 0.004870
Validation Loss: 0.00511909
Epoch [4/200], Train Loss: 0.004940
Validation Loss: 0.00511923
Epoch [5/200], Train Loss: 0.004947
Validation Loss: 0.00511722
Epoch [6/200], Train Loss: 0.004839
Validation Loss: 0.00511465
Epoch [7/200], Train Loss: 0.004885
Validation Loss: 0.00511026
Epoch [8/200], Train Loss: 0.004864
Validation Loss: 0.00509508
Epoch [9/200], Train Loss: 0.004818
Validation Loss: 0.00504813
Epoch [10/200], Train Loss: 0.004785
Validation Loss: 0.00503846
Epoch [11/200], Train Loss: 0.004862
Validation Loss: 0.00504119
Epoch [12/200], Train Loss: 0.004763
Validation Loss: 0.00502461
Epoch [13/200], Train Loss: 0.004816
Validation Loss: 0.00502212
Epoch [14/200], Train Loss: 0.004848
Validation Loss: 0.00502953
Epoch [15/200], Train Loss: 0.004834
Validation Loss: 0.00502364
Epoch [16/200], Train Loss: 0.004767
Validation Loss: 0.00502693
Epoch [17/200], Train Loss: 0.004823
Validation Loss: 0.00501751
Epoch [18/200], Train Loss: 0.004818
Validation Loss: 0.00502351
Epoch [19/200], Train Loss: 0.004748
Validation Loss: 0.00501444
Epoch [20/200], Train Loss: 0.004788
Validation Loss: 0.00501820
Epoch [21/200], Train Loss: 0.004775
Validation Loss: 0.00501327
Epoch [22/200], Train Loss: 0.004873
Validation Loss: 0.00501528
Epoch [23/200], Train Loss: 0.004780
Validation Loss: 0.00501167
Epoch [24/200], Train Loss: 0.004766
Validation Loss: 0.00501683
Epoch [25/200], Train Loss: 0.004744
Validation Loss: 0.00501436
Epoch [26/200], Train Loss: 0.004742
Validation Loss: 0.00501217
Epoch [27/200], Train Loss: 0.004794
Validation Loss: 0.00501074
Epoch [28/200], Train Loss: 0.004828
Validation Loss: 0.00500992
Epoch [29/200], Train Loss: 0.004771
Validation Loss: 0.00501239
Epoch [30/200], Train Loss: 0.004842
Validation Loss: 0.00501285
Epoch [31/200], Train Loss: 0.004763
Validation Loss: 0.00501179
Epoch [32/200], Train Loss: 0.004762
Validation Loss: 0.00500613
Epoch [33/200], Train Loss: 0.004806
Validation Loss: 0.00500459
Epoch [34/200], Train Loss: 0.004820
Validation Loss: 0.00500464
Epoch [35/200], Train Loss: 0.004766
Validation Loss: 0.00501249
Epoch [36/200], Train Loss: 0.004819
Validation Loss: 0.00500393
Epoch [37/200], Train Loss: 0.004745
Validation Loss: 0.00500229
Epoch [38/200], Train Loss: 0.004821
Validation Loss: 0.00500073
Epoch [39/200], Train Loss: 0.004788
Validation Loss: 0.00500884
Epoch [40/200], Train Loss: 0.004867
Validation Loss: 0.00500156
Epoch [41/200], Train Loss: 0.004811
Validation Loss: 0.00501348
Epoch [42/200], Train Loss: 0.004758
Validation Loss: 0.00500695
Epoch [43/200], Train Loss: 0.004759
Validation Loss: 0.00499648
Epoch [44/200], Train Loss: 0.004716
Validation Loss: 0.00499571
Epoch [45/200], Train Loss: 0.004748
Validation Loss: 0.00499287
Epoch [46/200], Train Loss: 0.004773
Validation Loss: 0.00499346
Epoch [47/200], Train Loss: 0.004800
Validation Loss: 0.00499418
Epoch [48/200], Train Loss: 0.004802
Validation Loss: 0.00500054
Epoch [49/200], Train Loss: 0.004738
Validation Loss: 0.00499647
Epoch [50/200], Train Loss: 0.004751
Validation Loss: 0.00499374
Epoch [51/200], Train Loss: 0.004760
Validation Loss: 0.00499282
Epoch [52/200], Train Loss: 0.004783
Validation Loss: 0.00499685
Epoch [53/200], Train Loss: 0.004838
Validation Loss: 0.00499242
Epoch [54/200], Train Loss: 0.004852
Validation Loss: 0.00500098
Epoch [55/200], Train Loss: 0.004825
Validation Loss: 0.00498972
Epoch [56/200], Train Loss: 0.004776
Validation Loss: 0.00498994
Epoch [57/200], Train Loss: 0.004753
Validation Loss: 0.00499025
Epoch [58/200], Train Loss: 0.004787
Validation Loss: 0.00499121
Epoch [59/200], Train Loss: 0.004763
Validation Loss: 0.00498935
Epoch [60/200], Train Loss: 0.004795
Validation Loss: 0.00499074
Epoch [61/200], Train Loss: 0.004773
Validation Loss: 0.00500237
Epoch [62/200], Train Loss: 0.004909
Validation Loss: 0.00499122
Epoch [63/200], Train Loss: 0.004750
Validation Loss: 0.00499026
Epoch [64/200], Train Loss: 0.004823
Validation Loss: 0.00498858
Epoch [65/200], Train Loss: 0.004797
Validation Loss: 0.00499022
Epoch [66/200], Train Loss: 0.004788
Validation Loss: 0.00498734
Epoch [67/200], Train Loss: 0.004800
Validation Loss: 0.00498757
Epoch [68/200], Train Loss: 0.004775
Validation Loss: 0.00498939
Epoch [69/200], Train Loss: 0.004797
Validation Loss: 0.00499045
Epoch [70/200], Train Loss: 0.004799
Validation Loss: 0.00498733
Epoch [71/200], Train Loss: 0.004823
Validation Loss: 0.00498798
Epoch [72/200], Train Loss: 0.004740
Validation Loss: 0.00498661
Epoch [73/200], Train Loss: 0.004785
Validation Loss: 0.00499029
Epoch [74/200], Train Loss: 0.004809
Validation Loss: 0.00498761
Epoch [75/200], Train Loss: 0.004785
Validation Loss: 0.00499796
Epoch [76/200], Train Loss: 0.004757
Validation Loss: 0.00498684
Epoch [77/200], Train Loss: 0.004788
Validation Loss: 0.00498834
Epoch [78/200], Train Loss: 0.004795
Validation Loss: 0.00498630
Epoch [79/200], Train Loss: 0.004749
Validation Loss: 0.00498501
Epoch [80/200], Train Loss: 0.004764
Validation Loss: 0.00498828
Epoch [81/200], Train Loss: 0.004840
Validation Loss: 0.00498904
Epoch [82/200], Train Loss: 0.004768
Validation Loss: 0.00498598
Epoch [83/200], Train Loss: 0.004745
Validation Loss: 0.00498429
Epoch [84/200], Train Loss: 0.004877
Validation Loss: 0.00498886
Epoch [85/200], Train Loss: 0.004717
Validation Loss: 0.00498810
Epoch [86/200], Train Loss: 0.004743
Validation Loss: 0.00498586
Epoch [87/200], Train Loss: 0.004708
Validation Loss: 0.00498549
Epoch [88/200], Train Loss: 0.004758
Validation Loss: 0.00498350
Epoch [89/200], Train Loss: 0.004817
Validation Loss: 0.00498591
Epoch [90/200], Train Loss: 0.004774
Validation Loss: 0.00498849
Epoch [91/200], Train Loss: 0.004737
Validation Loss: 0.00498425
Epoch [92/200], Train Loss: 0.004778
Validation Loss: 0.00498460
Epoch [93/200], Train Loss: 0.004795
Validation Loss: 0.00499186
Epoch [94/200], Train Loss: 0.004778
Validation Loss: 0.00498451
Epoch [95/200], Train Loss: 0.004789
Validation Loss: 0.00498393
Epoch [96/200], Train Loss: 0.004765
Validation Loss: 0.00498800
Epoch [97/200], Train Loss: 0.004767
Validation Loss: 0.00498404
Epoch [98/200], Train Loss: 0.004790
Validation Loss: 0.00498633
Early stopping triggered

Evaluating model for: Lamp
Run 128/144 completed in 677.80 seconds with: {'MAE': np.float32(2.3563232), 'MSE': np.float32(150.76917), 'RMSE': np.float32(12.27881), 'SAE': np.float32(0.15135095), 'NDE': np.float32(0.9810662)}

Run 129/144: hidden=256, seq_len=720, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.005658
Validation Loss: 0.00369532
Epoch [2/200], Train Loss: 0.005011
Validation Loss: 0.00360875
Epoch [3/200], Train Loss: 0.005005
Validation Loss: 0.00368821
Epoch [4/200], Train Loss: 0.004988
Validation Loss: 0.00359460
Epoch [5/200], Train Loss: 0.004952
Validation Loss: 0.00355784
Epoch [6/200], Train Loss: 0.004978
Validation Loss: 0.00355620
Epoch [7/200], Train Loss: 0.004929
Validation Loss: 0.00355209
Epoch [8/200], Train Loss: 0.004913
Validation Loss: 0.00355833
Epoch [9/200], Train Loss: 0.004941
Validation Loss: 0.00355430
Epoch [10/200], Train Loss: 0.004899
Validation Loss: 0.00354239
Epoch [11/200], Train Loss: 0.004870
Validation Loss: 0.00353545
Epoch [12/200], Train Loss: 0.004925
Validation Loss: 0.00353230
Epoch [13/200], Train Loss: 0.004883
Validation Loss: 0.00353181
Epoch [14/200], Train Loss: 0.004884
Validation Loss: 0.00352605
Epoch [15/200], Train Loss: 0.004867
Validation Loss: 0.00351877
Epoch [16/200], Train Loss: 0.004863
Validation Loss: 0.00351735
Epoch [17/200], Train Loss: 0.004880
Validation Loss: 0.00351503
Epoch [18/200], Train Loss: 0.004900
Validation Loss: 0.00351270
Epoch [19/200], Train Loss: 0.004907
Validation Loss: 0.00351058
Epoch [20/200], Train Loss: 0.004875
Validation Loss: 0.00351301
Epoch [21/200], Train Loss: 0.004904
Validation Loss: 0.00350872
Epoch [22/200], Train Loss: 0.004878
Validation Loss: 0.00350968
Epoch [23/200], Train Loss: 0.004854
Validation Loss: 0.00350978
Epoch [24/200], Train Loss: 0.004854
Validation Loss: 0.00350788
Epoch [25/200], Train Loss: 0.004881
Validation Loss: 0.00350835
Epoch [26/200], Train Loss: 0.004872
Validation Loss: 0.00350936
Epoch [27/200], Train Loss: 0.004874
Validation Loss: 0.00350670
Epoch [28/200], Train Loss: 0.004850
Validation Loss: 0.00350718
Epoch [29/200], Train Loss: 0.004835
Validation Loss: 0.00350618
Epoch [30/200], Train Loss: 0.004855
Validation Loss: 0.00350696
Epoch [31/200], Train Loss: 0.004825
Validation Loss: 0.00350761
Epoch [32/200], Train Loss: 0.004888
Validation Loss: 0.00350399
Epoch [33/200], Train Loss: 0.004892
Validation Loss: 0.00350854
Epoch [34/200], Train Loss: 0.004844
Validation Loss: 0.00350491
Epoch [35/200], Train Loss: 0.004837
Validation Loss: 0.00350574
Epoch [36/200], Train Loss: 0.004905
Validation Loss: 0.00350335
Epoch [37/200], Train Loss: 0.004856
Validation Loss: 0.00350930
Epoch [38/200], Train Loss: 0.004853
Validation Loss: 0.00350341
Epoch [39/200], Train Loss: 0.004843
Validation Loss: 0.00350540
Epoch [40/200], Train Loss: 0.004860
Validation Loss: 0.00350479
Epoch [41/200], Train Loss: 0.004842
Validation Loss: 0.00350610
Epoch [42/200], Train Loss: 0.004839
Validation Loss: 0.00350443
Epoch [43/200], Train Loss: 0.004833
Validation Loss: 0.00350308
Epoch [44/200], Train Loss: 0.004862
Validation Loss: 0.00350742
Epoch [45/200], Train Loss: 0.004858
Validation Loss: 0.00350534
Epoch [46/200], Train Loss: 0.004875
Validation Loss: 0.00350106
Epoch [47/200], Train Loss: 0.004861
Validation Loss: 0.00350970
Epoch [48/200], Train Loss: 0.004893
Validation Loss: 0.00350387
Epoch [49/200], Train Loss: 0.004844
Validation Loss: 0.00350402
Epoch [50/200], Train Loss: 0.004875
Validation Loss: 0.00350225
Epoch [51/200], Train Loss: 0.004851
Validation Loss: 0.00350572
Epoch [52/200], Train Loss: 0.004869
Validation Loss: 0.00350071
Epoch [53/200], Train Loss: 0.004878
Validation Loss: 0.00350596
Epoch [54/200], Train Loss: 0.004856
Validation Loss: 0.00350169
Epoch [55/200], Train Loss: 0.004883
Validation Loss: 0.00349991
Epoch [56/200], Train Loss: 0.004859
Validation Loss: 0.00350766
Epoch [57/200], Train Loss: 0.004884
Validation Loss: 0.00349933
Epoch [58/200], Train Loss: 0.004843
Validation Loss: 0.00350175
Epoch [59/200], Train Loss: 0.004842
Validation Loss: 0.00350176
Epoch [60/200], Train Loss: 0.004833
Validation Loss: 0.00349988
Epoch [61/200], Train Loss: 0.004891
Validation Loss: 0.00349972
Epoch [62/200], Train Loss: 0.004835
Validation Loss: 0.00350484
Epoch [63/200], Train Loss: 0.004850
Validation Loss: 0.00349873
Epoch [64/200], Train Loss: 0.004896
Validation Loss: 0.00349792
Epoch [65/200], Train Loss: 0.004840
Validation Loss: 0.00350854
Epoch [66/200], Train Loss: 0.004873
Validation Loss: 0.00349748
Epoch [67/200], Train Loss: 0.004859
Validation Loss: 0.00349905
Epoch [68/200], Train Loss: 0.004851
Validation Loss: 0.00350318
Epoch [69/200], Train Loss: 0.004846
Validation Loss: 0.00349883
Epoch [70/200], Train Loss: 0.004854
Validation Loss: 0.00349777
Epoch [71/200], Train Loss: 0.004876
Validation Loss: 0.00349958
Epoch [72/200], Train Loss: 0.004833
Validation Loss: 0.00350099
Epoch [73/200], Train Loss: 0.004843
Validation Loss: 0.00349675
Epoch [74/200], Train Loss: 0.004857
Validation Loss: 0.00349754
Epoch [75/200], Train Loss: 0.004863
Validation Loss: 0.00350233
Epoch [76/200], Train Loss: 0.004821
Validation Loss: 0.00349844
Epoch [77/200], Train Loss: 0.004889
Validation Loss: 0.00349516
Epoch [78/200], Train Loss: 0.004832
Validation Loss: 0.00350878
Epoch [79/200], Train Loss: 0.004890
Validation Loss: 0.00349408
Epoch [80/200], Train Loss: 0.004858
Validation Loss: 0.00349950
Epoch [81/200], Train Loss: 0.004849
Validation Loss: 0.00350133
Epoch [82/200], Train Loss: 0.004808
Validation Loss: 0.00349656
Epoch [83/200], Train Loss: 0.004870
Validation Loss: 0.00349397
Epoch [84/200], Train Loss: 0.004830
Validation Loss: 0.00349979
Epoch [85/200], Train Loss: 0.004839
Validation Loss: 0.00349682
Epoch [86/200], Train Loss: 0.004802
Validation Loss: 0.00349719
Epoch [87/200], Train Loss: 0.004860
Validation Loss: 0.00349482
Epoch [88/200], Train Loss: 0.004823
Validation Loss: 0.00350031
Epoch [89/200], Train Loss: 0.004815
Validation Loss: 0.00349323
Epoch [90/200], Train Loss: 0.004830
Validation Loss: 0.00349684
Epoch [91/200], Train Loss: 0.004842
Validation Loss: 0.00349868
Epoch [92/200], Train Loss: 0.004845
Validation Loss: 0.00349409
Epoch [93/200], Train Loss: 0.004807
Validation Loss: 0.00349787
Epoch [94/200], Train Loss: 0.004866
Validation Loss: 0.00349299
Epoch [95/200], Train Loss: 0.004833
Validation Loss: 0.00350039
Epoch [96/200], Train Loss: 0.004884
Validation Loss: 0.00349058
Epoch [97/200], Train Loss: 0.004822
Validation Loss: 0.00350616
Epoch [98/200], Train Loss: 0.004823
Validation Loss: 0.00349304
Epoch [99/200], Train Loss: 0.004841
Validation Loss: 0.00349615
Epoch [100/200], Train Loss: 0.004803
Validation Loss: 0.00350075
Epoch [101/200], Train Loss: 0.004833
Validation Loss: 0.00349184
Epoch [102/200], Train Loss: 0.004871
Validation Loss: 0.00349754
Epoch [103/200], Train Loss: 0.004833
Validation Loss: 0.00349812
Epoch [104/200], Train Loss: 0.004817
Validation Loss: 0.00349423
Epoch [105/200], Train Loss: 0.004810
Validation Loss: 0.00349550
Epoch [106/200], Train Loss: 0.004818
Validation Loss: 0.00349332
Early stopping triggered

Evaluating model for: Lamp
Run 129/144 completed in 169.52 seconds with: {'MAE': np.float32(2.9644861), 'MSE': np.float32(178.16992), 'RMSE': np.float32(13.348031), 'SAE': np.float32(0.005859338), 'NDE': np.float32(0.98037016)}

Run 130/144: hidden=256, seq_len=720, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.004942
Validation Loss: 0.00358014
Epoch [2/200], Train Loss: 0.004922
Validation Loss: 0.00356986
Epoch [3/200], Train Loss: 0.004907
Validation Loss: 0.00357748
Epoch [4/200], Train Loss: 0.004940
Validation Loss: 0.00356044
Epoch [5/200], Train Loss: 0.004905
Validation Loss: 0.00356314
Epoch [6/200], Train Loss: 0.004885
Validation Loss: 0.00354674
Epoch [7/200], Train Loss: 0.004895
Validation Loss: 0.00353627
Epoch [8/200], Train Loss: 0.004861
Validation Loss: 0.00352333
Epoch [9/200], Train Loss: 0.004852
Validation Loss: 0.00351580
Epoch [10/200], Train Loss: 0.004867
Validation Loss: 0.00351413
Epoch [11/200], Train Loss: 0.004869
Validation Loss: 0.00350718
Epoch [12/200], Train Loss: 0.004867
Validation Loss: 0.00353203
Epoch [13/200], Train Loss: 0.004891
Validation Loss: 0.00350535
Epoch [14/200], Train Loss: 0.004836
Validation Loss: 0.00350486
Epoch [15/200], Train Loss: 0.004814
Validation Loss: 0.00350436
Epoch [16/200], Train Loss: 0.004838
Validation Loss: 0.00351241
Epoch [17/200], Train Loss: 0.004878
Validation Loss: 0.00350300
Epoch [18/200], Train Loss: 0.004845
Validation Loss: 0.00350866
Epoch [19/200], Train Loss: 0.004848
Validation Loss: 0.00350274
Epoch [20/200], Train Loss: 0.004852
Validation Loss: 0.00350795
Epoch [21/200], Train Loss: 0.004853
Validation Loss: 0.00350399
Epoch [22/200], Train Loss: 0.004851
Validation Loss: 0.00350368
Epoch [23/200], Train Loss: 0.004857
Validation Loss: 0.00350323
Epoch [24/200], Train Loss: 0.004876
Validation Loss: 0.00350183
Epoch [25/200], Train Loss: 0.004825
Validation Loss: 0.00350413
Epoch [26/200], Train Loss: 0.004842
Validation Loss: 0.00350045
Epoch [27/200], Train Loss: 0.004871
Validation Loss: 0.00350591
Epoch [28/200], Train Loss: 0.004820
Validation Loss: 0.00350310
Epoch [29/200], Train Loss: 0.004865
Validation Loss: 0.00350127
Epoch [30/200], Train Loss: 0.004842
Validation Loss: 0.00350375
Epoch [31/200], Train Loss: 0.004821
Validation Loss: 0.00349991
Epoch [32/200], Train Loss: 0.004853
Validation Loss: 0.00350264
Epoch [33/200], Train Loss: 0.004821
Validation Loss: 0.00350347
Epoch [34/200], Train Loss: 0.004848
Validation Loss: 0.00350046
Epoch [35/200], Train Loss: 0.004856
Validation Loss: 0.00350309
Epoch [36/200], Train Loss: 0.004901
Validation Loss: 0.00349882
Epoch [37/200], Train Loss: 0.004922
Validation Loss: 0.00350358
Epoch [38/200], Train Loss: 0.004861
Validation Loss: 0.00350054
Epoch [39/200], Train Loss: 0.004857
Validation Loss: 0.00350010
Epoch [40/200], Train Loss: 0.004832
Validation Loss: 0.00349979
Epoch [41/200], Train Loss: 0.004814
Validation Loss: 0.00350014
Epoch [42/200], Train Loss: 0.004834
Validation Loss: 0.00349812
Epoch [43/200], Train Loss: 0.004822
Validation Loss: 0.00349715
Epoch [44/200], Train Loss: 0.004852
Validation Loss: 0.00350530
Epoch [45/200], Train Loss: 0.004823
Validation Loss: 0.00349679
Epoch [46/200], Train Loss: 0.004850
Validation Loss: 0.00349909
Epoch [47/200], Train Loss: 0.004874
Validation Loss: 0.00350212
Epoch [48/200], Train Loss: 0.004832
Validation Loss: 0.00349815
Epoch [49/200], Train Loss: 0.004825
Validation Loss: 0.00349791
Epoch [50/200], Train Loss: 0.004851
Validation Loss: 0.00349790
Epoch [51/200], Train Loss: 0.004828
Validation Loss: 0.00349984
Epoch [52/200], Train Loss: 0.004847
Validation Loss: 0.00349582
Epoch [53/200], Train Loss: 0.004821
Validation Loss: 0.00350073
Epoch [54/200], Train Loss: 0.004829
Validation Loss: 0.00349414
Epoch [55/200], Train Loss: 0.004840
Validation Loss: 0.00350154
Epoch [56/200], Train Loss: 0.004870
Validation Loss: 0.00349347
Epoch [57/200], Train Loss: 0.004817
Validation Loss: 0.00350439
Epoch [58/200], Train Loss: 0.004838
Validation Loss: 0.00349350
Epoch [59/200], Train Loss: 0.004851
Validation Loss: 0.00349933
Epoch [60/200], Train Loss: 0.004872
Validation Loss: 0.00349602
Epoch [61/200], Train Loss: 0.004871
Validation Loss: 0.00349869
Epoch [62/200], Train Loss: 0.004828
Validation Loss: 0.00349842
Epoch [63/200], Train Loss: 0.004861
Validation Loss: 0.00349422
Epoch [64/200], Train Loss: 0.004818
Validation Loss: 0.00350148
Epoch [65/200], Train Loss: 0.004830
Validation Loss: 0.00349355
Epoch [66/200], Train Loss: 0.004828
Validation Loss: 0.00350058
Early stopping triggered

Evaluating model for: Lamp
Run 130/144 completed in 136.96 seconds with: {'MAE': np.float32(3.211329), 'MSE': np.float32(179.6053), 'RMSE': np.float32(13.4016905), 'SAE': np.float32(0.1456354), 'NDE': np.float32(0.9843116)}

Run 131/144: hidden=256, seq_len=720, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.005107
Validation Loss: 0.00359979
Epoch [2/200], Train Loss: 0.005008
Validation Loss: 0.00365721
Epoch [3/200], Train Loss: 0.004956
Validation Loss: 0.00357757
Epoch [4/200], Train Loss: 0.004956
Validation Loss: 0.00357728
Epoch [5/200], Train Loss: 0.004947
Validation Loss: 0.00357676
Epoch [6/200], Train Loss: 0.004951
Validation Loss: 0.00358732
Epoch [7/200], Train Loss: 0.004972
Validation Loss: 0.00358249
Epoch [8/200], Train Loss: 0.004897
Validation Loss: 0.00357515
Epoch [9/200], Train Loss: 0.004946
Validation Loss: 0.00357261
Epoch [10/200], Train Loss: 0.004929
Validation Loss: 0.00357623
Epoch [11/200], Train Loss: 0.004987
Validation Loss: 0.00357072
Epoch [12/200], Train Loss: 0.004903
Validation Loss: 0.00356873
Epoch [13/200], Train Loss: 0.004953
Validation Loss: 0.00355712
Epoch [14/200], Train Loss: 0.004878
Validation Loss: 0.00355642
Epoch [15/200], Train Loss: 0.004892
Validation Loss: 0.00353405
Epoch [16/200], Train Loss: 0.004881
Validation Loss: 0.00352103
Epoch [17/200], Train Loss: 0.004917
Validation Loss: 0.00350887
Epoch [18/200], Train Loss: 0.004835
Validation Loss: 0.00350838
Epoch [19/200], Train Loss: 0.004828
Validation Loss: 0.00351168
Epoch [20/200], Train Loss: 0.004830
Validation Loss: 0.00350891
Epoch [21/200], Train Loss: 0.004910
Validation Loss: 0.00350572
Epoch [22/200], Train Loss: 0.004872
Validation Loss: 0.00351374
Epoch [23/200], Train Loss: 0.004864
Validation Loss: 0.00350383
Epoch [24/200], Train Loss: 0.004857
Validation Loss: 0.00350942
Epoch [25/200], Train Loss: 0.004868
Validation Loss: 0.00350437
Epoch [26/200], Train Loss: 0.004883
Validation Loss: 0.00350724
Epoch [27/200], Train Loss: 0.004863
Validation Loss: 0.00350738
Epoch [28/200], Train Loss: 0.004877
Validation Loss: 0.00350377
Epoch [29/200], Train Loss: 0.004831
Validation Loss: 0.00350674
Epoch [30/200], Train Loss: 0.004877
Validation Loss: 0.00350210
Epoch [31/200], Train Loss: 0.004854
Validation Loss: 0.00350968
Epoch [32/200], Train Loss: 0.004839
Validation Loss: 0.00350196
Epoch [33/200], Train Loss: 0.004853
Validation Loss: 0.00350388
Epoch [34/200], Train Loss: 0.004925
Validation Loss: 0.00350185
Epoch [35/200], Train Loss: 0.004866
Validation Loss: 0.00351091
Epoch [36/200], Train Loss: 0.004876
Validation Loss: 0.00350029
Epoch [37/200], Train Loss: 0.004831
Validation Loss: 0.00350609
Epoch [38/200], Train Loss: 0.004864
Validation Loss: 0.00350040
Epoch [39/200], Train Loss: 0.004871
Validation Loss: 0.00350224
Epoch [40/200], Train Loss: 0.004861
Validation Loss: 0.00350094
Epoch [41/200], Train Loss: 0.004882
Validation Loss: 0.00350316
Epoch [42/200], Train Loss: 0.004877
Validation Loss: 0.00349826
Epoch [43/200], Train Loss: 0.004848
Validation Loss: 0.00350395
Epoch [44/200], Train Loss: 0.004806
Validation Loss: 0.00349944
Epoch [45/200], Train Loss: 0.004875
Validation Loss: 0.00349773
Epoch [46/200], Train Loss: 0.004817
Validation Loss: 0.00350608
Epoch [47/200], Train Loss: 0.004824
Validation Loss: 0.00349609
Epoch [48/200], Train Loss: 0.004877
Validation Loss: 0.00349837
Epoch [49/200], Train Loss: 0.004869
Validation Loss: 0.00350181
Epoch [50/200], Train Loss: 0.004851
Validation Loss: 0.00349449
Epoch [51/200], Train Loss: 0.004855
Validation Loss: 0.00350223
Epoch [52/200], Train Loss: 0.004834
Validation Loss: 0.00349756
Epoch [53/200], Train Loss: 0.004893
Validation Loss: 0.00349568
Epoch [54/200], Train Loss: 0.004863
Validation Loss: 0.00350391
Epoch [55/200], Train Loss: 0.004839
Validation Loss: 0.00349449
Epoch [56/200], Train Loss: 0.004856
Validation Loss: 0.00349618
Epoch [57/200], Train Loss: 0.004866
Validation Loss: 0.00349791
Epoch [58/200], Train Loss: 0.004855
Validation Loss: 0.00349545
Epoch [59/200], Train Loss: 0.004827
Validation Loss: 0.00349660
Epoch [60/200], Train Loss: 0.004813
Validation Loss: 0.00349612
Early stopping triggered

Evaluating model for: Lamp
Run 131/144 completed in 149.42 seconds with: {'MAE': np.float32(3.13679), 'MSE': np.float32(179.56482), 'RMSE': np.float32(13.40018), 'SAE': np.float32(0.08009544), 'NDE': np.float32(0.98420066)}

Run 132/144: hidden=256, seq_len=720, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 938 windows

Epoch [1/200], Train Loss: 0.005007
Validation Loss: 0.00361557
Epoch [2/200], Train Loss: 0.004981
Validation Loss: 0.00360868
Epoch [3/200], Train Loss: 0.004985
Validation Loss: 0.00357772
Epoch [4/200], Train Loss: 0.004939
Validation Loss: 0.00357821
Epoch [5/200], Train Loss: 0.005003
Validation Loss: 0.00358620
Epoch [6/200], Train Loss: 0.004965
Validation Loss: 0.00359044
Epoch [7/200], Train Loss: 0.004983
Validation Loss: 0.00357756
Epoch [8/200], Train Loss: 0.004968
Validation Loss: 0.00357739
Epoch [9/200], Train Loss: 0.004946
Validation Loss: 0.00358550
Epoch [10/200], Train Loss: 0.004968
Validation Loss: 0.00357911
Epoch [11/200], Train Loss: 0.004900
Validation Loss: 0.00358033
Epoch [12/200], Train Loss: 0.004928
Validation Loss: 0.00357526
Epoch [13/200], Train Loss: 0.004961
Validation Loss: 0.00357356
Epoch [14/200], Train Loss: 0.004947
Validation Loss: 0.00357219
Epoch [15/200], Train Loss: 0.004937
Validation Loss: 0.00356015
Epoch [16/200], Train Loss: 0.004885
Validation Loss: 0.00354783
Epoch [17/200], Train Loss: 0.004914
Validation Loss: 0.00352461
Epoch [18/200], Train Loss: 0.004878
Validation Loss: 0.00350736
Epoch [19/200], Train Loss: 0.004900
Validation Loss: 0.00351539
Epoch [20/200], Train Loss: 0.004840
Validation Loss: 0.00350605
Epoch [21/200], Train Loss: 0.004869
Validation Loss: 0.00352594
Epoch [22/200], Train Loss: 0.004904
Validation Loss: 0.00350464
Epoch [23/200], Train Loss: 0.004839
Validation Loss: 0.00351099
Epoch [24/200], Train Loss: 0.004813
Validation Loss: 0.00350217
Epoch [25/200], Train Loss: 0.004860
Validation Loss: 0.00351107
Epoch [26/200], Train Loss: 0.004835
Validation Loss: 0.00350339
Epoch [27/200], Train Loss: 0.004814
Validation Loss: 0.00350125
Epoch [28/200], Train Loss: 0.004892
Validation Loss: 0.00350206
Epoch [29/200], Train Loss: 0.004890
Validation Loss: 0.00349954
Epoch [30/200], Train Loss: 0.004834
Validation Loss: 0.00350528
Epoch [31/200], Train Loss: 0.004829
Validation Loss: 0.00349955
Epoch [32/200], Train Loss: 0.004893
Validation Loss: 0.00350131
Epoch [33/200], Train Loss: 0.004896
Validation Loss: 0.00349865
Epoch [34/200], Train Loss: 0.004861
Validation Loss: 0.00349984
Epoch [35/200], Train Loss: 0.004867
Validation Loss: 0.00349779
Epoch [36/200], Train Loss: 0.004826
Validation Loss: 0.00349751
Epoch [37/200], Train Loss: 0.004857
Validation Loss: 0.00349718
Epoch [38/200], Train Loss: 0.004875
Validation Loss: 0.00349538
Epoch [39/200], Train Loss: 0.004889
Validation Loss: 0.00350412
Epoch [40/200], Train Loss: 0.004867
Validation Loss: 0.00349458
Epoch [41/200], Train Loss: 0.004910
Validation Loss: 0.00350190
Epoch [42/200], Train Loss: 0.004800
Validation Loss: 0.00349496
Epoch [43/200], Train Loss: 0.004837
Validation Loss: 0.00349731
Epoch [44/200], Train Loss: 0.004878
Validation Loss: 0.00349971
Epoch [45/200], Train Loss: 0.004847
Validation Loss: 0.00349501
Epoch [46/200], Train Loss: 0.004818
Validation Loss: 0.00350732
Epoch [47/200], Train Loss: 0.004833
Validation Loss: 0.00349373
Epoch [48/200], Train Loss: 0.004844
Validation Loss: 0.00350017
Epoch [49/200], Train Loss: 0.004879
Validation Loss: 0.00349826
Epoch [50/200], Train Loss: 0.004829
Validation Loss: 0.00350016
Epoch [51/200], Train Loss: 0.004848
Validation Loss: 0.00349392
Epoch [52/200], Train Loss: 0.004847
Validation Loss: 0.00350067
Epoch [53/200], Train Loss: 0.004818
Validation Loss: 0.00349829
Epoch [54/200], Train Loss: 0.004864
Validation Loss: 0.00349362
Epoch [55/200], Train Loss: 0.004842
Validation Loss: 0.00350068
Epoch [56/200], Train Loss: 0.004825
Validation Loss: 0.00349435
Epoch [57/200], Train Loss: 0.004880
Validation Loss: 0.00349726
Epoch [58/200], Train Loss: 0.004857
Validation Loss: 0.00349871
Epoch [59/200], Train Loss: 0.004847
Validation Loss: 0.00349617
Epoch [60/200], Train Loss: 0.004869
Validation Loss: 0.00349781
Epoch [61/200], Train Loss: 0.004849
Validation Loss: 0.00350096
Epoch [62/200], Train Loss: 0.004838
Validation Loss: 0.00349581
Epoch [63/200], Train Loss: 0.004899
Validation Loss: 0.00349679
Epoch [64/200], Train Loss: 0.004865
Validation Loss: 0.00350330
Early stopping triggered

Evaluating model for: Lamp
Run 132/144 completed in 221.43 seconds with: {'MAE': np.float32(3.197215), 'MSE': np.float32(179.52602), 'RMSE': np.float32(13.398732), 'SAE': np.float32(0.13561219), 'NDE': np.float32(0.9840943)}

Run 133/144: hidden=256, seq_len=1080, stride=0.1, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.005260
Validation Loss: 0.00484180
Epoch [2/200], Train Loss: 0.004991
Validation Loss: 0.00475981
Epoch [3/200], Train Loss: 0.004942
Validation Loss: 0.00473729
Epoch [4/200], Train Loss: 0.004918
Validation Loss: 0.00472179
Epoch [5/200], Train Loss: 0.004928
Validation Loss: 0.00471637
Epoch [6/200], Train Loss: 0.004899
Validation Loss: 0.00470736
Epoch [7/200], Train Loss: 0.004912
Validation Loss: 0.00470713
Epoch [8/200], Train Loss: 0.004906
Validation Loss: 0.00470723
Epoch [9/200], Train Loss: 0.004901
Validation Loss: 0.00470446
Epoch [10/200], Train Loss: 0.004915
Validation Loss: 0.00470354
Epoch [11/200], Train Loss: 0.004891
Validation Loss: 0.00470315
Epoch [12/200], Train Loss: 0.004899
Validation Loss: 0.00470182
Epoch [13/200], Train Loss: 0.004911
Validation Loss: 0.00470115
Epoch [14/200], Train Loss: 0.004891
Validation Loss: 0.00470022
Epoch [15/200], Train Loss: 0.004913
Validation Loss: 0.00470255
Epoch [16/200], Train Loss: 0.004899
Validation Loss: 0.00469823
Epoch [17/200], Train Loss: 0.004873
Validation Loss: 0.00469759
Epoch [18/200], Train Loss: 0.004872
Validation Loss: 0.00469624
Epoch [19/200], Train Loss: 0.004873
Validation Loss: 0.00469450
Epoch [20/200], Train Loss: 0.004872
Validation Loss: 0.00469321
Epoch [21/200], Train Loss: 0.004889
Validation Loss: 0.00469355
Epoch [22/200], Train Loss: 0.004892
Validation Loss: 0.00469652
Epoch [23/200], Train Loss: 0.004870
Validation Loss: 0.00469277
Epoch [24/200], Train Loss: 0.004891
Validation Loss: 0.00469873
Epoch [25/200], Train Loss: 0.004886
Validation Loss: 0.00468659
Epoch [26/200], Train Loss: 0.004875
Validation Loss: 0.00469515
Epoch [27/200], Train Loss: 0.004861
Validation Loss: 0.00468463
Epoch [28/200], Train Loss: 0.004865
Validation Loss: 0.00468052
Epoch [29/200], Train Loss: 0.004884
Validation Loss: 0.00468528
Epoch [30/200], Train Loss: 0.004868
Validation Loss: 0.00467471
Epoch [31/200], Train Loss: 0.004839
Validation Loss: 0.00466998
Epoch [32/200], Train Loss: 0.004871
Validation Loss: 0.00467688
Epoch [33/200], Train Loss: 0.004859
Validation Loss: 0.00466468
Epoch [34/200], Train Loss: 0.004844
Validation Loss: 0.00464814
Epoch [35/200], Train Loss: 0.004848
Validation Loss: 0.00465121
Epoch [36/200], Train Loss: 0.004819
Validation Loss: 0.00463869
Epoch [37/200], Train Loss: 0.004821
Validation Loss: 0.00462323
Epoch [38/200], Train Loss: 0.004803
Validation Loss: 0.00461810
Epoch [39/200], Train Loss: 0.004798
Validation Loss: 0.00461961
Epoch [40/200], Train Loss: 0.004805
Validation Loss: 0.00463613
Epoch [41/200], Train Loss: 0.004775
Validation Loss: 0.00462422
Epoch [42/200], Train Loss: 0.004777
Validation Loss: 0.00458530
Epoch [43/200], Train Loss: 0.004749
Validation Loss: 0.00457345
Epoch [44/200], Train Loss: 0.004766
Validation Loss: 0.00457040
Epoch [45/200], Train Loss: 0.004737
Validation Loss: 0.00454510
Epoch [46/200], Train Loss: 0.004720
Validation Loss: 0.00457753
Epoch [47/200], Train Loss: 0.004740
Validation Loss: 0.00452138
Epoch [48/200], Train Loss: 0.004686
Validation Loss: 0.00450113
Epoch [49/200], Train Loss: 0.004653
Validation Loss: 0.00446513
Epoch [50/200], Train Loss: 0.004642
Validation Loss: 0.00445607
Epoch [51/200], Train Loss: 0.004634
Validation Loss: 0.00445394
Epoch [52/200], Train Loss: 0.004607
Validation Loss: 0.00440102
Epoch [53/200], Train Loss: 0.004642
Validation Loss: 0.00440784
Epoch [54/200], Train Loss: 0.004575
Validation Loss: 0.00434840
Epoch [55/200], Train Loss: 0.004537
Validation Loss: 0.00432310
Epoch [56/200], Train Loss: 0.004503
Validation Loss: 0.00429076
Epoch [57/200], Train Loss: 0.004477
Validation Loss: 0.00424779
Epoch [58/200], Train Loss: 0.004487
Validation Loss: 0.00438609
Epoch [59/200], Train Loss: 0.004465
Validation Loss: 0.00421349
Epoch [60/200], Train Loss: 0.004382
Validation Loss: 0.00415962
Epoch [61/200], Train Loss: 0.004351
Validation Loss: 0.00412637
Epoch [62/200], Train Loss: 0.004311
Validation Loss: 0.00411214
Epoch [63/200], Train Loss: 0.004311
Validation Loss: 0.00406417
Epoch [64/200], Train Loss: 0.004259
Validation Loss: 0.00415571
Epoch [65/200], Train Loss: 0.004253
Validation Loss: 0.00401105
Epoch [66/200], Train Loss: 0.004218
Validation Loss: 0.00402425
Epoch [67/200], Train Loss: 0.004177
Validation Loss: 0.00393623
Epoch [68/200], Train Loss: 0.004122
Validation Loss: 0.00390275
Epoch [69/200], Train Loss: 0.004104
Validation Loss: 0.00385176
Epoch [70/200], Train Loss: 0.004056
Validation Loss: 0.00382823
Epoch [71/200], Train Loss: 0.004007
Validation Loss: 0.00376198
Epoch [72/200], Train Loss: 0.003952
Validation Loss: 0.00372351
Epoch [73/200], Train Loss: 0.003882
Validation Loss: 0.00367885
Epoch [74/200], Train Loss: 0.003875
Validation Loss: 0.00364072
Epoch [75/200], Train Loss: 0.003783
Validation Loss: 0.00347868
Epoch [76/200], Train Loss: 0.003690
Validation Loss: 0.00338226
Epoch [77/200], Train Loss: 0.003623
Validation Loss: 0.00331691
Epoch [78/200], Train Loss: 0.003580
Validation Loss: 0.00329705
Epoch [79/200], Train Loss: 0.003517
Validation Loss: 0.00316522
Epoch [80/200], Train Loss: 0.003444
Validation Loss: 0.00311871
Epoch [81/200], Train Loss: 0.003418
Validation Loss: 0.00312267
Epoch [82/200], Train Loss: 0.003353
Validation Loss: 0.00298599
Epoch [83/200], Train Loss: 0.003255
Validation Loss: 0.00291787
Epoch [84/200], Train Loss: 0.003176
Validation Loss: 0.00288018
Epoch [85/200], Train Loss: 0.003245
Validation Loss: 0.00304281
Epoch [86/200], Train Loss: 0.003190
Validation Loss: 0.00284569
Epoch [87/200], Train Loss: 0.003050
Validation Loss: 0.00272712
Epoch [88/200], Train Loss: 0.002964
Validation Loss: 0.00263971
Epoch [89/200], Train Loss: 0.002912
Validation Loss: 0.00259851
Epoch [90/200], Train Loss: 0.002867
Validation Loss: 0.00254796
Epoch [91/200], Train Loss: 0.002819
Validation Loss: 0.00252710
Epoch [92/200], Train Loss: 0.002776
Validation Loss: 0.00249793
Epoch [93/200], Train Loss: 0.002732
Validation Loss: 0.00244018
Epoch [94/200], Train Loss: 0.002689
Validation Loss: 0.00240080
Epoch [95/200], Train Loss: 0.002659
Validation Loss: 0.00239522
Epoch [96/200], Train Loss: 0.002621
Validation Loss: 0.00234475
Epoch [97/200], Train Loss: 0.002605
Validation Loss: 0.00235262
Epoch [98/200], Train Loss: 0.002591
Validation Loss: 0.00229326
Epoch [99/200], Train Loss: 0.002545
Validation Loss: 0.00228367
Epoch [100/200], Train Loss: 0.002515
Validation Loss: 0.00227492
Epoch [101/200], Train Loss: 0.002485
Validation Loss: 0.00221671
Epoch [102/200], Train Loss: 0.002451
Validation Loss: 0.00219541
Epoch [103/200], Train Loss: 0.002423
Validation Loss: 0.00217833
Epoch [104/200], Train Loss: 0.002389
Validation Loss: 0.00216966
Epoch [105/200], Train Loss: 0.002364
Validation Loss: 0.00214568
Epoch [106/200], Train Loss: 0.002352
Validation Loss: 0.00214577
Epoch [107/200], Train Loss: 0.002340
Validation Loss: 0.00209363
Epoch [108/200], Train Loss: 0.002306
Validation Loss: 0.00209012
Epoch [109/200], Train Loss: 0.002283
Validation Loss: 0.00206178
Epoch [110/200], Train Loss: 0.002266
Validation Loss: 0.00205475
Epoch [111/200], Train Loss: 0.002256
Validation Loss: 0.00204402
Epoch [112/200], Train Loss: 0.002233
Validation Loss: 0.00203542
Epoch [113/200], Train Loss: 0.002211
Validation Loss: 0.00201397
Epoch [114/200], Train Loss: 0.002182
Validation Loss: 0.00201111
Epoch [115/200], Train Loss: 0.002174
Validation Loss: 0.00200598
Epoch [116/200], Train Loss: 0.002166
Validation Loss: 0.00196091
Epoch [117/200], Train Loss: 0.002139
Validation Loss: 0.00195179
Epoch [118/200], Train Loss: 0.002118
Validation Loss: 0.00193625
Epoch [119/200], Train Loss: 0.002125
Validation Loss: 0.00193055
Epoch [120/200], Train Loss: 0.002089
Validation Loss: 0.00191511
Epoch [121/200], Train Loss: 0.002070
Validation Loss: 0.00190393
Epoch [122/200], Train Loss: 0.002054
Validation Loss: 0.00188756
Epoch [123/200], Train Loss: 0.002054
Validation Loss: 0.00187732
Epoch [124/200], Train Loss: 0.002045
Validation Loss: 0.00186983
Epoch [125/200], Train Loss: 0.002035
Validation Loss: 0.00185310
Epoch [126/200], Train Loss: 0.002028
Validation Loss: 0.00185758
Epoch [127/200], Train Loss: 0.002012
Validation Loss: 0.00184389
Epoch [128/200], Train Loss: 0.002005
Validation Loss: 0.00182489
Epoch [129/200], Train Loss: 0.001982
Validation Loss: 0.00181464
Epoch [130/200], Train Loss: 0.001972
Validation Loss: 0.00181707
Epoch [131/200], Train Loss: 0.001942
Validation Loss: 0.00182992
Epoch [132/200], Train Loss: 0.001948
Validation Loss: 0.00181134
Epoch [133/200], Train Loss: 0.001935
Validation Loss: 0.00179717
Epoch [134/200], Train Loss: 0.001924
Validation Loss: 0.00177954
Epoch [135/200], Train Loss: 0.001923
Validation Loss: 0.00178401
Epoch [136/200], Train Loss: 0.001909
Validation Loss: 0.00177317
Epoch [137/200], Train Loss: 0.001895
Validation Loss: 0.00177486
Epoch [138/200], Train Loss: 0.001887
Validation Loss: 0.00174524
Epoch [139/200], Train Loss: 0.001890
Validation Loss: 0.00174133
Epoch [140/200], Train Loss: 0.001876
Validation Loss: 0.00173042
Epoch [141/200], Train Loss: 0.001869
Validation Loss: 0.00172503
Epoch [142/200], Train Loss: 0.001854
Validation Loss: 0.00172005
Epoch [143/200], Train Loss: 0.001852
Validation Loss: 0.00172326
Epoch [144/200], Train Loss: 0.001839
Validation Loss: 0.00171271
Epoch [145/200], Train Loss: 0.001837
Validation Loss: 0.00170426
Epoch [146/200], Train Loss: 0.001822
Validation Loss: 0.00169428
Epoch [147/200], Train Loss: 0.001803
Validation Loss: 0.00170015
Epoch [148/200], Train Loss: 0.001821
Validation Loss: 0.00169411
Epoch [149/200], Train Loss: 0.001788
Validation Loss: 0.00166633
Epoch [150/200], Train Loss: 0.001789
Validation Loss: 0.00166185
Epoch [151/200], Train Loss: 0.001789
Validation Loss: 0.00166456
Epoch [152/200], Train Loss: 0.001772
Validation Loss: 0.00166534
Epoch [153/200], Train Loss: 0.001765
Validation Loss: 0.00164746
Epoch [154/200], Train Loss: 0.001764
Validation Loss: 0.00164923
Epoch [155/200], Train Loss: 0.001756
Validation Loss: 0.00163652
Epoch [156/200], Train Loss: 0.001740
Validation Loss: 0.00164396
Epoch [157/200], Train Loss: 0.001746
Validation Loss: 0.00163225
Epoch [158/200], Train Loss: 0.001738
Validation Loss: 0.00162511
Epoch [159/200], Train Loss: 0.001731
Validation Loss: 0.00161743
Epoch [160/200], Train Loss: 0.001718
Validation Loss: 0.00160844
Epoch [161/200], Train Loss: 0.001707
Validation Loss: 0.00160541
Epoch [162/200], Train Loss: 0.001701
Validation Loss: 0.00159066
Epoch [163/200], Train Loss: 0.001689
Validation Loss: 0.00160582
Epoch [164/200], Train Loss: 0.001692
Validation Loss: 0.00158704
Epoch [165/200], Train Loss: 0.001680
Validation Loss: 0.00157488
Epoch [166/200], Train Loss: 0.001667
Validation Loss: 0.00157480
Epoch [167/200], Train Loss: 0.001671
Validation Loss: 0.00156848
Epoch [168/200], Train Loss: 0.001667
Validation Loss: 0.00156368
Epoch [169/200], Train Loss: 0.001658
Validation Loss: 0.00155564
Epoch [170/200], Train Loss: 0.001643
Validation Loss: 0.00156137
Epoch [171/200], Train Loss: 0.001636
Validation Loss: 0.00156074
Epoch [172/200], Train Loss: 0.001635
Validation Loss: 0.00154800
Epoch [173/200], Train Loss: 0.001631
Validation Loss: 0.00153960
Epoch [174/200], Train Loss: 0.001626
Validation Loss: 0.00154013
Epoch [175/200], Train Loss: 0.001626
Validation Loss: 0.00153186
Epoch [176/200], Train Loss: 0.001616
Validation Loss: 0.00152508
Epoch [177/200], Train Loss: 0.001608
Validation Loss: 0.00152018
Epoch [178/200], Train Loss: 0.001602
Validation Loss: 0.00152032
Epoch [179/200], Train Loss: 0.001597
Validation Loss: 0.00151358
Epoch [180/200], Train Loss: 0.001592
Validation Loss: 0.00151071
Epoch [181/200], Train Loss: 0.001593
Validation Loss: 0.00150965
Epoch [182/200], Train Loss: 0.001598
Validation Loss: 0.00151106
Epoch [183/200], Train Loss: 0.001585
Validation Loss: 0.00149941
Epoch [184/200], Train Loss: 0.001562
Validation Loss: 0.00150274
Epoch [185/200], Train Loss: 0.001569
Validation Loss: 0.00149300
Epoch [186/200], Train Loss: 0.001563
Validation Loss: 0.00149238
Epoch [187/200], Train Loss: 0.001569
Validation Loss: 0.00148820
Epoch [188/200], Train Loss: 0.001559
Validation Loss: 0.00149528
Epoch [189/200], Train Loss: 0.001551
Validation Loss: 0.00147936
Epoch [190/200], Train Loss: 0.001537
Validation Loss: 0.00148660
Epoch [191/200], Train Loss: 0.001537
Validation Loss: 0.00147822
Epoch [192/200], Train Loss: 0.001539
Validation Loss: 0.00147042
Epoch [193/200], Train Loss: 0.001526
Validation Loss: 0.00147174
Epoch [194/200], Train Loss: 0.001522
Validation Loss: 0.00146500
Epoch [195/200], Train Loss: 0.001528
Validation Loss: 0.00144977
Epoch [196/200], Train Loss: 0.001512
Validation Loss: 0.00145120
Epoch [197/200], Train Loss: 0.001512
Validation Loss: 0.00144829
Epoch [198/200], Train Loss: 0.001506
Validation Loss: 0.00145246
Epoch [199/200], Train Loss: 0.001497
Validation Loss: 0.00145191
Epoch [200/200], Train Loss: 0.001501
Validation Loss: 0.00145682

Evaluating model for: Lamp
Run 133/144 completed in 1278.76 seconds with: {'MAE': np.float32(1.2341888), 'MSE': np.float32(40.776073), 'RMSE': np.float32(6.3856144), 'SAE': np.float32(0.19677179), 'NDE': np.float32(0.5158243)}

Run 134/144: hidden=256, seq_len=1080, stride=0.1, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.004962
Validation Loss: 0.00476050
Epoch [2/200], Train Loss: 0.004957
Validation Loss: 0.00472876
Epoch [3/200], Train Loss: 0.004914
Validation Loss: 0.00471954
Epoch [4/200], Train Loss: 0.004891
Validation Loss: 0.00471110
Epoch [5/200], Train Loss: 0.004882
Validation Loss: 0.00470329
Epoch [6/200], Train Loss: 0.004886
Validation Loss: 0.00470321
Epoch [7/200], Train Loss: 0.004881
Validation Loss: 0.00470163
Epoch [8/200], Train Loss: 0.004865
Validation Loss: 0.00470073
Epoch [9/200], Train Loss: 0.004877
Validation Loss: 0.00470400
Epoch [10/200], Train Loss: 0.004904
Validation Loss: 0.00469947
Epoch [11/200], Train Loss: 0.004897
Validation Loss: 0.00469807
Epoch [12/200], Train Loss: 0.004891
Validation Loss: 0.00470737
Epoch [13/200], Train Loss: 0.004868
Validation Loss: 0.00469604
Epoch [14/200], Train Loss: 0.004873
Validation Loss: 0.00469466
Epoch [15/200], Train Loss: 0.004875
Validation Loss: 0.00469246
Epoch [16/200], Train Loss: 0.004876
Validation Loss: 0.00471952
Epoch [17/200], Train Loss: 0.004875
Validation Loss: 0.00469108
Epoch [18/200], Train Loss: 0.004861
Validation Loss: 0.00469023
Epoch [19/200], Train Loss: 0.004882
Validation Loss: 0.00469446
Epoch [20/200], Train Loss: 0.004867
Validation Loss: 0.00468910
Epoch [21/200], Train Loss: 0.004870
Validation Loss: 0.00468801
Epoch [22/200], Train Loss: 0.004867
Validation Loss: 0.00470942
Epoch [23/200], Train Loss: 0.004888
Validation Loss: 0.00468766
Epoch [24/200], Train Loss: 0.004879
Validation Loss: 0.00469290
Epoch [25/200], Train Loss: 0.004867
Validation Loss: 0.00469239
Epoch [26/200], Train Loss: 0.004854
Validation Loss: 0.00468559
Epoch [27/200], Train Loss: 0.004882
Validation Loss: 0.00468749
Epoch [28/200], Train Loss: 0.004870
Validation Loss: 0.00468419
Epoch [29/200], Train Loss: 0.004885
Validation Loss: 0.00469715
Epoch [30/200], Train Loss: 0.004865
Validation Loss: 0.00469564
Epoch [31/200], Train Loss: 0.004879
Validation Loss: 0.00468548
Epoch [32/200], Train Loss: 0.004854
Validation Loss: 0.00468330
Epoch [33/200], Train Loss: 0.004883
Validation Loss: 0.00468292
Epoch [34/200], Train Loss: 0.004875
Validation Loss: 0.00468494
Epoch [35/200], Train Loss: 0.004864
Validation Loss: 0.00468286
Epoch [36/200], Train Loss: 0.004890
Validation Loss: 0.00469176
Epoch [37/200], Train Loss: 0.004843
Validation Loss: 0.00468189
Epoch [38/200], Train Loss: 0.004872
Validation Loss: 0.00468212
Epoch [39/200], Train Loss: 0.004848
Validation Loss: 0.00468176
Epoch [40/200], Train Loss: 0.004866
Validation Loss: 0.00470121
Epoch [41/200], Train Loss: 0.004858
Validation Loss: 0.00468725
Epoch [42/200], Train Loss: 0.004864
Validation Loss: 0.00468675
Epoch [43/200], Train Loss: 0.004864
Validation Loss: 0.00468010
Epoch [44/200], Train Loss: 0.004880
Validation Loss: 0.00468577
Epoch [45/200], Train Loss: 0.004851
Validation Loss: 0.00467837
Epoch [46/200], Train Loss: 0.004850
Validation Loss: 0.00467547
Epoch [47/200], Train Loss: 0.004844
Validation Loss: 0.00467143
Epoch [48/200], Train Loss: 0.004839
Validation Loss: 0.00467085
Epoch [49/200], Train Loss: 0.004870
Validation Loss: 0.00466595
Epoch [50/200], Train Loss: 0.004846
Validation Loss: 0.00466228
Epoch [51/200], Train Loss: 0.004833
Validation Loss: 0.00464556
Epoch [52/200], Train Loss: 0.004812
Validation Loss: 0.00462921
Epoch [53/200], Train Loss: 0.004792
Validation Loss: 0.00460985
Epoch [54/200], Train Loss: 0.004781
Validation Loss: 0.00459022
Epoch [55/200], Train Loss: 0.004749
Validation Loss: 0.00458291
Epoch [56/200], Train Loss: 0.004725
Validation Loss: 0.00452511
Epoch [57/200], Train Loss: 0.004738
Validation Loss: 0.00451206
Epoch [58/200], Train Loss: 0.004717
Validation Loss: 0.00467666
Epoch [59/200], Train Loss: 0.004697
Validation Loss: 0.00446486
Epoch [60/200], Train Loss: 0.004637
Validation Loss: 0.00444154
Epoch [61/200], Train Loss: 0.004622
Validation Loss: 0.00440416
Epoch [62/200], Train Loss: 0.004609
Validation Loss: 0.00442628
Epoch [63/200], Train Loss: 0.004574
Validation Loss: 0.00436000
Epoch [64/200], Train Loss: 0.004543
Validation Loss: 0.00436649
Epoch [65/200], Train Loss: 0.004526
Validation Loss: 0.00431488
Epoch [66/200], Train Loss: 0.004482
Validation Loss: 0.00431421
Epoch [67/200], Train Loss: 0.004444
Validation Loss: 0.00421894
Epoch [68/200], Train Loss: 0.004419
Validation Loss: 0.00420210
Epoch [69/200], Train Loss: 0.004379
Validation Loss: 0.00415542
Epoch [70/200], Train Loss: 0.004318
Validation Loss: 0.00413193
Epoch [71/200], Train Loss: 0.004330
Validation Loss: 0.00413098
Epoch [72/200], Train Loss: 0.004263
Validation Loss: 0.00404233
Epoch [73/200], Train Loss: 0.004185
Validation Loss: 0.00396426
Epoch [74/200], Train Loss: 0.004124
Validation Loss: 0.00389190
Epoch [75/200], Train Loss: 0.004065
Validation Loss: 0.00382741
Epoch [76/200], Train Loss: 0.003987
Validation Loss: 0.00372417
Epoch [77/200], Train Loss: 0.003906
Validation Loss: 0.00365582
Epoch [78/200], Train Loss: 0.003792
Validation Loss: 0.00349378
Epoch [79/200], Train Loss: 0.003760
Validation Loss: 0.00347214
Epoch [80/200], Train Loss: 0.003657
Validation Loss: 0.00333918
Epoch [81/200], Train Loss: 0.003552
Validation Loss: 0.00317930
Epoch [82/200], Train Loss: 0.003423
Validation Loss: 0.00299212
Epoch [83/200], Train Loss: 0.003275
Validation Loss: 0.00285240
Epoch [84/200], Train Loss: 0.003135
Validation Loss: 0.00272045
Epoch [85/200], Train Loss: 0.003049
Validation Loss: 0.00265841
Epoch [86/200], Train Loss: 0.002962
Validation Loss: 0.00254758
Epoch [87/200], Train Loss: 0.002840
Validation Loss: 0.00248581
Epoch [88/200], Train Loss: 0.002779
Validation Loss: 0.00244611
Epoch [89/200], Train Loss: 0.002718
Validation Loss: 0.00237134
Epoch [90/200], Train Loss: 0.002645
Validation Loss: 0.00232529
Epoch [91/200], Train Loss: 0.002620
Validation Loss: 0.00229426
Epoch [92/200], Train Loss: 0.002520
Validation Loss: 0.00226040
Epoch [93/200], Train Loss: 0.002473
Validation Loss: 0.00218357
Epoch [94/200], Train Loss: 0.002460
Validation Loss: 0.00218677
Epoch [95/200], Train Loss: 0.002455
Validation Loss: 0.00214579
Epoch [96/200], Train Loss: 0.002371
Validation Loss: 0.00211234
Epoch [97/200], Train Loss: 0.002327
Validation Loss: 0.00210293
Epoch [98/200], Train Loss: 0.002288
Validation Loss: 0.00207261
Epoch [99/200], Train Loss: 0.002246
Validation Loss: 0.00201665
Epoch [100/200], Train Loss: 0.002217
Validation Loss: 0.00197621
Epoch [101/200], Train Loss: 0.002211
Validation Loss: 0.00196948
Epoch [102/200], Train Loss: 0.002203
Validation Loss: 0.00199709
Epoch [103/200], Train Loss: 0.002129
Validation Loss: 0.00191489
Epoch [104/200], Train Loss: 0.002114
Validation Loss: 0.00191087
Epoch [105/200], Train Loss: 0.002071
Validation Loss: 0.00188610
Epoch [106/200], Train Loss: 0.002054
Validation Loss: 0.00187720
Epoch [107/200], Train Loss: 0.002034
Validation Loss: 0.00184214
Epoch [108/200], Train Loss: 0.002010
Validation Loss: 0.00184626
Epoch [109/200], Train Loss: 0.002004
Validation Loss: 0.00182356
Epoch [110/200], Train Loss: 0.001975
Validation Loss: 0.00180501
Epoch [111/200], Train Loss: 0.001962
Validation Loss: 0.00182044
Epoch [112/200], Train Loss: 0.001939
Validation Loss: 0.00178019
Epoch [113/200], Train Loss: 0.001921
Validation Loss: 0.00178335
Epoch [114/200], Train Loss: 0.001908
Validation Loss: 0.00175135
Epoch [115/200], Train Loss: 0.001893
Validation Loss: 0.00172384
Epoch [116/200], Train Loss: 0.001863
Validation Loss: 0.00171487
Epoch [117/200], Train Loss: 0.001843
Validation Loss: 0.00170266
Epoch [118/200], Train Loss: 0.001823
Validation Loss: 0.00169673
Epoch [119/200], Train Loss: 0.001817
Validation Loss: 0.00167666
Epoch [120/200], Train Loss: 0.001797
Validation Loss: 0.00164850
Epoch [121/200], Train Loss: 0.001788
Validation Loss: 0.00166892
Epoch [122/200], Train Loss: 0.001765
Validation Loss: 0.00164793
Epoch [123/200], Train Loss: 0.001747
Validation Loss: 0.00161552
Epoch [124/200], Train Loss: 0.001743
Validation Loss: 0.00160819
Epoch [125/200], Train Loss: 0.001732
Validation Loss: 0.00159424
Epoch [126/200], Train Loss: 0.001716
Validation Loss: 0.00157974
Epoch [127/200], Train Loss: 0.001692
Validation Loss: 0.00157461
Epoch [128/200], Train Loss: 0.001685
Validation Loss: 0.00157433
Epoch [129/200], Train Loss: 0.001680
Validation Loss: 0.00155050
Epoch [130/200], Train Loss: 0.001829
Validation Loss: 0.00167017
Epoch [131/200], Train Loss: 0.001757
Validation Loss: 0.00157451
Epoch [132/200], Train Loss: 0.001686
Validation Loss: 0.00154216
Epoch [133/200], Train Loss: 0.001659
Validation Loss: 0.00151676
Epoch [134/200], Train Loss: 0.001649
Validation Loss: 0.00153267
Epoch [135/200], Train Loss: 0.001638
Validation Loss: 0.00151311
Epoch [136/200], Train Loss: 0.001625
Validation Loss: 0.00152311
Epoch [137/200], Train Loss: 0.001609
Validation Loss: 0.00149953
Epoch [138/200], Train Loss: 0.001595
Validation Loss: 0.00149022
Epoch [139/200], Train Loss: 0.001587
Validation Loss: 0.00147291
Epoch [140/200], Train Loss: 0.001584
Validation Loss: 0.00148509
Epoch [141/200], Train Loss: 0.001574
Validation Loss: 0.00146017
Epoch [142/200], Train Loss: 0.001557
Validation Loss: 0.00146097
Epoch [143/200], Train Loss: 0.001549
Validation Loss: 0.00144544
Epoch [144/200], Train Loss: 0.001555
Validation Loss: 0.00144495
Epoch [145/200], Train Loss: 0.001546
Validation Loss: 0.00145140
Epoch [146/200], Train Loss: 0.001546
Validation Loss: 0.00143478
Epoch [147/200], Train Loss: 0.001520
Validation Loss: 0.00143941
Epoch [148/200], Train Loss: 0.001508
Validation Loss: 0.00141179
Epoch [149/200], Train Loss: 0.001505
Validation Loss: 0.00141761
Epoch [150/200], Train Loss: 0.001497
Validation Loss: 0.00140326
Epoch [151/200], Train Loss: 0.001485
Validation Loss: 0.00139408
Epoch [152/200], Train Loss: 0.001473
Validation Loss: 0.00138100
Epoch [153/200], Train Loss: 0.001472
Validation Loss: 0.00138060
Epoch [154/200], Train Loss: 0.001462
Validation Loss: 0.00137148
Epoch [155/200], Train Loss: 0.001457
Validation Loss: 0.00139179
Epoch [156/200], Train Loss: 0.001454
Validation Loss: 0.00136404
Epoch [157/200], Train Loss: 0.001443
Validation Loss: 0.00135861
Epoch [158/200], Train Loss: 0.001441
Validation Loss: 0.00135162
Epoch [159/200], Train Loss: 0.001429
Validation Loss: 0.00134043
Epoch [160/200], Train Loss: 0.001417
Validation Loss: 0.00133584
Epoch [161/200], Train Loss: 0.001427
Validation Loss: 0.00132847
Epoch [162/200], Train Loss: 0.001405
Validation Loss: 0.00133436
Epoch [163/200], Train Loss: 0.001405
Validation Loss: 0.00132497
Epoch [164/200], Train Loss: 0.001402
Validation Loss: 0.00133108
Epoch [165/200], Train Loss: 0.001392
Validation Loss: 0.00132833
Epoch [166/200], Train Loss: 0.001388
Validation Loss: 0.00131168
Epoch [167/200], Train Loss: 0.001379
Validation Loss: 0.00131304
Epoch [168/200], Train Loss: 0.001375
Validation Loss: 0.00131035
Epoch [169/200], Train Loss: 0.001372
Validation Loss: 0.00129888
Epoch [170/200], Train Loss: 0.001356
Validation Loss: 0.00129398
Epoch [171/200], Train Loss: 0.001345
Validation Loss: 0.00127283
Epoch [172/200], Train Loss: 0.001347
Validation Loss: 0.00126826
Epoch [173/200], Train Loss: 0.001337
Validation Loss: 0.00127720
Epoch [174/200], Train Loss: 0.001335
Validation Loss: 0.00126319
Epoch [175/200], Train Loss: 0.001326
Validation Loss: 0.00127973
Epoch [176/200], Train Loss: 0.001320
Validation Loss: 0.00125579
Epoch [177/200], Train Loss: 0.001332
Validation Loss: 0.00124842
Epoch [178/200], Train Loss: 0.001317
Validation Loss: 0.00124563
Epoch [179/200], Train Loss: 0.001299
Validation Loss: 0.00123214
Epoch [180/200], Train Loss: 0.001294
Validation Loss: 0.00123266
Epoch [181/200], Train Loss: 0.001296
Validation Loss: 0.00122480
Epoch [182/200], Train Loss: 0.001282
Validation Loss: 0.00124959
Epoch [183/200], Train Loss: 0.001288
Validation Loss: 0.00121637
Epoch [184/200], Train Loss: 0.001272
Validation Loss: 0.00121631
Epoch [185/200], Train Loss: 0.001264
Validation Loss: 0.00120926
Epoch [186/200], Train Loss: 0.001263
Validation Loss: 0.00120060
Epoch [187/200], Train Loss: 0.001259
Validation Loss: 0.00119025
Epoch [188/200], Train Loss: 0.001251
Validation Loss: 0.00119070
Epoch [189/200], Train Loss: 0.001238
Validation Loss: 0.00119418
Epoch [190/200], Train Loss: 0.001236
Validation Loss: 0.00118235
Epoch [191/200], Train Loss: 0.001238
Validation Loss: 0.00118487
Epoch [192/200], Train Loss: 0.001218
Validation Loss: 0.00116144
Epoch [193/200], Train Loss: 0.001223
Validation Loss: 0.00116969
Epoch [194/200], Train Loss: 0.001221
Validation Loss: 0.00116755
Epoch [195/200], Train Loss: 0.001211
Validation Loss: 0.00115153
Epoch [196/200], Train Loss: 0.001215
Validation Loss: 0.00117768
Epoch [197/200], Train Loss: 0.001207
Validation Loss: 0.00115394
Epoch [198/200], Train Loss: 0.001205
Validation Loss: 0.00115245
Epoch [199/200], Train Loss: 0.001198
Validation Loss: 0.00114552
Epoch [200/200], Train Loss: 0.001192
Validation Loss: 0.00114556

Evaluating model for: Lamp
Run 134/144 completed in 1752.08 seconds with: {'MAE': np.float32(0.9531616), 'MSE': np.float32(32.592396), 'RMSE': np.float32(5.708975), 'SAE': np.float32(0.19528279), 'NDE': np.float32(0.4611655)}

Run 135/144: hidden=256, seq_len=1080, stride=0.1, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.005026
Validation Loss: 0.00477945
Epoch [2/200], Train Loss: 0.004997
Validation Loss: 0.00478176
Epoch [3/200], Train Loss: 0.004972
Validation Loss: 0.00476869
Epoch [4/200], Train Loss: 0.004989
Validation Loss: 0.00475414
Epoch [5/200], Train Loss: 0.004926
Validation Loss: 0.00471031
Epoch [6/200], Train Loss: 0.004892
Validation Loss: 0.00470672
Epoch [7/200], Train Loss: 0.004895
Validation Loss: 0.00471301
Epoch [8/200], Train Loss: 0.004885
Validation Loss: 0.00470326
Epoch [9/200], Train Loss: 0.004905
Validation Loss: 0.00470361
Epoch [10/200], Train Loss: 0.004904
Validation Loss: 0.00470486
Epoch [11/200], Train Loss: 0.004876
Validation Loss: 0.00469953
Epoch [12/200], Train Loss: 0.004903
Validation Loss: 0.00469835
Epoch [13/200], Train Loss: 0.004876
Validation Loss: 0.00469617
Epoch [14/200], Train Loss: 0.004864
Validation Loss: 0.00469716
Epoch [15/200], Train Loss: 0.004872
Validation Loss: 0.00469355
Epoch [16/200], Train Loss: 0.004894
Validation Loss: 0.00470648
Epoch [17/200], Train Loss: 0.004891
Validation Loss: 0.00469048
Epoch [18/200], Train Loss: 0.004875
Validation Loss: 0.00468998
Epoch [19/200], Train Loss: 0.004915
Validation Loss: 0.00469385
Epoch [20/200], Train Loss: 0.004894
Validation Loss: 0.00468931
Epoch [21/200], Train Loss: 0.004867
Validation Loss: 0.00468697
Epoch [22/200], Train Loss: 0.004901
Validation Loss: 0.00470047
Epoch [23/200], Train Loss: 0.004869
Validation Loss: 0.00469600
Epoch [24/200], Train Loss: 0.004866
Validation Loss: 0.00468703
Epoch [25/200], Train Loss: 0.004853
Validation Loss: 0.00468687
Epoch [26/200], Train Loss: 0.004880
Validation Loss: 0.00468709
Epoch [27/200], Train Loss: 0.004859
Validation Loss: 0.00468540
Epoch [28/200], Train Loss: 0.004876
Validation Loss: 0.00468772
Epoch [29/200], Train Loss: 0.004875
Validation Loss: 0.00470346
Epoch [30/200], Train Loss: 0.004901
Validation Loss: 0.00469030
Epoch [31/200], Train Loss: 0.004868
Validation Loss: 0.00469076
Epoch [32/200], Train Loss: 0.004871
Validation Loss: 0.00468641
Epoch [33/200], Train Loss: 0.004865
Validation Loss: 0.00468389
Epoch [34/200], Train Loss: 0.004868
Validation Loss: 0.00468513
Epoch [35/200], Train Loss: 0.004874
Validation Loss: 0.00468984
Epoch [36/200], Train Loss: 0.004868
Validation Loss: 0.00468974
Epoch [37/200], Train Loss: 0.004880
Validation Loss: 0.00469027
Epoch [38/200], Train Loss: 0.004867
Validation Loss: 0.00469565
Epoch [39/200], Train Loss: 0.004882
Validation Loss: 0.00468510
Epoch [40/200], Train Loss: 0.004862
Validation Loss: 0.00468823
Epoch [41/200], Train Loss: 0.004863
Validation Loss: 0.00468419
Epoch [42/200], Train Loss: 0.004863
Validation Loss: 0.00468950
Epoch [43/200], Train Loss: 0.004842
Validation Loss: 0.00469080
Early stopping triggered

Evaluating model for: Lamp
Run 135/144 completed in 471.34 seconds with: {'MAE': np.float32(2.565816), 'MSE': np.float32(146.99785), 'RMSE': np.float32(12.124267), 'SAE': np.float32(0.07896204), 'NDE': np.float32(0.9793884)}

Run 136/144: hidden=256, seq_len=1080, stride=0.1, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 3002 windows

Epoch [1/200], Train Loss: 0.005013
Validation Loss: 0.00478474
Epoch [2/200], Train Loss: 0.005003
Validation Loss: 0.00477756
Epoch [3/200], Train Loss: 0.004985
Validation Loss: 0.00477547
Epoch [4/200], Train Loss: 0.004984
Validation Loss: 0.00476826
Epoch [5/200], Train Loss: 0.004938
Validation Loss: 0.00472946
Epoch [6/200], Train Loss: 0.004896
Validation Loss: 0.00471144
Epoch [7/200], Train Loss: 0.004885
Validation Loss: 0.00471845
Epoch [8/200], Train Loss: 0.004917
Validation Loss: 0.00470279
Epoch [9/200], Train Loss: 0.004904
Validation Loss: 0.00470463
Epoch [10/200], Train Loss: 0.004877
Validation Loss: 0.00470056
Epoch [11/200], Train Loss: 0.004896
Validation Loss: 0.00470061
Epoch [12/200], Train Loss: 0.004854
Validation Loss: 0.00470111
Epoch [13/200], Train Loss: 0.004884
Validation Loss: 0.00469430
Epoch [14/200], Train Loss: 0.004865
Validation Loss: 0.00469416
Epoch [15/200], Train Loss: 0.004898
Validation Loss: 0.00469483
Epoch [16/200], Train Loss: 0.004886
Validation Loss: 0.00469235
Epoch [17/200], Train Loss: 0.004892
Validation Loss: 0.00469235
Epoch [18/200], Train Loss: 0.004863
Validation Loss: 0.00469162
Epoch [19/200], Train Loss: 0.004894
Validation Loss: 0.00469756
Epoch [20/200], Train Loss: 0.004897
Validation Loss: 0.00468930
Epoch [21/200], Train Loss: 0.004874
Validation Loss: 0.00469319
Epoch [22/200], Train Loss: 0.004879
Validation Loss: 0.00468690
Epoch [23/200], Train Loss: 0.004873
Validation Loss: 0.00468670
Epoch [24/200], Train Loss: 0.004853
Validation Loss: 0.00468567
Epoch [25/200], Train Loss: 0.004887
Validation Loss: 0.00468680
Epoch [26/200], Train Loss: 0.004876
Validation Loss: 0.00468525
Epoch [27/200], Train Loss: 0.004865
Validation Loss: 0.00468858
Epoch [28/200], Train Loss: 0.004850
Validation Loss: 0.00468648
Epoch [29/200], Train Loss: 0.004861
Validation Loss: 0.00468517
Epoch [30/200], Train Loss: 0.004869
Validation Loss: 0.00468612
Epoch [31/200], Train Loss: 0.004861
Validation Loss: 0.00468692
Epoch [32/200], Train Loss: 0.004884
Validation Loss: 0.00468537
Epoch [33/200], Train Loss: 0.004853
Validation Loss: 0.00468637
Epoch [34/200], Train Loss: 0.004884
Validation Loss: 0.00469044
Epoch [35/200], Train Loss: 0.004869
Validation Loss: 0.00468534
Epoch [36/200], Train Loss: 0.004890
Validation Loss: 0.00468933
Epoch [37/200], Train Loss: 0.004869
Validation Loss: 0.00468617
Epoch [38/200], Train Loss: 0.004862
Validation Loss: 0.00468511
Epoch [39/200], Train Loss: 0.004860
Validation Loss: 0.00469050
Epoch [40/200], Train Loss: 0.004867
Validation Loss: 0.00469488
Epoch [41/200], Train Loss: 0.004870
Validation Loss: 0.00468493
Epoch [42/200], Train Loss: 0.004857
Validation Loss: 0.00468522
Epoch [43/200], Train Loss: 0.004867
Validation Loss: 0.00469845
Epoch [44/200], Train Loss: 0.004860
Validation Loss: 0.00469493
Epoch [45/200], Train Loss: 0.004890
Validation Loss: 0.00468576
Epoch [46/200], Train Loss: 0.004853
Validation Loss: 0.00468724
Epoch [47/200], Train Loss: 0.004855
Validation Loss: 0.00468352
Epoch [48/200], Train Loss: 0.004879
Validation Loss: 0.00468652
Epoch [49/200], Train Loss: 0.004846
Validation Loss: 0.00469300
Epoch [50/200], Train Loss: 0.004890
Validation Loss: 0.00468501
Epoch [51/200], Train Loss: 0.004869
Validation Loss: 0.00468358
Epoch [52/200], Train Loss: 0.004867
Validation Loss: 0.00468429
Epoch [53/200], Train Loss: 0.004855
Validation Loss: 0.00468725
Epoch [54/200], Train Loss: 0.004869
Validation Loss: 0.00468795
Epoch [55/200], Train Loss: 0.004852
Validation Loss: 0.00468689
Epoch [56/200], Train Loss: 0.004848
Validation Loss: 0.00468518
Epoch [57/200], Train Loss: 0.004877
Validation Loss: 0.00468460
Early stopping triggered

Evaluating model for: Lamp
Run 136/144 completed in 908.67 seconds with: {'MAE': np.float32(2.305358), 'MSE': np.float32(146.91988), 'RMSE': np.float32(12.121051), 'SAE': np.float32(0.1436466), 'NDE': np.float32(0.9791224)}

Run 137/144: hidden=256, seq_len=1080, stride=0.25, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.005589
Validation Loss: 0.00245034
Epoch [2/200], Train Loss: 0.004863
Validation Loss: 0.00262539
Epoch [3/200], Train Loss: 0.005055
Validation Loss: 0.00249409
Epoch [4/200], Train Loss: 0.004953
Validation Loss: 0.00243845
Epoch [5/200], Train Loss: 0.004891
Validation Loss: 0.00243987
Epoch [6/200], Train Loss: 0.004987
Validation Loss: 0.00245621
Epoch [7/200], Train Loss: 0.004961
Validation Loss: 0.00244919
Epoch [8/200], Train Loss: 0.004955
Validation Loss: 0.00243971
Epoch [9/200], Train Loss: 0.004932
Validation Loss: 0.00244139
Epoch [10/200], Train Loss: 0.004981
Validation Loss: 0.00243714
Epoch [11/200], Train Loss: 0.005010
Validation Loss: 0.00243515
Epoch [12/200], Train Loss: 0.004984
Validation Loss: 0.00243843
Epoch [13/200], Train Loss: 0.004850
Validation Loss: 0.00243910
Epoch [14/200], Train Loss: 0.004834
Validation Loss: 0.00243600
Epoch [15/200], Train Loss: 0.004901
Validation Loss: 0.00244452
Epoch [16/200], Train Loss: 0.004858
Validation Loss: 0.00244269
Epoch [17/200], Train Loss: 0.004870
Validation Loss: 0.00244406
Epoch [18/200], Train Loss: 0.004878
Validation Loss: 0.00244028
Epoch [19/200], Train Loss: 0.004903
Validation Loss: 0.00244489
Epoch [20/200], Train Loss: 0.004874
Validation Loss: 0.00244059
Epoch [21/200], Train Loss: 0.004915
Validation Loss: 0.00244056
Early stopping triggered

Evaluating model for: Lamp
Run 137/144 completed in 54.10 seconds with: {'MAE': np.float32(3.6611977), 'MSE': np.float32(226.02231), 'RMSE': np.float32(15.034039), 'SAE': np.float32(0.12870495), 'NDE': np.float32(0.9793701)}

Run 138/144: hidden=256, seq_len=1080, stride=0.25, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.004969
Validation Loss: 0.00245020
Epoch [2/200], Train Loss: 0.005007
Validation Loss: 0.00247377
Epoch [3/200], Train Loss: 0.004893
Validation Loss: 0.00244331
Epoch [4/200], Train Loss: 0.004993
Validation Loss: 0.00246813
Epoch [5/200], Train Loss: 0.004990
Validation Loss: 0.00243497
Epoch [6/200], Train Loss: 0.004896
Validation Loss: 0.00244054
Epoch [7/200], Train Loss: 0.004929
Validation Loss: 0.00245310
Epoch [8/200], Train Loss: 0.004863
Validation Loss: 0.00243285
Epoch [9/200], Train Loss: 0.004926
Validation Loss: 0.00244416
Epoch [10/200], Train Loss: 0.004833
Validation Loss: 0.00245915
Epoch [11/200], Train Loss: 0.004879
Validation Loss: 0.00243469
Epoch [12/200], Train Loss: 0.004857
Validation Loss: 0.00245383
Epoch [13/200], Train Loss: 0.004815
Validation Loss: 0.00243880
Epoch [14/200], Train Loss: 0.005072
Validation Loss: 0.00245847
Epoch [15/200], Train Loss: 0.004843
Validation Loss: 0.00243572
Epoch [16/200], Train Loss: 0.004859
Validation Loss: 0.00245544
Epoch [17/200], Train Loss: 0.004917
Validation Loss: 0.00243817
Epoch [18/200], Train Loss: 0.004900
Validation Loss: 0.00243847
Early stopping triggered

Evaluating model for: Lamp
Run 138/144 completed in 63.39 seconds with: {'MAE': np.float32(3.642325), 'MSE': np.float32(225.7956), 'RMSE': np.float32(15.026496), 'SAE': np.float32(0.1407769), 'NDE': np.float32(0.978879)}

Run 139/144: hidden=256, seq_len=1080, stride=0.25, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.005063
Validation Loss: 0.00255609
Epoch [2/200], Train Loss: 0.005046
Validation Loss: 0.00247982
Epoch [3/200], Train Loss: 0.004913
Validation Loss: 0.00245001
Epoch [4/200], Train Loss: 0.005058
Validation Loss: 0.00246455
Epoch [5/200], Train Loss: 0.005027
Validation Loss: 0.00248013
Epoch [6/200], Train Loss: 0.004906
Validation Loss: 0.00245763
Epoch [7/200], Train Loss: 0.005006
Validation Loss: 0.00245808
Epoch [8/200], Train Loss: 0.005000
Validation Loss: 0.00246580
Epoch [9/200], Train Loss: 0.004924
Validation Loss: 0.00245128
Epoch [10/200], Train Loss: 0.004967
Validation Loss: 0.00245003
Epoch [11/200], Train Loss: 0.004933
Validation Loss: 0.00243356
Epoch [12/200], Train Loss: 0.004807
Validation Loss: 0.00244045
Epoch [13/200], Train Loss: 0.004865
Validation Loss: 0.00246346
Epoch [14/200], Train Loss: 0.004922
Validation Loss: 0.00244734
Epoch [15/200], Train Loss: 0.004902
Validation Loss: 0.00244027
Epoch [16/200], Train Loss: 0.004907
Validation Loss: 0.00243444
Epoch [17/200], Train Loss: 0.004980
Validation Loss: 0.00243654
Epoch [18/200], Train Loss: 0.004928
Validation Loss: 0.00244164
Epoch [19/200], Train Loss: 0.004903
Validation Loss: 0.00243524
Epoch [20/200], Train Loss: 0.004888
Validation Loss: 0.00244554
Epoch [21/200], Train Loss: 0.004919
Validation Loss: 0.00243948
Early stopping triggered

Evaluating model for: Lamp
Run 139/144 completed in 93.51 seconds with: {'MAE': np.float32(3.6887221), 'MSE': np.float32(225.82965), 'RMSE': np.float32(15.02763), 'SAE': np.float32(0.10074393), 'NDE': np.float32(0.97895265)}

Run 140/144: hidden=256, seq_len=1080, stride=0.25, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 1214 windows

Epoch [1/200], Train Loss: 0.005052
Validation Loss: 0.00255295
Epoch [2/200], Train Loss: 0.005007
Validation Loss: 0.00245769
Epoch [3/200], Train Loss: 0.004996
Validation Loss: 0.00245519
Epoch [4/200], Train Loss: 0.004992
Validation Loss: 0.00248556
Epoch [5/200], Train Loss: 0.004924
Validation Loss: 0.00246326
Epoch [6/200], Train Loss: 0.004964
Validation Loss: 0.00245858
Epoch [7/200], Train Loss: 0.005014
Validation Loss: 0.00247282
Epoch [8/200], Train Loss: 0.005015
Validation Loss: 0.00245913
Epoch [9/200], Train Loss: 0.005031
Validation Loss: 0.00246468
Epoch [10/200], Train Loss: 0.004941
Validation Loss: 0.00246072
Epoch [11/200], Train Loss: 0.004957
Validation Loss: 0.00245858
Epoch [12/200], Train Loss: 0.004858
Validation Loss: 0.00245770
Epoch [13/200], Train Loss: 0.004909
Validation Loss: 0.00243333
Epoch [14/200], Train Loss: 0.004851
Validation Loss: 0.00245088
Epoch [15/200], Train Loss: 0.004845
Validation Loss: 0.00242867
Epoch [16/200], Train Loss: 0.004893
Validation Loss: 0.00245864
Epoch [17/200], Train Loss: 0.004910
Validation Loss: 0.00243208
Epoch [18/200], Train Loss: 0.004886
Validation Loss: 0.00243218
Epoch [19/200], Train Loss: 0.004861
Validation Loss: 0.00245153
Epoch [20/200], Train Loss: 0.004836
Validation Loss: 0.00243280
Epoch [21/200], Train Loss: 0.004842
Validation Loss: 0.00244679
Epoch [22/200], Train Loss: 0.004902
Validation Loss: 0.00243429
Epoch [23/200], Train Loss: 0.004937
Validation Loss: 0.00244824
Epoch [24/200], Train Loss: 0.004856
Validation Loss: 0.00243284
Epoch [25/200], Train Loss: 0.004954
Validation Loss: 0.00244133
Early stopping triggered

Evaluating model for: Lamp
Run 140/144 completed in 160.87 seconds with: {'MAE': np.float32(3.737005), 'MSE': np.float32(225.49174), 'RMSE': np.float32(15.016382), 'SAE': np.float32(0.020781478), 'NDE': np.float32(0.97821975)}

Run 141/144: hidden=256, seq_len=1080, stride=0.5, num_layers=2
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.005558
Validation Loss: 0.00565051
Epoch [2/200], Train Loss: 0.004857
Validation Loss: 0.00525228
Epoch [3/200], Train Loss: 0.004699
Validation Loss: 0.00521196
Epoch [4/200], Train Loss: 0.004682
Validation Loss: 0.00526953
Epoch [5/200], Train Loss: 0.004663
Validation Loss: 0.00524311
Epoch [6/200], Train Loss: 0.004643
Validation Loss: 0.00519887
Epoch [7/200], Train Loss: 0.004630
Validation Loss: 0.00519580
Epoch [8/200], Train Loss: 0.004641
Validation Loss: 0.00521020
Epoch [9/200], Train Loss: 0.004605
Validation Loss: 0.00520474
Epoch [10/200], Train Loss: 0.004593
Validation Loss: 0.00519072
Epoch [11/200], Train Loss: 0.004608
Validation Loss: 0.00517951
Epoch [12/200], Train Loss: 0.004689
Validation Loss: 0.00517417
Epoch [13/200], Train Loss: 0.004568
Validation Loss: 0.00517031
Epoch [14/200], Train Loss: 0.004624
Validation Loss: 0.00516573
Epoch [15/200], Train Loss: 0.004611
Validation Loss: 0.00516440
Epoch [16/200], Train Loss: 0.004577
Validation Loss: 0.00516263
Epoch [17/200], Train Loss: 0.004617
Validation Loss: 0.00515892
Epoch [18/200], Train Loss: 0.004635
Validation Loss: 0.00515403
Epoch [19/200], Train Loss: 0.004646
Validation Loss: 0.00514910
Epoch [20/200], Train Loss: 0.004593
Validation Loss: 0.00514441
Epoch [21/200], Train Loss: 0.004576
Validation Loss: 0.00514069
Epoch [22/200], Train Loss: 0.004598
Validation Loss: 0.00513815
Epoch [23/200], Train Loss: 0.004625
Validation Loss: 0.00513539
Epoch [24/200], Train Loss: 0.004594
Validation Loss: 0.00513131
Epoch [25/200], Train Loss: 0.004584
Validation Loss: 0.00512699
Epoch [26/200], Train Loss: 0.004513
Validation Loss: 0.00512377
Epoch [27/200], Train Loss: 0.004629
Validation Loss: 0.00512452
Epoch [28/200], Train Loss: 0.004593
Validation Loss: 0.00512002
Epoch [29/200], Train Loss: 0.004592
Validation Loss: 0.00511669
Epoch [30/200], Train Loss: 0.004590
Validation Loss: 0.00511628
Epoch [31/200], Train Loss: 0.004551
Validation Loss: 0.00511629
Epoch [32/200], Train Loss: 0.004565
Validation Loss: 0.00511439
Epoch [33/200], Train Loss: 0.004585
Validation Loss: 0.00511400
Epoch [34/200], Train Loss: 0.004563
Validation Loss: 0.00511293
Epoch [35/200], Train Loss: 0.004590
Validation Loss: 0.00511142
Epoch [36/200], Train Loss: 0.004551
Validation Loss: 0.00511196
Epoch [37/200], Train Loss: 0.004587
Validation Loss: 0.00511268
Epoch [38/200], Train Loss: 0.004581
Validation Loss: 0.00511168
Epoch [39/200], Train Loss: 0.004536
Validation Loss: 0.00511064
Epoch [40/200], Train Loss: 0.004637
Validation Loss: 0.00511261
Epoch [41/200], Train Loss: 0.004532
Validation Loss: 0.00510973
Epoch [42/200], Train Loss: 0.004600
Validation Loss: 0.00511089
Epoch [43/200], Train Loss: 0.004584
Validation Loss: 0.00511161
Epoch [44/200], Train Loss: 0.004572
Validation Loss: 0.00511232
Epoch [45/200], Train Loss: 0.004567
Validation Loss: 0.00511029
Epoch [46/200], Train Loss: 0.004572
Validation Loss: 0.00510960
Epoch [47/200], Train Loss: 0.004629
Validation Loss: 0.00511110
Epoch [48/200], Train Loss: 0.004577
Validation Loss: 0.00510996
Epoch [49/200], Train Loss: 0.004588
Validation Loss: 0.00510971
Epoch [50/200], Train Loss: 0.004541
Validation Loss: 0.00511064
Epoch [51/200], Train Loss: 0.004557
Validation Loss: 0.00511046
Epoch [52/200], Train Loss: 0.004593
Validation Loss: 0.00511046
Epoch [53/200], Train Loss: 0.004620
Validation Loss: 0.00511113
Epoch [54/200], Train Loss: 0.004558
Validation Loss: 0.00510820
Epoch [55/200], Train Loss: 0.004585
Validation Loss: 0.00510868
Epoch [56/200], Train Loss: 0.004557
Validation Loss: 0.00511073
Epoch [57/200], Train Loss: 0.004595
Validation Loss: 0.00511063
Epoch [58/200], Train Loss: 0.004593
Validation Loss: 0.00510917
Epoch [59/200], Train Loss: 0.004549
Validation Loss: 0.00510890
Epoch [60/200], Train Loss: 0.004557
Validation Loss: 0.00510971
Epoch [61/200], Train Loss: 0.004552
Validation Loss: 0.00511057
Epoch [62/200], Train Loss: 0.004567
Validation Loss: 0.00510967
Epoch [63/200], Train Loss: 0.004573
Validation Loss: 0.00510921
Epoch [64/200], Train Loss: 0.004595
Validation Loss: 0.00510910
Early stopping triggered

Evaluating model for: Lamp
Run 141/144 completed in 85.13 seconds with: {'MAE': np.float32(3.1174157), 'MSE': np.float32(208.92589), 'RMSE': np.float32(14.454268), 'SAE': np.float32(0.30010468), 'NDE': np.float32(0.98035026)}

Run 142/144: hidden=256, seq_len=1080, stride=0.5, num_layers=3
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.004668
Validation Loss: 0.00521661
Epoch [2/200], Train Loss: 0.004632
Validation Loss: 0.00521416
Epoch [3/200], Train Loss: 0.004583
Validation Loss: 0.00520254
Epoch [4/200], Train Loss: 0.004656
Validation Loss: 0.00520052
Epoch [5/200], Train Loss: 0.004627
Validation Loss: 0.00518894
Epoch [6/200], Train Loss: 0.004660
Validation Loss: 0.00518122
Epoch [7/200], Train Loss: 0.004592
Validation Loss: 0.00517235
Epoch [8/200], Train Loss: 0.004685
Validation Loss: 0.00517040
Epoch [9/200], Train Loss: 0.004601
Validation Loss: 0.00515496
Epoch [10/200], Train Loss: 0.004589
Validation Loss: 0.00514872
Epoch [11/200], Train Loss: 0.004637
Validation Loss: 0.00513590
Epoch [12/200], Train Loss: 0.004602
Validation Loss: 0.00512278
Epoch [13/200], Train Loss: 0.004520
Validation Loss: 0.00512492
Epoch [14/200], Train Loss: 0.004564
Validation Loss: 0.00511051
Epoch [15/200], Train Loss: 0.004580
Validation Loss: 0.00510843
Epoch [16/200], Train Loss: 0.004546
Validation Loss: 0.00510945
Epoch [17/200], Train Loss: 0.004625
Validation Loss: 0.00510894
Epoch [18/200], Train Loss: 0.004564
Validation Loss: 0.00510630
Epoch [19/200], Train Loss: 0.004595
Validation Loss: 0.00512004
Epoch [20/200], Train Loss: 0.004535
Validation Loss: 0.00510785
Epoch [21/200], Train Loss: 0.004536
Validation Loss: 0.00511233
Epoch [22/200], Train Loss: 0.004551
Validation Loss: 0.00511340
Epoch [23/200], Train Loss: 0.004575
Validation Loss: 0.00510858
Epoch [24/200], Train Loss: 0.004594
Validation Loss: 0.00511087
Epoch [25/200], Train Loss: 0.004574
Validation Loss: 0.00510882
Epoch [26/200], Train Loss: 0.004582
Validation Loss: 0.00511009
Epoch [27/200], Train Loss: 0.004569
Validation Loss: 0.00510722
Epoch [28/200], Train Loss: 0.004597
Validation Loss: 0.00511214
Early stopping triggered

Evaluating model for: Lamp
Run 142/144 completed in 51.32 seconds with: {'MAE': np.float32(3.0544152), 'MSE': np.float32(209.21837), 'RMSE': np.float32(14.464383), 'SAE': np.float32(0.37901613), 'NDE': np.float32(0.9810363)}

Run 143/144: hidden=256, seq_len=1080, stride=0.5, num_layers=4
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.004837
Validation Loss: 0.00523829
Epoch [2/200], Train Loss: 0.004631
Validation Loss: 0.00525155
Epoch [3/200], Train Loss: 0.004680
Validation Loss: 0.00524479
Epoch [4/200], Train Loss: 0.004628
Validation Loss: 0.00521886
Epoch [5/200], Train Loss: 0.004609
Validation Loss: 0.00522977
Epoch [6/200], Train Loss: 0.004656
Validation Loss: 0.00523618
Epoch [7/200], Train Loss: 0.004614
Validation Loss: 0.00522266
Epoch [8/200], Train Loss: 0.004647
Validation Loss: 0.00521661
Epoch [9/200], Train Loss: 0.004688
Validation Loss: 0.00521599
Epoch [10/200], Train Loss: 0.004619
Validation Loss: 0.00521508
Epoch [11/200], Train Loss: 0.004640
Validation Loss: 0.00521476
Epoch [12/200], Train Loss: 0.004653
Validation Loss: 0.00521649
Epoch [13/200], Train Loss: 0.004647
Validation Loss: 0.00521411
Epoch [14/200], Train Loss: 0.004594
Validation Loss: 0.00520962
Epoch [15/200], Train Loss: 0.004668
Validation Loss: 0.00520764
Epoch [16/200], Train Loss: 0.004635
Validation Loss: 0.00520526
Epoch [17/200], Train Loss: 0.004649
Validation Loss: 0.00520319
Epoch [18/200], Train Loss: 0.004587
Validation Loss: 0.00519888
Epoch [19/200], Train Loss: 0.004593
Validation Loss: 0.00519386
Epoch [20/200], Train Loss: 0.004586
Validation Loss: 0.00518754
Epoch [21/200], Train Loss: 0.004660
Validation Loss: 0.00518052
Epoch [22/200], Train Loss: 0.004577
Validation Loss: 0.00516699
Epoch [23/200], Train Loss: 0.004634
Validation Loss: 0.00515415
Epoch [24/200], Train Loss: 0.004573
Validation Loss: 0.00513793
Epoch [25/200], Train Loss: 0.004572
Validation Loss: 0.00512303
Epoch [26/200], Train Loss: 0.004544
Validation Loss: 0.00511134
Epoch [27/200], Train Loss: 0.004637
Validation Loss: 0.00511366
Epoch [28/200], Train Loss: 0.004619
Validation Loss: 0.00510915
Epoch [29/200], Train Loss: 0.004609
Validation Loss: 0.00511263
Epoch [30/200], Train Loss: 0.004548
Validation Loss: 0.00511169
Epoch [31/200], Train Loss: 0.004566
Validation Loss: 0.00511033
Epoch [32/200], Train Loss: 0.004546
Validation Loss: 0.00511609
Epoch [33/200], Train Loss: 0.004555
Validation Loss: 0.00511252
Epoch [34/200], Train Loss: 0.004576
Validation Loss: 0.00511064
Epoch [35/200], Train Loss: 0.004570
Validation Loss: 0.00511259
Epoch [36/200], Train Loss: 0.004577
Validation Loss: 0.00510982
Epoch [37/200], Train Loss: 0.004502
Validation Loss: 0.00510737
Epoch [38/200], Train Loss: 0.004538
Validation Loss: 0.00511797
Epoch [39/200], Train Loss: 0.004603
Validation Loss: 0.00510865
Epoch [40/200], Train Loss: 0.004542
Validation Loss: 0.00510688
Epoch [41/200], Train Loss: 0.004604
Validation Loss: 0.00511388
Epoch [42/200], Train Loss: 0.004546
Validation Loss: 0.00510873
Epoch [43/200], Train Loss: 0.004614
Validation Loss: 0.00510882
Epoch [44/200], Train Loss: 0.004637
Validation Loss: 0.00510734
Epoch [45/200], Train Loss: 0.004588
Validation Loss: 0.00510653
Epoch [46/200], Train Loss: 0.004553
Validation Loss: 0.00510872
Epoch [47/200], Train Loss: 0.004530
Validation Loss: 0.00510607
Epoch [48/200], Train Loss: 0.004572
Validation Loss: 0.00510915
Epoch [49/200], Train Loss: 0.004534
Validation Loss: 0.00510731
Epoch [50/200], Train Loss: 0.004605
Validation Loss: 0.00510679
Epoch [51/200], Train Loss: 0.004598
Validation Loss: 0.00510561
Epoch [52/200], Train Loss: 0.004563
Validation Loss: 0.00510731
Epoch [53/200], Train Loss: 0.004572
Validation Loss: 0.00510532
Epoch [54/200], Train Loss: 0.004544
Validation Loss: 0.00510789
Epoch [55/200], Train Loss: 0.004551
Validation Loss: 0.00510431
Epoch [56/200], Train Loss: 0.004558
Validation Loss: 0.00510582
Epoch [57/200], Train Loss: 0.004613
Validation Loss: 0.00511069
Epoch [58/200], Train Loss: 0.004529
Validation Loss: 0.00510325
Epoch [59/200], Train Loss: 0.004533
Validation Loss: 0.00510406
Epoch [60/200], Train Loss: 0.004609
Validation Loss: 0.00511292
Epoch [61/200], Train Loss: 0.004577
Validation Loss: 0.00510365
Epoch [62/200], Train Loss: 0.004562
Validation Loss: 0.00510210
Epoch [63/200], Train Loss: 0.004522
Validation Loss: 0.00510488
Epoch [64/200], Train Loss: 0.004577
Validation Loss: 0.00510899
Epoch [65/200], Train Loss: 0.004541
Validation Loss: 0.00510150
Epoch [66/200], Train Loss: 0.004626
Validation Loss: 0.00510475
Epoch [67/200], Train Loss: 0.004566
Validation Loss: 0.00510320
Epoch [68/200], Train Loss: 0.004587
Validation Loss: 0.00510218
Epoch [69/200], Train Loss: 0.004551
Validation Loss: 0.00510280
Epoch [70/200], Train Loss: 0.004546
Validation Loss: 0.00510192
Epoch [71/200], Train Loss: 0.004517
Validation Loss: 0.00510151
Epoch [72/200], Train Loss: 0.004603
Validation Loss: 0.00510456
Epoch [73/200], Train Loss: 0.004555
Validation Loss: 0.00510012
Epoch [74/200], Train Loss: 0.004554
Validation Loss: 0.00510001
Epoch [75/200], Train Loss: 0.004537
Validation Loss: 0.00510156
Epoch [76/200], Train Loss: 0.004547
Validation Loss: 0.00510210
Epoch [77/200], Train Loss: 0.004553
Validation Loss: 0.00509907
Epoch [78/200], Train Loss: 0.004515
Validation Loss: 0.00510022
Epoch [79/200], Train Loss: 0.004577
Validation Loss: 0.00510217
Epoch [80/200], Train Loss: 0.004573
Validation Loss: 0.00510048
Epoch [81/200], Train Loss: 0.004586
Validation Loss: 0.00509996
Epoch [82/200], Train Loss: 0.004548
Validation Loss: 0.00509741
Epoch [83/200], Train Loss: 0.004592
Validation Loss: 0.00509909
Epoch [84/200], Train Loss: 0.004544
Validation Loss: 0.00509841
Epoch [85/200], Train Loss: 0.004530
Validation Loss: 0.00509757
Epoch [86/200], Train Loss: 0.004554
Validation Loss: 0.00509814
Epoch [87/200], Train Loss: 0.004548
Validation Loss: 0.00509704
Epoch [88/200], Train Loss: 0.004566
Validation Loss: 0.00509828
Epoch [89/200], Train Loss: 0.004562
Validation Loss: 0.00509851
Epoch [90/200], Train Loss: 0.004537
Validation Loss: 0.00509557
Epoch [91/200], Train Loss: 0.004540
Validation Loss: 0.00509596
Epoch [92/200], Train Loss: 0.004565
Validation Loss: 0.00509671
Epoch [93/200], Train Loss: 0.004574
Validation Loss: 0.00509743
Epoch [94/200], Train Loss: 0.004624
Validation Loss: 0.00509595
Epoch [95/200], Train Loss: 0.004594
Validation Loss: 0.00509464
Epoch [96/200], Train Loss: 0.004553
Validation Loss: 0.00509541
Epoch [97/200], Train Loss: 0.004601
Validation Loss: 0.00509591
Epoch [98/200], Train Loss: 0.004549
Validation Loss: 0.00509368
Epoch [99/200], Train Loss: 0.004554
Validation Loss: 0.00509447
Epoch [100/200], Train Loss: 0.004542
Validation Loss: 0.00509932
Epoch [101/200], Train Loss: 0.004585
Validation Loss: 0.00509567
Epoch [102/200], Train Loss: 0.004549
Validation Loss: 0.00509342
Epoch [103/200], Train Loss: 0.004571
Validation Loss: 0.00509539
Epoch [104/200], Train Loss: 0.004574
Validation Loss: 0.00509435
Epoch [105/200], Train Loss: 0.004575
Validation Loss: 0.00509393
Epoch [106/200], Train Loss: 0.004552
Validation Loss: 0.00509257
Epoch [107/200], Train Loss: 0.004550
Validation Loss: 0.00509523
Epoch [108/200], Train Loss: 0.004536
Validation Loss: 0.00509518
Epoch [109/200], Train Loss: 0.004551
Validation Loss: 0.00509347
Epoch [110/200], Train Loss: 0.004562
Validation Loss: 0.00509251
Epoch [111/200], Train Loss: 0.004547
Validation Loss: 0.00509310
Epoch [112/200], Train Loss: 0.004585
Validation Loss: 0.00509868
Epoch [113/200], Train Loss: 0.004598
Validation Loss: 0.00509386
Epoch [114/200], Train Loss: 0.004579
Validation Loss: 0.00509312
Epoch [115/200], Train Loss: 0.004571
Validation Loss: 0.00509367
Epoch [116/200], Train Loss: 0.004577
Validation Loss: 0.00509681
Epoch [117/200], Train Loss: 0.004565
Validation Loss: 0.00509362
Epoch [118/200], Train Loss: 0.004566
Validation Loss: 0.00509278
Epoch [119/200], Train Loss: 0.004566
Validation Loss: 0.00509284
Epoch [120/200], Train Loss: 0.004494
Validation Loss: 0.00509422
Early stopping triggered

Evaluating model for: Lamp
Run 143/144 completed in 275.99 seconds with: {'MAE': np.float32(2.9397628), 'MSE': np.float32(208.4801), 'RMSE': np.float32(14.43884), 'SAE': np.float32(0.30248278), 'NDE': np.float32(0.9793033)}

Run 144/144: hidden=256, seq_len=1080, stride=0.5, num_layers=5
Training and evaluating model for: Lamp
Dataset length: 618 windows

Epoch [1/200], Train Loss: 0.004752
Validation Loss: 0.00522235
Epoch [2/200], Train Loss: 0.004703
Validation Loss: 0.00524885
Epoch [3/200], Train Loss: 0.004690
Validation Loss: 0.00522531
Epoch [4/200], Train Loss: 0.004598
Validation Loss: 0.00522595
Epoch [5/200], Train Loss: 0.004635
Validation Loss: 0.00523621
Epoch [6/200], Train Loss: 0.004679
Validation Loss: 0.00522594
Epoch [7/200], Train Loss: 0.004639
Validation Loss: 0.00521988
Epoch [8/200], Train Loss: 0.004613
Validation Loss: 0.00522053
Epoch [9/200], Train Loss: 0.004647
Validation Loss: 0.00521946
Epoch [10/200], Train Loss: 0.004630
Validation Loss: 0.00522161
Epoch [11/200], Train Loss: 0.004632
Validation Loss: 0.00522137
Epoch [12/200], Train Loss: 0.004703
Validation Loss: 0.00522027
Epoch [13/200], Train Loss: 0.004626
Validation Loss: 0.00521793
Epoch [14/200], Train Loss: 0.004613
Validation Loss: 0.00521721
Epoch [15/200], Train Loss: 0.004659
Validation Loss: 0.00521777
Epoch [16/200], Train Loss: 0.004688
Validation Loss: 0.00521825
Epoch [17/200], Train Loss: 0.004651
Validation Loss: 0.00521514
Epoch [18/200], Train Loss: 0.004647
Validation Loss: 0.00521335
Epoch [19/200], Train Loss: 0.004663
Validation Loss: 0.00521217
Epoch [20/200], Train Loss: 0.004680
Validation Loss: 0.00521017
Epoch [21/200], Train Loss: 0.004592
Validation Loss: 0.00520501
Epoch [22/200], Train Loss: 0.004668
Validation Loss: 0.00520233
Epoch [23/200], Train Loss: 0.004609
Validation Loss: 0.00519444
Epoch [24/200], Train Loss: 0.004611
Validation Loss: 0.00518290
Epoch [25/200], Train Loss: 0.004645
Validation Loss: 0.00516744
Epoch [26/200], Train Loss: 0.004582
Validation Loss: 0.00514426
Epoch [27/200], Train Loss: 0.004536
Validation Loss: 0.00512196
Epoch [28/200], Train Loss: 0.004556
Validation Loss: 0.00511007
Epoch [29/200], Train Loss: 0.004631
Validation Loss: 0.00510966
Epoch [30/200], Train Loss: 0.004635
Validation Loss: 0.00510781
Epoch [31/200], Train Loss: 0.004598
Validation Loss: 0.00511326
Epoch [32/200], Train Loss: 0.004529
Validation Loss: 0.00511098
Epoch [33/200], Train Loss: 0.004586
Validation Loss: 0.00510986
Epoch [34/200], Train Loss: 0.004539
Validation Loss: 0.00510887
Epoch [35/200], Train Loss: 0.004583
Validation Loss: 0.00511756
Epoch [36/200], Train Loss: 0.004565
Validation Loss: 0.00510571
Epoch [37/200], Train Loss: 0.004618
Validation Loss: 0.00510750
Epoch [38/200], Train Loss: 0.004569
Validation Loss: 0.00510612
Epoch [39/200], Train Loss: 0.004533
Validation Loss: 0.00510531
Epoch [40/200], Train Loss: 0.004609
Validation Loss: 0.00511725
Epoch [41/200], Train Loss: 0.004578
Validation Loss: 0.00510459
Epoch [42/200], Train Loss: 0.004589
Validation Loss: 0.00510599
Epoch [43/200], Train Loss: 0.004517
Validation Loss: 0.00510846
Epoch [44/200], Train Loss: 0.004560
Validation Loss: 0.00510529
Epoch [45/200], Train Loss: 0.004580
Validation Loss: 0.00510259
Epoch [46/200], Train Loss: 0.004588
Validation Loss: 0.00510564
Epoch [47/200], Train Loss: 0.004533
Validation Loss: 0.00510360
Epoch [48/200], Train Loss: 0.004584
Validation Loss: 0.00510202
Epoch [49/200], Train Loss: 0.004592
Validation Loss: 0.00510226
Epoch [50/200], Train Loss: 0.004605
Validation Loss: 0.00510506
Epoch [51/200], Train Loss: 0.004564
Validation Loss: 0.00510133
Epoch [52/200], Train Loss: 0.004560
Validation Loss: 0.00510039
Epoch [53/200], Train Loss: 0.004551
Validation Loss: 0.00510502
Epoch [54/200], Train Loss: 0.004574
Validation Loss: 0.00510322
Epoch [55/200], Train Loss: 0.004527
Validation Loss: 0.00509916
Epoch [56/200], Train Loss: 0.004597
Validation Loss: 0.00510436
Epoch [57/200], Train Loss: 0.004577
Validation Loss: 0.00510127
Epoch [58/200], Train Loss: 0.004566
Validation Loss: 0.00509808
Epoch [59/200], Train Loss: 0.004635
Validation Loss: 0.00510520
Epoch [60/200], Train Loss: 0.004570
Validation Loss: 0.00509995
Epoch [61/200], Train Loss: 0.004560
Validation Loss: 0.00509774
Epoch [62/200], Train Loss: 0.004566
Validation Loss: 0.00509973
Epoch [63/200], Train Loss: 0.004562
Validation Loss: 0.00509957
Epoch [64/200], Train Loss: 0.004594
Validation Loss: 0.00509848
Epoch [65/200], Train Loss: 0.004581
Validation Loss: 0.00509683
Epoch [66/200], Train Loss: 0.004573
Validation Loss: 0.00509670
Epoch [67/200], Train Loss: 0.004581
Validation Loss: 0.00509788
Epoch [68/200], Train Loss: 0.004559
Validation Loss: 0.00509602
Epoch [69/200], Train Loss: 0.004617
Validation Loss: 0.00509642
Epoch [70/200], Train Loss: 0.004604
Validation Loss: 0.00509626
Epoch [71/200], Train Loss: 0.004576
Validation Loss: 0.00509478
Epoch [72/200], Train Loss: 0.004550
Validation Loss: 0.00509474
Epoch [73/200], Train Loss: 0.004546
Validation Loss: 0.00509636
Epoch [74/200], Train Loss: 0.004553
Validation Loss: 0.00509627
Epoch [75/200], Train Loss: 0.004560
Validation Loss: 0.00509467
Epoch [76/200], Train Loss: 0.004513
Validation Loss: 0.00509365
Epoch [77/200], Train Loss: 0.004566
Validation Loss: 0.00509557
Epoch [78/200], Train Loss: 0.004550
Validation Loss: 0.00509548
Epoch [79/200], Train Loss: 0.004553
Validation Loss: 0.00509347
Epoch [80/200], Train Loss: 0.004594
Validation Loss: 0.00509507
Epoch [81/200], Train Loss: 0.004544
Validation Loss: 0.00509333
Epoch [82/200], Train Loss: 0.004583
Validation Loss: 0.00509360
Epoch [83/200], Train Loss: 0.004522
Validation Loss: 0.00509345
Epoch [84/200], Train Loss: 0.004571
Validation Loss: 0.00509550
Epoch [85/200], Train Loss: 0.004496
Validation Loss: 0.00509284
Epoch [86/200], Train Loss: 0.004561
Validation Loss: 0.00509297
Epoch [87/200], Train Loss: 0.004542
Validation Loss: 0.00509680
Epoch [88/200], Train Loss: 0.004661
Validation Loss: 0.00509442
Epoch [89/200], Train Loss: 0.004555
Validation Loss: 0.00509233
Epoch [90/200], Train Loss: 0.004622
Validation Loss: 0.00509429
Epoch [91/200], Train Loss: 0.004530
Validation Loss: 0.00509339
Epoch [92/200], Train Loss: 0.004503
Validation Loss: 0.00509192
Epoch [93/200], Train Loss: 0.004598
Validation Loss: 0.00509364
Epoch [94/200], Train Loss: 0.004546
Validation Loss: 0.00509233
Epoch [95/200], Train Loss: 0.004581
Validation Loss: 0.00509284
Epoch [96/200], Train Loss: 0.004567
Validation Loss: 0.00509327
Epoch [97/200], Train Loss: 0.004597
Validation Loss: 0.00509190
Epoch [98/200], Train Loss: 0.004533
Validation Loss: 0.00509153
Epoch [99/200], Train Loss: 0.004584
Validation Loss: 0.00509458
Epoch [100/200], Train Loss: 0.004521
Validation Loss: 0.00509172
Epoch [101/200], Train Loss: 0.004606
Validation Loss: 0.00509353
Epoch [102/200], Train Loss: 0.004529
Validation Loss: 0.00509114
Epoch [103/200], Train Loss: 0.004578
Validation Loss: 0.00509283
Epoch [104/200], Train Loss: 0.004558
Validation Loss: 0.00509298
Epoch [105/200], Train Loss: 0.004555
Validation Loss: 0.00509206
Epoch [106/200], Train Loss: 0.004604
Validation Loss: 0.00509306
Epoch [107/200], Train Loss: 0.004540
Validation Loss: 0.00509134
Epoch [108/200], Train Loss: 0.004573
Validation Loss: 0.00509215
Epoch [109/200], Train Loss: 0.004602
Validation Loss: 0.00509288
Epoch [110/200], Train Loss: 0.004578
Validation Loss: 0.00509157
Epoch [111/200], Train Loss: 0.004539
Validation Loss: 0.00509106
Epoch [112/200], Train Loss: 0.004607
Validation Loss: 0.00509452
Epoch [113/200], Train Loss: 0.004584
Validation Loss: 0.00509155
Epoch [114/200], Train Loss: 0.004539
Validation Loss: 0.00509118
Epoch [115/200], Train Loss: 0.004562
Validation Loss: 0.00509217
Epoch [116/200], Train Loss: 0.004538
Validation Loss: 0.00509112
Epoch [117/200], Train Loss: 0.004546
Validation Loss: 0.00509349
Epoch [118/200], Train Loss: 0.004542
Validation Loss: 0.00509246
Epoch [119/200], Train Loss: 0.004514
Validation Loss: 0.00509157
Epoch [120/200], Train Loss: 0.004579
Validation Loss: 0.00509133
Epoch [121/200], Train Loss: 0.004517
Validation Loss: 0.00509138
Early stopping triggered

Evaluating model for: Lamp
Run 144/144 completed in 405.63 seconds with: {'MAE': np.float32(2.9456449), 'MSE': np.float32(208.46593), 'RMSE': np.float32(14.438349), 'SAE': np.float32(0.28698778), 'NDE': np.float32(0.97926986)}
     hidden_size  seq_length  stride  num_layers  eval_result
99           256         120    0.10           5     0.220106
98           256         120    0.10           4     0.224418
97           256         120    0.10           3     0.264457
50           128         120    0.10           4     0.278625
51           128         120    0.10           5     0.282488
..           ...         ...     ...         ...          ...
137          256        1080    0.25           3     3.642325
136          256        1080    0.25           2     3.661198
138          256        1080    0.25           4     3.688722
88           128        1080    0.25           2     3.714610
139          256        1080    0.25           5     3.737005

[144 rows x 5 columns]
